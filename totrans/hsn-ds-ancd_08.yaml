- en: Unsupervised Learning in Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before discussing unsupervised learning, it might be a good idea to introduce
    supervised learning since most of us will be familiar with functions discussed
    in the previous chapters. For a function of *y=f(x)*, usually we have values for
    independent variables of *x[1]*, *x[2]*, ... *x[n]* and a set of corresponding
    values for a dependent variable of *y*. In previous chapters, we have discussed
    various types of functions, such as the single-factor linear model. Our task is
    to figure out the format of the function, given a set of input values. For supervised
    learning, we have two datasets: the **training data** and **test data**. For the
    training dataset, it has a set of input variables and related output values (also
    called a **supervisory signal**). A supervised learning algorithm analyzes the
    training data and produces an inferred function. Then, we apply this inferred
    function to map the given test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supervised learning, we don't have *y* for unsupervised learning. In
    other words, unsupervised learning is used to draw inferences from datasets consisting
    of input data without labelled responses. In a sense, for supervised learning,
    we have both *x* and *y*, while for unsupervised learning, we have *x* only. In
    other words, for one algorithm, we have both input and output, while for the other,
    we have inputs only. The most common unsupervised learning method is Cluster Analysis,
    which is used for exploratory data analysis to find hidden patterns or groupings
    in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introduction to Python packages: `scipy`, `contrastive`, and `sklearn` (scikit-learn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Introduction to R packages: `rattle`, `randomUniformForest`, and `Rmixmod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation using Julia
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task view for Cluster Analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For unsupervised learning, we try to reorganize data or classify it into different
    groups based on certain traits or characteristics. For this purpose, we can use
    certain rules to categorize our dataset. For example, we could classify them into
    different groups based on investors'' characteristics, such as age, education
    level, background, job types, living city, profession, salary level, and house
    ownership. For instance, they could be classified into four types of investors:
    aggressive, risk averse, risk neutral, and extremely risk averse. After that,
    financial institutions could design and market specific financial products targeting
    different groups.'
  prefs: []
  type: TYPE_NORMAL
- en: To plan an equitable income tax policy, governments could classify potential
    taxpayers based on various criteria, such as income level and whether a person
    has a certain disability. Then, they could design distinct tax plans to target
    various groups with varying social welfare status. Such a tax policy could be
    more equitable than a simple progressive tax scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Another example is that investors could have over 5,000 stocks available to
    buy. It might be a good idea to group them according to various benchmarks, such
    as riskiness, profitability, familiarity, reputation, local/national/international,
    social responsibility, and transaction cost. After that, financial institutions
    could form different mutual funds which could be sold to different investors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is used to summarize the idea: grouping data according
    to certain criteria. The data is drawn from two normal distributions with the
    same standard deviation, but with different means. Later in the chapter, we will
    show the related code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b4187cf8-03f4-4341-ad53-3f97dded5985.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The logic for clustering or grouping is that the distance between group members
    is less than that between groups. In other words, the similarity among members
    within each group is higher than that between groups. For instance, if we have
    ZIP codes of many houses, we could estimate the distance between each pair of
    houses and classify them into different communities. For numerical values, such
    as *(x[1],y[1]), (x[2],y[2]),* the difference could be defined by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b10edad-8c57-48b6-b27c-c359b6bec3e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code generates a graph that shows the distance between two points.
    This is also called Euclidean distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code, `method="euclidean"` could be omitted since it is the
    default setting. The distance is `2.236` and the graph is ignored for brevity.
    The potential phrases for the method variable are `euclidean`, *maximum,* *manhattan,* *canberra,
    binary*, and *minkowski.* The general form is called the Minkowski distance, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f61bc53c-d682-49d6-8419-fa2105049b87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Obviously, when *n* takes a value of `2`, Equation (2) is the same as Equation
    (1). On the other hand, when *p = 1*, this is equivalent to the Manhattan distance,
    as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c343a8b-dca2-439e-a8da-f0b2f48923e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When *x* and *y* differ in terms of sizes, we could scale them by using the
    following formulae:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f63d00d9-12c2-4936-a943-da2fa2912e76.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The previous scaling method is based on the assumption that *x* and *y* are
    independent, where ![](img/9dd0a259-0c4d-42a4-ba5d-d3053294379a.png) (![](img/d7e8ba0b-c561-48da-94cb-a8ce3b7cc2d5.png))
    is the new data point for *x (y)*, *![](img/c435e423-edbb-4d77-8d07-2ae9739095ee.png) (![](img/f052afff-8055-4c10-bf01-ffd5be193128.png))*
    is the mean for *x (y)*, and *![](img/fd28ba58-bedd-4525-b74d-0f5d66adca14.png)(![](img/267ec792-db42-4961-a745-3f2f711035bc.png))*
    is the standard deviation of *x(y)*. If researchers put more weight on the direction
    of the data rather than the magnitude, then the cosine distance could be used.
    The cosine of two non-zero vectors can be derived by using the Euclidean dot product
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d3dc554e-a590-4c82-9854-0b85304a67f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *θ* is the angle between those two vectors. Also, we could view *cos(θ)*
    as a scaling factor, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Obviously, when the *x* and *y* vector has the exact same direction, *cos(θ)*
    has the highest value of 1\. When they have opposite directions, *cos(θ)* has
    a negative value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given two vectors A and B, the cosine similarity, *cos(θ)*, is represented
    using a dot product and magnitude in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fdd66593-f9f3-462b-9208-28839851f4f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For categorical data points, they could not be ranked because of non-numeric
    values. For these cases, we can calculate the following similarity index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/188cf78b-353d-4ba4-b053-cf148e7599c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where *n[matched]* is the number of matched treats and *n[total]* is the number
    of total treats. When we have both categorical and numeric data, we can estimate
    both types of distances first when choosing a weighted final value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/793aebfe-458d-417d-a729-797743f6ba10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Assuming *d[num]* is the distance based on the numerical data and *d[cat]*
    is the distance based on the categorical data, we have the following equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97b9fe03-c9ef-4c8a-bc93-3a0cef53b2a5.png)'
  prefs: []
  type: TYPE_IMG
- en: where *w[num]* is the weight of the numerical value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have two sets of data, *X* and *Y*. For the *X* set, we have
    *x[1]*, *x[2]*, ..., *x[n]*, while for the *Y* set, we have *y[1]*, *y[2]*, ...,
    and *y[m]*. In other words, for *X*, there are *n* observations, while for *Y*,
    there are *m* observations. For a pair of clusters, we have several ways to define
    their linkages (see the following table):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Linkage** | **Math formula** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| Single | ![](img/8697c477-f14c-4853-828a-2e22494f5788.png) | Shortest distance
    between the closest members of the two clusters |'
  prefs: []
  type: TYPE_TB
- en: '| Complete | ![](img/f3330f9d-1a50-4304-b9aa-1c5bd30acccd.png) | Longest distance
    between the members that are farthest apart (most dissimilar) |'
  prefs: []
  type: TYPE_TB
- en: '| Average | ![](img/5276dbe0-2eb7-45e3-aa7a-91807909229f.png) | Step 1: Get
    distances between all pairsStep 2: Calculate the average |'
  prefs: []
  type: TYPE_TB
- en: '| Centroid Method | ![](img/d4109ed8-9024-4a5b-9881-dca7811cb3bd.png) | Step
    1: Find the mean vector locationsStep 2: Get the distance between the two locations
    |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 Four types of linkages between two clusters
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use some hypothetical values to see how to get those linkages. To make
    our estimate easier to understand, we have just five values (see the following
    code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5944b5d-2969-4c19-86dc-5cb6a77fcf73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the centers of these two clusters, we could guess that the first one should
    be about (65,140), while the second one should be around (170, 40) (see the following
    output code):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Hierarchical clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In hierarchical clustering, the two most similar clusters are combined and continue
    to combine until all objects are in the same cluster. Hierarchical clustering
    produces a tree called a **dendrogram** that shows the hierarchy of the clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show this concept, let''s start by looking at the dataset called `animals`
    embedded in the R package called `cluster`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In total, we have 20 observations with 6 characteristics: `war`, `fly`, `ver`,
    `end`, `gro`, and `hai`, illustrated by their column names. The following table
    shows their definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **#** | **Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | `war` | warm-blooded |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | `fly` | can fly |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | `ver` | vertebrate |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | `end` | endangered |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | `gro` | live in groups |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | `hai` | have hair |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 Definitions of column names for animals
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we have 20 observations with 6 characters to describe each of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The `mona()` function is **MONothetic Analysis Clustering of Binary Variables**,
    and returns a list representing a divisive hierarchical clustering of a dataset
    with only binary variables. The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b8abf3f-6597-4204-bf00-40285101ff6f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous output, we can see that there are five groups. For example,
    the first observation called `ant` belongs to group 4\. Its vector is `[0, 0,
    0, 0,1,0]`. On the other hand, for the second observation called `bee`, it does
    not belong to any of those five groups (that is, its group is zero). Its six-value
    binary vector is `[0,1,0,0,1,1]`. To visually present the outcome, we have the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b03d7f2-c031-4386-8464-9e5dc3986394.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous graph, it can be seen that the first observation belongs
    to group 4, while the second one does not belong to any of those five groups.
    To draw a dendrogram for the dataset called `animals`, we have the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Refer to the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97a7f5cd-723f-476c-b916-4611a44e919f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the next example, we generate two sets of random numbers drawn from a normal
    distribution with different means, `0` and `80`, with the same standard deviation
    of `8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output graph is shown in the following diagram. From it, we can see that
    those random values belong to two distinct groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/69b490fb-8dcb-46b3-9202-c6029484f852.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also note that the  `clara()` function from the program has the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: k-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of k-means clustering is to partition *n* observations into *k*
    clusters, where each observation belongs to the cluster with the nearest mean.
    This results in a partitioning of the data space into a **Voronoi diagram**. In
    mathematics, a Voronoi diagram is a partitioning of a plane into regions based
    on the distance to points in a specific subset of the plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Borrowed from Priyadarsini ([https://www.kaggle.com/maitree/kmeans-unsupervised-learning-using-wine-dataset/data](https://www.kaggle.com/maitree/kmeans-unsupervised-learning-using-wine-dataset/data)),
    the slightly modified code is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d25be667-5e59-440c-88b7-dc5f4d1569f9.png)'
  prefs: []
  type: TYPE_IMG
- en: This square matrix shows the similarity between each combination of treatments.
    Blue values are all positive, representing similarity, while red values are negative,
    representing dissimilarity.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Python packages – scipy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The submodule from the `scipy` package is called `scipy.cluster`. With the
    following code, we can find all embedded functions. The document for this submodule
    is available here at [https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The next screenshot shows the related output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07935c9d-d3d2-4b69-9d2a-2ff805549fd1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we use the `scipy.cluster._hierarchy` function. This example is borrowed
    as well ([https://stackoverflow.com/questions/21638130/tutorial-for-scipy-cluster-hierarchy](https://stackoverflow.com/questions/21638130/tutorial-for-scipy-cluster-hierarchy)).
    The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The related graphs are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9f2ad3cf-df57-4201-9eb1-c31a6222d1fd.png)'
  prefs: []
  type: TYPE_IMG
- en: The methods used are single and complete in terms of estimating the distance
    between two points. The top middle graph shows the clustering with two groups,
    while the top right one shows the clustering for five groups.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Python packages – contrastive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To install the Python package called `contrastive`, we issue the following
    command after launching the Anaconda prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the top part of the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e28a64d-0e6b-4b2c-a4c6-45d85f13561c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the `dir()` and `print()` functions, we could find more information about
    a function embedded in the Python package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31d9c78d-cc2d-4f0c-bc13-3f0164441a45.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction to Python packages – sklearn (scikit-learn)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s look at the functions contained in the Python package called
    `sklearn`. The code has just three lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee25739c-0211-4257-89ed-1b4dbaf9dad6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For one specific submodule, it is called `sklearn.cluster`, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9ceacea-0503-459c-9ee5-8d8f72436b0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, we can show many embedded datasets by using the following three
    lines of Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e85dd7e0-2720-41f0-a4df-bc9c9f8d09b6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, if we want to use a dataset called `iris`, we can apply the `load_iris()`
    function, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The first few lines are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78d793f6-bbc2-4122-ba7e-561f86b2151c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code is one example of using the module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of the previous code shows the potential groups they belong to.
    The next example is by *Michel, Gramfort and Varoquaux* (2010), [http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py](http://scikit-learn.org/stable/auto_examples/cluster/plot_ward_structured_vs_unstructured.html#sphx-glr-auto-examples-cluster-plot-ward-structured-vs-unstructured-py).
    The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/749edf45-95c9-4f24-b49a-45c034ac5492.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous graphs, we visually view groups by their colors. For example,
    the left graph shows us there are six groups. The next example is about Agglomerative
    Clustering on a 2D embedding of digits. Since the program is too long, we would
    not include it here. You can find the program at [http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py](http://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py).
    The related graph output is shown in the following screenshot. The graph shows
    several groups based on a few different definitions of distance (linkage):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/82def4b1-5195-49ee-9f01-9fc1953a61e2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the definitions of distance of **ward**, **average**, or **complete**,
    we have three ways to generate clusters. For each method, various colors represent
    different clusters. The last example is looking at the `iris` dataset, which is
    associated with **Principal Component Analysis** (**PCA**). PCA uses an orthogonal
    transformation to convert a set of observations of possibly correlated variables
    into a set of values of linearly uncorrelated variables called **principal components** ([http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html))[.](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_iris.html)
    The related 3D picture is shown here. The graph shows the groups for these three
    types:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/83b77261-d427-415f-a5fa-3c8e72f649db.png)'
  prefs: []
  type: TYPE_IMG
- en: One useful link is [http://scikit-learn.org/stable/modules/clustering.html](http://scikit-learn.org/stable/modules/clustering.html).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to R packages – rattle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before discussing one or two examples of using `rattle`, it might be a good
    idea to discuss an R package called `rattle.data`. As its name suggests, we could
    guess that it contains data used by `rattle`. It is a good idea to use a small
    dataset to generate a dendrogram. For the next case, we use the first `20` observations
    from a dataset called `wine`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'To launch `rattle`, we have the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'For data, we choose R Dataset, then choose `x`, as shown in the following screenshot.
    To save space, only the top part is presented here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ede71328-7e7c-42df-9206-5cd13e04c788.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot shows our choice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17d38ba4-ee8c-4f14-95ae-cf360dc70211.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous screenshot, we see 14 observations. Click Clusters, with
    a default of `10` clusters, and Dendrogram. See the result in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b713d931-ff37-4e00-babf-cef265c52817.png)'
  prefs: []
  type: TYPE_IMG
- en: The previous dendrogram is a tree diagram that is used to show the relationship
    for a clustering analysis. Based on the definition of distance or relationship,
    we have a link between the closed associated one pair. Then, we move one step
    up to link this pair with another adjacent point or observation. The procedure
    continues until we reach the final step of just one group.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to R packages – randomUniformForest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The R package, `randomUniformForest` is for classification, regression, and
    unsupervised learning. The basic block for a random forest is the decision tree
    model. To make our classification, researchers add some random effect. Because
    of this, the random forest method performs better than the decision tree model.
    We can use the following code to install, load, and get help about this specific
    R package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s look at a relatively simple program that tries to classify different
    plants into four groups by using the random forest method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'It is obvious that the function name of `unsupervised.randomUniformForest`
    is quite long. To make our program more readable, we generate a short name called
    `shortName` instead. The related output is shown in the following screenshot,
    where it illustrates four groups by color:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa4f83d4-327a-4bb3-81c9-026a929f7e18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second example presents several groups based on the wine quality, such
    as alcohol concentration, color, and dilution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09a517ab-ae5d-4c5a-9842-8628aa80eec5.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction to R packages – Rmixmod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This R package is for supervised, unsupervised, and semi-supervised classification
    with **MIXture MODelling** (interface of MIXMOD software). First, let''s look
    at a dataset called `birds`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'From the previous output, we know that there are `69` observations with `5` characteristics.
    The following example code is designed to plot bars based on eyebrow and collar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The graphs are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2327af5-5f00-40f4-ba15-e1f8b139cda2.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementation using Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For Julia, we use the package called `Clustering`. The next example is borrowed
    from *Lin, Regier, and Arslan* (2016) with a minor modification ([https://github.com/JuliaStats/Clustering.jl/blob/master/test/affprop.jl](https://github.com/JuliaStats/Clustering.jl/blob/master/test/affprop.jl)).
    First, we generate a set of random numbers. Then, replace the values along the
    main diagonal line with the median values. Then, the program classifies them into
    different groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/80add43a-f72b-43ec-8d58-112f57ead85c.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the output, we see a few groups and which numbers belong to which group.
  prefs: []
  type: TYPE_NORMAL
- en: Task view for Cluster Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapters, we have discussed various kinds of task views. A
    task view is a set of R packages organized by one or a few experts around a topic.
    For the **Cluster Analysis**, [https://CRAN.R-project.org/view=Cluster](https://CRAN.R-project.org/view=Cluster),
    the top part is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68937f40-a789-46c7-959e-22890acc7fa9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As discussed in *[Chapter 6](c812a40e-eb24-4bb8-8af5-1cfe1834ec77.xhtml), Managing
    Packages*, we can use just three lines of R code to install these cluster-related
    packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In total, there are about 154 R packages included in the previous task view
    called **Cluster,** as of March 21, 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have discussed unsupervised learning. In particular, we
    have explained hierarchical clustering and k-means clustering. As for R and Python,
    we have explained several related packages:'
  prefs: []
  type: TYPE_NORMAL
- en: R: `rattle`, `Rmixmod`, and `randomUniformForest`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python: `scipy.cluster`, `contrastive`, and `sklearn`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several real-world examples have also been used to illustrate the applications
    of these packages in detail.
  prefs: []
  type: TYPE_NORMAL
- en: For the next chapter, we will discuss supervised learning, such as classification,
    the k-nearest neighbors algorithm, Bayes' classifiers, reinforcement learning,
    and specific R and Python-related modules, such as `RTextTools` and `sklearn`.
    In addition, we will discuss implementation via R, Python, Julia, and Octave.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions and exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does *unsupervised learning* mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the major difference between unsupervised learning and supervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we install the Python package `sklearn`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discuss the relationship between distance and clustering classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we define the distance between two objects?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For non-numeric values, how do we define a distance between two members?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For R, we could find a set of related packages related to unsupervised learning
    called `cluster`. Is there any task view, or similar super package, for Python?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, generate the following set of random numbers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Then, based on the various definitions of distance, estimate the distances between
    those four groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following set of data, estimate the minimum, maximum, and average distances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: What is the usage of a dendrogram?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Draw a dendrogram by using all wine data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate 20 random numbers with a mean of 1.2 and standard deviation of 2 from
    a normal distribution. Then draw a dendrogram.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using a five-year monthly historical price data for 30 stocks, estimate their
    annualized standard deviations and means. Classify them into different groups.
    The source of data is Yahoo!Finance ([http://finance.yahoo.com](http://finance.yahoo.com)).
    Note that the following formulae are used to calculate an annualized standard
    deviation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/9f277317-4d64-44b3-a79d-1796a5aeacc9.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *σ[annual]* is the annualized standard deviation, *σ[monthly]* is the
    standard deviation based on monthly returns, ![](img/55c15b72-5583-4adc-b383-60f548c17314.png) is
    the annualized mean return, and ![](img/6e626250-a85a-4787-a4ed-f19fb52240f3.png) is
    the monthly mean return.
  prefs: []
  type: TYPE_NORMAL
- en: For the R package called `cluster`, what is the meaning of the `votes.repub`
    dataset? Using that dataset, conduct a Cluster Analysis and draw a dendogram tree.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find more information about the `linkage_tree()` function contained in the `sklearn.cluster`
    submodule. (Python)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the `rattle` package, how do we save an R script?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
