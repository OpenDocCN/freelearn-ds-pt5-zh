- en: Chapter 5. Data Analysis on Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The field of data analytics at scale has been evolving like never before. Various
    libraries and tools were developed for data analysis with a rich set of algorithms.
    On a parallel line, distributed computing techniques were evolving with time,
    to process huge datasets at scale. These two traits had to converge, and that
    was the primary intention behind the development of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: The previous two chapters outlined the technology aspects of data science. It
    covered some fundamentals on the DataFrame API, Datasets, streaming data  and
    how it facilitated data representation through DataFrames that R and Python users
    were familiar with. After introducing this API, we saw how operating on datasets
    became easier than ever. We also looked at how Spark SQL played a background role
    in supporting the DataFrame API with its robust features and optimization techniques.
    In this chapter, we are going to cover the scientific aspect of big data analysis
    and learn various data analytics techniques that can be executed on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite for this chapter, a basic understanding of the DataFrame
    API and statistics fundamentals is good to have. However, we have tried to make
    the content as simple as possible and covered some important fundamentals in detail
    so that anyone can get started with statistical analysis on Spark. The topics
    covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Data analytics life cycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data consolidation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data cleansing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data transformation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data distributions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measures of location
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Measures of spread
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary statistics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphical techniques
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferential statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discrete probability distributions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous probability distributions
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Standard error
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence level
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Margin of error and confidence interval
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Variability in population
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimating sample size
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi-square test
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: F-test
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Data analytics life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For most real-world projects, there is some defined sequence of steps to be
    followed. However, there are no universally agreed upon definitions or boundaries
    for data analytics and data science. Generally, the term "data analytics" encompasses
    the techniques and processes involved in examining data, discovering useful insights,
    and communicating them. The term "data science" can be best treated as an interdisciplinary
    field drawing from *statistics*, *computer science*, and *mathematics*. Both terms
    deal with processing raw data to derive knowledge or insights, usually in an iterative
    fashion, and some people use them interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on diverse business requirements, there are different ways of approaching
    problems but there is no unique standard process that fits in well with all possible
    scenarios. A typical process workflow can be summarized as a cycle of formulating
    a question, exploring, hypothesizing, validating the hypothesis, analyzing the
    results, and starting all over again. This is depicted in the following figure
    with the thick arrows. From a data perspective, the workflow consists of data
    acquisition, preprocessing, exploring the data, modeling, and communicating the
    results. This is shown in the figure as circles. Analysis and visualization happen
    at every stage, right from data collection to results communication. The data
    analytics workflow encompasses all the activities shown in both views:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Data analytics life cycle](img/image_05_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The most important thing in the entire life cycle is the question at hand. Data
    that might contain an answer (relevant data!) to that question comes next. Depending
    on the question, the first task is to collect the right data from one or more
    data sources as needed. Organizations often maintain **data lakes**, which are
    humongous repositories of data in their original format.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to clean/transform the data to the desired format. Data cleansing
    is also called data munging, data wrangling, or data dredging. This involves activities
    such as missing value treatment and outlier treatment upon assessing the quality
    of the data at hand. You may also have to aggregate/plot the data for better understanding.
    This process of formulating the final data matrix to work with is touted as the
    most time-consuming step. This also happens to be an underestimated component
    that is considered to be part of preprocessing, along with other activities such
    as feature extraction and data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The crux of data science, that is, training models and extracting patterns,
    comes next, which requires heavy use of statistics and machine learning. The final
    step is publishing the results.
  prefs: []
  type: TYPE_NORMAL
- en: The remaining sections in this chapter delve deeper into each of these steps
    and how they can be implemented using Spark. Some basics of statistics are also
    included so as to enable the reader to follow the code snippets with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data acquisition, or data collection, is the very first step in any data science
    project. Usually, you won't find the complete set of required data in one place
    as it is distributed across **line-of-business** (**LOB**) applications and systems.
  prefs: []
  type: TYPE_NORMAL
- en: The majority of this section has already been covered in the previous chapter,
    which outlined how to source data from different data sources and store the data
    in DataFrames for easier analysis. There is a built-in mechanism in Spark to fetch
    data from some of the common data sources and the *Data Source API* is provided
    for the ones not supported out of the box on Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of the data acquisition and preparation phases,
    let us assume a scenario and try to address all the steps involved with example
    code snippets. The scenario is such that employee data is present across native
    RDDs, JSON files, and on a SQL server. So, let''s see how we can get those to
    Spark DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data quality has always been a pervasive problem in the industry. The presence
    of incorrect or inconsistent data can produce misleading results of your analysis.
    Implementing better algorithm or building better models will not help much if
    the data is not cleansed and prepared well, as per the requirement. There is an
    industry jargon called **data engineering** that refers to data sourcing and preparation.
    This is typically done by data scientists and in a few organizations, there is
    a dedicated team for this purpose. However, while preparing data, a scientific
    perspective is often needed to do it right. As an example, you may not just do
    *mean substitution* to treat missing values and look into data distribution to
    find more appropriate values to substitute. Another such example is that you may
    not just look at a box plot or scatter plot to look for outliers, as there could
    be multivariate outliers which are not visible if you plot a single variable.
    There are different approaches, such as **Gaussian Mixture Models** (**GMMs**)
    and **Expectation Maximization** (**EM**) algorithms that use **Mahalanobis distance**
    to look for multivariate outliers.
  prefs: []
  type: TYPE_NORMAL
- en: The data preparation phase is an extremely important phase, not only for the
    algorithms to work properly, but also for you to develop a better understanding
    of your data so that you can take the right approach while implementing an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data has been acquired from different sources, the next step is to
    consolidate them all so that the data as a whole can be cleaned, formatted, and
    transformed to the format needed for your analysis. Please note that you might
    have to take samples of data from the sources, depending on the scenario, and
    then prepare the data for further analysis. Various sampling techniques that can
    be used are discussed later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Data consolidation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will take a look at how to combine data acquired from various
    data sources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After integrating data from those sources, the final dataset (in this case
    it is `final_data`) should be of the following format (just example data):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **emp_id** | **name** | **age** | **role** | **salary** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | John | 25 | Associate | 10,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Ray | 35 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Mike | 24 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Jane | 28 | Associate | null |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Kevin | 26 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Vincent | 35 | Senior Manager | 22,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | James | 38 | Senior Manager | 20,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Shane | 32 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Larry | 29 | Manager | 10,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | Kimberly | 29 | Associate | 8,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | Alex | 28 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | Garry | 25 | Manager | 12.000 $ |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | Max | 31 | Manager | 12,000 $ |'
  prefs: []
  type: TYPE_TB
- en: Data cleansing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you have the data consolidated in one place, it is extremely important
    that you spend enough time and effort in cleaning it before analyzing it. This
    is an iterative process because you have to validate the actions you have taken
    on the data and continue till you are satisfied with the data quality. It is advisable
    that you spend time analyzing the causes of anomalies you detect in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Some level of impurity in data usually exists in any dataset. There can be various
    kinds of issues with data, but we are going to address a few common cases, such
    as missing values, duplicate values, transforming, or formatting (adding or removing
    digits from a number, splitting a column into two, merging two columns into one).
  prefs: []
  type: TYPE_NORMAL
- en: Missing value treatment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are various ways of handling missing values. One way is dropping rows
    containing missing values. We may want to drop a row even if a single column has
    missing value, or may have different strategies for different columns. We may
    want to retain the row as long as the total number of missing values in that row
    are under a threshold. Another approach may be to replace nulls with a constant
    value, say the mean value in case of numeric variables.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will not be providing some examples in both Scala and Python
    and will try to cover various scenarios to give you a broader perspective.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Outlier treatment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Understanding what an outlier is also important to treat it well. To put it
    simply, an outlier is a data point that does not share the same characteristics
    as the rest of the data points. Example: if you have a dataset of schoolchildren
    and there are a few age values in the range of 30-40 then they could be outliers.
    Let us look into a different example now: if you have a dataset where a variable
    can have data points only in two ranges, say, in the 10-20 or 80-90 range, then
    the data points (say, 40 or 55) with values in between these two ranges could
    also be outliers. In this example, 40 or 55 do not belong to the 10-20 range,
    nor do they belong to the 80-90 range, and are outliers.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, there can be univariate outliers and there can be multivariate outliers
    as well. We will focus on univariate outliers in this book for simplicity's sake
    as Spark MLlib may not have all the algorithms needed at the time of writing this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: In order to treat the outliers, you have to first see if there are outliers.
    There are different ways, such as summary statistics and plotting techniques,
    to find the outliers. You can use the built-in library functions such as `matplotlib`
    of Python to visualize your data. You can do so by connecting to Spark through
    a notebook (for example, Jupyter) so that the visuals can be generated, which
    may not be possible on a command shell.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you find outliers, you can either delete the rows containing outliers
    or impute the mean values in place of outliers or do something more relevant,
    as applicable to your case. Let us have a look at the mean substitution method
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Duplicate values treatment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are different ways of treating the duplicate records in a dataset. We
    will demonstrate those in the following code snippets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Data transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There can be various kinds of data transformation needs and every case is mostly
    unique. We are going to cover some basic types of transformations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Merging two columns into one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding characters/numbers to the existing ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deleting or replacing characters/numbers from the existing ones
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing date formats
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now that we are familiar with basic examples, let us put together a somewhat
    complex example. You might have noticed that the date column in Authors data has
    different date formats. In some cases, month is followed by day, and vice versa.
    Such anomalies are common in the real world, wherein data might be collected from
    different sources. Here, we are looking at a case where the date column has data
    points with many different date formats. We need to standardize all the different
    date formats into one format. To do so, we first have to create a **user-defined
    function** (**udf**) that can take care of the different formats and convert those
    to one common format.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: That lines up the date of birth strings neatly. We can keep fine-tuning the
    udf as we encounter more varieties of date formats.
  prefs: []
  type: TYPE_NORMAL
- en: At this stage, before getting started with data analysis, it is extremely important
    that you should pause for a moment and re-evaluate the actions you have taken
    from starting data acquisition to cleaning and transforming it. There have been
    a lot of cases where tremendous time and effort involved went for a toss and led
    to project failure because of incorrect data being analyzed and modeled. Such
    cases became perfect examples of a famous computer adage - **Garbage In, Garbage
    Out** (**GIGO**).
  prefs: []
  type: TYPE_NORMAL
- en: Basics of statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The field of statistics is all about using mathematical procedures to summarize
    the raw facts and figures of a dataset in some meaningful way so that it makes
    sense to you. This includes, and is not limited to: gathering data, analyzing
    it, interpreting it, and representing it.'
  prefs: []
  type: TYPE_NORMAL
- en: The field of statistics exists mainly because it is usually impossible to collect
    data for the entire population. So using statistical techniques, we estimate the
    population parameters using the sample statistics by addressing the uncertainties.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will cover some basic statistics and analysis techniques
    on which we are going to build up our complete understanding of the concepts covered
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'The study of statistics can be broadly categorized into two main branches:'
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inferential statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram depicts these two terms and shows how we estimate the
    population parameters from samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Basics of statistics](img/image_05_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Before we get started on these, it is important to get some familiarity with
    sampling and distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Through sampling techniques, we just take a portion of the population dataset
    and work on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sampling](img/image_05_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'But why do we sample? The following are various reasons for sampling:'
  prefs: []
  type: TYPE_NORMAL
- en: Difficult to get the entire population's data; for example, the heights of the
    citizens of any country.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to process the entire dataset. When we talk about big data computing
    platforms such as Spark, the scope of this challenge nearly disappears. However,
    there can be scenarios where you have to treat the entire data at hand as a sample
    and extrapolate your analysis result to a future time or to a larger population.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficult to plot voluminous data to visualize it. There can be technical limitations
    to it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For validation of your analysis or validation of your predictive models - especially
    when you are working with small datasets and you have to rely on cross-validation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For effective sampling, there are two important constraints: one is determining
    the sample size and the other is the technique to choose for sampling. The sample
    size greatly influences the estimation of population parameters. We will cover
    this aspect later in this chapter after covering some of the prerequisite basics.
    In this section, we will focus on sampling techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: There are various probability-based (the probability of each sample being selected
    is known) and non-probability-based (the probability of each sample being selected
    is not known) sampling techniques available, but we are going to limit our discussion
    to probability-based techniques only.
  prefs: []
  type: TYPE_NORMAL
- en: Simple random sample
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **simple random sample** (**SRS**) is the most basic type of probability
    sampling method, where every element has the same probability of being chosen.
    This means that every possible sample of *n* elements has an equal chance of selection.
  prefs: []
  type: TYPE_NORMAL
- en: Systematic sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Systematic sampling is probably the simplest of all probability-based sampling
    techniques, where every *kth* element of the population is sampled. So this is
    otherwise known as interval sampling. It starts with a fixed starting point chosen
    at random and then an interval is estimated (the *kth* element, where *k = (population
    size)/(sample size)*). Here, the progression through the elements is circled to
    start from the beginning when it reaches the end till your sample size is reached.
  prefs: []
  type: TYPE_NORMAL
- en: Stratified sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This sampling technique is preferred when the subgroups or subpopulations within
    the population vary, because other sampling techniques might not help extract
    a sample that is a good representative of the population. Through stratified sampling,
    the population is divided into homogeneous subgroups called **strata** and a sample
    is taken by randomly selecting the subjects from those strata in proportion to
    the population. So, the stratum size to population size ratio is maintained in
    the sample as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We only looked at sampling on DataFrames; there are MLlib library functions
    such as `sampleByKey` and `sampleByKeyExact` to do stratified sampling on RDDs
    of key-value pairs. Check out `spark.util.random` package for Bernoulli , Poisson
    or Random samplers.
  prefs: []
  type: TYPE_NORMAL
- en: Data distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding how your data is distributed is one of the primary tasks you need
    to perform to turn data into information. Analyzing the distributions of the variables
    helps detect the outliers, visualize the trends in the data, and can also shape
    up your understanding for the data at hand. This helps in thinking right and taking
    the right approaches in solving a business problem. Plotting the distributions
    makes it visually more intuitive and we will cover this aspect in the *Descriptive
    statistics* section.
  prefs: []
  type: TYPE_NORMAL
- en: Frequency distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Frequency distribution explains which values a variable takes and how often
    it takes those values. It is usually represented with a table with each possible
    value with its corresponding number of occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s consider an example where we roll a six-sided die 100 times and observe
    the following frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequency distributions](img/Chapter-5_NEw.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Frequency Table
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you might observe different distributions on every set of 100 rolls
    of the die because it would depend on chance.
  prefs: []
  type: TYPE_NORMAL
- en: 'At times, you might be interested in the proportions of occurrences instead
    of just occurrences. In the preceding die roll example, we rolled the die 100
    times in total, so the proportionate distribution or the **relative frequency
    distribution** would appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Frequency distributions](img/Final-5-RT-3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Relative Frequency Table
  prefs: []
  type: TYPE_NORMAL
- en: Probability distributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the same example of die rolling, we know that a total probability of 1 is
    distributed across all faces of the die. This means that a probability of 1/6
    (approximately 0.167) is associated with face 1 through face 6\. Irrespective
    of the number of times you roll a die (a fair die!), the same probability of 1/6
    would be distributed evenly on all sides of the die. So, if you plot this distribution,
    it would appear as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Probability distributions](img/Chapter-new.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Probability Distribution
  prefs: []
  type: TYPE_NORMAL
- en: We looked at three kinds of distributions here - frequency distributions, relative
    frequency distribution, and probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: This probability distribution is actually the distribution of the population.
    In real-world cases, at times we have prior knowledge of the population distribution
    (in our example, it is the probability of 0.167 on all six sides of a fair die)
    and at times we don't. In scenarios where we don't have the population distribution,
    finding the distribution of the population itself becomes part of your inferential
    statistics. Also, unlike the fair die example, where the same probability is associated
    with all the sides, there can be different probabilities associated with the values
    a variable can take and they can follow a particular type of distribution as well.
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to reveal the secret! The relation between the relative frequency
    distribution and the probability distribution is the basis of statistical inference.
    The relative frequency distributions are also called empirical distributions based
    on what we observe in the samples we take (here, it is a sample of 100). As discussed
    earlier, the empirical distributions of every 100 rolls of the die would differ
    depending on chance. Now, the larger the number of rolls, the closer will be the
    relative frequency distribution to the probability distribution. So, the relative
    frequencies of an infinite number of die rolls is the probability distribution,
    which in turn is the population distribution.
  prefs: []
  type: TYPE_NORMAL
- en: There are various kinds of probability distributions, which are again categorized
    into two, based on the type of variable - categorical or continuous. We will cover
    these distributions in detail in the subsequent sections of this chapter. However,
    we should know what these categories imply! Categorical variables can take on
    only a few categories; for example, pass/fail, zero/one, cancer/malignant are
    examples of categorical variables with two categories. Similarly, a categorical
    variable can have more categories, such as red/green/blue, type1/type2/type3/type4,
    and so on. Continuous variables can take on any value in a given range and measured
    on a continuous scale, for example, age, height, salary, and so on. Theoretically,
    there can be an infinite number of possible values between any two values of a
    continuous variable. For example, between 5'6" and 6'4" height values (foot and
    inch scale), there can be many fractional values possible. The same holds true
    when measured in a centimeter scale as well.
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we learnt how distributions are formed. In this section,
    we will learn how to describe them through descriptive statistics. There are two
    important components of a distribution that can help describe it, which are its
    location and its spread.
  prefs: []
  type: TYPE_NORMAL
- en: Measures of location
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A measure of location is a single value that describes where the center of the
    data lies. The three most common measures of location are mean, median, and mode.
  prefs: []
  type: TYPE_NORMAL
- en: Mean
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By far the most common and widely used measure of central tendency is the **mean**,
    which is otherwise known as the average. Whether it is a sample or a population,
    the mean or average is the summation of all the elements divided by the total
    number of elements.
  prefs: []
  type: TYPE_NORMAL
- en: Median
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **median** is the middle value of a series of data when sorted in any order
    so that half of the data is greater than the median and the other half smaller.
    When there are two middle values (with an even number of data items), the median
    is the average of those middle two. Medians are better measures of location when
    the data has outliers (extreme values).
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **mode** is the most frequent data item. It can be determined for both qualitative
    and quantitative data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: //Reusing data_new created in duplicated value treatment
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Measures of spread
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Measures of spread describe how close or scattered the data is for a particular
    variable or data item.
  prefs: []
  type: TYPE_NORMAL
- en: Range
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The range is the difference between the smallest and largest values of a variable.
    One disadvantage to it is that it does not take into account every value in the
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To find the variability in the dataset, we can subtract each value from the
    mean, square them up so it gets rid of the negative signs (also scales up the
    magnitude), and then sum them all and divide by the total number of values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Variance](img/image_05_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: If the data is more spread out, the variance will be a large number. One disadvantage
    to it is that it gives undue weight to the outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Standard deviation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like variance, standard deviation is also a measure of dispersion within the
    data. Variance had the limitation that the unit of data was also squared along
    with the data, so it was difficult to relate the variance with the values in the
    dataset. So, standard deviation is calculated as the square root of the variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard deviation](img/image_05_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Summary statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The summary statistics of a dataset is extremely useful information that gives
    us a quick insight into the data at hand. Using the function `colStats` available
    in statistics, we can obtain a multivariate statistical summary of `RDD[Vector]`
    which contains column-wise max, min, mean, variance, number of non-zeros, and
    the total count. Let us explore this through some code examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apache Spark MLlib RDD-based API is in maintenance mode starting Spark 2.0\.
    They are expected to deprecated in 2.2+ and removed in Spark 3.0.
  prefs: []
  type: TYPE_NORMAL
- en: Graphical techniques
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To understand the behavior of your data points, you may have to plot them and
    see. You need a platform, however, to visualize your data in terms of *box plots*,
    *scatter plots*, or *histograms*, to name a few. The iPython/Jupyter notebook
    or any other third-party notebook supported by Spark can be used for data visualization
    in your browser itself. Databricks provides their own notebook. Visualization
    is covered in its own chapter and this chapter focuses on the complete life cycle.
    However, Spark provides histogram data preparation out of the box so that bucket
    ranges and frequencies may be transferred to the client machine as against the
    complete dataset. The following example shows the same.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Inferential statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We saw that descriptive statistics were extremely useful in describing and
    presenting data, but they did not provide a way to use the sample statistics to
    infer the population parameters or to validate any hypothesis we might have made.
    So, the techniques of inferential statistics surfaced to address such requirements.
    Some of the important uses of inferential statistics are:'
  prefs: []
  type: TYPE_NORMAL
- en: Estimation of population parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Please note that a sample can never represent a population perfectly because
    every time we sample, it naturally incurs sampling errors, hence the need for
    inferential statistics! Let us spend some time understanding the various types
    of probability distributions that can help infer the population parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Discrete probability distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Discrete probability distributions are used to model data that is discrete in
    nature, which means that data can only take on certain values, such as integers.
    Unlike categorical variables, discrete variables can take on only numeric data,
    especially count data from a set of distinct whole values. Also, the sum of probabilities
    of all possible values of a random variable is one. The discrete probability distributions
    are described in terms of probability mass function. There can be various types
    of discrete probability distributions. The following are a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bernoulli distribution is a type of distribution that describes the trials having
    only two possible outcomes, such as success/failure, head/tail, the face value
    of a six-sided die is 4 or not, the message sent was received or not, and so on.
    Bernoulli distribution can be generalized for any categorical variable with two
    or more possible outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take the example of "students'' pass rate for an exam" where 0.6 (60
    percent) is the probability **P** of the students passing the exam and 0.4 (40
    percent) is the probability (**1-P**) for the students to fail in the exam. Let
    us denote fail as **0** and pass as **1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Bernoulli distribution](img/image_05_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Such distributions cannot answer questions such as the expected pass rate of
    a student, because the expected value (μ) is going to be some fraction that this
    distribution cannot take. It can only mean that if you sample 1,000 students,
    then 600 would pass and 400 would fail.
  prefs: []
  type: TYPE_NORMAL
- en: Binomial distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This distribution can describe a series of Bernoulli trials (each with only
    two possible outcomes). Also, it assumes that the outcome of one trial does not
    affect the subsequent trials and that the probability of any event occurring is
    the same on every trial. An example of binomial distribution is tossing a coin
    five times. Here, the outcome of the first toss does not influence the outcome
    of the second toss, and the probability associated with each outcome is the same
    on all tosses.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *n* is the number of trials and *p* is the probability of success in every
    trial, then the mean (μ) of this binomial distribution would be given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '*μ = n * p*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The variance (σ2x) would be given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '*σ2x = n*p*(1-p).*'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, a random variable *X* that follows binomial distribution with parameters
    *n* and *p*, we can write as *X ~ B(n, p)*. For such a distribution, the probability
    of getting exactly *k* successes in *n* trials can be described by the probability
    mass function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Binomial distribution](img/image_05_012.jpg)![Binomial distribution](img/image_05_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: here, k = 0, 1, 2, ..., n
  prefs: []
  type: TYPE_NORMAL
- en: Sample problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us assume a hypothetical scenario. Suppose 24 percent of companies in a
    city announced they would provide support to the tsunami-affected areas of the
    country as part of their CSR activity. In a sample of 20 companies chosen at random,
    find the probability of the number of companies that have announced they will
    help tsunami-affected areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Exactly three
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Less than three
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three or more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Solution**:'
  prefs: []
  type: TYPE_NORMAL
- en: The sample size = *n* = 20.
  prefs: []
  type: TYPE_NORMAL
- en: The probability that a company chosen at random has announced it will help =
    *P = 0.24*.
  prefs: []
  type: TYPE_NORMAL
- en: a) P(x = 3) = ^(20)C[3] (0.24)³ (0.76) ^(17) = 0.15
  prefs: []
  type: TYPE_NORMAL
- en: b) P(x < 3) = P(0) + P(1) + P(2)
  prefs: []
  type: TYPE_NORMAL
- en: = (0.76) ^(20) + ^(20)C[1] (0.24) (0.76)^(19) + ^(20)C[2] (0.24)² (0.76)^(18)
  prefs: []
  type: TYPE_NORMAL
- en: = 0.0041 + 0.0261 + 0.0783 = 0.11
  prefs: []
  type: TYPE_NORMAL
- en: c) P(x >= 3) = 1 - P(x <= 2) = 1- 0.11 = 0.89
  prefs: []
  type: TYPE_NORMAL
- en: Note that binomial distribution is widely used in scenarios where you want to
    model the success rate in a sample of size *n* drawn from a population of size
    *N*, with replacement. If it is done without replacement then the draws will no
    longer be independent and hence will not follow binomial distribution rightly.
    However, such scenarios do exist and can be modeled using different types of distributions,
    such as hypergeometric distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Poisson distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Poisson distribution can describe the probability of a given number of independent
    events that occur with a known average rate in a fixed interval of time or space.
    Please note that the events should only have binary outcomes such as success or
    failure, for example, the number of phone calls you receive per day or the number
    of cars passing a signal per hour. You need to carefully take a closer look at
    these examples. Please note here that you do not have the opposite half of this
    information, that is, how many phone calls you did not receive per day or how
    many cars did not pass that signal. Such data points do not have the other half
    of the information. On the contrary, if I say that 30 out of 50 students passed
    in an exam, you can easily infer that 20 students have failed! You have this other
    half of the information.
  prefs: []
  type: TYPE_NORMAL
- en: 'If *µ* is the mean number of events occurring (a known average rate in a fixed
    interval of time or space) then the probability of *k* events occurring at the
    same interval can be described by the probability mass function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Poisson distribution](img/image_05_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: here, *k* = 0, 1, 2, 3...
  prefs: []
  type: TYPE_NORMAL
- en: The preceding equation describes the Poisson distribution.
  prefs: []
  type: TYPE_NORMAL
- en: For a Poisson distribution, mean and variance are the same. Also, the Poisson
    distribution tends to be more symmetric as its mean or variance increases.
  prefs: []
  type: TYPE_NORMAL
- en: Sample problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Suppose you knew that the mean number of calls to a fire station on a weekday
    is eight. What is the probability that on a given weekday there would be 11 calls?
    This problem can be solved using the following formula based on the Poisson distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/image_05_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Continuous probability distributions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Continuous probability distributions are used to model data that is continuous
    in nature, which means that data can only take on any value within a specified
    range. So we deal with probabilities associated with intervals and not with any
    particular value as it is zero. Continuous probability distributions are the theoretical
    models of experiments; it is a relative frequency distribution built from an infinite
    number of observations. This means that when you reduce the interval, the number
    of observations increases, and as the number of observations increases more and
    more and approaches infinity, it forms a continuous probability distribution.
    The total area under the curve is one and to find the probability associated with
    any particular range, we have to find the area under the curve. Therefore, continuous
    distributions are normally described in terms of **probability density function**
    (**PDF**) which is of the following type:'
  prefs: []
  type: TYPE_NORMAL
- en: P(a ≤ X ≤ b) = a∫^b f(x) dx
  prefs: []
  type: TYPE_NORMAL
- en: There can be various types of continuous probability distributions. The following
    sections are a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Normal distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A normal distribution is a simple, straightforward, yet very important continuous
    probability distribution. It is otherwise known as a Gaussian distribution or
    **bell curve** because of its appearance when plotted. Also, for a perfect normal
    distribution, the mean, median, and mode are all the same.
  prefs: []
  type: TYPE_NORMAL
- en: Many naturally occurring phenomena follow a normal distribution (they may follow
    a different distribution as well!), such as the heights of people, errors in measurement,
    and so on. However, normal distributions are not suitable to model variables that
    are highly skewed or are inherently positive (for example, share prices or students'
    test scores where the difficulty level was minimal). Such variables may be better
    described by different distributions or by the normal distribution after a data
    transformation (like logarithmic transformation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Normal distributions can be described using two descriptors: mean for the location
    of the center and standard deviation for the spread (height and width). The probability
    density function that represents a normal distribution is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Normal distribution](img/image_05_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: One of the reasons this normal distribution tops the chart for popularity is
    because of the **Central Limit Theorem** (**CLT**). It states that, regardless
    of the population distribution, the mean of samples independently drawn from same
    population distribution is distributed almost normally and this normality increases
    more and more with the increase in sample size. This behavior is actually the
    basis of statistical hypothesis testing.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, every normal distribution, irrespective of its mean and standard
    deviation, follows an empirical rule (68-95-99.7 rule) which states that about
    68 percent of the area under the curve falls within one standard deviation of
    the mean, 95 percent of the area under the curve falls within two standard deviations
    of the mean, and around 99.7 percent of the area under the curve falls within
    three standard deviations of the mean.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to find the probability of an event, you can either use integral calculus
    or transform the distribution into a standard normal distribution as explained
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Standard normal distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A standard normal distribution is a type of normal distribution with mean *0*
    and standard deviation *1*. Such a distribution is rarely found naturally. It
    is designed mainly to find the area under the curve of a normal distribution (instead
    of integrating using calculus) or to normalize the data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose a random variable *X* is normally distributed with mean (*μ*) and standard
    deviation (*σ*), then the random variable *Z* will have a standard normal distribution
    with mean *0* and standard deviation *1*. The value of *Z* can be found as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard normal distribution](img/image_05_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Since data can be standardized in this manner, the data points can be represented
    and interpreted as *how many standard deviations away from the mean* they lie
    in the distribution. It helps in comparing two distributions with different scales.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the applications of a normal distribution in scenarios where one
    wants to find what percent would fall under a specified range - assuming that
    the distribution is approximately normal.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: If the time a shopkeeper operates the shop on a given day follows the normal
    distribution with *μ* = *8* hours and *σ* = *0.5* hours, what is the probability
    that he stays at the shop for less than 7.5 hours?
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability distribution would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard normal distribution](img/image_05_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Data distribution
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard normal distribution](img/Capture.jpg)![Standard normal distribution](img/image_05_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Standard normal distribution
  prefs: []
  type: TYPE_NORMAL
- en: 'So the probability that the shopkeeper stays at the shop for less than 7.5
    hours is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '*P(z = -1) = 0.1587 = 15.87*'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This was figured out using the Z-table.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that normality in a dataset is mostly an approximation. You first
    need to check the normality of the data and then proceed further if your analysis
    is based on the assumption of normality in data. There are various different ways
    to check for normality: you can opt for techniques such as histogram (with a curve
    fitted with the mean and standard deviation of the data), normal probability plot,
    or QQ plot.'
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Chi-square distribution is one of the most widely used distributions in statistical
    inference. It is a special case of gamma distribution, which is useful in modeling
    skewed distributions of the variables that are not negative. It states that, if
    a random variable *X* is normally distributed and *Z* is one of its standard normal
    variables, then *Z[2]* will have a X[²] distribution with one degree of freedom.
    Similarly, if we take many such random independent standard normal variables from
    the same distribution, square them and add them up, then that will also follow
    X[²] distribution as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Z[12] + Z[22] + ... + Z[k2]* will have X[2] distribution with *k* degrees
    of freedom.'
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square distribution is mainly used for the inference of population variance
    or population standard deviation given the sample variance or standard deviation.
    This is because X[2] distribution is defined in an alternative way, in terms of
    the ratio of sample variance to population variance.
  prefs: []
  type: TYPE_NORMAL
- en: To justify this point, let us take a random sample (*x[1]*, *x[2]*,...,*xn*)
    from a normal distribution with variance ![Chi-square distribution](img/Ch.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample mean would be given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chi-square distribution](img/image_05_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'However, the sample variance is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chi-square distribution](img/image_05_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Considering the preceding mentioned facts, we can define the chi-square statistic
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chi-square distribution](img/image_05_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (Remember ![Chi-square distribution](img/image_05_024.jpg) and *Z[2]* will have
    X[2] distribution.)
  prefs: []
  type: TYPE_NORMAL
- en: So , ![Chi-square distribution](img/image_05_025.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the sampling distribution of the chi-square statistic will follow
    a chi-square distribution with *(n-1)* degrees of freedom.
  prefs: []
  type: TYPE_NORMAL
- en: 'The probability density function of a chi-square distribution with *n* degrees
    of freedom and gamma function *Г* is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chi-square distribution](img/image_05_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For a *χ2* distribution with *k* degrees of freedom, mean (*µ*) = *k* and variance
    (*σ2*) = *2k.*
  prefs: []
  type: TYPE_NORMAL
- en: Please note that chi-square distributions are positively skewed, but the degree
    of skewness decreases with the increase in the degree of freedom and approaches
    a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Sample problem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Find the 90 percent confidence interval for the variance and standard deviation
    for the price in dollars for adult single movie tickets. The data given represents
    a selected sample of nationwide movie theaters. Assume the variable is normally
    distributed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given sample (in $): 10, 08, 07, 11, 12, 06, 05, 09, 15, 12'
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '*N* = *10*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mean of sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/Mean-of-sample.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Variance of sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/Variance.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Standard deviation of sample:'
  prefs: []
  type: TYPE_NORMAL
- en: S = sqrt(9.61)
  prefs: []
  type: TYPE_NORMAL
- en: 'Degree of freedom:'
  prefs: []
  type: TYPE_NORMAL
- en: 10-1 = 9
  prefs: []
  type: TYPE_NORMAL
- en: Now we need to find the 90 percent confidence interval, which means that 10
    percent of the data will be left over in the tails.
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/image_05_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let us use the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/image_05_028.jpg)![Sample problem](img/image_05_029.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Then we can either find the chi-square value using a table or a computer program.
  prefs: []
  type: TYPE_NORMAL
- en: To find the middle 90 percent confidence interval, we can consider the left
    95 percent and right 5 percent.
  prefs: []
  type: TYPE_NORMAL
- en: 'So after substituting the numbers, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Sample problem](img/image_05_030.jpg)![Sample problem](img/image_05_031.jpg)![Sample
    problem](img/image_05_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, we can conclude that we are 90 percent confident that the standard deviation
    for the price of a single movie ticket of the population (all tickets in the nation)
    is between $2.26 and $5.10 based on a sample of 10 nationwide movie ticket prices.
  prefs: []
  type: TYPE_NORMAL
- en: Student's t-distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Student's t-distribution is used in estimating the mean of a normally distributed
    population in the case where the population standard deviation is not known or
    the sample size is too small. In such cases, both *μ* and *σ* are unknown and
    population parameters are estimated only through the sample.
  prefs: []
  type: TYPE_NORMAL
- en: This distribution is bell-shaped and symmetric like normal distribution, but
    has heavier tails. The t-distribution becomes a normal distribution when the sample
    size is large.
  prefs: []
  type: TYPE_NORMAL
- en: Let us take a random sample (*x1*, *x2*,...,*xn*) from a normal distribution
    with mean *μ* and variance *σ2*.
  prefs: []
  type: TYPE_NORMAL
- en: The sample mean would be ![Student's t-distribution](img/image_05_033.jpg) and
    sample variance ![Student's t-distribution](img/image_05_034.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the above-mentioned facts, the t-statistic can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Student''s t-distribution](img/image_05_035.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The sampling distribution of the t-statistic will follow a t-distribution with
    *(n-1)* **degrees of freedom** (**df** ). The higher the degree of freedom, the
    closer will be the t-distribution to the standard normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: The mean of a t-distribution (*μ*) = *0* and variance *(σ2) = df/df-2*
  prefs: []
  type: TYPE_NORMAL
- en: Now, just to make things clearer, let us look back for a moment and consider
    the scenario where the population *σ* is known. When the population is normally
    distributed, the sample mean *x̄* is mostly normally distributed regardless of
    the sample size and any linear transformation of *x̄* such as ![Student's t-distribution](img/image_05_037.jpg)
    will also follow a normal distribution.
  prefs: []
  type: TYPE_NORMAL
- en: What if the population is not normally distributed? Even then, the distribution
    of *x̄* (which is the sampling distribution) or ![Student's t-distribution](img/image_05_037.jpg) will
    follow a normal distribution as per CLT when the sample size is large enough!
  prefs: []
  type: TYPE_NORMAL
- en: The other scenario is that the population *σ* is unknown. With this, if the
    population is normally distributed, the sample mean *x̄* is mostly normally distributed,
    but the random variable ![Student's t-distribution](img/image_05_039.jpg) will
    not follow a normal distribution; it follows a t-distribution with *(n-1)* degrees
    of freedom. The reason is because of the randomness of *S* in the denominator,
    it is different for different samples.
  prefs: []
  type: TYPE_NORMAL
- en: In the above case, if the population is not normally distributed, the distribution
    of ![Student's t-distribution](img/image_05_040.jpg) will follow a normal distribution
    as per CLT with sufficiently large sample sizes (and not with the small sample
    size!). So, with the large sample size, the distribution of ![Student's t-distribution](img/image_05_040.jpg) follows
    a normal distribution, and it is safe to assume that it follows t-distribution
    because t-distribution approaches normality with an increase in the sample size.
  prefs: []
  type: TYPE_NORMAL
- en: F-distribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In statistical inference, F-distribution is used to study the variance of two
    normally distributed populations. It states that the sampling distribution of
    the sample variances from two independent normally distributed populations with
    the same population variance follow F-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: If the sample variance of sample 1 is ![F-distribution](img/image_05_041.jpg) and
    if the sample variance of sample 2 is ![F-distribution](img/image_05_042.jpg) then,
    ![F-distribution](img/image_05_043.jpg) will have F-distribution (*σ12 = σ22*).
  prefs: []
  type: TYPE_NORMAL
- en: From the above fact, we can also say that ![F-distribution](img/image_05_044.jpg) will
    also follow F-distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section of chi-square distribution, we can also say that
  prefs: []
  type: TYPE_NORMAL
- en: '![F-distribution](img/image_05_045.jpg) will also follow F-distribution with
    *n1-1* and *n2-1* degrees of freedom. For each combination of these degrees of
    freedoms, there would be different F-distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: Standard error
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The standard deviation of the sampling distribution of a statistic (such as
    mean or variance) is called the **standard error** (**SE**), a measure of variability.
    In other words, the **standard error of the mean** (**SEM**) can be defined as
    the standard deviation of the sample mean's estimate of a population mean.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you increase the sample sizes, the sampling distribution of the mean gets
    more and more normal and the standard deviation gets smaller. It is proved that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard error](img/image_05_046.jpg)'
  prefs: []
  type: TYPE_IMG
- en: (*n* being the sample size)
  prefs: []
  type: TYPE_NORMAL
- en: '![Standard error](img/image_05_047.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The smaller the standard error, the more representative the sample will be of
    the overall population. Also, the larger the sample size, the smaller the standard
    error.
  prefs: []
  type: TYPE_NORMAL
- en: SE is very important in other measures of statistical inference, such as margin
    of error and confidence interval.
  prefs: []
  type: TYPE_NORMAL
- en: Confidence level
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is a measure of how certain you would like to be (the probability) in estimating
    the population parameter through sample statistics so that the expected values
    would fall within a desired range or confidence interval. It is calculated by
    subtracting the significance level (*α*) from *1* (that is, *confidence level
    = 1 - α*). So, if *α = 0.05*, the confidence level would be *1-0.05 = 0.95*
  prefs: []
  type: TYPE_NORMAL
- en: Usually, the higher the confidence level, the higher the sample size required.
    However, there are often trade-offs and you have to decide on how confident you
    would like to be so that you can estimate the sample size needed for your confidence
    level.
  prefs: []
  type: TYPE_NORMAL
- en: Margin of error and confidence interval
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed already, since a sample can never be a 100 percent representative
    of the population, estimating the population parameter through inference will
    always have some margin of error due to sampling errors. Usually, the bigger the
    sample size, the smaller the margin of error. However, you have to decide on how
    much error to allow, and estimating a proper sample size required would depend
    on that.
  prefs: []
  type: TYPE_NORMAL
- en: So, the range of values below and above the sample statistic based on the margin
    of error is called the **confidence interval**. In other words, a confidence interval
    is a range of numbers within which we believe the true population parameter to
    fall a certain percentage of the time (confidence level).
  prefs: []
  type: TYPE_NORMAL
- en: Please note here that a statement such as "I am 95 percent confident that the
    confidence interval contains the true value" could be misleading! The right way
    of stating this could be "*If I take an infinite number of samples of the same
    size, then 95 percent of the time the confidence interval would contains the true
    value".*
  prefs: []
  type: TYPE_NORMAL
- en: For example, when you put the confidence level as 95 percent and the confidence
    interval as 4 percent for a sample statistic 58 (here, 58 is any sample statistic
    such as mean, variance, or standard deviation), you can say that you are 95 percent
    sure that the true percentage of the population is between 58 - 4 = 54 percent
    and 58 + 4 = 62 percent.
  prefs: []
  type: TYPE_NORMAL
- en: Variability in the population
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The variability in the population is one of the most important factors we should
    consider in our inferential statistics. It plays an important role in estimating
    the sample size. No matter what sampling algorithm you choose that can best represent
    the population, the sample size still plays a crucial role - and this is obvious!
  prefs: []
  type: TYPE_NORMAL
- en: If the variation in the population is more, then the sample size required would
    also be more.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating sample size
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We already covered sampling techniques in the previous sections. In this section,
    we will discuss how to estimate the sample size. Assume you have to prove a concept
    or to assess the result of some action, then you take some relevant data and try
    to prove your point. However, how would you ensure you have enough data? Samples
    that are too big waste time and resources, and samples that are too small may
    lead to misleading results. Estimating the sample size depends majorly on factors
    such as the margin of error or confidence interval, confidence level, and variability
    in the population.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: The college president asks the statistics teacher to estimate the average age
    of the students at their college. How large a sample is necessary? The statistics
    teacher would like to be 99 percent confident that the estimate should be accurate
    within 1 year. From a previous study, the standard deviation of the ages is known
    to be 3 years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Estimating sample size](img/image_05_048.jpg)![Estimating sample size](img/image_05_049.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hypothesis testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hypothesis testing is about testing the assumptions made for the population
    parameters. This helps in determining whether a result is statistically significant
    or has occurred by chance. It is the most important instrument of statistical
    research. We will discuss some of the testing to see how variables are related
    to each other in the population.
  prefs: []
  type: TYPE_NORMAL
- en: Null and alternate hypotheses
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The null hypothesis (denoted as H0) is often the initial claim about the population
    parameter, and it is mostly indicative of *no effect* or *no relation*. In our
    hypothesis testing, our aim is to invalidate and reject the null hypothesis to
    be able to accept the alternate hypothesis (denoted as H1). The alternate hypothesis
    is indicative of some effect in your experiment. While experimenting, please note
    here that you either reject the null hypothesis or fail to reject the null hypothesis.
    If you are successful in rejecting the null hypothesis then the alternate hypothesis
    is to be considered and if you fail to reject the null hypothesis then the null
    hypothesis is considered (though it may not be true!).
  prefs: []
  type: TYPE_NORMAL
- en: So, we usually hope to get a very small P-value (lower than the defined significance
    level alpha) to be able to reject the null hypothesis. If the P-value is greater
    than alpha, then you fail to reject the null hypothesis.
  prefs: []
  type: TYPE_NORMAL
- en: Chi-square test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the statistical inference techniques are used to estimate the population
    parameters or to test a hypothesis using the sample statistics such as *mean*.
    However, the chi-square statistic takes a completely different approach by examining
    the whole distribution or the relationship between two distributions. In the field
    of inferential statistics, many test statistics resemble a chi-square distribution.
    The most common tests using this distribution are the chi-square test of goodness
    of fit (one-way tables) and chi-square test of independence (two-way tables).
    The *goodness of fit* test is done when you want to see if the sample data follows
    the same distribution in the population and the *independence* test is done when
    you want to see if two categorical variables are related to each other in the
    population.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input data types determine whether to conduct a *goodness of fit* or *independence*
    test without specifying them as switches explicitly. So, if you provide a vector
    as input, then the *goodness of fit* test is conducted and if you provide a matrix
    as input, then the *independence* test is conducted. In either case, a vector
    of frequencies of events or a contingency matrix is provided as input which you
    need to compute first. Let us explore these through examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: F-test
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have already covered how to calculate the F-statistic in the previous sections.
    Now we will solve a sample problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem:'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You want to test the belief that the income of Master's degree holders shows
    greater variability than the income of Bachelor's degree holders. A random sample
    of 21 graduates and a random sample of 30 Masters were taken. The standard deviation
    of the graduates sample was $180 and that of the Masters sample was $112.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The null hypothesis is: *H[0] : σ[1]² =σ[2]²*'
  prefs: []
  type: TYPE_NORMAL
- en: Given that *S[1] = $180*, *n[1] = 21*, and *S[2] = $112*, *n[2] = 30*
  prefs: []
  type: TYPE_NORMAL
- en: Considering the level of significance to be *α = 0.05*
  prefs: []
  type: TYPE_NORMAL
- en: '*F = S[1]² /S[2]² = 180²/112² = 2.58*'
  prefs: []
  type: TYPE_NORMAL
- en: From the F-table with the significance level 0.05, df1=20 and df2=29, we can
    see that the F-value is 1.94
  prefs: []
  type: TYPE_NORMAL
- en: Since the computed value of F is greater than the table value of F, we can reject
    the null hypothesis and conclude that *σ[1]² >σ[2]* ^(*2*) .
  prefs: []
  type: TYPE_NORMAL
- en: Correlations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Correlations provide a way to measure the statistical dependence between two
    random variables that are numeric in nature. This shows the extent to which the
    two variables change with each other. There are basically two types of correlation
    measures: Pearson and Spearman. Pearson is more appropriate for interval scale
    data, such as temperature, height, and so on. Spearman is more appropriate for
    ordinal scale, such as a satisfaction survey where 1 is less satisfied and 5 is
    most satisfied. Also, Pearson is calculated based on true values and is useful
    in finding linear relationships whereas Spearman is based on rank order and is
    useful in finding monotonic relationships. The monotonic relationship means that
    the variables do change together, but not at a constant rate. Please note that
    both of these correlation measures can only measure linear or monotonic relationships
    and are not capable of depicting any other kind of relationships such as non-linear
    relationships.'
  prefs: []
  type: TYPE_NORMAL
- en: In Spark, both of these are supported. If you input two `RDD[Double]`, the output
    is a *Double* and if you input an `RDD[Vector]`, the output is a *Correlation
    Matrix*. In both Scala and Python implementations, if you do not provide the type
    of correlation as input, then the default considered is always Pearson.
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '**Scala**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly covered the steps involved in the data science life
    cycle, such as data acquisition, data preparation, and data exploration through
    descriptive statistics. We also learnt to estimate the population parameters through
    sample statistics using some popular tools and techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We explained the basics of statistics from both theoretical and practical aspects
    by going deeper into the fundamentals in a few areas to be able to solve business
    problems. Finally, we learnt a few examples on how statistical analysis can be
    performed on Apache Spark, leveraging the out-of-the-box features, which was basically
    the objective behind this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss more details of the machine learning part of data science in
    the next chapter as we have already built statistical understanding in this chapter.
    Learnings from this chapter should help connect to the machine learning algorithms
    in a more informed way.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supported statistics by Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/mllib-statistics.html](http://spark.apache.org/docs/latest/mllib-statistics.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Plotting features of Databricks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.cloud.databricks.com/docs/latest/databricks_guide/04%20Visualizations/4%20Matplotlib%20and%20GGPlot.html](https://docs.cloud.databricks.com/docs/latest/databricks_guide/04%20Visualizations/4%20Matplotlib%20and%20GGPlot.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Detailed information on OOTB library functions of MLLIB stats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.Statistics$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.Statistics%24)'
  prefs: []
  type: TYPE_NORMAL
