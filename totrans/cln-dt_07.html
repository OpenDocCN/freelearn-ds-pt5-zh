<html><head></head><body><div><div><div><div><h1 class="title"><a id="ch07"/>Chapter 7. RDBMS Cleaning Techniques</h1></div></div></div><p>Home refrigerators all come with shelves and most have one or two drawers for vegetables. But if you ever visit a home organization store or talk to a professional organizer, you will learn that there are also numerous additional storage options, including egg containers, cheese boxes, soda can dispensers, wine bottle holders, labeling systems for leftovers, and stackable, color-coded bins in a variety of sizes. But do we really need all these extras? To answer this, ask yourself these questions, are my frequently used foods easy to find? Are food items taking up more space than they should? Are leftovers clearly labeled so I remember what they are and when I made them? If our answers are <em>no</em>, organization experts say that containers and labels can help us optimize storage, reduce waste, and make our lives easier.</p><p>The <a id="id435" class="indexterm"/>same is true in our <strong>Relational Database Management System</strong> (<strong>RDBMS</strong>). As the classic long-term data storage solution, RDBMS is a standard part of the modern data science toolkit. But all too often, we are guilty of just depositing data in the database, with little thought about the details. In this chapter, we will learn how to design an RDBMS that goes beyond <em>two shelves and a drawer</em>. We will learn a few techniques that will ensure our RDBMS is optimizing storage, reducing waste, and making our lives easier. Specifically, we will:</p><div><ul class="itemizedlist"><li class="listitem">Learn how to find anomalies in our RDBMS data</li><li class="listitem">Learn several strategies to clean different types of problematic data</li><li class="listitem">Learn when and how to create new tables for your cleaned data, including creating both child tables and lookup tables</li><li class="listitem">Learn how to document the rules governing the changes you made</li></ul></div><div><div><div><div><h1 class="title"><a id="ch07lvl1sec39"/>Getting ready</h1></div></div></div><p>To set up the examples in this chapter, we will be working with a popular dataset called <strong>Sentiment140</strong>. This<a id="id436" class="indexterm"/> dataset was created to help learn about the positive and negative sentiments in messages on Twitter. We are not really concerned with sentiment analysis in this book, but we are going to use this dataset to practice cleaning data after it has already been imported into a relational database.</p><p>To get <a id="id437" class="indexterm"/>started with the Sentiment140 dataset, you will need a MySQL server set up and ready to go, just like in the earlier Enron examples.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec40"/>Step one – download and examine Sentiment140</h1></div></div></div><p>The<a id="id438" class="indexterm"/> version of the Sentiment140 data that we want to <a id="id439" class="indexterm"/>use is the original set of two files available directly <a id="id440" class="indexterm"/>from the Sentiment140 project at <a class="ulink" href="http://help.sentiment140.com/for-students">http://help.sentiment140.com/for-students</a>. This ZIP file of tweets and their positive and negative polarity (or sentiment, on a scale of 0, 2, or 4) was created by some graduate students at Stanford University. Since this file was made publicly available, the original Sentiment140 files have been added by other websites and made available as part of many larger collections of tweets. For this chapter, we will use the original Sentiment140 text file, which is either available as a link from the preceding site or by following the precise path to <a class="ulink" href="http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip">http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip</a>.</p><p>Download the ZIP file, extract it, and take a look at the two CSV files inside using your text editor. Right away, you will notice that one file has many more lines than the other, but both these files have the same number of columns in them. The data is comma-delimited, and each column has been enclosed in double quotes. The description of each column can be found on the <code class="literal">for-students</code> page linked in the preceding section.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec41"/>Step two – clean for database import</h1></div></div></div><p>For our <a id="id441" class="indexterm"/>purposes—learning how to clean data—it will be sufficient to load the smaller of these files into a single MySQL database table. Everything we need to do to learn, we can accomplish with the smaller file, the one called <code class="literal">testdata.manual.2009.06.14.csv</code>.</p><p>As we are looking at the data, we may notice a few areas that will trip us up if we try to import this file directly into MySQL. One of the trouble spots is located at line 28 in the file:</p><div><pre class="programlisting">"4","46","Thu May 14 02:58:07 UTC 2009","""booz allen""", </pre></div><p>Do you see the triple quotation marks <code class="literal">"""</code> right before the <code class="literal">booz</code> keyword and after the word <code class="literal">allen</code>? The same issue comes up later on line 41 with double quotation marks around the song title <code class="literal">P.Y.T</code>:</p><div><pre class="programlisting">"4","131","Sun May 17 15:05:03 UTC 2009","Danny Gokey","VickyTigger","I'm listening to ""P.Y.T"" by Danny Gokey…"</pre></div><p>The problem with these extra quotation marks is that the MySQL import routine will use the quotation marks to delimit the column text. This will produce an error, as MySQL will think that the line has more columns than there really are.</p><p>To fix this, in <a id="id442" class="indexterm"/>our text editor, we can use <strong>Find and Replace</strong> to replace all instances of <code class="literal">"""</code> with <code class="literal">"</code> (double quote) and all instances of <code class="literal">""</code> with <code class="literal">'</code> (single quote).</p><div><h3 class="title"><a id="tip24"/>Tip</h3><p>These <code class="literal">""</code> could also probably be removed entirely with very little negative effect on this cleaning exercise. To do this, we would simply search for <code class="literal">""</code> and replace it with nothing. But if you want to stick close to the original intent of the tweet, a single quote (or even an escaped double quote like this <code class="literal">\"</code>) is a safe choice for a replacement character.</p></div><p>Save this cleaned file to a new filename, something like <code class="literal">cleanedTestData.csv</code>. We are now ready to import it into MySQL.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec42"/>Step three – import the data into MySQL in a single table</h1></div></div></div><p>To load<a id="id443" class="indexterm"/> our somewhat cleaner data file into MySQL, we will need to revisit the CSV-to-SQL techniques from the <em>Importing spreadsheet data into MySQL</em> section in <a class="link" title="Chapter 3. Workhorses of Clean Data – Spreadsheets and Text Editors" href="part0024.xhtml#aid-MSDG2">Chapter 3</a>, <em>Workhorses of Clean Data – Spreadsheets and Text Editors</em>:</p><div><ol class="orderedlist arabic"><li class="listitem">From the command line, navigate to the directory where you have saved the file you created in step two. This is the file we are going to import into MySQL.</li><li class="listitem">Then, launch your MySQL client, and connect to your database server:<div><pre class="programlisting">user@machine:~/sentiment140$ mysql -hlocalhost -umsquire -p
Enter password:</pre></div></li><li class="listitem">Enter your password, and after you are logged in, create a database within MySQL to hold the table, as follows:<div><pre class="programlisting">mysql&gt; CREATE DATABASE sentiment140;
mysql&gt; USE sentiment140;</pre></div></li><li class="listitem">Next, we need to create a table to hold the data. The data type and lengths for each column should represent our best attempt to match the data we have. Some of the columns will be varchars, and each of them will need a length. As we might not know what those lengths should be, we can use our cleaning tools to discern an appropriate range.</li><li class="listitem">If we open our CSV file in Excel (Google Spreadsheets will work just fine for this as well), we can run some simple functions to find the maximum lengths of some of our text fields. The <code class="literal">len()</code> function, for example, gives the length <a id="id444" class="indexterm"/>of a text string in characters, and the <code class="literal">max()</code> function can tell us the highest number in a range. With our CSV file open, we can apply these functions to see how long our varchar columns in MySQL should be.<p>The following screenshot shows a method to use functions to solve this problem. It shows the <code class="literal">length()</code> function applied to column <strong>G</strong>, and the <code class="literal">max()</code> function used in column <strong>H</strong> but applied to column <strong>G</strong>.</p><div><img src="img/image00294.jpeg" alt="Step three – import the data into MySQL in a single table"/></div><p style="clear:both; height: 1em;"> </p><p>Columns <strong>G</strong> and <strong>H</strong> show how to get the length of a text column in Excel and then get the maximum value.</p></li><li class="listitem">To calculate these maximum lengths more quickly, we can also take an Excel shortcut. The following array formula can work to quickly combine the maximum value with the length of a text column in a single cell—just make sure you press <em>Ctrl</em> + <em>Shift</em> + <em>Enter</em> after typing this nested function rather than just <em>Enter</em>:<div><pre class="programlisting">=max(len(f1:f498))</pre></div><p>This nested function can be applied to any text column to get the maximum length of the text in that column, and it only uses a single cell to do this without requiring any intermediate length calculations.</p></li></ol><div></div><p>After we run these functions, it turns out that the maximum length for any of our tweets is 144 characters.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec73"/>Detecting and cleaning abnormalities</h2></div></div></div><p>You <a id="id445" class="indexterm"/>might be wondering how a tweet in this dataset <a id="id446" class="indexterm"/>could possibly be 144 characters long as Twitter limits all tweets to a maximum length of 140 characters. It turns out that in the sentiment140 dataset, the <strong>&amp;</strong> character was sometimes translated to the HTML equivalent code, <code class="literal">&amp;amp</code>, but not always. Some other HTML code was used too, for instance, sometimes, the <strong>&lt;</strong> character became <code class="literal">&amp;lt;</code> and <strong>&gt;</strong> became <code class="literal">&amp;gt;</code>. So, for a few very long tweets, this addition of just a few more characters can easily push this tweet over the length limit of 140. As we know that these HTML-coded characters were not what the original person tweeted, and because we see that they happen sometimes but not all the time, we<a id="id447" class="indexterm"/> call these <strong>data abnormalities</strong>.</p><p>To clean<a id="id448" class="indexterm"/> these, we have two choices. We can either go ahead<a id="id449" class="indexterm"/> and import the messy data into the database and try to clean it there, or we can attempt to clean it first in Excel or a text editor. To show the difference in these two techniques, we will do both here. First, we will use find and replace in our spreadsheet or text editor to try to convert the characters shown in the following table. We can import the CSV file into Excel and see how much cleaning we can do there:</p><div><table border="1"><colgroup><col/><col/><col/><col/></colgroup><thead><tr><th valign="bottom">
<p>HTML code</p>
</th><th valign="bottom">
<p>Replace with</p>
</th><th valign="bottom">
<p>The count of instances</p>
</th><th valign="bottom">
<p>The Excel function used to find count</p>
</th></tr></thead><tbody><tr><td valign="top">
<p><code class="literal">&amp;lt;</code></p>
</td><td valign="top">
<p><code class="literal">&lt;</code></p>
</td><td valign="top">
<p>6</p>
</td><td valign="top">
<p><code class="literal">=COUNTIF(F1:F498,"*&amp;lt*")</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">&amp;gt;</code></p>
</td><td valign="top">
<p><code class="literal">&gt;</code></p>
</td><td valign="top">
<p>5</p>
</td><td valign="top">
<p><code class="literal">=COUNTIF(F1:F498,"*&amp;gt*")</code></p>
</td></tr><tr><td valign="top">
<p><code class="literal">&amp;amp;</code></p>
</td><td valign="top">
<p><code class="literal">&amp;</code></p>
</td><td valign="top">
<p>24</p>
</td><td valign="top">
<p><code class="literal">=COUNTIF(F1:F498,"*&amp;amp*")</code></p>
</td></tr></tbody></table></div><p>The first two character swaps work fine with <strong>Find and Replace</strong> in Excel. The <code class="literal">&amp;lt</code>; and <code class="literal">&amp;gt</code>; HTML-encoded characters are changed. Take a look at the text like this:</p><div><pre class="programlisting">I'm listening to 'P.Y.T' by Danny Gokey &amp;lt;3 &amp;lt;3 &amp;lt;3 Aww, he's so amazing. I &amp;lt;3 him so much :)</pre></div><p>The preceding becomes text like this:</p><div><pre class="programlisting">I'm listening to 'P.Y.T' by Danny Gokey &lt;3 &lt;3 &lt;3 Aww, he's so amazing. I &lt;3 him so much :)</pre></div><p>However, when we attempt to use Excel to find <code class="literal">&amp;amp;</code> and replace it with <code class="literal">&amp;</code>, you may run into an error, as shown:</p><div><img src="img/image00295.jpeg" alt="Detecting and cleaning abnormalities"/></div><p style="clear:both; height: 1em;"> </p><p>Some <a id="id450" class="indexterm"/>operating systems and versions of Excel have a problem <a id="id451" class="indexterm"/>with our selection of the <strong>&amp;</strong> character as a replacement. At this point, if we run into this error, we could take a few different approaches:</p><div><ul class="itemizedlist"><li class="listitem">We could use our search engine of choice to attempt to find an Excel solution to this error</li><li class="listitem">We could move our CSV text data into a text editor and perform the find and replace function in there</li><li class="listitem">We could forge ahead and throw the data into the database despite it having the weird <code class="literal">&amp;amp</code>; characters in it and then attempt to clean it inside the database</li></ul></div><p>Normally, I would be in favor of not moving dirty data into the database if it is even remotely possible to clean it outside of the database. However, as this is a chapter about cleaning inside a database, let's go ahead and import the half-cleaned data into the database, and we will work on cleaning the <code class="literal">&amp;amp</code>; issue once the data is inside the table.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec74"/>Creating our table</h2></div></div></div><p>To<a id="id452" class="indexterm"/> move our half-cleaned data into the database, we will first write our <code class="literal">CREATE</code> statement and then run it on our MySQL database. The <code class="literal">CREATE</code> statement is shown as follows:</p><div><pre class="programlisting">mysql&gt; CREATE TABLE sentiment140 (
    -&gt;   polarity enum('0','2','4') DEFAULT NULL,
    -&gt;   id int(11) PRIMARY KEY,
    -&gt;   date_of_tweet varchar(28) DEFAULT NULL,
    -&gt;   query_phrase varchar(10) DEFAULT NULL,
    -&gt;   user varchar(10) DEFAULT NULL,
    -&gt;   tweet_text varchar(144) DEFAULT NULL
    -&gt; ) ENGINE=MyISAM DEFAULT CHARSET=utf8;</pre></div><div><h3 class="title"><a id="note16"/>Note</h3><p>This statement uses the simple and fast MyISAM engine as we do not anticipate needing any InnoDB features such as row-level locking or transactions. For more on the difference between MyISAM and InnoDB, there is a handy discussion of<a id="id453" class="indexterm"/> when to use each storage engine located here: <a class="ulink" href="http://stackoverflow.com/questions/20148/myisam-versus-innodb">http://stackoverflow.com/questions/20148/myisam-versus-innodb</a>.</p></div><p>You might notice that the code still requires 144 for the length of the <code class="literal">tweet_text</code> column. This is because we were unable to clean these columns with the <code class="literal">&amp;amp</code>; code in them. However, this does not bother me too much because I know that varchar columns will not use their extra space unless they need it. After all, this is why they are called varchar, or variable character, columns. But if this extra length really bothers you, you can alter the table later to only have 140 characters for that column.</p><p>Next, we will use the MySQL command line to run the following import statement from the location:</p><div><pre class="programlisting">mysql&gt; LOAD DATA LOCAL INFILE 'cleanedTestData.csv'
    -&gt;   INTO TABLE sentiment140
    -&gt;   FIELDS TERMINATED BY ',' ENCLOSED BY '"' ESCAPED BY '\'
    -&gt;   (polarity, id, date_of_tweet, query_phrase, user, tweet_text);</pre></div><p>This command loads the data from our cleaned CSV file into the new table we created. A success message will look like this, indicating that all 498 rows were loaded into the table:</p><div><pre class="programlisting">Query OK, 498 rows affected (0.00 sec)
Records: 498  Deleted: 0  Skipped: 0  Warnings: 0</pre></div><div><h3 class="title"><a id="tip25"/>Tip</h3><p>If you have access to a browser-based interface such as phpMyAdmin (or a desktop<a id="id454" class="indexterm"/> application such as MySQL Workbench or Toad for MySQL), all of these SQL commands can be completed inside these tools very easily and without having to type on the command line, for example, in phpMyAdmin, you can use the Import tab and upload the CSV file there. Just make sure that the data file is cleaned following the procedures in <em>Step two – clean for database import</em>, or you may get errors about having too many columns in your file. This error is caused by quotation mark problems.</p></div></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec43"/>Step four – clean the &amp;amp; character</h1></div></div></div><p>In the <a id="id455" class="indexterm"/>last step, we decided to postpone cleaning the <code class="literal">&amp;amp</code>; character because Excel was giving a weird error about it. Now that we have finished <em>Step three – import the data into MySQL in a single table</em> and our data is imported into MySQL, we can very easily clean the data using an <code class="literal">UPDATE</code> statement and the <code class="literal">replace()</code>string function. Here is the SQL query needed to take all instances of <code class="literal">&amp;amp</code>; and replace them with <code class="literal">&amp;</code>:</p><div><pre class="programlisting">UPDATE sentiment140 SET tweet_text = replace(tweet_text,'&amp;amp;', '&amp;');</pre></div><p>The <code class="literal">replace()</code>function works just like find and replace in Excel or in a text editor. We can see that tweet ID 594, which used to say <code class="literal">#at&amp;amp;t is complete fail</code>, now reads <code class="literal">#at&amp;t is complete fail</code>.</p></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec44"/>Step five – clean other mystery characters</h1></div></div></div><p>As we <a id="id456" class="indexterm"/>are perusing the <code class="literal">tweet_text</code> column, we may have noticed a few odd tweets, such as tweet IDs 613 and 2086:</p><div><pre class="programlisting">613, Talk is Cheap: Bing that, I?ll stick with Google
2086, Stanford University?s Facebook Profile</pre></div><p>The <code class="literal">?</code> character is what we should be concerned about. As with the HTML-encoded characters we saw earlier, this character issue is also very likely an artifact of a prior conversion between character sets. In this case, there was probably some kind of high-ASCII or Unicode <a id="id457" class="indexterm"/>apostrophe (sometimes called a <strong>smart quote</strong>) in the original tweet, but when the data was converted into a lower-order character set, such as plain ASCII, that particular flavor of apostrophe was simply changed to a <code class="literal">?</code>.</p><p>Depending on what we plan to do with the data, we might not want to leave out the <code class="literal">?</code> character, for example, if we are performing word counting or text mining, it may be very important that we convert <code class="literal">I?ll</code> to <code class="literal">I'll</code> and <code class="literal">University?s</code> to <code class="literal">University's</code>. If we decide that this is important, then our job is to detect the tweets, where this error happened, and then devise a strategy to convert the question mark back to a single quote. The trick, of course, is that we cannot just replace every question mark in the <code class="literal">tweet_text</code> column with a single quote character, because some tweets have question marks in them that should be left alone.</p><p>To locate the <a id="id458" class="indexterm"/>problem characters, we can run some SQL queries that attempts to locate the problems using a regular expression. We are interested in question marks that appear in odd places, such as with an alphabetic character immediately following them. Here is an initial pass at a regular SQL expression using the MySQL <code class="literal">REGEXP</code> feature. Running this will give us a rough idea of where the problem question marks might reside:</p><div><pre class="programlisting">SELECT id, tweet_text
FROM sentiment140
WHERE tweet_text
REGEXP '\\?[[:alpha:]]+';</pre></div><p>This SQL regular expression asks for any question mark characters that are immediately followed by one or more alphabetic characters. The SQL query yields six rows, four of which turn out to have <a id="id459" class="indexterm"/>odd question marks and two of which are <strong>false positives</strong>. False positives are tweets that matched our pattern but that should not actually be changed. The<a id="id460" class="indexterm"/> two false positives are tweets with IDs <strong>234</strong> and <strong>2204</strong>. These two included question marks as part of a legitimate URL. Tweets <strong>139</strong>, <strong>224</strong>, <strong>613</strong>, and <strong>2086</strong> are <strong>true positives</strong>, which means tweets that were correctly detected as anomalous and should be changed. All the results are shown in the following phpMyAdmin screenshot:</p><div><img src="img/image00296.jpeg" alt="Step five – clean other mystery characters"/></div><p style="clear:both; height: 1em;"> </p><p>Tweet <strong>139</strong> is strange, though. It has a question mark before the word <strong>Obama</strong>, as if the name of a news article was being quoted, but there is no matching quote (or missing quote) at the end of the string. Was this supposed to be some other character? This might actually be a false positive too, or at least not enough of a true positive to actually make us fix it. While we are looking at the tweets closely, <strong>224</strong> also has an extra strange question mark in a place where it does not seem to belong.</p><p>If we are going to write a <code class="literal">replace()</code> function to change problematic question marks to single quotes, we will somehow need to write a regular expression that matches only the true positives and does not match any of the false positives. However, as this dataset is small, and there are only four true positives—or three if we decide <strong>139</strong> does not need to be cleaned—then <a id="id461" class="indexterm"/>we could just clean the true positives by hand. This is especially a good idea as we have a few questions about other possible issues such as the extra question mark in tweet <strong>224</strong>.</p><p>In this case, as we only have three problem rows, it will be faster to simply run three small <code class="literal">UPDATE</code> commands on the data rather than attempting to craft the perfect regular expression. Here is the SQL query to take care of tweets <strong>224</strong> (first issue only), <strong>613</strong>, and <strong>2086</strong>:</p><div><pre class="programlisting">UPDATE sentiment140 SET tweet_text = 'Life''s a bitch? and so is Dick Cheney. #p2 #bipart #tlot #tcot #hhrs #GOP #DNC http://is.gd/DjyQ' WHERE id = 224;

UPDATE sentiment140 SET tweet_text = 'Talk is Cheap: Bing that, I''ll stick with Google. http://bit.ly/XC3C8' WHERE id = 613;

UPDATE sentiment140 SET tweet_text = 'Stanford University''s Facebook Profile is One of the Most Popular Official University Pages - http://tinyurl.com/p5b3fl' WHERE id = 2086;</pre></div><div><h3 class="title"><a id="note17"/>Note</h3><p>Note that we had to escape our single quotes in these update statements. In MySQL, the escape character is either the backslash or single quote itself. These examples show the single quote as the escape character.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec75"/>Step six – clean the dates</h2></div></div></div><p>If we take a look at the <code class="literal">date_of_tweet</code> column, we see that we created it as a simple variable character field, <code class="literal">varchar(30)</code>. What is so wrong with that? Well, suppose we want to<a id="id462" class="indexterm"/> put the tweets in order from earliest to latest. Right now, we cannot use a simple SQL <code class="literal">ORDER BY</code> clause and get the proper date order, because we will get an alphabetical order instead. All Fridays will come before any Mondays, and May will always come after June. We can test this with the following SQL query:</p><div><pre class="programlisting">SELECT id, date_of_tweet
FROM sentiment140
ORDER BY date_of_tweet;</pre></div><p>The first few rows are in order but down near row 28, we start to see a problem:</p><div><pre class="programlisting">2018  Fri May 15 06:45:54 UTC 2009
2544  Mon Jun 08 00:01:27 UTC 2009
…
3  Mon May 11 03:17:40 UTC 2009</pre></div><p><code class="literal">May 11</code> does not come after <code class="literal">May 15</code> or <code class="literal">June 8</code>. To fix this, we will need to create a new column that cleans these date strings and turns them into proper MySQL datetime data types. We learned in the <em>Converting between data types</em> section in <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em>, that MySQL works best when dates and time are stored as native <strong>date</strong>, <strong>time</strong>, or <strong>datetime</strong> types. The format to insert a datetime type looks<a id="id463" class="indexterm"/> like<a id="id464" class="indexterm"/> this: <code class="literal">YYYY-MM-DD HH:MM:SS</code>. But this is not <a id="id465" class="indexterm"/>what our data looks like in the <code class="literal">date_of_tweet</code> column.</p><p>There are <a id="id466" class="indexterm"/>numerous built-in MySQL functions that can help us format our messy date string into the preferred format. By doing this, we will be able to take advantage of MySQL's ability to perform math on the dates and time, for example, finding the difference between two dates or times or sorting items properly in the order of their dates or times.</p><p>To get our string into a MySQL-friendly datetime type, we will perform the following procedure:</p><div><ol class="orderedlist arabic"><li class="listitem">Alter the table to include a new column, the purpose of which is to hold the new datetime information. We can call this new column <code class="literal">date_of_tweet_new</code> or <code class="literal">date_clean</code> or some other name that clearly differentiates it from the original <code class="literal">date_of_tweet</code> column. The SQL query to perform this task is as follows:<div><pre class="programlisting">ALTER TABLE sentiment140
ADD date_clean DATETIME NULL
AFTER date_of_tweet;</pre></div></li><li class="listitem">Perform an update on each row, during which we format the old date string into a properly formatted datetime type instead of a string and add the new value into the newly created <code class="literal">date_clean</code> column. The SQL to perform this task is as follows:<div><pre class="programlisting">UPDATE sentiment140
SET date_clean = str_to_date(date_of_tweet, '%a %b %d %H:%i:%s UTC %Y');</pre></div></li></ol><div></div><p>At this point, we have a new column that has been populated with the clean datetime. Recall that the old <code class="literal">date_of_tweet</code> column was flawed in that it was not sorting dates properly. To test whether the dates are being sorted correctly now, we can select our data in the order of the new column:</p><div><pre class="programlisting">SELECT id, date_of_tweet 
FROM sentiment140
ORDER BY date_clean;</pre></div><p>We see that the rows are now perfectly sorted, with the May 11 date coming first, and no dates are out of order.</p><p>Should we remove the old <code class="literal">date</code> column? This is up to you. If you are worried that you may have made a mistake or that you might need to have the original data for some reason, then by <a id="id467" class="indexterm"/>all means, keep it. But if you feel like removing it, simply drop the column, as shown:</p><div><pre class="programlisting">ALTER TABLE sentiment140 
DROP date_of_tweet;</pre></div><p>You could also create a copy of the Sentiment140 table that has the original columns in it as a backup.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec45"/>Step seven – separate user mentions, hashtags, and URLs</h1></div></div></div><p>Another<a id="id468" class="indexterm"/> problem with this data right now is that there are lots of interesting<a id="id469" class="indexterm"/> pieces of information hidden inside the <code class="literal">tweet_text</code> column, for example, consider all the times that a person directs a tweet to the attention of another person using the <code class="literal">@</code> symbol before their username. This is called a <strong>mention</strong> on Twitter. It might be interesting to count how many times a particular person is mentioned or how many times they are mentioned in conjunction with a particular keyword. Another <a id="id470" class="indexterm"/>interesting piece of data hidden in some of the tweets is <strong>hashtags</strong>; for example, the tweet with ID 2165 discusses the concepts of jobs and babysitting using the <code class="literal">#jobs</code> and <code class="literal">#sittercity</code> hashtags.</p><p>This same tweet also includes an external, non-Twitter <strong>URL</strong>. We can extract each of these mentions, hashtags, and URLs and save them separately in the database.</p><p>This task will be similar to how we cleaned the dates, but with one important difference. In the case of the dates, we only had one possible corrected version of the date, so it was sufficient to add a single new column to hold the new, cleaned version. With mentions, hashtags, and URLs, however, we may have zero or more in a single <code class="literal">tweet_text</code> value, for example, the tweet we looked at earlier (ID 2165) had two hashtags in it, as does this tweet (ID 2223):</p><div><pre class="programlisting">HTML 5 Demos! Lots of great stuff to come! Yes, I'm excited. :) http://htmlfive.appspot.com #io2009 #googleio</pre></div><p>This tweet has zero mentions, one URL, and two hashtags. Tweet 13078 includes three mentions but no hashtags or URLs:</p><div><pre class="programlisting">Monday already. Iran may implode. Kitchen is a disaster. @annagoss seems happy. @sebulous had a nice weekend and @goldpanda is great. whoop.</pre></div><p>We will need to change our database structure to hold these new pieces of information—hashtags, URLs, and mentions—all the while keeping in mind that a given tweet can have a lot of these in it.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec76"/>Create some new tables</h2></div></div></div><p>Following relational database theory, we should avoid creating columns that will store multivalue <a id="id471" class="indexterm"/>attributes, for example, if a tweet has three hashtags, we should not just deposit all three hashtags into a single column. The impact of this rule for us is that we cannot just copy the <code class="literal">ALTER</code> procedure we used for the date cleaning problem earlier.</p><p>Instead, we need to create three new tables: <code class="literal">sentiment140_mentions</code>, <code class="literal">sentiment140_urls</code>, and <code class="literal">sentiment140_hashtags</code>. The primary key for each new table will be a synthetic ID column, and each table will include just two other columns: <code class="literal">tweet_id</code>, which ties this new table back to the original <code class="literal">sentiment140</code> table, and the actual extracted text of the hashtag, mention, or URL. Here are three <code class="literal">CREATE</code> statements to create the tables we need:</p><div><pre class="programlisting"><strong>CREATE TABLE IF NOT EXISTS sentiment140_mentions</strong> (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  mention varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;

<strong>CREATE TABLE IF NOT EXISTS sentiment140_hashtags</strong> (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  hashtag varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;

<strong>CREATE TABLE IF NOT EXISTS sentiment140_urls</strong> (
  id int(11) NOT NULL AUTO_INCREMENT,
  tweet_id int(11) NOT NULL,
  url varchar(144) NOT NULL,
  PRIMARY KEY (id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8;</pre></div><div><h3 class="title"><a id="note18"/>Note</h3><p>These tables do not use foreign keys back to the original <code class="literal">sentiment140</code> tweet table. If you would like to add these, that is certainly possible. For the purposes of learning how to clean this dataset, however, it is not necessary.</p></div><p>Now that our tables are created, it is time to fill them with the data that we have carefully extracted from <code class="literal">tweet_text column</code>. We will work on each extraction case separately, starting with user mentions.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec77"/>Extract user mentions</h2></div></div></div><p>To design<a id="id472" class="indexterm"/> a procedure that can handle the extraction of the user mentions, let's first review what we know about mentions in tweets:</p><div><ul class="itemizedlist"><li class="listitem">The user mention always starts with the <strong>@</strong> sign</li><li class="listitem">The user mention is the word that immediately follows the <strong>@</strong> sign</li><li class="listitem">If there is a space after <strong>@</strong>, it is not a user mention</li><li class="listitem">There are no spaces inside the user mention itself</li><li class="listitem">As e-mail addresses also use <strong>@</strong>, we should be mindful of them</li></ul></div><p>Using these <a id="id473" class="indexterm"/>rules, we can construct some valid user mentions:</p><div><ul class="itemizedlist"><li class="listitem">@foo</li><li class="listitem">@foobar1</li><li class="listitem">@_1foobar_</li></ul></div><p>We can <a id="id474" class="indexterm"/>construct some examples of invalid user mentions:</p><div><ul class="itemizedlist"><li class="listitem">@ foo (the space following the @ invalidates it)</li><li class="listitem">foo@bar.com (bar.com is not recognized)</li><li class="listitem">@foo bar (only @foo will be recognized)</li><li class="listitem">@foo.bar (only @foo will be recognized)</li></ul></div><div><h3 class="title"><a id="note19"/>Note</h3><p>In this example, we assume that we do not care about the difference between a regular <code class="literal">@mention</code> and.<code class="literal">@mention</code>, sometimes called a dot-mention. These are tweets with a period in front of the <code class="literal">@</code> sign. They are designed to push a tweet to all of the user's followers.</p></div><p>As this rule set is more complicated than what we can execute efficiently in SQL, it is preferable to write a simple little script to clean these tweets using some regular expressions. We can write this type of script in any language that can connect to our database, such as Python or PHP. As we used PHP to connect to the database in <a class="link" title="Chapter 2. Fundamentals – Formats, Types, and Encodings" href="part0020.xhtml#aid-J2B82">Chapter 2</a>, <em>Fundamentals – Formats, Types, and Encodings</em>, let's use a quick PHP script here as well. This script connects to the database, searches for user mentions in the <code class="literal">tweet_text</code> column, and moves any found mentions into the new <code class="literal">sentiment140_mentions</code> table:</p><div><pre class="programlisting">&lt;?php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'sentiment140')
    or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

// pull out the tweets
$select_query = "SELECT id, tweet_text FROM sentiment140";
$select_result = mysqli_query($dbc, $select_query);

// die if the query failed
if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// pull out the mentions, if any
$mentions = array();
while($row = mysqli_fetch_array($select_result))
{
    if (preg_match_all(
        "/(?&lt;!\pL)@(\pL+)/iu",
        $row["tweet_text"],
        $mentions
    ))
    { 
        foreach ($mentions[0] as $name)
        {
            $insert_query = "INSERT into sentiment140_mentions (id, tweet_id, mention) VALUES (NULL," . $row["id"] . ",'$name')";
            echo "&lt;br /&gt;$insert_query";
            $insert_result = mysqli_query($dbc, $insert_query);
            // die if the query failed
            if (!$insert_result)
                die ("INSERT failed! [$insert_query]" .  mysqli_error());
        }
    }
}
?&gt;</pre></div><p>After <a id="id475" class="indexterm"/>running this little script on the <code class="literal">sentiment140</code> table, we see that 124 unique user mentions have been extracted out of the 498 original tweets. A few interesting things about this script include that it will handle Unicode characters in usernames, even though this dataset does not happen to have any. We can test this by quickly inserting a test row at the end of the <code class="literal">sentiment140</code> table, for example:</p><div><pre class="programlisting">INSERT INTO sentiment140 (id, tweet_text) VALUES(99999, "This is a @тест");</pre></div><p>Then, run the script again; you will see that a row has been added to the <code class="literal">sentiment140_mentions</code> table, with the <code class="literal">@тест</code> Unicode user mention successfully extracted. In<a id="id476" class="indexterm"/> the next section, we will build a similar script to extract hashtags.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec78"/>Extract hashtags</h2></div></div></div><p>Hashtags<a id="id477" class="indexterm"/> have their own rules, which are slightly different to user mentions. Here is a list of some of the rules we can use to determine whether something is a hashtag:</p><div><ul class="itemizedlist"><li class="listitem">Hashtags start with the <code class="literal">#</code> sign</li><li class="listitem">The hashtag is the word that immediately follows the <code class="literal">#</code> sign</li><li class="listitem">Hashtags can have underscores in them but no spaces and no other punctuation</li></ul></div><p>The PHP code to extract hashtags into their own table is mostly identical to the user mentions code, with the exception of the regular expression in the middle of the code. We can simply change the <code class="literal">$mentions</code> variable to <code class="literal">$hashtags</code>, and then adjust the regular expression to look like this:</p><div><pre class="programlisting">if (preg_match_all(
        "/(#\pL+)/iu",
        $row["tweet_text"],
        $hashtags
    ))</pre></div><p>This regular expression says that we are interested in matching case-insensitive Unicode letter characters. Then, we need to change our <code class="literal">INSERT</code> line to use the correct table and column names like this:</p><div><pre class="programlisting">$insert_query = "INSERT INTO sentiment140_hashtags (id, tweet_id, hashtag) VALUES (NULL," . $row["id"] . ",'$name')";</pre></div><p>When we successfully run this script, we see that 54 hashtags have been added to the <code class="literal">sentiment140_hashtags</code> table. Many more of the tweets have multiple hashtags, even more than the tweets that had multiple user mentions, for example, we can see right away that tweets 174 and 224 both have several embedded hashtags.</p><p>Next, we will use this same skeleton script and modify it again to extract URLs.</p></div><div><div><div><div><h2 class="title"><a id="ch07lvl2sec79"/>Extract URLs</h2></div></div></div><p>Pulling <a id="id478" class="indexterm"/>out the URLs from the text can be as simple as looking for any string that starts with <em>http://</em> or <em>https://</em>, or it could get a lot more complex depending on what types of URLs the text string includes, for example, some strings might include <em>file:// </em>URLs or torrent links, such as magnet URLs, or other types of unusual links. In the case of our Twitter data, we have it somewhat easier, as the URLs that were included in our dataset all start with HTTP. So, we could be lazy and just design a simple regular expression to extract any string that follows http:// or https://. This regular expression would just look like this:</p><div><pre class="programlisting">if (preg_match_all(
        "!https?://\S+!",
        $row["tweet_text"],
        $urls
    ))</pre></div><p>However, if <a id="id479" class="indexterm"/>we do a bit of hunting on our favorite search engine, it turns out that we can easily find some pretty impressive and useful generic URL matching patterns that will handle more sophisticated link patterns. The reason that this is useful is that if we write our URL extraction to handle more sophisticated cases, then it will still work if our data changes in the future.</p><p>A very<a id="id480" class="indexterm"/> well-documented URL pattern matching routine is given on the <a class="ulink" href="http://daringfireball.net/2010/07/improved_regex_for_matching_urls">http://daringfireball.net/2010/07/improved_regex_for_matching_urls</a> website. The following code shows how to modify our PHP code to use this pattern for URL extraction in the Sentiment140 dataset:</p><div><pre class="programlisting">&lt;?php
// connect to db
$dbc = mysqli_connect('localhost', 'username', 'password', 'sentiment140')
    or die('Error connecting to database!' . mysqli_error());
$dbc-&gt;set_charset("utf8");

// pull out the tweets
$select_query = "SELECT id, tweet_text FROM sentiment140";
$select_result = mysqli_query($dbc, $select_query);

// die if the query failed
if (!$select_result)
    die ("SELECT failed! [$select_query]" .  mysqli_error());

// pull out the URLS, if any
$urls = array();
$pattern  = '#\b(([\w-]+://?|www[.])[^\s()&lt;&gt;]+(?:\([\w\d]+\)|([^[:punct:]\s]|/)))#';

while($row = mysqli_fetch_array($select_result))
{
    echo "&lt;br/&gt;working on tweet id: " . $row["id"];
    if (preg_match_all(
        $pattern,
        $row["tweet_text"],
        $urls
    ))
    {
        foreach ($urls[0] as $name)
        {
            echo "&lt;br/&gt;----url: ".$name;
            $insert_query = "INSERT into sentiment140_urls (id, tweet_id, url)
                VALUES (NULL," . $row["id"] . ",'$name')";
            echo "&lt;br /&gt;$insert_query";
            $insert_result = mysqli_query($dbc, $insert_query);
            // die if the query failed
            if (!$insert_result)
                die ("INSERT failed! [$insert_query]" .mysqli_error());
        }
    }
}
?&gt;</pre></div><p>This<a id="id481" class="indexterm"/> program is nearly identical to the mention extracting program we wrote earlier, with two exceptions. First, we stored the regular expression pattern in a variable called <code class="literal">$pattern</code>, as it was long and complicated. Second, we made small changes to our database <code class="literal">INSERT</code> command, just as we did for the hasthtag extraction.</p><p>A full line-by-line explanation of the regular expression pattern is available on its original website, but the short explanation is that the pattern shown will match any URL protocol, such as http:// or file://, and it also attempts to match valid domain name patterns as well and directory/file patterns a few levels deep. The source website provides its own test dataset too if you want to see the variety of patterns that it will match and a few known patterns that will definitely <em>not</em> match.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec46"/>Step eight – cleaning for lookup tables</h1></div></div></div><p>In the <em>Step seven – Separate user mentions, hashtags, and URLs</em> section, we created new tables to<a id="id482" class="indexterm"/> hold the extracted hashtags, user mentions, and URLs, and then provided a way to link each row back to the original table via the <code class="literal">id</code> column. We followed the rules of database normalization by creating new tables that represent the one-to-many relationship between a tweet and user mentions, between a tweet and hashtags, or between a tweet and URLs. In this step, we will continue optimizing this table for performance and efficiency.</p><p>The column we are concerned with now is the <code class="literal">query_phrase</code> column. Looking at the column data, we can see that it contains the same phrases repeated over and over. These were apparently the search phrases that were originally used to locate and select the tweets that now exist in this dataset. Of the 498 tweets in the <code class="literal">sentiment140</code> table, how many of the query phrases are repeated over and over? We can use the following SQL to detect this:</p><div><pre class="programlisting">SELECT count(DISTINCT query_phrase)
FROM sentiment140;</pre></div><p>The <a id="id483" class="indexterm"/>query result shows that there are only 80 distinct query phrases, but these are used over and over in the 498 rows.</p><p>This may not seem like a problem in a table of 498 rows, but if we had an extremely large table, such as with hundreds of millions of rows, we should be concerned with two things about this column. First, duplicating these strings over and over takes up unnecessary space in the database, and second, searching for distinct string values is very slow.</p><p>To solve this<a id="id484" class="indexterm"/> problem, we will create a <strong>lookup table</strong> of query values. Each query string will exist only once in this new table, and we will also create an ID number for each one. Then, we will change the original table to use these new numeric values rather than the string values that it is using now. Our procedure to accomplish this is as follows:</p><div><ol class="orderedlist arabic"><li class="listitem">Create a new lookup table:<div><pre class="programlisting">CREATE TABLE sentiment140_queries (
  query_id int(11) NOT NULL AUTO_INCREMENT,
  query_phrase varchar(25) NOT NULL,
  PRIMARY KEY (query_id)
) ENGINE=MyISAM DEFAULT CHARSET=utf8 AUTO_INCREMENT=1;</pre></div></li><li class="listitem">Populate the lookup table with the distinct query phrases and automatically give each one a <code class="literal">query_id</code> number:<div><pre class="programlisting">INSERT INTO sentiment140_queries (query_phrase)
SELECT DISTINCT query_phrase FROM sentiment140;</pre></div></li><li class="listitem">Create a new column in the original table to hold the query phrase number:<div><pre class="programlisting">ALTER TABLE sentiment140
ADD query_id INT NOT NULL AFTER query_phrase;</pre></div></li><li class="listitem">Make a backup of the <code class="literal">sentiment140</code> table in case the next step goes wrong. Any time we perform <code class="literal">UPDATE</code> on a table, it is a good idea to make a backup. To create a copy of the <code class="literal">sentiment140</code> table, we can use a tool like phpMyAdmin to copy the table easily (use the <strong>Operations</strong> tab). Alternately, we can recreate a copy of the table and then import into it the rows from the original table, as shown in the following SQL:<div><pre class="programlisting">CREATE TABLE sentiment140_backup(
  polarity int(1) DEFAULT NULL,
  id int(5)NOT NULL,
  date_of_tweet varchar(30) CHARACTER SET utf8 DEFAULT NULL ,
  date_clean datetime DEFAULT NULL COMMENT 'holds clean, formatted date_of_tweet',
  query_id int(11) NOT  NULL,
  user varchar(25) CHARACTER SET utf8 DEFAULT NULL,
  tweet_text varchar(144) CHARACTER SET utf8 DEFAULT NULL ,
  PRIMARY KEY (id)) ENGINE=MyISAM DEFAULT CHARSET=utf8;

SET SQL_MODE='NO_AUTO_VALUE_ON_ZERO';
INSERT INTO sentiment140_backup SELECT * FROM sentiment140;</pre></div></li><li class="listitem">Populate the <a id="id485" class="indexterm"/>new column with the correct number. To do this, we join the two tables together on their text column, then look up to the correct number value from the lookup table, and insert it into the <code class="literal">sentiment140</code> table. In the following query, each table has been given an alias, <code class="literal">s</code> and <code class="literal">sq</code>:<div><pre class="programlisting">UPDATE sentiment140 s
INNER JOIN sentiment140_queries sq
ON s.query_phrase = sq.query_phrase
SET s.query_id = sq.query_id;</pre></div></li><li class="listitem">Remove the old <code class="literal">query_phrase</code> column in the <code class="literal">sentiment140</code> table:<div><pre class="programlisting">ALTER TABLE sentiment140
DROP query_phrase;</pre></div></li></ol><div></div><p>At this point, we have an effective way to create a list of phrases, as follows. These are shown in the alphabetical order:</p><div><pre class="programlisting">SELECT query_phrase
FROM sentiment140_queries
ORDER BY 1;</pre></div><p>We can also find out the tweets with a given phrase (<code class="literal">baseball</code>) by performing a join between the two tables:</p><div><pre class="programlisting">SELECT s.id, s.tweet_text, sq.query_phrase
FROM sentiment140 s
INNER JOIN sentiment140_queries sq
  ON s.query_id = sq.query_id
WHERE sq.query_phrase = 'baseball';</pre></div><p>At this point, we have a cleaned <code class="literal">sentiment140</code> table and four new tables to hold various extracted and cleaned values, including hashtags, user mentions, URLs, and query phrases. Our <code class="literal">tweet_text</code> and <code class="literal">date_clean</code> columns are clean, and we have a lookup table for query phrases.</p><div><div><div><div><h2 class="title"><a id="ch07lvl2sec80"/>Step nine – document what you did</h2></div></div></div><p>With nine steps of cleaning and multiple languages and tools in use, there is no doubt there'll be a point at which we will make a mistake and have to repeat a step. If we had to describe to someone else what we did, we will almost certainly have trouble remembering the exact steps and all the reasons why we did each thing.</p><p>To save <a id="id486" class="indexterm"/>ourselves mistakes along the way, it is essential that we keep a log of our cleaning steps. At a minimum, the log should contain these in the order in which they were performed:</p><div><ul class="itemizedlist"><li class="listitem">Every SQL statement</li><li class="listitem">Every Excel function or text editor routine, including screenshots if necessary</li><li class="listitem">Every script</li><li class="listitem">Notes and comments about why you did each thing</li></ul></div><p>Another excellent idea is to create a backup of the tables at each stage, for example, we created a backup just before we performed <code class="literal">UPDATE</code> on the <code class="literal">sentiment140</code> table, and we discussed performing backups after we created the new <code class="literal">date_clean</code> column. Backups are easy to do and you can always drop the backed-up table later if you decide you do not need it.</p></div></div>
<div><div><div><div><h1 class="title"><a id="ch07lvl1sec47"/>Summary</h1></div></div></div><p>In this chapter, we used a sample dataset, a collection of tweets called Sentiment140, to learn how to clean and manipulate data in a relational database management system. We performed a few basic cleaning procedures in Excel, and then we reviewed how to get the data out of a CSV file and into the database. At this point, the rest of the cleaning procedures were performed inside the RDBMS itself. We learned how to manipulate strings into proper dates, and then we worked on extracting three kinds of data from within the tweet text, ultimately moving these extracted values to new, clean tables. Next, we learned how to create a lookup table of values that are currently stored inefficiently, thus allowing us to update the original table with efficient, numeric lookup values. Finally, because we performed a lot of steps and because there is always the potential for mistakes or miscommunication about what we did, we reviewed some strategies to document our cleaning procedures.</p><p>In the next chapter, we will switch our perspective away from cleaning what has been given to us toward preparing cleaned data for others to use. We will learn some best practices to create datasets that require the least amount of cleaning by others.</p></div></body></html>