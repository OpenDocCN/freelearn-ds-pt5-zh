["```py\nscala> import org.apache.spark.ml.feature.RegexTokenizer\nimport org.apache.spark.ml.feature.RegexTokenizer\nscala> val date_pattern: String = \"\\\\d{1,4}[/ -]\\\\d{1,4}[/ -]\\\\d{1,4}\"\ndate_pattern: String = \\d{1,4}[/ -]\\d{1,4}[/ -]\\d{1,4}\nscala> val textDF  = spark.createDataFrame(Seq(\n    (1, \"Hello 1996-12-12 this 1-21-1111 is a 18-9-96 text \"),\n    (2, \"string with dates in different 01/02/89 formats\"))).\n    toDF(\"LineNo\",\"Text\")\ntextDF: org.apache.spark.sql.DataFrame = [LineNo: int, Text: string]\nscala> val date_regex = new RegexTokenizer().\n        setInputCol(\"Text\").setOutputCol(\"dateStr\").\n        setPattern(date_pattern).setGaps(false)\ndate_regex: org.apache.spark.ml.feature.RegexTokenizer = regexTok_acdbca6d1c4c\nscala> date_regex.transform(textDF).select(\"dateStr\").show(false)\n+--------------------------------+\n|dateStr                         |\n+--------------------------------+\n|[1996-12-12, 1-21-1111, 18-9-96]|\n|[01/02/89]                      |\n+--------------------------------+\n```", "```py\n// Example-1: Extract date like strings from text\n>>> from pyspark.ml.feature import RegexTokenizer\n>>> date_pattern = \"\\\\d{1,4}[/ -]\\\\d{1,4}[/ -]\\\\d{1,4}\"\n>>> textDF  = spark.createDataFrame([\n        [1, \"Hello 1996-12-12 this 1-21-1111 is a 18-9-96 text \"],\n        [2, \"string with dates in different 01/02/89 formats\"]]).toDF(\n        \"LineNo\",\"Text\")\n>>> date_regex = RegexTokenizer(inputCol=\"Text\",outputCol=\"dateStr\",\n            gaps=False, pattern=date_pattern)\n>>> date_regex.transform(textDF).select(\"dateStr\").show(5,False)\n+--------------------------------+\n|dateStr                         |\n+--------------------------------+\n|[1996-12-12, 1-21-1111, 18-9-96]|\n|[01/02/89]                      |\n+--------------------------------+\n```", "```py\n//Step1: Load text containing @ from source file\nscala> val path = \"<Your path>/tweets.json\"\npath: String = <Your path>/tweets.json\nscala> val raw_df = spark.read.text(path).filter($\"value\".contains(\"@\"))\nraw_df: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [value: string]\n//Step2: Split the text to words and filter out non-tag words\nscala> val df1 = raw_df.select(explode(split('value, \" \")).as(\"word\")).\n        filter($\"word\".startsWith(\"@\"))\ndf1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [word: string]\n//Step3: compute tag-wise counts and report top 5\nscala> df1.groupBy($\"word\").agg(count($\"word\")).\n        orderBy($\"count(word)\".desc).show(5)\n+------------+-----------+\n+                                                     \n|        word|count(word)|\n+------------+-----------+\n|@ApacheSpark|         15|\n|    @SSKapci|          9|\n|@databricks:|          4|\n|     @hadoop|          4|\n| @ApacheApex|          4|\n+------------+-----------+\n```", "```py\n>> from pyspark.sql.functions import explode, split\n//Step1: Load text containing @ from source file\n>>> path =\"<Your path>/tweets.json\"\n>>> raw_df1 = spark.read.text(path)\n>>> raw_df = raw_df1.where(\"value like '%@%'\")\n>>> \n//Step2: Split the text to words and filter out non-tag words\n>>> df = raw_df.select(explode(split(\"value\",\" \")))\n>>> df1 = df.where(\"col like '@%'\").toDF(\"word\")\n>>> \n//Step3: compute tag-wise counts and report top 5\n>>> df1.groupBy(\"word\").count().sort(\n     \"count\",ascending=False).show(5)\n+------------+-----+\n+                                                        \n|        word|count|\n+------------+-----+\n|@ApacheSpark|   15|\n|    @SSKapci|    9|\n|@databricks:|    4|\n| @ApacheApex|    4|\n|     @hadoop|    4|\n+------------+-----+\n\n```", "```py\nscala> import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\nimport org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel}\nscala> import org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.DataFrame\nscala> import org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.linalg.Vector\nscala> val df: DataFrame = spark.createDataFrame(Seq(\n  (0, Array(\"ant\", \"bat\", \"cat\", \"dog\", \"eel\")),\n  (1, Array(\"dog\",\"bat\", \"ant\", \"bat\", \"cat\"))\n)).toDF(\"id\", \"words\")\ndf: org.apache.spark.sql.DataFrame = [id: int, words: array<string>]\nscala>\n// Fit a CountVectorizerModel from the corpus \n// Minimum occurrences (DF) is 2 and pick 10 top words(vocabsize) only scala> val cvModel: CountVectorizerModel = new CountVectorizer().\n        setInputCol(\"words\").setOutputCol(\"features\").\n        setMinDF(2).setVocabSize(10).fit(df)\ncvModel: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_7e79157ba561\n// Check vocabulary. Words are arranged as per frequency \n// eel is dropped because it is below minDF = 2 scala> cvModel.vocabulary\nres6: Array[String] = Array(bat, dog, cat, ant)\n//Apply the model on document\nscala> val cvDF: DataFrame = cvModel.transform(df)\ncvDF: org.apache.spark.sql.DataFrame = [id: int, words: array<string> ... 1 more field]\n//Check the word count scala> cvDF.select(\"features\").collect().foreach(row =>\nprintln(row(0).asInstanceOf[Vector].toDense))\n\n[1.0,1.0,1.0,1.0]\n[2.0,1.0,1.0,1.0]\n```", "```py\n>>> from pyspark.ml.feature import CountVectorizer,CountVectorizerModel\n>>> from pyspark.ml.linalg import Vector\n>>> \n// Define source DataFrame\n>>> df = spark.createDataFrame([\n    [0, [\"ant\", \"bat\", \"cat\", \"dog\", \"eel\"]],\n    [1, [\"dog\",\"bat\", \"ant\", \"bat\", \"cat\"]]\n  ]).toDF(\"id\", \"words\")\n>>> \n// Fit a CountVectorizerModel from the corpus\n// Minimum occorrences (DF) is 2 and pick 10 top words(vocabsize) only\n>>> cvModel = CountVectorizer(inputCol=\"words\", outputCol=\"features\",\n        minDF = 2, vocabSize = 10).fit(df)\n>>> \n// Check vocabulary. Words are arranged as per frequency\n// eel is dropped because it is below minDF = 2\n>>> cvModel.vocabulary\n[u'bat', u'ant', u'cat', u'dog']\n//Apply the model on document\n>>> cvDF = cvModel.transform(df)\n//Check the word count\n>>> cvDF.show(2,False)\n+---+-------------------------+-------------------------------+\n|id |words                    |features                       |\n+---+-------------------------+-------------------------------+\n|0  |[ant, bat, cat, dog, eel]|(4,[0,1,2,3],[1.0,1.0,1.0,1.0])|\n|1  |[dog, bat, ant, bat, cat]|(4,[0,1,2,3],[2.0,1.0,1.0,1.0])|\n+---+-------------------------+-------------------------------+\n```", "```py\n |id | text                  \n +---+-------------------------+-------------------------------+\n |0  | \"ant\", \"bat\", \"cat\", \"dog\", \"eel\"     \n |1  | \"dog\",\"bat\", \"ant\", \"bat\", \"cat\"\n```", "```py\nid| text                               | Vector \n--|------------------------------------|-------------------- \n0 | \"ant\", \"bat\", \"cat\", \"dog\", \"eel\" |[1.0,1.0,1.0,1.0] \n1 | \"dog\",\"bat\", \"ant\", \"bat\", \"cat\"   |[2.0,1.0,1.0,1.0]\n\n```", "```py\n// Example 4: define CountVectorizerModel with a-priori vocabulary\nscala> val cvm: CountVectorizerModel = new CountVectorizerModel(\n        Array(\"ant\", \"bat\", \"cat\")).\n        setInputCol(\"words\").setOutputCol(\"features\")\ncvm: org.apache.spark.ml.feature.CountVectorizerModel = cntVecModel_ecbb8e1778d5\n\n//Apply on the same data. Feature order corresponds to a-priory vocabulary order scala> cvm.transform(df).select(\"features\").collect().foreach(row =>\n        println(row(0).asInstanceOf[Vector].toDense))\n[1.0,1.0,1.0]\n[1.0,2.0,1.0]\n```", "```py\nscala> import org.apache.spark.ml.feature.StopWordsRemover\nimport org.apache.spark.ml.feature.StopWordsRemover\nscala> import org.apache.spark.sql.DataFrame\nimport org.apache.spark.sql.DataFrame\nscala> import org.apache.spark.ml.linalg.Vector\nimport org.apache.spark.ml.linalg.Vector\nscala> val rawdataDF = spark.createDataFrame(Seq(\n        (0, Array(\"I\", \"ate\", \"the\", \"cake\")),\n        (1, Array(\"John \", \"had\", \"a\", \" tennis\", \"racquet\")))).\n        toDF(\"id\",\"raw_text\")\nrawdataDF: org.apache.spark.sql.DataFrame = [id: int, raw_text: array<string>]\nscala> val remover = new StopWordsRemover().setInputCol(\"raw_text\").\n                setOutputCol(\"processed_text\")\nremover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_55edbac88edb\nscala> remover.transform(rawdataDF).show(truncate=false)\n+---+---------------------------------+-------------------------+\n|id |raw_text                         |processed_text           |\n+---+---------------------------------+-------------------------+\n|0  |[I, ate, the, cake]              |[ate, cake]              |\n|1  |[John , had, a,  tennis, racquet]|[John ,  tennis, racquet]|\n+---+---------------------------------+-------------------------+\n```", "```py\n>>> from pyspark.ml.feature import StopWordsRemover\n>>> RawData = sqlContext.createDataFrame([\n    (0, [\"I\", \"ate\", \"the\", \"cake\"]),\n    (1, [\"John \", \"had\", \"a\", \" tennis\", \"racquet\"])\n    ], [\"id\", \"raw_text\"])\n>>> \n>>> remover = StopWordsRemover(inputCol=\"raw_text\",\n        outputCol=\"processed_text\")\n>>> remover.transform(RawData).show(truncate=False)\n+---+---------------------------------+-------------------------+\n|id |raw_text                         |processed_text           |\n+---+---------------------------------+-------------------------+\n|0  |[I, ate, the, cake]              |[ate, cake]              |\n|1  |[John , had, a,  tennis, racquet]|[John ,  tennis, racquet]|\n+---+---------------------------------+-------------------------+\n```", "```py\n id | raw_text \n----|---------- \n 0  | [I, ate, the, cake] \n 1  | [John, had, a, tennis, racquet] \n\n```", "```py\n\n id | raw_text                       | processed_text \n----|--------------------------------|-------------------- \n 0  | [I, ate, the, cake]            |  [ate, cake] \n 1  |[John, had, a, tennis, racquet] |[John, tennis, racquet] \n\n```", "```py\nscala> import org.apache.spark.ml.feature.Word2Vec\nimport org.apache.spark.ml.feature.Word2Vec\n\n//Step1: Load text file and split to words scala> val path = \"<Your path>/RobertFrost.txt\"\npath: String = <Your path>/RobertFrost.txt\nscala> val raw_text = spark.read.text(path).select(\n        split('value, \" \") as \"words\")\nraw_text: org.apache.spark.sql.DataFrame = [words: array<string>]\n\n//Step2: Prepare features vector of size 4 scala> val resultDF = new Word2Vec().setInputCol(\"words\").\n        setOutputCol(\"features\").setVectorSize(4).\n        setMinCount(2).fit(raw_text).transform(raw_text)\nresultDF: org.apache.spark.sql.DataFrame = [words: array<string>, features: vector]\n\n//Examine results scala> resultDF.show(5)\n+--------------------+--------------------+\n|               words|            features|\n+--------------------+--------------------+\n|[Whose, woods, th...|[-0.0209098898340...|\n|[His, house, is, ...|[-0.0013444167044...|\n|[He, will, not, s...|[-0.0058525378408...|\n|[To, watch, his, ...|[-0.0189630933296...|\n|[My, little, hors...|[-0.0084691265597...|\n+--------------------+--------------------+\n```", "```py\n>>> from pyspark.ml.feature import Word2Vec\n>>> from pyspark.sql.functions import explode, split\n>>>\n\n//Step1: Load text file and split to words >>> path = \"<Your path>/RobertFrost.txt\"\n>>> raw_text = spark.read.text(path).select(\n        split(\"value\",\" \")).toDF(\"words\")\n\n//Step2: Prepare features vector of size 4 >>> resultDF = Word2Vec(inputCol=\"words\",outputCol=\"features\",\n                 vectorSize=4, minCount=2).fit(\n                 raw_text).transform(raw_text)\n\n//Examine results scala> resultDF.show(5)\n+--------------------+--------------------+\n|               words|            features|\n+--------------------+--------------------+\n|[Whose, woods, th...|[-0.0209098898340...|\n|[His, house, is, ...|[-0.0013444167044...|\n|[He, will, not, s...|[-0.0058525378408...|\n|[To, watch, his, ...|[-0.0189630933296...|\n|[My, little, hors...|[-0.0084691265597...|\n+--------------------+--------------------+\n```", "```py\n\n input |1-gram sequence  | 2-gram sequence | 3-gram sequence \n-------|-----------------|-----------------|--------------- \n apple | a,p,p,l,e       |  ap,pp,pl,le    |  app,ppl,ple \n\n```", "```py\nscala> import org.apache.spark.ml.feature.NGram\nimport org.apache.spark.ml.feature.NGram\nscala> val wordDF = spark.createDataFrame(Seq(\n        (0, Array(\"Hi\", \"I\", \"am\", \"a\", \"Scientist\")),\n        (1, Array(\"I\", \"am\", \"just\", \"learning\", \"Spark\")),\n        (2, Array(\"Coding\", \"in\", \"Scala\", \"is\", \"easy\"))\n        )).toDF(\"label\", \"words\")\n\n//Create an ngram model with 3 words length (default is 2) scala> val ngramModel = new NGram().setInputCol(\n                \"words\").setOutputCol(\"ngrams\").setN(3)\nngramModel: org.apache.spark.ml.feature.NGram = ngram_dc50209cf693\n\n//Apply on input data frame scala> ngramModel.transform(wordDF).select(\"ngrams\").show(false)\n+--------------------------------------------------+\n|ngrams                                            |\n+--------------------------------------------------+\n|[Hi I am, I am a, am a Scientist]                 |\n|[I am just, am just learning, just learning Spark]|\n|[Coding in Scala, in Scala is, Scala is easy]     |\n+--------------------------------------------------+\n\n//Apply the model on another dataframe, Word2Vec raw_text scala>ngramModel.transform(raw_text).select(\"ngrams\").take(1).foreach(println)\n[WrappedArray(Whose woods these, woods these are, these are I, are I think, I think I, think I know.)]\n```", "```py\n>>> from pyspark.ml.feature import NGram\n>>> wordDF = spark.createDataFrame([\n         [0, [\"Hi\", \"I\", \"am\", \"a\", \"Scientist\"]],\n         [1, [\"I\", \"am\", \"just\", \"learning\", \"Spark\"]],\n         [2, [\"Coding\", \"in\", \"Scala\", \"is\", \"easy\"]]\n         ]).toDF(\"label\", \"words\")\n\n//Create an ngram model with 3 words length (default is 2) >>> ngramModel = NGram(inputCol=\"words\", outputCol= \"ngrams\",n=3)\n>>> \n\n//Apply on input data frame >>> ngramModel.transform(wordDF).select(\"ngrams\").show(4,False)\n+--------------------------------------------------+\n|ngrams                                            |\n+--------------------------------------------------+\n|[Hi I am, I am a, am a Scientist]                 |\n|[I am just, am just learning, just learning Spark]|\n|[Coding in Scala, in Scala is, Scala is easy]     |\n+--------------------------------------------------+\n\n//Apply the model on another dataframe from Word2Vec example >>> ngramModel.transform(resultDF).select(\"ngrams\").take(1)\n[Row(ngrams=[u'Whose woods these', u'woods these are', u'these are I', u'are I think', u'I think I', u'think I know.'])]\n```", "```py\n// Step 1: Define a udf to assign a category // One or more similar words are treated as one category (eg survey, poll)\n// If input list contains any of the words in a category list, it is assigned to that category\n// \"General\" is assigned if none of the categories matched\nscala> import scala.collection.mutable.WrappedArray\nimport scala.collection.mutable.WrappedArray\nscala> val findCategory = udf ((words: WrappedArray[String]) =>\n    { var idx = 0; var category : String = \"\"\n    val categories : List[Array[String]] =  List(\n     Array(\"Python\"), Array(\"Hadoop\",\"hadoop\"),\n     Array(\"survey\",\"poll\"),\n      Array(\"event\",\"training\", \"Meetup\", \"summit\",\n          \"talk\", \"talks\", \"Setting\",\"sessions\", \"workshop\"),\n     Array(\"resource\",\"Guide\",\"newsletter\", \"Blog\"))\n    while(idx < categories.length && category.isEmpty ) {\n        if (!words.intersect(categories(idx)).isEmpty) {\n         category = categories(idx)(0) }  //First word in the category list\n     idx += 1 }\n    if (category.isEmpty) {\n    category = \"General\"  }\n    category\n  })\nfindCategory: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(ArrayType(StringType,true))))\n\n//UDF to convert category to a numerical label scala> val idxCategory = udf ((category: String) =>\n        {val catgMap = Map({\"General\"->1},{\"event\"->2},{\"Hadoop\"->3},\n                             {\"Python\"->4},{\"resource\"->5})\n         catgMap(category)})\nidxCategory: org.apache.spark.sql.expressions.UserDefinedFunction =\nUserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))\nscala> val labels = Array(\"General\",\"event\",\"Hadoop\",\"Python\",\"resource\")\n //Step 2: Prepare train data \n//Step 2a: Extract \"text\" data and split to words scala> val path = \"<Your path>/tweets_train.txt\"\npath: String = <Your path>../work/tweets_train.txt\nscala> val pattern = \"\"text\":\"\npattern: String = \"text\":\nscala> val raw_text = spark.read.text(path).filter($\"value\".contains(pattern)).\n               select(split('value, \" \") as \"words\")\nraw_text: org.apache.spark.sql.DataFrame = [words: array<string>]\nscala>\n\n//Step 2b: Assign a category to each line scala> val train_cat_df = raw_text.withColumn(\"category\",\n\nfindCategory(raw_text(\"words\"))).withColumn(\"label\",idxCategory($\"category\"))\ntrain_cat_df: org.apache.spark.sql.DataFrame = [words: array<string>, category:\nstring ... 1 more field]\n\n//Step 2c: Examine categories scala> train_cat_df.groupBy($\"category\").agg(count(\"category\")).show()\n+--------+---------------+                                                     \n|category|count(category)|\n+--------+---------------+\n| General|            146|\n|resource|              1|\n|  Python|              2|\n|   event|             10|\n|  Hadoop|              6|\n+--------+---------------+ \n\n//Step 3: Build pipeline scala> import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.Pipeline\nscala> import org.apache.spark.ml.feature.{StopWordsRemover, CountVectorizer,\n                  IndexToString}\nimport org.apache.spark.ml.feature.{StopWordsRemover, CountVectorizer,\nStringIndexer, IndexToString}\nscala> import org.apache.spark.ml.classification.NaiveBayes\nimport org.apache.spark.ml.classification.NaiveBayes\nscala>\n\n//Step 3a: Define pipeline stages \n//Stop words should be removed first scala> val stopw = new StopWordsRemover().setInputCol(\"words\").\n                setOutputCol(\"processed_words\")\nstopw: org.apache.spark.ml.feature.StopWordsRemover = stopWords_2fb707daa92e\n//Terms to term frequency converter scala> val cv = new CountVectorizer().setInputCol(\"processed_words\").\n             setOutputCol(\"features\")\ncv: org.apache.spark.ml.feature.CountVectorizer = cntVec_def4911aa0bf\n//Define model scala> val model = new NaiveBayes().\n                setFeaturesCol(\"features\").\n                setLabelCol(\"label\")\nmodel: org.apache.spark.ml.classification.NaiveBayes = nb_f2b6c423f12c\n//Numerical prediction label to category converter scala> val lc = new IndexToString().setInputCol(\"prediction\").\n              setOutputCol(\"predictedCategory\").\n              setLabels(labels)\nlc: org.apache.spark.ml.feature.IndexToString = idxToStr_3d71be25382c\n //Step 3b: Build pipeline with desired stages scala> val p = new Pipeline().setStages(Array(stopw,cv,model,lc))\np: org.apache.spark.ml.Pipeline = pipeline_956942e70b3f\n //Step 4: Process train data and get predictions \n//Step 4a: Execute pipeline with train data scala> val resultsDF = p.fit(train_cat_df).transform(train_cat_df)\nresultsDF: org.apache.spark.sql.DataFrame = [words: array<string>, category:\nstring ... 7 more fields]\n\n//Step 4b: Examine results scala> resultsDF.select(\"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n|   event|            event|\n|   event|            event|\n| General|          General|\n+--------+-----------------+\n //Step 4c: Look for prediction mismatches scala> resultsDF.filter(\"category != predictedCategory\").select(\n         \"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n| General|            event|\n| General|           Hadoop|\n|resource|           Hadoop|\n+--------+-----------------+\n //Step 5: Evaluate model using test data \n//Step5a: Prepare test data scala> val path = \"<Your path> /tweets.json\"\npath: String = <Your path>/tweets.json\nscala> val raw_test_df =\nspark.read.text(path).filter($\"value\".contains(pattern)).\n               select(split('value, \" \") as \"words\"\n\nraw_test_df: org.apache.spark.sql.DataFrame = [words: array<string>]\nscala> val test_cat_df = raw_test_df.withColumn(\"category\",\n\nfindCategory(raw_test_df(\"words\")))withColumn(\"label\",idxCategory($\"category\"))\ntest_cat_df: org.apache.spark.sql.DataFrame = [words: array<string>, category:\nstring ... 1 more field]\nscala> test_cat_df.groupBy($\"category\").agg(count(\"category\")).show()\n+--------+---------------+                                                     \n|category|count(category)|\n+--------+---------------+\n| General|              6|\n|   event|             11|\n+--------+---------------+\n //Step 5b: Run predictions on test data scala> val testResultsDF = p.fit(test_cat_df).transform(test_cat_df)\ntestResultsDF: org.apache.spark.sql.DataFrame = [words: array<string>,\ncategory: string ... 7 more fields]\n//Step 5c:: Examine results\nscala> testResultsDF.select(\"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n| General|            event|\n|   event|          General|\n|   event|          General|\n+--------+-----------------+\n\n//Step 5d: Look for prediction mismatches scala> testResultsDF.filter(\"category != predictedCategory\").select(\n         \"category\",\"predictedCategory\").show()\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n|   event|          General|\n|   event|          General|\n+--------+-----------------+\n```", "```py\n// Step 1: Initialization \n//Step1a: Define a udfs to assign a category // One or more similar words are treated as one category (eg survey, poll)\n// If input list contains any of the words in a category list, it is assigned to that category\n// \"General\" is assigned if none of the categories matched\n>>> def findCategory(words):\n        idx = 0; category  = \"\"\n        categories = [[\"Python\"], [\"Hadoop\",\"hadoop\"],\n          [\"survey\",\"poll\"],[\"event\",\"training\", \"Meetup\", \"summit\",\n          \"talk\", \"talks\", \"Setting\",\"sessions\", \"workshop\"],\n          [\"resource\",\"Guide\",\"newsletter\", \"Blog\"]]\n        while(not category and idx < len(categories)):\n          if len(set(words).intersection(categories[idx])) > 0:\n             category = categories[idx][0] #First word in the category list\n          else:\n             idx+=1\n        if not category:   #No match found\n          category = \"General\"\n        return category\n>>> \n//Step 1b: Define udf to convert string category to a numerical label >>> def idxCategory(category):\n       catgDict = {\"General\" :1, \"event\" :2, \"Hadoop\" :2,\n             \"Python\": 4, \"resource\" : 5}\n       return catgDict[category]\n>>> \n//Step 1c: Register UDFs >>> from pyspark.sql.functions import udf\n>>> from pyspark.sql.types import StringType, IntegerType\n>>> findCategoryUDF = udf(findCategory, StringType())\n>>> idxCategoryUDF = udf(idxCategory, IntegerType())\n\n//Step 1d: List categories >>> categories =[\"General\",\"event\",\"Hadoop\",\"Python\",\"resource\"]\n//Step 2: Prepare train data \n//Step 2a: Extract \"text\" data and split to words >>> from pyspark.sql.functions import split\n>>> path = \"../work/tweets_train.txt\"\n>>> raw_df1 = spark.read.text(path)\n>>> raw_df = raw_df1.where(\"value like '%\"text\":%'\").select(\n             split(\"value\", \" \")).toDF(\"words\")\n\n//Step 2b: Assign a category to each line >>> train_cat_df = raw_df.withColumn(\"category\",\\\n        findCategoryUDF(\"words\")).withColumn(\n        \"label\",idxCategoryUDF(\"category\"))\n\n//Step 2c: Examine categories scala> train_cat_df.groupBy(\"category\").count().show()\n+--------+---------------+                                                     \n|category|count(category)|\n+--------+---------------+\n| General|            146|\n|resource|              1|\n|  Python|              2|\n|   event|             10|\n|  Hadoop|              6|\n+--------+---------------+\n\n//Step 3: Build pipeline >>> from pyspark.ml import Pipeline\n>>> from pyspark.ml.feature import StopWordsRemover, CountVectorizer,\nIndexToString\n>>> from pyspark.ml.classification import NaiveBayes\n>>>\n\n//Step 3a: Define pipeline stages \n//Stop words should be removed first >>> stopw = StopWordsRemover(inputCol = \"words\",\n                  outputCol = \"processed_words\")\n//Terms to term frequency converter >>> cv = CountVectorizer(inputCol = \"processed_words\",\n             outputCol = \"features\")\n//Define model >>> model = NaiveBayes(featuresCol=\"features\",\n                   labelCol = \"label\")\n//Numerical prediction label to category converter >>> lc = IndexToString(inputCol = \"prediction\",\n           outputCol = \"predictedCategory\",\n           labels = categories)\n>>> \n\n//Step 3b: Build pipeline with desired stages >>> p = Pipeline(stages = [stopw,cv,model,lc])\n>>> \n //Step 4: Process train data and get predictions \n//Step 4a: Execute pipeline with train data >>> resultsDF = p.fit(train_cat_df).transform(train_cat_df)\n\n//Step 4b: Examine results >>> resultsDF.select(\"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n|   event|            event|\n|   event|            event|\n| General|          General|\n+--------+-----------------+\n //Step 4c: Look for prediction mismatches >>> resultsDF.filter(\"category != predictedCategory\").select(\n         \"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n|  Python|           Hadoop|\n|  Python|           Hadoop|\n|  Hadoop|            event|\n+--------+-----------------+\n //Step 5: Evaluate model using test data \n//Step5a: Prepare test data >>> path = \"<Your path>/tweets.json\">>> raw_df1 = spark.read.text(path)\n>>> raw_test_df = raw_df1.where(\"va\nue like '%\"text\":%'\").select(\n               split(\"value\", \" \")).toDF(\"words\")\n>>> test_cat_df = raw_test_df.withColumn(\"category\",\n        findCategoryUDF(\"words\")).withColumn(\n        \"label\",idxCategoryUDF(\"category\"))\n>>> test_cat_df.groupBy(\"category\").count().show()\n+--------+---------------+                                                     \n|category|count(category)|\n+--------+---------------+\n| General|              6|\n|   event|             11|\n+--------+---------------+\n //Step 5b: Run predictions on test data >>> testResultsDF = p.fit(test_cat_df).transform(test_cat_df)\n//Step 5c:: Examine results >>> testResultsDF.select(\"category\",\"predictedCategory\").show(3)\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n| General|          General|\n|   event|            event|\n|   event|            event|\n+--------+-----------------+\n//Step 5d: Look for prediction mismatches >>> testResultsDF.filter(\"category != predictedCategory\").select(\n         \"category\",\"predictedCategory\").show()\n+--------+-----------------+\n|category|predictedCategory|\n+--------+-----------------+\n|   event|          General|\n|   event|          General|\n+--------+-----------------+\n```", "```py\nscala> import scala.util.Random\nimport scala.util.Random\nscala> import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nscala> import org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\n//Create a RowMatrix of 6 rows and 5 columns scala> var vlist: Array[Vector] = Array()\nvlist: Array[org.apache.spark.mllib.linalg.Vector] = Array()\nscala> for (i <- 1 to 6) vlist = vlist :+ Vectors.dense(\n       Array.fill(5)(Random.nextInt*1.0))\nscala> val rows_RDD = sc.parallelize(vlist)\nrows_RDD: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] =\nParallelCollectionRDD[0] at parallelize at <console>:29\nscala> val row_matrix = new RowMatrix(rows_RDD)\nrow_matrix: org.apache.spark.mllib.linalg.distributed.RowMatrix = org.apache.spark.mllib.linalg.distributed.RowMatrix@348a6639\n //SVD example for top 3 singular values scala> val SVD_result = row_matrix.computeSVD(3)\nSVD_result:\norg.apache.spark.mllib.linalg.SingularValueDecomposition[org.apache.spark.mlli\n.linalg.distributed.RowMatrix,org.apache.spark.mllib.linalg.Matrix] =\nSingularValueDecomposition(null,\n[4.933482776606544E9,3.290744495921952E9,2.971558550447048E9],\n-0.678871347405378    0.054158900880961904  -0.23905281217240534\n0.2278187940802       -0.6393277579229861   0.078663353163388\n0.48824560481341733   0.3139021297613471    -0.7800061948839081\n-0.4970903877201546   2.366428606359744E-4  -0.3665502780139027\n0.041829015676406664  0.6998515759330556    0.4403374382132576    )\n\nscala> SVD_result.s   //Show the singular values (strengths)\nres1: org.apache.spark.mllib.linalg.Vector =\n[4.933482776606544E9,3.290744495921952E9,2.971558550447048E9]\n\n//PCA example to compute top 2 principal components scala> val PCA_result = row_matrix.computePrincipalComponents(2)\nPCA_result: org.apache.spark.mllib.linalg.Matrix =\n-0.663822435334425    0.24038790854106118\n0.3119085619707716    -0.30195355896094916\n0.47440026368044447   0.8539858509513869\n-0.48429601343640094  0.32543904517535094\n-0.0495437635382354   -0.12583837216152594\n```"]