- en: Clustering into K Clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is a technique for dividing data into clusters, with the same features
    in the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to use the *k*-means clustering algorithm, using an example involving household
    incomes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to classify features by clustering them first with the features, along with
    the known classes, using an example of gender classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to implement the *k*-means clustering algorithm in Python in the *Implementation
    of k-means clustering algorithm* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of house ownership and how to choose an appropriate number of clusters
    for your analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use the example of house ownership to scale a given set of numerical
    data appropriately to improve the accuracy of classification by using a clustering
    algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An understanding of how different numbers of clusters alter the meaning of the
    dividing line between those clusters, using an example of document clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Household incomes – clustering into k clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For example, let's look at households whose annual earnings in USD are $40,000,
    $55,000, $70,000, $100,000, $115,000, $130,000 and $135,000\. Then, if we were
    to combine those households into two clusters, taking their earnings as a measure
    of similarity, the first cluster would have those households earning 40 k, 55
    k, and 70 k, while the second cluster would include those households earning 100
    k, 115 k, 130 k, and 135 k.
  prefs: []
  type: TYPE_NORMAL
- en: This is because 40k and 135k are furthest away from each other so, because we
    want to have two clusters, these have to be in different clusters. 55 K is closer
    to 40 k than to 135 k, so 40 k and 55 k will be in the same cluster. Similarly,
    130 k and 135 k will be in the same cluster. 70 K is closer to 40 k and 55 k than
    to 130 K and 135 k, so 70 k should be in the cluster with 40 k and 55 k. 115 K
    is closer to 130 k and 135 k than to the first cluster containing 40 k, 55 k, and
    70 k, so it will be in the second cluster. Finally, 100 k is closer to the second
    cluster, containing 115 k, 130 k, and 135 k, so it will be located there. Therefore,
    the first cluster will include 40 k, 55 k, and 70  households. The second cluster
    will include the 100 k, 115 k, 130 k, and 135 k households.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering the features of groups with similar properties and assigning a cluster
    to a feature is a form of classification. It is up to a data scientist to interpret
    the result of the clustering and what classification it induces. Here, the cluster
    containing the households with annual incomes of USD 40 k, 55 k, and 70 k represents
    a class of households with a low income. The second cluster, including households
    with an annual income of USD 100 k, 115 k, 130 k, and 135 k, represents a class
    of households with a high income.
  prefs: []
  type: TYPE_NORMAL
- en: We clustered the households into the two clusters in an informal way based on
    intuition and common sense. There are clustering algorithms that cluster data
    according to precise rules. These algorithms include a fuzzy c-means clustering
    algorithm, a hierarchical clustering algorithm, a Gaussian (EM) clustering algorithm,
    a quality threshold clustering algorithm, and a *k*-means clustering algorithm,
    which is the focus of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The *k*-means clustering algorithm classifies given points into *k* groups in
    such a way that the distance between members of the same group is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: The *k*-means clustering algorithm determines the initial *k*-centroids (points
    in a cluster center)—one for each cluster. Then, each feature is classified into
    the cluster whose centroid is closest to that feature. After classifying all the
    features, we have formed an initial *k* clusters.
  prefs: []
  type: TYPE_NORMAL
- en: For each cluster, we recomputed the centroid to be the average of the points
    in that cluster. After we have moved the centroids, we recompute the classes again.
    The features in the classes may change. In this case, we have to recompute the
    centroids again. If the centroids no longer move, then the *k*-means clustering
    algorithm terminates.
  prefs: []
  type: TYPE_NORMAL
- en: Picking the initial k-centroids
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could pick the initial *k*-centroids to be any of the *k* features in the
    data to be classified. But, ideally, we would like to pick points that belong
    to different clusters from the very outset. Therefore, we may want to aim to maximize
    their mutual distance in a certain way. Simplifying the process, we could pick
    the first centroid to be any point from the features. The second could be the
    one that is furthest from the first. The third could be the one that is furthest
    from both the first and second, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Computing a centroid of a given cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A centroid of a cluster is just an average of the points in that cluster. If
    a cluster contains one-dimensional points with the coordinates *x[1], x[2], …,
    x[n]*, then the centroid of that cluster would be  ![](img/a1e22e13-4d22-4526-a133-86bca036067f.png).
    If a cluster contains two-dimensional points with the coordinates *(x[1],y[1]),(x[2],y[2]),…,(x[n],y[n])*,
    then the *x*-coordinate of the centroid of the cluster would have the value *(1/n)*(x[1]+x[2]+...+x[n])*,
    and the *y*-coordinate would have the value ![](img/afbc1a34-7864-4496-a187-513dc5e5d8f1.png).
  prefs: []
  type: TYPE_NORMAL
- en: This computation generalizes easily to higher dimensions. If the value of the
    higher-dimensional features for the *x*-coordinate are *x[1], x[2], …, x[n]*,
    then the value of the *x*-coordinate for the centroid is ![](img/afbc1a34-7864-4496-a187-513dc5e5d8f1.png).
  prefs: []
  type: TYPE_NORMAL
- en: Using the k-means clustering algorithm on the household income example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will apply the *k*-clustering algorithm to the household income example.
    In the beginning, we have households with incomes of $40,000, $55,000, $70,000,
    $100,000, $115,000, $130,000 and $135,000.
  prefs: []
  type: TYPE_NORMAL
- en: The first centroid to be picked up can be any feature, for example, $70,000\.
    The second centroid should be the feature that is furthest from the first one,
    that is 135 k, since 135 k minus 70 k is 65 k, which is the greatest difference
    between any other feature and 70 k. Thus, 70 k is the centroid of the first cluster,
    while 135 k is the centroid of the second cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now, by taking the difference, 40 k, 55 k, 70 k, and 100 k are closer to 70
    k than to 135 k, so they will be in the first cluster, while 115 k, 130 k, and
    135 k are closer to 135 k than to 70 k, so they will be in the second cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we have classified the features according to the initial centroids, we
    recompute the centroids. The centroid of the first cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee300cca-5ab2-44aa-ab19-2c686e619d9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The centroid of the second cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/179e82f3-1388-4a6c-b804-02c8bd469ee9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the new centroids, we reclassify the features as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The first cluster containing the centroid 66.25 k will contain the features
    40 k, 55 k, and 70 k
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second cluster containing the centroid 126.66 k will contain the features
    100 k, 115 k, 130 k, and 135 k
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We notice that the 100 k feature moved from the first cluster to the second,
    since it is now closer to the centroid of the second cluster (*distance |100 k-126.66
    k|=26.66 k*) than to the centroid of the first cluster (*distance |100 k-66.25
    k|=33.75 k*). Since the features in the clusters have changed, we have to recompute
    the centroids again.
  prefs: []
  type: TYPE_NORMAL
- en: 'The centroid of the first cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5779d6d-ccc3-4ba7-ada3-509f3df61c4c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The centroid of the second cluster is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28b0ff92-f3a2-4778-82cd-d6d8e878de27.png)'
  prefs: []
  type: TYPE_IMG
- en: Using these centroids, we reclassify the features into the clusters. The first
    centroid, 55 k, will contain the features 40 k, 55 k, and 70 k. The second centroid,
    120 k, will contain the features 100 k, 115 k, 130 k, and 135 k. Thus, when the
    centroids were updated, the clusters did not change. Hence, their centroids will
    remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the algorithm terminates with the two clusters: the first cluster
    having the features 40 k, 55 k, and 70 k, and the second cluster having the features
    100 k, 115 k, 130 k, and 135 k.'
  prefs: []
  type: TYPE_NORMAL
- en: Gender classification – clustering to classify
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following data is taken from the gender classification example, *Problem
    6*, Chapter 2, *Naive Bayes*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Height in cm** | **Weight in kg** | **Hair length** | **Gender** |'
  prefs: []
  type: TYPE_TB
- en: '| 180 | 75 | Short | Male |'
  prefs: []
  type: TYPE_TB
- en: '| 174 | 71 | Short | Male |'
  prefs: []
  type: TYPE_TB
- en: '| 184 | 83 | Short | Male |'
  prefs: []
  type: TYPE_TB
- en: '| 168 | 63 | Short | Male |'
  prefs: []
  type: TYPE_TB
- en: '| 178 | 70 | Long | Male |'
  prefs: []
  type: TYPE_TB
- en: '| 170 | 59 | Long | Female |'
  prefs: []
  type: TYPE_TB
- en: '| 164 | 53 | Short | Female |'
  prefs: []
  type: TYPE_TB
- en: '| 155 | 46 | Long | Female |'
  prefs: []
  type: TYPE_TB
- en: '| 162 | 52 | Long | Female |'
  prefs: []
  type: TYPE_TB
- en: '| 166 | 55 | Long | Female |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | 60 | Long | ? |'
  prefs: []
  type: TYPE_TB
- en: 'To simplify matters, we will remove the column entitled **Hair length**. We
    will also remove the column entitled **Gender**, since we would like to cluster
    the people in the table based on their height and weight. We would like to establish
    whether the eleventh person in the table is more likely to be a man or a woman
    using clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Height in cm** | **Weight in kg** |'
  prefs: []
  type: TYPE_TB
- en: '| 180 | 75 |'
  prefs: []
  type: TYPE_TB
- en: '| 174 | 71 |'
  prefs: []
  type: TYPE_TB
- en: '| 184 | 83 |'
  prefs: []
  type: TYPE_TB
- en: '| 168 | 63 |'
  prefs: []
  type: TYPE_TB
- en: '| 178 | 70 |'
  prefs: []
  type: TYPE_TB
- en: '| 170 | 59 |'
  prefs: []
  type: TYPE_TB
- en: '| 164 | 53 |'
  prefs: []
  type: TYPE_TB
- en: '| 155 | 46 |'
  prefs: []
  type: TYPE_TB
- en: '| 162 | 52 |'
  prefs: []
  type: TYPE_TB
- en: '| 166 | 55 |'
  prefs: []
  type: TYPE_TB
- en: '| 172 | 60 |'
  prefs: []
  type: TYPE_TB
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We may apply scaling to the initial data, but to simplify matters, we will use
    the unscaled data in the algorithm. We will cluster the data we have into two
    clusters, since there are two possibilities for gender—male or female. Then, we
    will aim to classify a person with a height of 172 cm and a weight of 60 kg as
    being more likely be a man if, and only if, there are more men in that cluster.
    The clustering algorithm is a very efficient technique. Thus, classifying in this
    way is very fast, especially if there are a lot of features to classify.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's apply the *k*-means clustering algorithm to the data we have. First,
    we pick the initial centroids. Let the first centroid, for example, be a person
    with a height of 180 cm and a weight of 75 kg, denoted in a vector as *(180,75)*.
    The point that is furthest away from *(180,75)* is *(155,46)*. So that will be
    the second centroid.
  prefs: []
  type: TYPE_NORMAL
- en: 'The points that are closer to the first centroid *(180,75)* by taking Euclidean
    distance are *(180,75)*, *(174,71)*, *(184,83)*, *(168,63), (178,70)*, *(170,59)*,
    and *(172,60)*. So these points will be in the first cluster. The points that
    are closer to the second centroid *(155,46)* are *(155,46)*, *(164,53)*, *(162,52)*,
    and *(166,55)*. So these points will be in the second cluster. The current situation
    involving these two clusters is displayed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ca5c8e6-7dd7-4d76-83ac-ea0752aebe07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.1: Clustering of people according to their height and weight'
  prefs: []
  type: TYPE_NORMAL
- en: Let's recompute the centroids of the clusters. The blue cluster with the features
    (*180,75)*, *(174,71)*, *(184,83)*, *(168,63)*, *(178,70), (170,59)*, and *(172,60)*
    will have the centroid *((180+174+184+168+178+170+172)/7,(75+71+83+63+70+59+60)/7)~(175.14,68.71)*.
  prefs: []
  type: TYPE_NORMAL
- en: The red cluster with the features *(155,46)*, *(164,53)*, *(162,52)*, and *(166,55)*
    will have the centroid *((155+164+162+166)/4,(46+53+52+55)/4)=(161.75, 51.5)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reclassifying the points using the new centroid, the classes of the points
    do not change. The blue cluster will have the points *(180,75)*, *(174,71)*, *(184,83)*,
    *(168,63)*, *(178,70)*, *(170,59)*, and *(172,60)*. The red cluster will have
    the points *(155,46)*, *(164,53)*, *(162,52)*, and *(166,55)*. Therefore, the
    clustering algorithm terminates with clusters, as depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/736d91cc-3378-4953-9286-8034a22357e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5.2: Clustering of people according to their height and weight'
  prefs: []
  type: TYPE_NORMAL
- en: Now we would like to classify the instance *(172,60)* as to whether that person
    is a male or a female. The instance *(172,60)* is in the blue cluster, so it is
    similar to the features in the blue cluster. Are the remaining features in the
    blue cluster more likely male or female? Five out of six features are male, while
    only one is female. Since the majority of the features are male in the blue cluster
    and the person *(172,60)* is in the blue cluster as well, we classify a person
    with a height of 172 cm and a weight of 60 kg as male.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of the k-means clustering algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now implement the *k*-means clustering algorithm. It takes a CSV file
    as input with one data item per line. A data item is converted into a point. The
    algorithms classify these points into the specified number of clusters. In the
    end, the clusters are visualized on a graph using the `matplotlib` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Input data from gender classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We save data from the gender classification example into the CSV file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Program output for gender classification data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We run the program, implementing the *k*-means clustering algorithm on the
    data from the gender classification example. The numerical argument `2` means
    that we would like to cluster the data into two clusters, as can be seen in the
    following block of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The program also outputs a graph, visible in *Figure 5.2.* The `last` parameter
    means that we would like the program to perform clustering until the final step.
    If we would like to display only the first step (step 0), we could change the
    last to `0` in order to run, as can be seen in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Upon execution of the program, we would get a graph of the clusters and their
    centroids in the initial step, as in *Figure 5.1*.
  prefs: []
  type: TYPE_NORMAL
- en: House ownership – choosing the number of clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take the example from the first chapter regarding house ownership:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Annual income in USD** | **House ownership status** |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 50,000 | Non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 37 | 34,000 | Non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | 40,000 | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | 30,000 | Non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 95,000 | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 78,000 | Non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 13,0000 | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 10,5000 | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 10,0000 | Non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 60,000 | Owner |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 80,000 | Peter |'
  prefs: []
  type: TYPE_TB
- en: We would like to predict whether Peter is a house owner using clustering.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Just as in the first chapter, we will have to scale the data, since the *income*
    axis is significantly greater and thus would diminish the impact of the *age*
    axis, which actually has a good predictive power in this kind of problem. This
    is because it is expected that older people have had more time to settle down,
    save money, and buy a house, as compared to younger people.
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply the same rescaling from [Chapter 1](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml), *Classification
    Using K Nearest Neighbors*, and obtain the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Age** | **Scaled age** | **Annual income in USD** | **Scaled annual income**
    | **House ownership status** |'
  prefs: []
  type: TYPE_TB
- en: '| 23 | 0.09375 | 50000 | 0.2 | non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 37 | 0.53125 | 34000 | 0.04 | non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 48 | 0.875 | 40000 | 0.1 | owner |'
  prefs: []
  type: TYPE_TB
- en: '| 52 | 1 | 30000 | 0 | non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 28 | 0.25 | 95000 | 0.65 | owner |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0.15625 | 78000 | 0.48 | non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 35 | 0.46875 | 130000 | 1 | owner |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 0.375 | 105000 | 0.75 | owner |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 0 | 100000 | 0.7 | non-owner |'
  prefs: []
  type: TYPE_TB
- en: '| 40 | 0.625 | 60000 | 0.3 | owner |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 0.9375 | 80000 | 0.5 | ? |'
  prefs: []
  type: TYPE_TB
- en: Given the table, we produce the input file for the algorithm and execute it,
    clustering the features into two clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for two clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/d27bf970-4129-4f73-a650-d9109a755968.png)'
  prefs: []
  type: TYPE_IMG
- en: The blue cluster contains scaled features – *(0.09375,0.2)*, *(0.25,0.65)*,
    *(0.15625,0.48)*, *(0.46875,1)*, *(0.375,0.75)*, and *(0,0.7),* and unscaled ones
    – *(23,50000)*, *(28,95000)*, *(25,78000)*, *(35,130000)*, *(32,105000)*, and *(20,100000)*.
    The red cluster contains scaled features –*(0.53125,0.04)*, *(0.875,0.1)*, *(1,0)*,
    *(0.625,0.3)*, and *(0.9375,0.5),* and unscaled ones *(37,34000)*, *(48,40000)*,
    *(52,30000)*, *(40,60000)*, and *(50,80000)*.
  prefs: []
  type: TYPE_NORMAL
- en: So, Peter belongs to the red cluster. What is the proportion of house owners
    in the red cluster, not counting Peter? 2/4, or 1/2, of the people in the red
    cluster are house owners. Thus, the red cluster, to which Peter belongs, does
    not seem to have a high predictive power in determining whether Peter would be
    a house owner or not. We may try to cluster the data into more clusters, in the
    hope that we would gain a purer cluster that could be more reliable for the prediction
    of house ownership for Peter. Therefore, let's try to cluster the data into three
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for three clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4e4dbc70-3394-483c-9e88-fcfc3cbcd199.png)'
  prefs: []
  type: TYPE_IMG
- en: The red cluster has stayed the same. So let's cluster the data into four clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for four clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/39fd3799-c004-48c0-bfa9-e87f0befb50b.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the red cluster, where Peter belongs, has changed. What is the proportion
    of house owners in the red cluster now? If we do not count Peter, 2/3 of people
    in the red cluster own a house. When we clustered into two or three clusters,
    the proportion was only ½, which did not tell us whether Peter is a house owner
    or not. Now, there is a majority of house owners in the red cluster, not counting
    Peter, so we have a stronger belief that Peter is also a house owner. However,
    2/3 is still a relatively low confidence level for classifying Peter as a house
    owner. Let's cluster the data into five clusters to see what happens.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for five clusters:**'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/51db4dd3-3ecb-4aa4-a035-8dfeb21018fb.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, the red cluster contains only Peter and a non-owner. This clustering suggests
    that Peter is more likely a non-owner as well. However, according to the previous
    cluster, Peter is more likely be an owner of a house. Therefore, it may not be
    so clear whether Peter owns a house or not. Collecting more data would improve
    our analysis and should be carried out before making a definite classification
    of this problem.
  prefs: []
  type: TYPE_NORMAL
- en: From our analysis, we noticed that a different number of clusters can result
    in a different result for a classification, since the nature of members in an
    individual cluster can change. After collecting more data, we should perform cross-validation
    to determine the number of clusters that classifies the data with the highest
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Document clustering – understanding the number of k clusters in a semantic context
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are given the following information about the frequency counts for the words
    *money* and *god(s)* in the following 17 books from the Project Gutenberg library:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Book number** | **Book name** | **Money as a %** | **God(s) as a %** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | *The Vedanta-Sutras, with the commentary by**Ramanuja*, by Trans. George
    Thibaut | 0 | 0.07 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | *The Mahabharata of Krishna-Dwaipayana Vyasa**-Adi Parva*, by Kisari
    Mohan Ganguli | 0 | 0.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | *The Mahabharata of Krishna-Dwaipayana**Vyasa, Pt. 2*, by Krishna-Dwaipayana
    Vyasa | 0.01 | 0.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | *The Mahabharata of Krishna-Dwaipayana Vyasa Bk.**3 Pt. 1*, by Krishna-Dwaipayana
    Vyasa | 0 | 0.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | *The Mahabharata of Krishna-Dwaipayana Vyasa**Bk. 4*, by Kisari Mohan
    Ganguli | 0 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | *The Mahabharata of Krishna-Dwaipayana Vyasa**Bk. 3 Pt. 2*, translated
    by Kisari Mohan Ganguli | 0 | 0.27 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | *The Vedanta-Sutras, with commentary* bySankaracarya | 0 | 0.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | *The King James Bible* | 0.02 | 0.59 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | *Paradise Regained*, by John Milton | 0.02 | 0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | *Imitation of Christ*, by Thomas A Kempis | 0.01 | 0.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | *The Koran*, as translated by Rodwell | 0.01 | 1.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | The Adventures of Tom Sawyer, complete, byMark Twain (Samuel Clemens)
    | 0.05 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | *The Adventures of Huckleberry Finn*, complete,by Mark Twain (Samuel
    Clemens) | 0.08 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | *Great Expectations*, by Charles Dickens | 0.04 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | *The Picture of Dorian Gray*, by Oscar Wilde | 0.03 | 0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | *The Adventures of Sherlock Holmes*, by Arthur Conan Doyle | 0.04 |
    0.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | *Metamorphosi**s*, by Franz KafkaTranslated by David Wyllie | 0.06 |
    0.03 |'
  prefs: []
  type: TYPE_TB
- en: We would like to cluster this dataset, based on the chosen frequency counts
    of the words, into groups according to their semantic context.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will perform rescaling, since the highest frequency count of the
    word *money* is 0.08 percent, whereas the highest frequency count of the word
    "god(s)" is 1.72%. So, we will divide the frequency count of money by 0.08, and
    the frequency count of god(s) by 1.72, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Book number** | **Money scaled** | **God(s) scaled** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0 | 0.0406976744 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 0.0988372093 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.125 | 0.0581395349 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0 | 0.1860465116 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 0 | 0.0348837209 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 0 | 0.1569767442 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 0 | 0.0348837209 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 0.25 | 0.3430232558 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.25 | 0.261627907 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0.125 | 0.4011627907 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 0.125 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 0.625 | 0.0058139535 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 0.5 | 0.0058139535 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 0.375 | 0.0174418605 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 0.5 | 0.0174418605 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 0.75 | 0.0174418605 |'
  prefs: []
  type: TYPE_TB
- en: Now that we have rescaled the data, let's apply the *k*-means clustering algorithm
    by trying to divide the data into a different amount of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for two clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/42911ca1-e009-4853-a961-b7eda68a1a91.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that clustering into two clusters divides books into religious
    ones, which are the ones in the blue cluster, and non-religious ones, which are
    the ones in the red cluster. Let's try to cluster the books into three clusters
    to observe how the algorithm will divide up the data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for three clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/baabeb91-1761-4629-b5d2-02c942e15b85.png)'
  prefs: []
  type: TYPE_IMG
- en: This time, the algorithm separated The Koran, from the religious books, into
    a green cluster. This is because, in fact, the word *god* is the fifth most commonly
    occurring word in The Koran. The clustering here happens to divide the books according
    to the writing style they were written in. Clustering into four clusters separates
    one book, which has a relatively high frequency of the word *money*, from the
    red cluster of non-religious books into a separate cluster. Now let's look at
    clustering into five clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for five clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ba0f2f79-1c06-45b9-97fa-73b1a7e1ff2b.png)'
  prefs: []
  type: TYPE_IMG
- en: This clustering further divides the blue cluster of the remaining religious
    books into a blue cluster of Hindu books and a gray cluster of Christian books.
  prefs: []
  type: TYPE_NORMAL
- en: We can use clustering in this way to group items with similar properties and
    then find similar items quickly based on the given example. The granularity of
    the clustering under the parameter *k* determines how similar we can expect the
    items in a group to be. The higher the parameter, the more similar the items in
    the cluster are going to be, albeit in a smaller number.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how the clustering of data is very efficient and
    can be used to facilitate the faster classification of new features by classifying
    a feature as belonging to the class that is represented in the cluster of that
    feature. An appropriate number of clusters can be determined through cross-validation,
    by choosing the one that results in the most accurate classification.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering orders data according to its similarity. The more clusters there
    are, the greater the similarity between the features in a cluster, but the fewer
    features in a cluster there are.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned that the *k*-means algorithm is a clustering algorithm that
    tries to cluster features in such a way that the mutual distance of the features
    in a cluster is minimized. To do this, the algorithm computes the centroid of
    each cluster and a feature belongs to the cluster whose centroid is closest to
    it. The algorithm finishes the computation of the clusters as soon as they or
    their centroids no longer change.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will analyze the relationship between dependent variables
    using mathematical regression. Unlike with the classification and clustering algorithms,
    regression analysis will be used to estimate the most probable value of a variable,
    such as weight, distance, or temperature.
  prefs: []
  type: TYPE_NORMAL
- en: Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Problem 1**: Compute the centroid of the following clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: a) 2, 3, 4
  prefs: []
  type: TYPE_NORMAL
- en: b) USD 100, USD 400, USD 1,000
  prefs: []
  type: TYPE_NORMAL
- en: c) (10,20), (40, 60), (0, 40)
  prefs: []
  type: TYPE_NORMAL
- en: d) (USD 200, 40 km), (USD 300, 60 km), (USD 500, 100 km), (USD 250, 200 km)
  prefs: []
  type: TYPE_NORMAL
- en: e) (1,2,4), (0,0,3), (10,20,5), (4,8,2), (5,0,1)
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 2**: Cluster the following datasets into two, three, and four clusters
    using the *k*-means clustering algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: a) 0, 2, 5, 4, 8, 10, 12, 11
  prefs: []
  type: TYPE_NORMAL
- en: b) (2,2), (2,5), (10,4), (3,5), (7,3), (5,9), (2,8), (4,10), (7,4), (4,4), (5,8),
    (9,3)
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 3**: We are given the ages of the couples and the number of children
    they have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Couple number** | **Wife''s age** | **Husband''s age** | **Number of children**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 48 | 49 | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 40 | 43 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 24 | 28 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 49 | 42 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 32 | 34 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 24 | 27 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 29 | 32 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 35 | 35 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 33 | 36 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 42 | 47 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 22 | 27 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 41 | 45 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 39 | 43 | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 36 | 38 | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 30 | 32 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 36 | 38 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| 17 | 36 | 39 | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 37 | 38 | ? |'
  prefs: []
  type: TYPE_TB
- en: We would like to guess, using clustering, how many children a couple has where
    the age of the husband is 37 and the age of the wife is 38.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Problem 1**:  a) **![](img/dbe2db56-78ad-4fb8-874e-3b405828a540.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: b) ![](img/05c3cee4-8cb6-42a3-a46d-1762b351aedd.png)
  prefs: []
  type: TYPE_NORMAL
- en: c)  ![](img/e5697e9f-d754-41e6-b0a4-1a9f1fb72978.png)
  prefs: []
  type: TYPE_NORMAL
- en: d)**![](img/cc01cdbe-b8d3-4609-ace6-a4054f085f4f.png)**
  prefs: []
  type: TYPE_NORMAL
- en: e) ![](img/135753ed-7971-4a68-998f-e25309588af5.png)
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 2**: a) We add a second coordinate and set it to `0` for all the
    features. This way, the distance between the features does not change and we can
    use the clustering algorithm we implemented earlier in this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '**For two clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '**For three clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**For four clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: b) We use the implemented algorithm again.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for two clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for three clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for four clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '**Problem 3**: We are given the ages of 17 couples and the number of children
    they have, and would like to find out how many children the 18^(th) couple has.
    We will use the first 14 couples as training data, and the next 3 couples for
    cross-validation to determine the number of clusters *k* that we will use to find
    out how many children the 18^(th) couple is expected to have.'
  prefs: []
  type: TYPE_NORMAL
- en: After clustering, we will say that a couple is likely to have roughly the number
    of children that is the average in that cluster. Using cross-validation, we will
    choose the number of clusters that will minimize the difference between the actual
    and predicted number of children. We will capture this difference for all the
    items in the cluster cumulatively as the square root of the squares of the differences
    between the number of children of each couple. This will minimize the variance
    of the random variable in relation to the predicted number of children for the
    18^(th) couple.
  prefs: []
  type: TYPE_NORMAL
- en: We will perform clustering into two, three, four, and five clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Output for two clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A couple listed for a cluster is of the form `(couple_number,(wife_age,husband_age))`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We would like to determine the expected number of children for the 15^(th)
    couple *(30,32)*, that is, where the wife is 30 years old and the husband is 32
    years old. *(30,32)* is in cluster 1\. The couples in cluster 1 are as follows:
    *(24.0, 28.0)*, *(32.0, 34.0)*, *(24.0, 27.0)*, *(29.0, 32.0)*, *(35.0, 35.0)*,
    *(33.0, 36.0)*, *(22.0, 27.0)*, and *(30.0, 32.0)*. Of these, and the first 14
    couples used for data purposes, the remaining couples are: *(24.0, 28.0)*, *(32.0,
    34.0)*, *(24.0, 27.0)*, *(29.0, 32.0)*, *(35.0, 35.0)*, *(33.0, 36.0)*, and *(22.0,
    27.0)*. The average number of children for these couples is *est15=8/7~1.14*.
    This is the estimated number of children for the 15^(th) couple, based on the
    data from the first 14 couples.'
  prefs: []
  type: TYPE_NORMAL
- en: The estimated number of children for the 16^(th) couple is *est16=23/7~3.29*.
    The estimated number of children for the 17^(th) couple is also *est17=23/7~3.29 *since
    both the 16^(th) and 17^(th) couples belong to the same cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will calculate the *E2* error (two for two clusters) between the estimated
    number of children (for example, denoted *est15* for the 15^(th) couple) and the
    actual number of children (for example, denoted *act15* for the 15^(th) couple)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/002cecdc-82e4-40f7-9781-a478866c51c4.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b2ca6e56-f0de-42e9-9227-cb25a7fd8498.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have calculated the *E2* error, we will calculate the errors in
    terms of the estimation with the other clusters. We will choose the number of
    clusters containing the fewest errors to estimate the number of children for the
    18^(th) couple.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output for three clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Now, the 15^(th) couple is in cluster 1, the 16^(th) couple is in cluster 2,
    and the 17^(th) couple is in cluster 2\. So the estimated number of children for
    each couple is *5/4=1.25*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *E3* error of the estimation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72664168-009d-4b4f-a5e4-833789796640.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Output for four clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The 15^(th) couple is in cluster 3, the 16^(th) is in cluster 2, and the 17^(th) is
    in cluster 2\. So, the estimated number of children for the 15^(th) couple is
    *5/4=1.25*. The estimated number of children for the 16^(th) and 17^(th) couples
    is 8/3=2.67.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *E4* error of the estimation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f995b78a-8035-4b00-91cc-f4593b3795c8.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Output for five clusters**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The 15^(th) couple is in cluster 3, the 16^(th) is in cluster 2, and the 17^(th) is
    in cluster 2\. So, the estimated number of children for the 15^(th) couple is
    1\. The estimated number of children for the 16^(th) and 17^(th) couples is 5/3=1.67.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *E5* error of the estimation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9781e3d2-892b-419f-8121-85ece972c9e4.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Using cross-validation to determine the outcome**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We used 14 couples as training data for the estimation and three other couples
    for cross-validation to find the best parameter of *k* clusters among the values
    2, 3, 4, and 5\. We could try to cluster into more clusters, but since we have
    such a relatively small amount of data, it should be sufficient to cluster into
    five clusters at most. Let''s summarize the errors arising from the estimation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of clusters** | **Error rate** |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2.17 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 2.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2.13 |'
  prefs: []
  type: TYPE_TB
- en: The error rate is lowest for **3** and **5** clusters. The fact that the error
    rate goes up for **4** clusters and then down again for **5** clusters could indicate
    that we don't have enough data to make a good estimate. A natural expectation
    would be that there are not local maxims of errors for values of *k* greater than
    2\. Moreover, the difference between the error rate for clustering with **3**
    and **5** clusters is very small, and one cluster out of **5** is smaller than
    one cluster out of **3**. For this reason, we choose 3 clusters over 5 to estimate
    the number of children for the 18^(th) couple.
  prefs: []
  type: TYPE_NORMAL
- en: When clustering into three clusters, the 18^(th) couple is in cluster **2**.
    Therefore, the estimated number of children for the 18^(th) couple is 1.25.
  prefs: []
  type: TYPE_NORMAL
