<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 4. Deep Dive into Inferential Statistics</h1></div></div></div><p class="calibre11">Our world is a big data generating machine. These day-to-day activities consist of random and complex events that can be used to better understandUnivariate distributions: Normal, gamma, binomial the world. To achieve this, we will try to gain a deeper understanding of the processes.</p><p class="calibre11">
<strong class="calibre19">Inferential statistics</strong> is to reach to a conclusion on the basis of evidence and reasoning gained from the sample data that is generalized for the population. Inferential statistics considers that there will be some sampling errors, which means the sample that we have drawn from the population may not be perfectly representing the population.</p><p class="calibre11">Inferential statistics include:</p><div><ul class="itemizedlist"><li class="listitem">Estimation</li><li class="listitem">Hypothesis testing</li></ul></div><p class="calibre11">What is the difference between a sample and population? A population is a collection of all the events or observations about which we want want to gain knowledge. But its size can be so huge that it is not always convenient or feasible to analyze every event of this observation. In such a scenario, we take a subset that well defines the population that we want to analyze. We refer to this subset as a sample of the population.</p><p class="calibre11">In the last chapter, we discussed descriptive statistics. Although inferential and descriptive statistics are both done on the same set of data, they are quite different. We may apply descriptive statistics only on this sample data, but inferential statistics make use of this sample data with others to make generalizations that are valid for the larger population.</p><p class="calibre11">Therefore, descriptive statistics provide the summary of the data numerically or graphically. It only helps us to understand the data that we have, but we cannot use these results to form a conclusion that is generalized for the whole population.</p><p class="calibre11">With inferential statistics, we try to build a conclusion that is applicable for the whole population. But inferential statistics is limited by two main conditions:</p><div><ul class="itemizedlist"><li class="listitem">Whether the sample data that we have actually represents the population or not</li><li class="listitem">Whether the calculated assumptions that we form to make the sample data represent the population are correct or not</li></ul></div><p class="calibre11">There is always a degree of uncertainty that the sample data taken from the population may or may not represent the population perfectly. Therefore, we make some estimations or assumptions to handle this uncertainty that again can have consequences on the results that we generate.</p><p class="calibre11">In Julia, we have various packages, which are used for inferential statistics. One such package is <code class="literal">Distributions.jl</code>, which provides functions related to probabilistic distributions. <code class="literal">Distributions.jl</code> covers the following statistical methods:</p><div><ul class="itemizedlist"><li class="listitem">Properties of distribution - mean, variance, skewness, and kurtosis (moment) and entropy</li><li class="listitem">Probability density/mass functions</li><li class="listitem">Characteristic functions</li><li class="listitem">Maximum likelihood estimation</li><li class="listitem"><strong class="calibre19">Maximum-A-Posteriori (MAP)</strong> probability estimate</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec34" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Installation</h1></div></div></div><p class="calibre11">
<code class="literal">Distributions.jl</code> is a registered Julia package so it can be added using:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Pkg.add("Distributions") 
</strong>
</pre><p class="calibre11">Further sections would require the package to be installed. So, we would assume you have added the package now.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec35" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding the sampling distribution</h1></div></div></div><p class="calibre11">The sampling distribution is the likelihood of gathering every possible statistic from a sample of a population that is taken randomly. Useful information can be derived using the sampling distribution without the complete knowledge of the population. Suppose we are calculating the sample mean but we don't know the population. Still, we can assume that the sample mean is within a certain number of standard deviations of the population mean.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec36" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding the normal distribution</h1></div></div></div><p class="calibre11">The normal distribution is the core of inferential statistics. It is like a bell curve (also called a Gaussian curve). Most of the complex processes can be defined by the normal distribution.</p><p class="calibre11">Let's see what a normal distribution looks like. First, we will import the necessary packages. We are including RDatasets now, but will be using it later:</p><p class="calibre11">
</p><div><img src="img/image_04_001.jpg" alt="Understanding the normal distribution" class="calibre118"/></div><p class="calibre11">
</p><p class="calibre11">We first set the seed and then explore the normal function:</p><p class="calibre11">
</p><div><img src="img/image_04_002.jpg" alt="Understanding the normal distribution" class="calibre119"/></div><p class="calibre11">
</p><p class="calibre11">As per the warning given, we can also use <code class="literal">fieldnames</code> instead of <code class="literal">names</code>. It is recommended to use <code class="literal">fieldnames</code> only from the newer versions of Julia.</p><p class="calibre11">Here, we can see that the Normal function is in the Distributions package and has the features Univariate and Continuous. The constructor of the <code class="literal">normal()</code> function accepts two parameters:</p><div><ul class="itemizedlist"><li class="listitem">Mean (μ)</li><li class="listitem">Standard deviation (σ)</li></ul></div><p class="calibre11">Let's instantiate a normal distribution. We will keep the mean (μ) as 1.0 and the standard deviation (σ) as <code class="literal">3.0</code>:</p><p class="calibre11">
</p><div><img src="img/image_04_003.jpg" alt="Understanding the normal distribution" class="calibre120"/></div><p class="calibre11">
</p><p class="calibre11">We can check the mean and standard deviation that we have kept:</p><p class="calibre11">
</p><div><img src="img/image_04_004.jpg" alt="Understanding the normal distribution" class="calibre121"/></div><p class="calibre11">
</p><p class="calibre11">Using this normal distribution object, we can now create a distribution using a random function:</p><p class="calibre11">
</p><div><img src="img/image_04_005.jpg" alt="Understanding the normal distribution" class="calibre122"/></div><p class="calibre11">
</p><p class="calibre11">To better understand the function, let's plot a histogram using Gadfly:</p><p class="calibre11">
</p><div><img src="img/image_04_006.jpg" alt="Understanding the normal distribution" class="calibre123"/></div><p class="calibre11">
</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Parameter estimation</h2></div></div></div><p class="calibre11">This is used to find out by what kind of distribution it is best described. We can use the <code class="literal">fit</code> function for this:</p><p class="calibre11">
</p><div><img src="img/image_04_007.jpg" alt="Parameter estimation" class="calibre124"/></div><p class="calibre11">
</p><p class="calibre11">We used <code class="literal">[1.0, 3.0]</code> to create the <code class="literal">x</code> and we can see that the estimates are quite close.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec37" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Type hierarchy in Distributions.jl</h1></div></div></div><p class="calibre11">The functions provided in <code class="literal">Distributions.jl</code> follow a hierarchy. Let's go through it to understand the capabilities of the package.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding Sampleable</h2></div></div></div><p class="calibre11">Sampleable is an abstract type that includes samplers and distributions from which one can draw samples. It is defined as follows:</p><p class="calibre11">
</p><div><img src="img/image_04_008.jpg" alt="Understanding Sampleable" class="calibre125"/></div><p class="calibre11">
</p><p class="calibre11">The kinds of samples that can be drawn are defined by the two parameter types:</p><div><ul class="itemizedlist"><li class="listitem">VariateForm:<div><ul class="itemizedlist1"><li class="listitem">Univariate: Scalar number</li><li class="listitem">Multivariate: Numeric vector</li><li class="listitem">Matrixvariate: Numeric matrix</li></ul></div><p class="calibre44">
</p></li><li class="listitem">ValueSupport:<div><ul class="itemizedlist1"><li class="listitem">Discrete: Int</li><li class="listitem">Continuous: Float64</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">We can extract the information about the sample that the Sampleable object generates. An array can contain multiple samples depending on the variate form. We can use various functions to get the information (let's assume <code class="literal">sampobj</code> is the sampleable object):</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">length(sampobj)</code>: As the name suggests, it gives the length of the sample, which is 1 when the object is Univariate</li><li class="listitem"><code class="literal">size(sampobj)</code>: This returns the shape of the sample</li><li class="listitem"><code class="literal">nsamples(sampobj, X)</code>: This returns the number of samples that are in X</li><li class="listitem"><code class="literal">eltype(sampobj)</code>: This returns the default type of elements in the sample</li><li class="listitem"><code class="literal">rand(sampobj, x)</code>: This returns x number of samples taken from the sample:<div><ul class="itemizedlist1"><li class="listitem">For <code class="literal">sampobj=univariate</code>, a vector of length x is returned</li><li class="listitem">For <code class="literal">sampobj=multivariate</code>, a matrix of x columns is returned</li><li class="listitem">For <code class="literal">sampobj=matrix-variate</code>, an array of a sample matrix is returned</li></ul></div><p class="calibre44">
</p></li></ul></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec3" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Representing probabilistic distributions</h3></div></div></div><p class="calibre11">To better represent probabilistic distributions, Distribution, which is a subtype of Sampleable, is used:</p><p class="calibre11">
</p><div><img src="img/image_04_009.jpg" alt="Representing probabilistic distributions" class="calibre126"/></div><p class="calibre11">
</p><p class="calibre11">For ease of use, we generally use <code class="literal">typealias</code> for commonly used distributions :</p><pre class="programlisting">
<strong class="calibre19">julia&gt; typealias UnivariateDistribution{S&lt;:ValueSupport}   Distribution{Univariate,S} 
 
julia&gt; typealias MultivariateDistribution{S&lt;:ValueSupport} Distribution{Multivariate,S} 
 
julia&gt; typealias MatrixDistribution{S&lt;:ValueSupport}       Distribution{Matrixvariate,S} 
</strong>
</pre></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec38" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Univariate distributions</h1></div></div></div><p class="calibre11">The distributions where each sample is scalar are Univariate distributions. We can categorize them further into two distributions based on the values they support:</p><div><ul class="itemizedlist"><li class="listitem">Univariate Continuous Distribution</li><li class="listitem">Univariate Discrete Distribution</li></ul></div><p class="calibre11">Abstract types:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; typealias UnivariateDistribution{S&lt;:ValueSupport} Distribution{Univariate,S} 
 
julia&gt; typealias DiscreteUnivariateDistribution   Distribution{Univariate, Discrete} 
julia&gt; typealias ContinuousUnivariateDistribution Distribution{Univariate, Continuous} 
</strong>
</pre><p class="calibre11">Many methods are implemented for Univariate distributions in the package, which provides necessary functionalities.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Retrieving parameters</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><code class="literal">params(distributionX)</code>: This will return a tuple of parameters</li><li class="listitem"><code class="literal">succprob(distributionX)</code>: This returns the probability of success</li><li class="listitem"><code class="literal">failprob(distributionX)</code>: This returns the probability of failure</li><li class="listitem"><code class="literal">dof(distributionX):</code> This returns the degree of freedom</li><li class="listitem"><code class="literal">ncategories(distributionX)</code>: This returns the number of categories</li><li class="listitem"><code class="literal">ntrials(distributionX)</code>: This returns the number of trials</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Statistical functions</h2></div></div></div><p class="calibre11">Common statistical functions such as <code class="literal">mean()</code>, <code class="literal">median()</code>, <code class="literal">mode()</code>, <code class="literal">std()</code>, <code class="literal">var()</code>, and so on, are applicable on these distributions.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Evaluation of probability</h2></div></div></div><p class="calibre11">In addition to various statistical functions, Julia also provides functions for evaluating probability:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">pdf(distributionX)</code>: <code class="literal">pdf</code> refers to the probability density function. It returns the probability vector of <code class="literal">distributionX</code>. A range of values can also be provided as the second argument to the function in the form of <code class="literal">a:b</code>.</li><li class="listitem"><code class="literal">cdf(distributionX)</code>: <code class="literal">cdf</code> refers to the cumulative distribution function.</li><li class="listitem"><code class="literal">insupport(distributionX,x)</code>: This supports function returns if the <code class="literal">distributionX, x</code> is in support or not.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sampling in Univariate distributions</h2></div></div></div><p class="calibre11">We have previously discussed random number generation. It can also be used to draw a sample from a distribution:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; rand(distributionX)</strong>
</pre><p class="calibre11">This will draw a single sample from <code class="literal">distributionX</code>. It uses multiple dispatch and we can provide other arguments depending on our needs:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; rand(distributionX,n) 
</strong>
</pre><p class="calibre11">This will return from the <code class="literal">distributionX</code> a vector of n independent samples.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding Discrete Univariate distributions and types</h2></div></div></div><p class="calibre11">Discrete Univariate Distribution is the super type of these distributions and the sample drawn from such distributions is an integer.</p><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec4" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Bernoulli distribution</h3></div></div></div><p class="calibre11">Bernoulli distribution is a discrete distribution. It has two possible outcomes, let's say these are <em class="calibre23">n=0</em> and <em class="calibre23">n=1</em>. Here, if we take <em class="calibre23">n=1</em> as success and its probability as <em class="calibre23">p</em>, then <em class="calibre23">n=0</em> is failure and has the probability <em class="calibre23">q=1-p</em> where <em class="calibre23">0&lt;p&lt;1</em>.</p><p class="calibre11">In Julia, Bernoulli distribution is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Bernoulli(p) 
</strong>
</pre><p class="calibre11">Here, <code class="literal">p</code> is the success rate (probability).</p></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Binomial distribution</h3></div></div></div><p class="calibre11">Binomial distribution is another discrete probability distribution. It is given by <em class="calibre23">P<sub class="calibre127">p</sub>(n|N)</em>, which is obtaining n number of successes out of N Bernoulli trials. After a sequence of independent trials, the number of successes obtained is the binomial distribution:</p><p class="calibre11">
</p><div><img src="img/B05321_04_09_02.jpg" alt="Binomial distribution" class="calibre128"/></div><p class="calibre11">
</p><p class="calibre11">This is a Binomial distribution with number of trials=1 and success rate, <code class="literal">p=0.5</code>:</p><p class="calibre11">
</p><div><img src="img/B05321_04_09_03.jpg" alt="Binomial distribution" class="calibre129"/></div><p class="calibre11">
</p><p class="calibre11">Here we have specified the number of <code class="literal">trials=5</code>. The success rate remains as default:</p><p class="calibre11">
</p><div><img src="img/B05321_04_09_04.jpg" alt="Binomial distribution" class="calibre130"/></div><p class="calibre11">
</p><p class="calibre11">We can also define the success rate. So, this will return a distribution with the number of trials=5 and success rate, <code class="literal">p=0.3</code>.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Continuous distributions</h2></div></div></div><p class="calibre11">Continuous Univariate Distribution is the super type of all the continuous univariate distributions, and each sample drawn from a continuous univariate distribution is of type <code class="literal">Float64</code>.</p><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec6" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Cauchy distribution</h3></div></div></div><p class="calibre11">Cauchy distribution is also called Lorentz distribution. It is a continuous distribution that describes the resonance behavior:</p><p class="calibre11">
</p><div><img src="img/B05321_04_09_05.jpg" alt="Cauchy distribution" class="calibre131"/></div><p class="calibre11">
</p><p class="calibre11">This gives the standard Cauchy distribution (location = 0.0, scale = 1.0):</p><p class="calibre11">
</p><div><img src="img/B05321_04_09_06.jpg" alt="Cauchy distribution" class="calibre132"/></div><p class="calibre11">
</p><p class="calibre11">We can pass parameters. This one gives us the Cauchy distribution with location <code class="literal">u</code> and scale <code class="literal">s</code>.</p></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec7" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chi distribution</h3></div></div></div><p class="calibre11">Chi distribution with k degrees of freedom is the distribution formed by the square root of a chi-squared random variable, which is the sum of squares of k independent variables that are normally distributed.</p><p class="calibre11">In Julia, it is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Chi(k) 
</strong>
</pre><p class="calibre11">This will form a Chi distribution with <code class="literal">k</code> degrees of freedom.</p><p class="calibre11">It is used to yield the correction factor in the unbiased estimation of the standard deviation of the normal distribution by dividing by the mean of the chi distribution.</p></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec8" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chi-square distribution</h3></div></div></div><p class="calibre11">Chi-square distribution with <em class="calibre23">k</em> degrees of freedom is the distribution of a sum of the squares of <em class="calibre23">k</em> independent standard normal random variables.</p><p class="calibre11">In Julia, it is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Chisq(k) 
</strong>
</pre><p class="calibre11">Here, <code class="literal">k</code> is the degree of freedom.</p><p class="calibre11">The significance of Chi-square distribution commonly used in chi-squared tests is:</p><div><ul class="itemizedlist"><li class="listitem">It is used to get the goodness of fit of an observed distribution</li><li class="listitem">Of a normal distribution, it is used to get the confidence interval estimation for a population standard deviation from a sample standard deviation</li><li class="listitem">It is also used to get the independence of classification criteria of qualitative data</li></ul></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec39" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Truncated distributions</h1></div></div></div><p class="calibre11">Sometimes it is required to limit a distribution within a specific domain or range and the result from restricting a distribution is called truncated distribution. These are useful when we can only record the events in a specified range or when a threshold is given:</p><p class="calibre11">
</p><div><img src="img/image_04_015.jpg" alt="Truncated distributions" class="calibre133"/></div><p class="calibre11">
</p><p class="calibre11">This is the truncated distribution when it is restricted between two constants. In Julia, it is implemented as follows:</p><p class="calibre11">
</p><div><img src="img/image_04_016.jpg" alt="Truncated distributions" class="calibre134"/></div><p class="calibre11">
</p><div><ol class="orderedlist"><li class="listitem1">The nontruncated case: −∞ = a, b = +∞.</li><li class="listitem1">The lower truncated case: −∞ &lt; a, b = +∞.</li><li class="listitem1">The upper truncated case: −∞ = a, b &lt; +∞.</li><li class="listitem1">The doubly truncated case: −∞ &lt; a, b &lt; +∞</li></ol></div><p class="calibre11">
</p><div><img src="img/image_04_017.jpg" alt="Truncated distributions" class="calibre135"/></div><p class="calibre11">.</p><p class="calibre11">However, a few statistical functions that are available to Univariate Distributions are available to general Truncated distributions too. The reason for non-availability of those functions is that it gets complex to compute because of the truncation.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec48" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Truncated normal distributions</h2></div></div></div><p class="calibre11">This is a special type of distribution in which the truncated distribution forms a normal distribution.</p><p class="calibre11">It can be made using the dedicated constructor, <code class="literal">TruncatedNormal</code>, or by providing the Normal constructor as an argument to the Truncated constructor:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; TruncatedNormal(mu, sigma, l, u) 
</strong>
</pre><p class="calibre11">As this is the normal distribution, the statistical functions that are not available to general truncated distributions are available to truncated normal distributions.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec40" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding multivariate distributions</h1></div></div></div><p class="calibre11">A multivariate probability distribution is one containing more than one random variable. There may or may not be any correlation among these random variables. A sample drawn from this distribution is a vector. <code class="literal">Distributions.jl</code> has implementations of commonly used multivariate functions—<em class="calibre23">Multinomial</em>, Multivariate<em class="calibre23"> Normal,</em> and <em class="calibre23">Dirichlet. </em>They are implemented as follows:</p><p class="calibre11">
</p><div><img src="img/image_04_018.jpg" alt="Understanding multivariate distributions" class="calibre135"/></div><p class="calibre11">
</p><p class="calibre11">Its type aliases are given as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; typealias MultivariateDistribution{S&lt;:ValueSupport} Distribution{Multivariate,S} 
 
julia&gt; typealias DiscreteMultivariateDistribution   Distribution{Multivariate, Discrete} 
julia&gt; typealias ContinuousMultivariateDistribution Distribution{Multivariate, Continuous} 
</strong>
</pre><p class="calibre11">Most of the methods available to Univariate distributions are also available to Multivariate distributions.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec49" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Multinomial distribution</h2></div></div></div><p class="calibre11">This generalizes the binomial distribution. Suppose that over a finite set of size k of a categorical distribution, we take <em class="calibre23">n</em> independent draws.</p><p class="calibre11">Let's represent this as : <em class="calibre23">X = X<sub class="calibre127">1</sub>, X<sub class="calibre127">2</sub>, ............ X<sub class="calibre127">k</sub></em>.</p><p class="calibre11">Then this <em class="calibre23">X</em> represents a multinomial distribution whose every sample is a k-dimensional integer vector that sums to <em class="calibre23">n</em>.</p><p class="calibre11">In Julia, it is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Multinomial(n, p) 
</strong>
</pre><p class="calibre11">Here, p represents the probability vector and we are creating the distribution with n trials.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec50" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Multivariate normal distribution</h2></div></div></div><p class="calibre11">This is a multidimensional generalization of the normal distribution:</p><p class="calibre11">
</p><div><img src="img/B05321_04_14.jpg" alt="Multivariate normal distribution" class="calibre136"/></div><p class="calibre11">
</p><p class="calibre11">Reasons why multivariate normal distribution is important:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Mathematical simplicity</strong>: It is easier to work with this distribution</li><li class="listitem"><strong class="calibre19">Multivariate version of the Central Limit Theorem</strong>: If we have a collection of random vectors <em class="calibre23">X<sub class="calibre127">1</sub>, X<sub class="calibre127">2</sub>,…,X<sub class="calibre127">n</sub></em> that are independent and identically distributed, then the sample mean vector, x¯x¯, is going to be approximately multivariate normally distributed for large samples</li><li class="listitem">It is used in the modeling of many natural phenomena</li></ul></div><p class="calibre11">
</p><div><img src="img/image_04_020.jpg" alt="Multivariate normal distribution" class="calibre137"/></div><p class="calibre11">
</p><p class="calibre11">There are three types of covariances matrices that are implemented:</p><div><ul class="itemizedlist"><li class="listitem">Full covariance.</li><li class="listitem">Diagonal covariance.</li><li class="listitem">Isotropic covariance.</li></ul></div><pre class="programlisting">
<strong class="calibre19">julia&gt; typealias FullNormal MvNormal{PDMat,    Vector{Float64}} 
julia&gt; typealias DiagNormal MvNormal{PDiagMat, Vector{Float64}} 
julia&gt; typealias IsoNormal  MvNormal{ScalMat,  Vector{Float64}} 
 
julia&gt; typealias ZeroMeanFullNormal MvNormal{PDMat,    ZeroVector{Float64}} 
julia&gt; typealias ZeroMeanDiagNormal MvNormal{PDiagMat, ZeroVector{Float64}} 
julia&gt; typealias ZeroMeanIsoNormal  MvNormal{ScalMat,  ZeroVector{Float64}} 
</strong>
</pre><p class="calibre11">The mean vector is either of an instance of <code class="literal">Vector{Float64}</code> or <code class="literal">ZeroVector{Float64}.ZeroVector{Float64}</code> is a vector filled with zeros.</p><p class="calibre11">A multivariate normal distribution is constructed in the following ways:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">MvNormal(mu, sig)</code>: <code class="literal">mu</code> refers to mean and sig refers to covariance</li><li class="listitem"><code class="literal">MvNormal(sig)</code>: We are not passing the mean, therefore the mean will be zero</li><li class="listitem"><code class="literal">MvNormal(d,sig)</code>: <code class="literal">d</code> refers to the dimension here</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec51" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Dirichlet distribution</h2></div></div></div><p class="calibre11">Dirichlet distribution represents the conjugate prior to the multinomial distribution. This refers to the condition that the posterior distribution is also a Dirichlet distribution if the prior distribution of the multinomial parameters is Dirichlet:</p><p class="calibre11">
</p><div><img src="img/B05321_04_16.jpg" alt="Dirichlet distribution" class="calibre138"/></div><p class="calibre11">
</p><p class="calibre11">This tells us that the Dirichlet is part of the Multivariate family and is a Continuous distribution:</p><p class="calibre11">
</p><div><img src="img/B05321_04_17.jpg" alt="Dirichlet distribution" class="calibre139"/></div><p class="calibre11">
</p><p class="calibre11">There are parameters accepted by the Dirichlet method. This is used as:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Dirichlet(alpha) 
</strong>
</pre><p class="calibre11">Here alpha is a vector:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Dirichlet(k, a) 
</strong>
</pre><p class="calibre11">In this, <code class="literal">a</code> is a positive scalar.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec41" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding matrixvariate distributions</h1></div></div></div><p class="calibre11">This is a distribution from which any sample drawn is of type matrix. Many of the methods that can be used with Univariate and Multivariate distributions can be used with Matrix-variate distributions.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec52" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Wishart distribution</h2></div></div></div><p class="calibre11">This is a type of matrix-variate distribution and is a generalization of the Chi-square distribution to two or more variables. It is constructed by adding the inner products of identically distributed, independent, and zero-mean multivariate normal random vectors. It is used as a model for the distribution of the sample covariance matrix for multivariate normal random data, after scaling by the sample size:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Wishart(v, S) 
</strong>
</pre><p class="calibre11">Here, <code class="literal">v</code> refers to the degrees of freedom and <code class="literal">S</code> is the base matrix.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec53" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Inverse-Wishart distribution</h2></div></div></div><p class="calibre11">This is the conjugate prior to the covariance matrix of a multivariate normal distribution. In Julia, it is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; InverseWishart(v, P) 
</strong>
</pre><p class="calibre11">This represents an Inverse-Wishart distribution with <code class="literal">v</code> degrees of freedom and base matrix <code class="literal">P</code>.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec42" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Distribution fitting</h1></div></div></div><p class="calibre11">Distribution fitting is the fitting of a probability distribution to a series of data to predict the probability of variable phenomena in a certain interval. We can get good predictions from the distribution, which is a close fit to the data. Depending on the characteristics of the distribution and of the phenomenon, some can be fitted more closely with the data:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; d = fit(Distribution_type, dataset) 
</strong>
</pre><p class="calibre11">This fits a distribution of type <code class="literal">Distribution_type</code> to a given dataset; <code class="literal">dataset.x</code> is of the array type and comprises all the samples. The fit function finds the best way to fit the distribution.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec54" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Distribution selection</h2></div></div></div><p class="calibre11">The distribution is selected by the symmetry or the skewness of the data with respect to the mean value.</p><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec9" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Symmetrical distributions</h3></div></div></div><p class="calibre11">For symmetrical distributions tending to have the bell curve, the normal distribution and the logistic distributions are most suited. When the kurtosis is higher, the values are spread far away from the center, and then one can also use Student's t-distribution.</p></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec10" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Skew distributions to the right</h3></div></div></div><p class="calibre11">Also called positive skewness, this is when the distance of the larger values from the mean is greater than the distance of the smaller values from the mean. In these scenarios, log-normal distribution, and log-logistic distribution are most suited. Also, the exponential distribution, the Weibull distribution, the Pareto distribution, and the Gumbel distribution can be suited in some of these scenarios.</p></div><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec11" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Skew distributions to the left</h3></div></div></div><p class="calibre11">The negative skewness or skewness to the left is when the distance of the smaller values from the mean is greater than the distance of the larger values from the mean. For such data, square-normal distribution, Gompertz distribution, and the inverted or mirrored Gumbel distributions are suited.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Maximum Likelihood Estimation</h2></div></div></div><p class="calibre11">
<strong class="calibre19">Maximum Likelihood Estimation</strong> (<strong class="calibre19">MLE</strong>) is a procedure of estimating the parameters for a given statistic, which makes the given distribution a maximum. It is an analytic maximization procedure.</p><p class="calibre11">For example, we have a sample from a population but for some reason we couldn't measure the whole population. We want to know some statistics of this population; this could be done by Maximum Likelihood Estimation assuming that the data is normally distributed. MLE would give the parametric values that would have the highest probability according to the data that we have and the given model.</p><p class="calibre11">Sample properties of MLEs:</p><div><ul class="itemizedlist"><li class="listitem">Unbiased minimum variance estimators (large sample size)</li><li class="listitem">Confidence bounds can be generated by calculating approximate normal distributions and approximate sample variances</li><li class="listitem">Can be used to test hypotheses about models and parameters</li></ul></div><p class="calibre11">Drawbacks to MLEs:</p><div><ul class="itemizedlist"><li class="listitem">MLEs can be heavy with a small number of failures (large sample size is not able to overcome this)</li><li class="listitem">Calculating MLEs requires solving complex non-linear equations</li></ul></div><p class="calibre11">MLE maximizes the likelihood function by selecting the set of values of the model parameters for the defined statistical model and given dataset. With large sample sizes (tending to infinity), MLEs have the following features:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Efficiency</strong>: The asymptotic mean squared error of the MLE is the lowest among all the consistent estimators</li><li class="listitem"><strong class="calibre19">Asymptotic normality</strong>: The MLE distribution tends to the Gaussian distribution with increase in sample size</li><li class="listitem"><strong class="calibre19">Consistency</strong>: The probability of the sequences converges to the estimated value</li><li class="listitem">After the correction for bias, it has second-order efficiency</li></ul></div><p class="calibre11">In Julia, we have a function for maximum likelihood estimation, <code class="literal">fit_mle</code>. This uses multiple dispatches:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; fit_mle(Distribution, dataset) 
</strong>
</pre><div><ul class="itemizedlist"><li class="listitem"><code class="literal">dataset</code> can be an array for univariate distribution.</li><li class="listitem"><code class="literal">dataset</code> is a matrix for multivariate distribution:</li></ul></div><pre class="programlisting">
<strong class="calibre19">        julia&gt; fit_mle(Distribution, weights, dataset)</strong>
</pre><div><ul class="itemizedlist"><li class="listitem">This includes an additional parameter, weights, which is an array of length <code class="literal">n</code>. <code class="literal">n</code> is equal to the number of samples contained in the dataset.</li></ul></div><p class="calibre11">At the time of writing, <code class="literal">fit_mle</code> has been implemented for the following most used distributions:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Univariate distributions</strong>: Normal, gamma, binomial, Bernoulli, categorical, uniform, laplace, exponential, geometric, and so on</li><li class="listitem"><strong class="calibre19">Multivariate distributions</strong>: Multinomial, multivariate normal, and Dirichlet</li></ul></div><p class="calibre11">As mentioned, the <code class="literal">fit_mle</code> uses multiple dispatches. The implementation for some of the distributions is quite different to the others.</p><p class="calibre11">As for Binomial distribution:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">fit_mle(BinomialDistribution, numOfTrials, dataset, weights)</code>: The number of trials is an additional parameter that represents it for each experiment. <code class="literal">weights</code> is an optional argument.</li></ul></div><p class="calibre11">As for categorical distribution:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">fit_mle(CategoricalDistribution, spaceSize, dataset, weights)</code>: <code class="literal">spaceSize</code> is an additional parameter that represents the number of distinct values. <code class="literal">weights</code> is an optional argument.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sufficient statistics</h2></div></div></div><p class="calibre11">Julia provides a function that can be used to generate the estimation and then apply the Maximum Likelihood Estimation (<code class="literal">fit_mle</code>).</p><p class="calibre11">Usage:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; gensuffstats = suffstats(Distribution, dataset, weights) 
</strong>
</pre><p class="calibre11">Here, <code class="literal">weights</code> is an optional parameter. This generates the sufficient statistics of the dataset, and now we can apply the <code class="literal">fit_mle</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt;  fit_mle(Distribution, gensuffstats) 
</strong>
</pre><p class="calibre11">The reason for using the sufficient statistics function is because it is more efficient.</p></div><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Maximum-a-Posteriori estimation</h2></div></div></div><p class="calibre11">This is also known as energy minimization. The parameter to be estimated, although it is unknown, is considered fixed unlike MLE, which is considered a random variable.</p><p class="calibre11">In Bayesian analysis, for the parameters that we want to estimate of the physical process, we may have priori information, which may have come from empirical evidence or other scientific knowledge. Such information can be encoded in the <strong class="calibre19">probability distribution function</strong> (<strong class="calibre19">pdf</strong>) on the parameter to be estimated:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; posterior(priori, suffst) 
</strong>
</pre><p class="calibre11">This returns the posterior distribution, which is based on the data provided by the sufficient statistics and is of the same type as a priori (the priori distribution).</p><p class="calibre11">You can generate the Maximum-a-Posteriori estimation as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; fit_map(priori, G, dataset[, weights]) 
</strong>
</pre><p class="calibre11">Here <code class="literal">G</code> is the likelihood model (or a distribution).</p><p class="calibre11">You can generate a completed distribution as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; complete(priori, G, params) 
</strong>
</pre><p class="calibre11">This will give a completed distribution given parameter, param, and likelihood model, G.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec43" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Confidence interval</h1></div></div></div><p class="calibre11">This describes the amount of uncertainty associated with the unknown population parameter in the estimated range of values of the population.</p><p class="calibre11">
</p><div><img src="img/image_04_023.jpg" alt="Confidence interval" class="calibre140"/></div><p class="calibre11">
</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Interpreting the confidence intervals</h2></div></div></div><p class="calibre11">Suppose it is given that the population mean is greater than 100 and less than 300, with a confidence interval of 95%.</p><p class="calibre11">General perception is that the chance of the population mean falling between 100 and 300 is 95%. This is wrong, as the population mean is not a random variable but is constant and doesn't change, and its probability of falling in any specified range is 0 to 1.</p><p class="calibre11">The uncertainty level associated with a sampling method is described by the confidence level. Suppose to select different samples and for each of these samples to compute a different interval estimate we used the same sampling method. The true population parameter would be included in some of these interval estimates, but not in every one.</p><p class="calibre11">So, the 95% confidence level means that the population parameter is included in 95% of the interval estimates.</p><p class="calibre11">Here are the steps to construct a confidence interval:</p><div><ul class="itemizedlist"><li class="listitem">A sample statistic is identified</li><li class="listitem">Select a confidence level</li><li class="listitem">Calculate the margin of error:<p class="calibre44">
<em class="calibre23">Margin of error = standard deviation (error) of statistic* critical value</em>
</p></li><li class="listitem">Describe the confidence level:<p class="calibre44">
<em class="calibre23">Confidence interval = margin of error + sample statistic</em>
</p></li></ul></div><p class="calibre11">In Julia, a confidence interval is calculated using the <code class="literal">ci</code> function. There are 12 methods for the generic function <code class="literal">ci</code>:</p><pre class="programlisting">ci(x::HypothesisTests.Btest) 
ci(x::HypothesisTests.BTest, alpha::Float64)
 
ci(x::HypothesisTests.BinomialTest) 
ci(x::HypothesisTests.BinomialTest, alpha::Float64)
 
ci(x::HypothesisTests.SignTest) 
ci(x::HypothesisTests.SignTest, alpha::Float64)
 
ci(x::HypothesisTests.FisherExactTest) 
ci(x::HypothesisTests.FisherExactTest, alpha::Float64)
ci(x::HypothesisTests.TTest) 
ci(x::HypothesisTests.TTest, alpha::Float64)
 
ci(x::HypothesisTests.PowerDivergenceTest) 
ci(x::HypothesisTests.PowerDivergenceTest, alpha::Float64) 
</pre><div><div><div><div><h3 class="title5"><a id="ch04lvl3sec12" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Usage</h3></div></div></div><p class="calibre11">To get the confidence interval of the Binomial proportions, it is used as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; ci(test::BinomialTest,alpha=0.05; tail=:both,method=:clopper_pearson) 
</strong>
</pre><p class="calibre11">This will compute the confidence interval that will have a coverage of 1-alpha. The method used is clopper pearson.</p>Other methods can also be used:
<div><ul class="itemizedlist"><li class="listitem">Wald interval (<code class="literal">:wald</code>)</li><li class="listitem">Wilson score interval (<code class="literal">:wilson</code>)</li><li class="listitem">Jeffreys interval (<code class="literal">:jeffrey</code>)</li><li class="listitem">Agresti Coull interval (<code class="literal">:agresti_coull</code>)</li></ul></div><p class="calibre11">To get the confidence interval of the multinomial proportions, it is used as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; ci(test::PowerDivergenceTest, alpha=0.05; tail=:both, method=:sison_glaz) 
</strong>
</pre><p class="calibre11">Other methods than <code class="literal">sison_glaz</code>:</p><div><ul class="itemizedlist"><li class="listitem">Bootstrap intervals (<code class="literal">:bootstrap</code>)</li><li class="listitem">Quesenberry, Hurst intervals (<code class="literal">:quesenberry_hurst</code>)</li><li class="listitem">Gold intervals (<code class="literal">:gold</code>)</li></ul></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec44" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding z-score</h1></div></div></div><p class="calibre11">Z-score refers to the standard deviations the element is away from the mean.</p><p class="calibre11">It is given by the following formula:</p><p class="calibre11">
</p><div><img src="img/image_04_024-1.jpg" alt="Understanding z-score" class="calibre141"/></div><p class="calibre11">
</p><p class="calibre11">Here <code class="literal">X</code> represents the value of the element, σ is the standard deviation, and μ is the population mean.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Interpreting z-scores</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><code class="literal">z-score&lt;0</code>: The element is less than the mean</li><li class="listitem"><code class="literal">z-score&gt;0</code>: The element is greater than the mean</li><li class="listitem"><code class="literal">z-score=0</code>: The element is equal to the mean</li><li class="listitem"><code class="literal">z-score=0.5</code>: The element is 0.5 SD greater than the mean</li></ul></div><p class="calibre11">In Julia, it is implemented as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; zscore(X,  μ, σ) 
</strong>
</pre><p class="calibre11">μ and σ are optional as they can be calculated by the function.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec45" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding the significance of the P-value</h1></div></div></div><p class="calibre11">The probability that a null-hypothesis will be rejected even if it is proven true is the p-value. When there is no difference between two measures, then the hypothesis is said to be a null-hypothesis.</p><p class="calibre11">For example, if there is a hypothesis that, in the game of football, every player who plays 90 minutes will also score a goal then the null hypothesis would be that there is no relation between the number of minutes played and the goals scored.</p><p class="calibre11">Another example would be a hypothesis that a person with blood group A will have higher blood pressure than the person with blood group B. In a null hypothesis, there will be no difference, that is, no relation between the blood type and the pressure.</p><p class="calibre11">The significance level is given by (α) and if the p-value is equal or less than it, then the null hypothesis is declared inconsistent or invalid. Such a hypothesis is rejected.</p><div><div><div><div><h2 class="title3"><a id="ch04lvl2sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>One-tailed and two-tailed test</h2></div></div></div><p class="calibre11">The following diagram represents the two-tails being used for a hypothesis test.</p><p class="calibre11">
</p><div><img src="img/5321_4_19.jpg" alt="One-tailed and two-tailed test" class="calibre142"/></div><p class="calibre11">
</p><p class="calibre11">In Julia, it is calculated as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; pvalue(test::HypothesisTest; tail=:both) 
</strong>
</pre><p class="calibre11">This will return the p-value for the two-tailed test. To get the p-value of the one-tailed test use <code class="literal">tail=:left</code> or <code class="literal">tail=:right</code>.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec46" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we dived deep into inferential statistics and learned about various concepts and methods in Julia to work with different kinds of datasets. We started with understanding the normal distribution, which is a must when dealing with statistics. In parallel, we started exploring Distributions.jl and various methods provided by Julia. We then moved on to Univariate distributions and understanding why they are so important. We also explored some other distributions, such as Chi, Chi-square, and Cauchy. Later in the chapter, we studied what z-score, p-value, one-tailed, and two-tailed tests are about. After studying the chapter, we should be able to understand the datasets and apply inferential statistics to gain insights as well as using the z-score and p-value to accept or reject our hypothesis.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch04lvl1sec47" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://docs.julialang.org/en/release-0.4/manual/">http://docs.julialang.org/en/release-0.4/manual/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/JuliaStats/Distributions.jl">https://github.com/JuliaStats/Distributions.jl</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf">https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://onlinecourses.science.psu.edu/">https://onlinecourses.science.psu.edu/</a></li></ul></div></div></div>



  </body></html>