<html><head></head><body>
        

                            
                    <h1 class="header-title">SQL, Math, and Wrapping it up</h1>
                
            
            
                
<p>Databases are attractive solutions for storing and accessing data. They supply the developer with an API that allows the structured organization of data, the ability to search that data in flexible ways, and the ability to store new data. When a database's capabilities are a requirement, there's often little room left for negotiation; the question is which database and not whether we should use one.</p>
<p>Despite this fact, the Unix command line provides a suite of tools that lets a developer view streams or files in many of the same ways as they would view a database. Given one or more files with data in it, we can use these tools to query that data without ever having to maintain a database or any of the things that go along with it, such as fixed schemas. Often, we can use this method for processing data instead of standing up a database server and dealing with the issues associated with the <strong>Extract</strong>, <strong>Transformation</strong>, and <strong>Load</strong> (<strong>ETL</strong>) of data into that database. Even better, our pipeline, and therefore our view of the data, can change over time, unlike the relatively static schemas of traditional databases.</p>
<p>Often, you'll need to perform computations on numerical data in your workflows. The command line has several tools that enable us to do this.</p>
<p>Bash itself has the capability to do some math in shell scripts. When a little more capability is required, two command-line tools, <kbd>bc</kbd> and <kbd>awk</kbd>, are capable of doing many types of calculations.</p>
<p>Sometimes, we may need the full power of a programming language and mathematics packages, such as Python and Pandas. While this isn't a tutorial on how to do data science in Python, in this chapter, we'll see how to interface your Python routines in line with other command-line tools and build a custom pipeline for your needs.</p>
<p>We'll also be using many of the tools that we have seen in this book to perform some real-world analysis on weather data.</p>
<p>So, to sum it up, in this chapter we will be looking at:</p>
<ul>
<li>Viewing data as columns using <kbd>cut</kbd></li>
<li>Using <kbd>grep</kbd> as a <kbd>WHERE</kbd> clause</li>
<li>Joining different sets of data using the <kbd>join</kbd> command</li>
<li>Simulating <kbd>SELECT</kbd> clauses using <kbd>awk</kbd></li>
<li>Learning how to use SQLite when a more fully-featured database is needed</li>
<li>Bash variable assignment</li>
<li>Basic bash arithmetic and comparisons</li>
<li>Math using <kbd>bc</kbd></li>
<li>Streaming calculations with <kbd>awk</kbd></li>
<li>Interfacing with python routines</li>
<li>Looking at the contents of a publicly available weather API</li>
<li>Scraping the API and storing the results in lightweight databases</li>
<li>Using the tools discussed in the previous chapters to analyze the data in the databases we've created</li>
<li>Drawing some conclusions about how accurate the weather forecast is</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">cut and viewing data as columnar</h1>
                
            
            
                
<p>The first thing you will likely need to do is partition data in files into rows of data and columns of data. We saw some transformations in the previous chapters that allow us to manipulate data one row at a time. For this chapter, we'll assume the rows of your data correspond with the lines of data in your files. If this isn't the case, this may be the first thing you want to do in your pipeline.</p>
<p>Given that we have some rows of data in our file or stream, we would like to view those rows in a columnar fashion, such as a traditional database. We can do this using the help of the <kbd>cut</kbd> command. <kbd>cut</kbd> will allow us to chop the lines of the file into columns by a delimiter, and to select which of those columns get passed through to the output.</p>
<p>If your data is a comma-separated or tab-separated file, <kbd>cut</kbd> is quite simple:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | head</strong><em> <br/></em></pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-800 image-border" src="img/7950562c-771d-46d9-b458-5cdb9ebd0a68.png" style="width:128.50em;height:28.25em;"/></p>
<p>In this command, we're telling <kbd>cut</kbd> that the delimiter is using <kbd>-d$'\t'</kbd>. Also, we use the <kbd>-f2,8</kbd> option to tell <kbd>cut</kbd> which of the columns we would like to pass from the input to the output. Note that we captured the header row of the data, which probably isn't desired. To skip it, add <kbd>tail -n +2</kbd> to the pipe:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | tail -n +2 | head</strong></pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-801 image-border" src="img/29c188b8-b3b5-44c1-9e8e-b1ff9d0b9a2a.png" style="width:144.33em;height:28.17em;"/></p>
<p>If your line is more complicated than a CSV or TSV, you may have to do more than one pass using cut, or possibly an intervening step using <kbd>awk</kbd> or <kbd>sed</kbd>. For example, in the book-review dataset, say we want to output the date field, but in year-month-date order. We can first select down to the date field, re-cut the date field into its constituent parts, and output them in the desired order:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f15 | cut -d$'-' -f2,3,1 | head</strong></pre>
<p class="mce-root"/>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-802 image-border" src="img/fba937fa-eb7d-43a1-a016-dfde2e6b7686.png" style="width:143.00em;height:27.50em;"/></p>
<p><kbd>cut</kbd> can also cut particular bytes or characters from a stream if you have fixed-width fields:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -c1-12 | head</strong><em><br/></em></pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-803 image-border" src="img/c4538db4-1f6d-4769-8a84-7b80e30107dc.png" style="width:114.67em;height:26.83em;"/></p>
<p>In the case of the book data, this isn't going to make much sense since the fields are variable-width, but sometimes it's just what you need.</p>
<p>Using <kbd>cut</kbd> in this fashion will be your tool for a SQL-like <kbd>SELECT</kbd> of particular characters in each row of your data.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">WHERE clauses</h1>
                
            
            
                
<p>The powerful grep regular-expression-matching tool we discussed in a previous chapter allows us to perform <kbd>WHERE</kbd> clauses on our files. The clause may be a bit less intuitive than a SQL <kbd>WHERE</kbd> clause, but we can do as much or more with grep as we can with the SQL <kbd>WHERE</kbd> clause. For example, perhaps we only care about accounts starting with the number <kbd>3</kbd>:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | tail -n +2 | grep "^3" | head</strong> <em><br/></em></pre>
<p>The following will be displayed on your screen:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-804 image-border" src="img/b17384d7-b377-4f51-8776-1776fc96bf5a.png" style="width:152.17em;height:27.58em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Join, for joining data</h1>
                
            
            
                
<p>Join works how an <kbd>INNER JOIN</kbd> might work in your SQL-style database. Two sorted files or streams are passed to the <kbd>join</kbd> command (see the section on sort to see how to <kbd>sort</kbd> your streams). The lines of the files must be sorted on the field you are attempting to join on. The <kbd>join</kbd> command will then output the results of the inner join on these two files, where if there's a matching field it will output the <kbd>join</kbd> key along with the remainder of the data lines of the first file concatenated with the second.</p>
<p>For example, say we would like to find users who are present both in the first review file and the second, and how many reviews they have in each. We can run the following <kbd>join</kbd> command:</p>
<pre><strong>join -j2 &lt;(zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2 | sort | uniq -c) &lt;(zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv.gz | cut -d$'\t' -f2 | sort | uniq -c) | head<em> </em></strong></pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-805 image-border" src="img/4ee4b7a3-0e09-4690-b5e5-f56271cf9f5d.png" style="width:161.33em;height:23.42em;"/></p>
<p>Here, we're using process substitution to slice the review files' data. This is done in parallel, increasing the speed of the process.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Group by and ordering</h1>
                
            
            
                
<p>We can perform a <kbd>GROUP BY</kbd> operation by using <kbd>sort</kbd> piped to <kbd>uniq -c</kbd> (as discussed in <a href="df05c890-510b-4e7e-8cc2-200f68f2febf.xhtml" target="_blank">Chapter 5</a>, <em>Loops, Functions, and String Processing</em>):</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f 2 | sort | uniq -c | head<em> </em></strong></pre>
<p class="CDPAlignLeft CDPAlign">The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-806 image-border" src="img/b7a2eabd-a1eb-48f2-b4f4-4ea27d0c84a7.png" style="width:130.50em;height:25.58em;"/></p>
<p>In the preceding example, we are simply counting how many reviews each user made. We might want to get the average review of each user, which can be done using <kbd>awk</kbd> associative arrays:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | awk '{sum[$1]+=$2;count[$1]+=1} END {for (i in sum) {print i,sum[i],count[i],sum[i]/count[i]}}' | head</strong> </pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-807 image-border" src="img/937340b2-4a8b-4126-a4cc-48930c9c597d.png" style="width:162.50em;height:24.92em;"/></p>
<p>Here, the output of the command is the ID, the sum of the reviews, the count of the reviews, and the average review for each user.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We can also sort the resulting data using the same tool, <kbd>sort</kbd>. For example, we can take our preceding <kbd>GROUP BY</kbd> example, and <kbd>ORDER BY</kbd> the number of reviews each user made to find the most prolific reviewers:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | awk '{sum[$1]+=$2;count[$1]+=1} END {for (i in sum) {print i,sum[i],count[i],sum[i]/count[i]}}' | sort -k3 -r -n | head</strong><em> <br/></em></pre>
<p>The preceding code produces these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-808 image-border" src="img/c226f381-1029-4d67-ad2b-edee031027c1.png" style="width:162.50em;height:24.58em;"/></p>
<p>The number of reviews each user made to find the most prolific reviewers</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Simulating selects</h1>
                
            
            
                
<p>In the previous sections, we saw how to <kbd>SELECT</kbd> data, inner <kbd>JOIN</kbd> data, and even do <kbd>GROUP BY</kbd> and <kbd>ORDER BY</kbd> operations on flat files or streams of data. Rounding out the commonly-used operations, we can also create sub-selected tables of data by simply wrapping a set of calls into a stream and then processing them further. This is what we've been doing using the piping model, but to illustrate a point, say we wanted to sub-select out of the grouped-by reviews only those reviewers who had between <kbd>100</kbd> and <kbd>200</kbd> reviews. We can take the command in the preceding example and <kbd>awk</kbd> it once more:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | cut -d$'\t' -f2,8 | awk '{sum[$1]+=$2;count[$1]+=1} END {for (i in sum) {print i,sum[i],count[i],sum[i]/count[i]}}' | sort -k3 -r -n | awk '$3 &gt;= 100 &amp;&amp; $3 &lt;=200' | head </strong></pre>
<p>The preceding code produces these results:</p>
<div><img class="aligncenter size-full wp-image-809 image-border" src="img/a1a328d6-07c8-486d-b271-fc58abeea197.png" style="width:162.50em;height:24.58em;"/></div>
<p>Sub-selecting out of the grouped-by reviews only those reviewers who had between 100 and 200 reviews</p>
<p class="mce-root"/>
<p class="mce-root">Using all of these tools, you saw how we can simulate most of the common SQL expressions on rows of file or stream data using the command line.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Keys to the kingdom</h1>
                
            
            
                
<p>Now that we can explore data with the command line and have mastered transforming text, we'll provide you with the keys to the kingdom. SQLite is a public domain library that implements a SQL engine and provides a <kbd>sqlite</kbd> command shell for interacting with database files. Unlike Oracle, MySQL, and other database engines that provide a network endpoint, sqlite is offline and locally driven by library calls to interact with a single file that is the entire database. This makes backups easy. Backups can be created by doing <kbd>cp database.sq3 backups/`date +%F`-database.sq3</kbd>. One can version control it, but that's unlikely to compress well with delta comparisons.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using SQLite</h1>
                
            
            
                
<p>Easy import of CSV files (with custom delimiter):</p>
<pre><strong>head -n21 amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv &gt; test.csv </strong><br/><strong>sqlite3 test.sq3 &lt;&lt;EOF</strong><br/><strong>.mode csv</strong><br/><strong>.separator "\t"</strong><br/><strong>.import test.csv test_reviews</strong><br/><strong>EOF</strong></pre>
<p>The data needs some massaging to get it into CSV format—it has a few problematic characters in the dataset – let's use some shell hackery to make it uniform:</p>
<pre><strong>COLS=`head  amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv | head -n1 | sed -e 's:^\|$:":g; s:\t:", ":g'`<br/><br/>VALUES=`head  amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv | tail -n1 | sed -e 's:^\|$:":g; s:\t:", ":g'`<br/><br/></strong><strong>sqlite3 reviews.sq3 "create table ‘aws-reviews' ( $COLS) ;"</strong> </pre>
<p>Show the tables by using the following command:</p>
<pre><strong>sqlite3 reviews.sq3 ".tables"<em> </em></strong></pre>
<p>The preceding code shows the tables in the database:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-810 image-border" src="img/38620955-256e-4f22-9ad6-45c5b3355570.png" style="width:31.08em;height:4.83em;"/></p>
<p>To show the datatypes for the table columns, run the following:</p>
<pre><strong> sqlite3 reviews.sq3 ".schema aws-reviews"</strong></pre>
<p>The preceding code produces this output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-811 image-border" src="img/6544e972-38fc-465d-9cbf-1ba65f3a7be8.png" style="width:162.50em;height:9.42em;"/></p>
<p>Showing the datatypes for the table columns</p>
<p>Load 20 lines of Amazon reviews into the sqlite database, named <kbd>reviews.sq3</kbd>, into the <kbd>aws_reviews</kbd> table:</p>
<pre><strong>head -n21 amazon_reviews_us_Digital_Ebook_Purchase_v1_00.tsv | sed '1d; s/"/""/g ; s/\t/", "/g;' | while read LINE ; do VALUES="\"${LINE}\"" ; sqlite3 reviews.sq3 "insert into aws_reviews ($COLS) VALUES ($VALUES) ;"; done </strong> <em> </em></pre>
<p>We read the first 21 lines. Our stream editor strips the first line (the header), escapes any double-quotes with a second pair of quotes (funky escaping, we know), and replaces the "tab" delimiter with a value separator that terminates the string and indicates it has a following element.</p>
<p>Then we convert the read <kbd>LINE</kbd> into our input <kbd>VALUES</kbd> by prepending a double-quote and appending a double-quote to finish properly formatting our values. Finally, our data is ready to insert into the table.</p>
<p>Note that sqlite3 uses a second quote character as a quote-escape sequence, similar to using <kbd>%%</kbd> with <kbd>printf</kbd> to get a literal <kbd>%</kbd> character.</p>
<p>Now we can query the data like any traditional database, because sqlite is a database engine in library form:</p>
<pre><strong>sqlite3 reviews.sq3 “select * from aws_reviews”<em> </em></strong></pre>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Math in bash itself</h1>
                
            
            
                
<p>Bash itself is able to do simple integer arithmetic. There are at least three different ways to accomplish this in bash.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using let</h1>
                
            
            
                
<p>You can use the let command to do simple bash arithmetic:</p>
<pre>$ let x=1<br/> $ echo $x<br/> 1<br/> $ let x=$x+1<br/> $ echo $x<br/>2</pre>


            

            
        
    

        

                            
                    <h1 class="header-title">Basic arithmetic</h1>
                
            
            
                
<p>You can do addition, subtraction, multiplication (be sure to escape the <kbd>*</kbd> operator with <kbd>\*</kbd>) and integer division<em>:</em></p>
<pre>expr 1 + 2<br/>3<br/>expr 3 \* 10<br/>30<em><br/></em></pre>
<p>The numbers must be separated by spaces.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Double-parentheses</h1>
                
            
            
                
<p>Similar to let, you can do simple integer arithmetic in bash using doubled parentheses:</p>
<pre>a=$((1 + 2))<br/>echo $a <br/>((a++))<br/>echo $a<br/> <br/>3<br/>4</pre>
<p>To see the full range of operations available in the shell, check out the GNU reference page: <a href="https://www.gnu.org/software/bash/manual/html_node/Shell-Arithmetic.html" target="_blank">https://www.gnu.org/software/bash/manual/html_node/Shell-Arithmetic.html</a>.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">bc, the unix basic calculator</h1>
                
            
            
                
<p><kbd>bc</kbd> is a calculator scripting language. Scripts in <kbd>bc</kbd> can be executed with the <kbd>bc</kbd> command. Imagine a <kbd>test.bc</kbd> file contains the following code:</p>
<pre>scale = 2;<br/>(10.0*2+2)/7;</pre>
<p>That means you can run <kbd>bc</kbd> like this:</p>
<pre>cat test.bc | bc<br/>3.14</pre>
<p><kbd>bc</kbd> can do far more than just divide two numbers. It's a fully-fledged scripting language on its own and you can do arbitrarily complex things with a <kbd>bc</kbd> script. A <kbd>bc</kbd> script might be the ending point of a pipeline of data, where, initially, the data files are massaged into a stream of data rows, and then a <kbd>bc</kbd> script is used to compute the values we're looking for. Let's illustrate this with a simple example.</p>
<p>In this example, we need to take a CSV data file and compute the average of the second number in each row, and also compute the sum of the fourth number in each row. Say we have a <kbd>bc</kbd> function to compute something interesting on these two numbers, such as a harmonic mean. We can use <kbd>awk</kbd> to output the numbers into a <kbd>bc</kbd> script and then feed the result into <kbd>bc</kbd> using a pipe.</p>
<p>So, say our <kbd>bc</kbd> function to compute the harmonic mean of two numbers looks like this:</p>
<pre>scale=5; <br/>define harmonic(x,y){ return 2.0/((1.0/x) + (1.0/y)); }<em> </em></pre>
<p>We can use <kbd>awk</kbd> to find the two numbers and construct the <kbd>bc</kbd> script, and then pipe it to <kbd>bc</kbd> to execute:</p>
<pre>awk '{s+=$2 ; f+=$4}END{print "scale=5;\n define harmonic(x,y){ return 2.0/((1.0/x) + (1.0/y)); } \n harmonic(",s/NR,",",f,")"}' data.txt | bc<em> <br/></em></pre>
<p>See the <kbd>bc</kbd> documentation at <a href="https://www.gnu.org/software/bc/manual/html_mono/bc.html">https://www.gnu.org/software/bc/manual/html_mono/bc.html</a> for more things you could do with <kbd>bc</kbd>.</p>
<p class="mce-root"/>


            

            
        
    

        

                            
                    <h1 class="header-title">Math in (g)awk</h1>
                
            
            
                
<p><kbd>awk</kbd> (including the <kbd>gnu</kbd> implementation, <kbd>gawk</kbd>) is designed to stream text processing, data extraction, and reporting. A large percentage of practical statistics is made up of counting things in specific ways, and this is one of the things <kbd>awk</kbd> excels at. Tallying totals, histograms, and grouped counts are all very easy in <kbd>awk</kbd>.</p>
<p>An <kbd>awk</kbd> program is structured as a set of patterns that are matched, and actions to take when those patterns are matched:</p>
<pre>pattern {action}<br/>pattern {action}<br/>pattern {action}<br/>…</pre>
<p>For each record (usually each line of text passed to <kbd>awk</kbd>), each pattern is tested to see whether the record matches, and if so, the action is taken. Additionally, each record is automatically split into a list of fields by a delimiter. The default action, if none is given, is to print the record. The default pattern is to match everything. There are two special patterns, <kbd>BEGIN</kbd> and <kbd>END</kbd>, which are matched only before any records are processed, or after, respectively.</p>
<p>The power of <kbd>awk</kbd> lies in its variables: variables can be used without a declaration. There's some special variables already available to you that are useful for math:</p>
<pre>$0: The text of the entire record.<br/>$1, $2, … : The text of the 1st, 2nd, etc fields in the record.<br/>NF: The number of fields in the current record.<br/>NR: The current count of records (equal to the total number of records in the END step)</pre>
<p>Additionally, you can assign values to your own variables. <kbd>awk</kbd> natively supplies variables that can hold strings, integers, floating point numbers, and regular expressions and associative arrays.</p>
<p>As an example, say we want to count the word frequency in the reviews of our test data. Run this code:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | tail -n +2 | head -n 10000 | cut -f14 | awk 'BEGIN {FS="[^a-zA-Z]+"}; {for (i=1;i&lt;NF;i++) words[$i] ++}; END {for (i in words) print words[i], i}' | head<em> </em></strong></pre>
<p class="mce-root"/>
<p>It will produce these results:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-812 image-border" src="img/731421e3-4a37-4ed0-8f2b-bac5e8d90b95.png" style="width:162.50em;height:24.58em;"/></p>
<p>Counting the word frequency in the reviews of our test data</p>
<p>Say we'd like to compute a histogram of the star values of the reviews. This is also very easy with <kbd>awk</kbd>:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | tail -n +2 | cut -f8 | awk '{star[$0]++}; END {for (i in star) print i,star[i]}'<em> </em></strong></pre>
<p>The preceding code produces this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-813 image-border" src="img/02dd2478-2f99-4129-bbae-36bc45a1d83d.png" style="width:160.17em;height:14.75em;"/></p>
<p>Computing a histogram of the star values of the reviews</p>
<p>We can see that four- and five-star reviews dominate this dataset.</p>
<p>Besides counting, <kbd>awk</kbd> is also great for manipulating the format of strings: look back at <a href="df05c890-510b-4e7e-8cc2-200f68f2febf.xhtml" target="_blank">Chapter 5</a>, <em>Loops, Functions, and String Processing</em>, for some examples of using <kbd>awk</kbd> for string manipulation.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Python (pandas, numpy, scikit-learn)</h1>
                
            
            
                
<p>Counting things often gets you to where you need to be, but sometimes more complex tools are required to do the job. Fortunately, we can write our own tools in the UNIX paradigm and use them in our workstream pipes along with our other command-line tools if we so desire.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>One such tool is python, along with popular data science libraries such as <kbd>pandas</kbd>, <kbd>numpy</kbd>, and <kbd>scikit-learn</kbd>. This isn't a text on all the great things those libraries can do for you (if you'd like to learn, a good place to start is the official python tutorial (<a href="https://docs.python.org/3/tutorial/">https://docs.python.org/3/tutorial/</a>) and the basics of Pandas data structures in the Pandas documentation (<a href="https://pandas.pydata.org/pandas-docs/stable/basics.html" target="_blank">https://pandas.pydata.org/pandas-docs/stable/basics.html</a>). Make sure you have Python, <kbd>pip</kbd>, and <kbd>pandas</kbd> installed before you continue (see <a href="d26c5d26-6302-4b9d-b6ce-62b1ab13db0d.xhtml">Chapter 1</a>, <em>Data Science at the Command Line and Setting It Up</em>).</p>
<p>If you want to connect your python program to a piped stream however, of course there are ways to do it. A simple method is to use the <kbd>sys</kbd> library. Say we have a small pandas program tuned to our dataset that computes the mean of a couple of the columns that we know are in the data:</p>
<pre>import sys<br/>import pandas as pd<br/> <br/>df = pd.read_csv(sys.stdin,sep='\t')<br/>print 'star rating mean',df['star_rating'].mean()<br/>print 'helpful votes mean', df['helpful_votes'].mean()</pre>
<p>Note how we get the data directly from the <kbd>sys.stdin</kbd> stream and pass that right to pandas' <kbd>read_csv</kbd> method (using tab as a separator). If we use this method, we can pipe the data right into the script:</p>
<pre><strong>zcat amazon_reviews_us_Digital_Ebook_Purchase_v1_01.tsv.gz | head -n 100 | python average.py<em> </em></strong></pre>
<p>The preceding code produces this output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-814 image-border" src="img/b437f3e9-be78-4c1b-b5be-21aff1aeaad3.png" style="width:136.00em;height:11.25em;"/></p>


            

            
        
    

        

                            
                    <h1 class="header-title">Analyzing weather data in bash</h1>
                
            
            
                
<p>The National Weather Service has an API to get weather data: <a href="https://forecast-v3.weather.gov/documentation">https://forecast-v3.weather.gov/documentation</a> . The API delivers forecast data over a lightweight HTTP interface. If you pass the correct URL and parameters to the web endpoint, the service will return JSON-formatted weather data. Let's take a look at an example of some data exploration we can do with this rich dataset.</p>
<p>The NWS provides both current weather data and forecasts. Let's say I'd like to see just how accurate NWS forecasts are. I'd like to do this over some amount of time, say a week. I'd like to save tomorrow's forecast, and then later on, compare those forecasts to what the temperature really was. For this example, let's look at the forecast highs, and the actual high temperatures. I'd like to do this for a single point in lat-lon.</p>
<p>Our overall plan will be to record the forecasts for the next day's high temperatures once a day in a CSV file. Once an hour, we'll record the actual temperature in another CSV file. Then, we'll write a script that compares these two files and computes the accuracy of each type of forecast (one-day forecast, two-day forecast, and so on) over multiple days.</p>
<p>First, we need to be able to query the right endpoint in the API. The weather service data is gridded into a set of grid locations. To find the grid for a particular lat-lon point, we can query the API:</p>
<pre><strong>curl -s "https://api.weather.gov/points/42.5,-71.5"</strong></pre>
<p>Querying the API returns the following:</p>
<pre><strong>{</strong><br/><strong>    "@context": [</strong><br/><strong>        "https://raw.githubusercontent.com/geojson/geojson-ld/master/contexts/geojson-base.jsonld",</strong><br/><br/><strong>        {</strong><br/><strong>            "wx": "https://api.weather.gov/ontology#",</strong><br/><strong>            "s": "https://schema.org/",</strong><br/><strong>            "geo": "http://www.opengis.net/ont/geosparql#",</strong><br/><strong>            "unit": "http://codes.wmo.int/common/unit/",</strong><br/><strong>            "@vocab": "https://api.weather.gov/ontology#",</strong><br/><strong>            "geometry": {</strong><br/><strong>                "@id": "s:GeoCoordinates",</strong><br/><strong>                "@type": "geo:wktLiteral"</strong><br/><strong>         }</strong><br/><strong>  [......]</strong><br/><strong>}</strong></pre>
<p>There's a lot of extraneous information in JSON, when we really only want the grid coordinates and the forecast region. Let's use the <kbd>jq</kbd> UNIX tool to parse this JSON and extract the relevant information:</p>
<pre><strong>curl -s "https://api.weather.gov/points/42.5,-71.5" | jq -r '.| "\(.properties.cwa) \(.properties.gridX) \(.properties.gridY)"'</strong></pre>
<p class="mce-root"/>
<p>The relevant information looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-815 image-border" src="img/737ef681-b54a-48c8-931d-5ff08789a98d.png" style="width:155.42em;height:9.50em;"/></p>
<p>Here, we've used <kbd>jq</kbd> to parse and format a bit of text that we could then insert into a URL, which we can re-curl for the forecast. Helpfully, however, the API actually gives us the entire URL of the forecast inside the JSON, in the <kbd>properties.forecastGridData</kbd> feature:</p>
<pre><strong>curl -s "https://api.weather.gov/points/42.5,-71.5" | jq -r '.| "\(.properties.forecastGridData)"' </strong></pre>
<p>The preceding code produces this output: </p>
<pre><strong>https://api.weather.gov/gridpoints/BOX/55,80</strong></pre>
<p>We're going to take this URL, <kbd>curl</kbd> it into <kbd>jq</kbd> again, and extract the high temperature forecasts for the next day. Using <kbd>jq</kbd>, we're going to format these into a CSV line that we'll later on append to our flat file data table. For this example, we're going to ignore time zones, and assume days start and end on Zulu time. Run this code:</p>
<pre><strong>curl -s "https://api.weather.gov/gridpoints/BOX/55,80" |  jq -r '[.properties.maxTemperature.values[1].validTime[0:10],.properties.maxTemperature.values[1].value] | @csv'<em> </em></strong></pre>
<p>It produces the following output:</p>
<pre><strong>"2018-06-22",23.88888888888897</strong></pre>
<p>The output will be different since you're running this after 2018-06-22.</p>
<p>Looks great! Save this command as is to a bash script, say <kbd>forecast.sh</kbd>, using the editor of your choice. Be sure to make the script executable with the <kbd>chmod</kbd> command:</p>
<pre><strong>$ chmod 700 forecast.sh </strong></pre>
<p class="mce-root"/>
<p>And let's <kbd>cat</kbd> the file to view the contents:</p>
<pre><strong>$ cat forecast.sh <br/>#!/bin/bash<br/>curl -s "https://api.weather.gov/gridpoints/BOX/55,80" | jq -r '[.properties.maxTemperature.values[1].validTime[0:10],.properties.maxTemperature.values[1].value] | @csv'</strong></pre>
<p>Let's add this to a cron[1] task and run this once a day at noon, and append the resulting line to a <kbd>.csv</kbd> file. Cron is a system utility that will run a command on a schedule. The schedules look something like this:</p>
<pre>&lt;minutes to run&gt; &lt;hours to run&gt; &lt;day of month to run&gt; &lt;month to run&gt; &lt;day of week to run&gt;</pre>
<p>So, if we'd like to run this once a day, we want to run it on a particular minute of a particular hour, but on every day, month, and day of week giving the following cron pattern, if say, we'd like to run at noon every day:</p>
<pre>0 12 * * *</pre>
<p>To add a script to cron's list, the <kbd>crontab</kbd> you'll need to run the command:</p>
<pre><strong>crontab -e</strong></pre>
<p>Add the following line to your <kbd>crontab</kbd>:</p>
<pre>0 12 * * * sh &lt;script location&gt;/forecast.sh &gt;&gt; &lt;data dir&gt;forecast.csv</pre>
<p>Now, every day the forecast will be appended to the file you specified.</p>
<p>To get the current weather data, we need to find the closest weather station to our gridpoint:</p>
<pre><strong>curl -s "https://api.weather.gov/gridpoints/BOX/55,80/stations" | jq -r '.observationStations[0]'<em> </em></strong></pre>
<p>The preceding code produces this output:</p>
<pre><strong>https://api.weather.gov/stations/KBED</strong></pre>
<p>The current weather is located at the following API point:</p>
<pre><strong>https://api.weather.gov/stations/KBED/observations/current</strong></pre>
<p class="mce-root"/>
<p>From this API point, we can grab a timestamp and current temperature:</p>
<pre><strong>curl -s "https://api.weather.gov/stations/KBED/observations/current" | jq -r '[.properties.timestamp[0:10],.properties.temperature.value]| @csv'</strong><em> <br/></em><strong>"2018-06-21",20.600000000000023<em> </em></strong></pre>
<p>Add this to a script file, and to your crontab as well, set to run every hour. To do this, we need to specify a minute but wildcard everything else in the cron pattern:</p>
<pre>0 * * * * sh &lt;script location&gt;/actual.sh &gt;&gt; &lt;data location&gt;/actual.csv</pre>
<p>We let this run for a couple of weeks to build our dataset.</p>
<p>Now, we want to take the maximum temperature we record each day, join that to the forecast we recorded for that day, and compute the difference. To find the max temperature for any given day, we can once again use <kbd>gawk</kbd>:</p>
<pre><strong>gawk  'BEGIN { FPAT = "([^,]+)|(\"[^\"]+\")" } {count[$1]++ ; max[$1] = (count[$1]==1||max[$1]&lt;$2)?$2:max[$1]} END{ for (i in max) print $i,max[$i]}' actual.csv </strong><br/><strong>"2018-06-22",18.900000000000034</strong></pre>
<p>Then, we can join this result back to our forecasts. Since the output is already sorted by date in a sortable YYYY-MM-DD order, we don't need to pre-sort. Run the following:</p>
<pre><strong> join -t',' &lt;(gawk  'BEGIN { FPAT = "([^,]+)|(\"[^\"]+\")" } {count[$1]++ ; max[$1] = (count[$1]==1||max[$1]&lt;$2)?$2:max[$1]} END{ for (i in max) print $i,max[$i]}' actual.csv ) forecast.csv<em> </em></strong></pre>
<p>The preceding code produces the following output:</p>
<pre><strong>"2018-06-22",18.900000000000034 ,23.88888888888897</strong><br/><strong>...</strong></pre>
<p>And we can pipe this stream to <kbd>awk</kbd> to compute the difference between the actual and predicted temperatures:</p>
<pre><strong>&gt; join -t',' &lt;(gawk  'BEGIN { FPAT = "([^,]+)|(\"[^\"]+\")" } {count[$1]++ ; max[$1] = (count[$1]==1||max[$1]&lt;$2)?$2:max[$1]} END{ for (i in max) print $i,max[$i]}' actual.csv ) forecast.csv | gawk 'BEGIN { FPAT = "([^,]+)|(\"[^\"]+\")" } {print $1,$2-$3}'<em> </em></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The preceding code results in the following:</p>
<pre><strong>"2018-06-22" -4.98889</strong></pre>
<p>We grabbed real data from the Internet, massaged it using a workflow, stored it into files, and computed numeric values with the data in the tables we made!</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we used <kbd>cut</kbd>, <kbd>grep</kbd>, <kbd>awk</kbd>, and <kbd>sort</kbd> to deeply inspect our data, as one would in a more traditional database. We then saw how sqlite can provide a lightweight alternative to other databases. Using these tools together, we were able to mine useful knowledge from our raw files.</p>
<p>We also saw how the command line offers several options for doing arithmetic and other mathematical operations. Simple arithmetic and grouped tallies can be performed using bash itself or <kbd>awk</kbd>. More complex mathematics can be done using a scripting language, such as <kbd>bc</kbd> or python, and be called like other command-line workflow tools.</p>
<p>Finally, we used many of the tools we discussed to produce a useful and interesting result from publicly-available data.</p>
<p>We hope that this book broadens your understanding of just how powerful the command line actually is, especially for data science. However, this is only the very beginning. There's a number of tools and other commands we haven't even mentioned, which are very powerful and deserve to be mentioned. <kbd>BashHTTPD</kbd> (<a href="https://github.com/avleen/bashttpd">https://github.com/avleen/bashttpd</a>) is a web server in bash; it may sound silly, but the shell can really do amazing things. <kbd>BashReduce</kbd> (<a href="https://github.com/erikfrey/bashreduce">https://github.com/erikfrey/bashreduce</a>) gives the user the ability to run bash commands over multiple machines/cores. You might have noticed some of the commands took a little while to run. We recommend taking a look at <kbd>BashReduce</kbd> to speed things up. Those who are familiar with the <kbd>MapReduce</kbd> concept should have no issue picking up and working with <kbd>BashReduce</kbd>.</p>
<p>We also want to mention that there are so many other great command-line tools out there; we could write about them forever. However, for this book, we decided to focus on the everyday commands and provide examples on how to use them. We hope you enjoyed this book!</p>


            

            
        
    </body></html>