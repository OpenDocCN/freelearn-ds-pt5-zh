["```py\ngit clone --bare /project/location/my-code /remote-location/my-code #copy your code history from a local git repo\ngit remote add usb file:///remote/location/my-code \n# add your remote as a file location\ngit remote add usb file:///remote/location/my-code \n# add your remote as a file location\ngit push usb master \n# push the code\n\n# Done! Other developers can set up your remote and pull updates:\ngit remote add usb file:///remote/location/my-code # add your remote as a file location\ngit pull usb mater # pull the code\n```", "```py\n├── AUTHORS.rst <- List of developers and maintainers.\n├── CHANGELOG.rst <- Changelog to keep track of new features and fixes.\n├── LICENSE.txt <- License as chosen on the command-line.\n├── README.md <- The top-level README for developers.\n├── configs <- Directory for configurations of model & application.\n├── data\n│ ├── external <- Data from third party sources.\n│ ├── interim <- Intermediate data that has been transformed.\n│ ├── processed <- The final, canonical data sets for modeling.\n│ └── raw <- The original, immutable data dump.\n├── docs <- Directory for Sphinx documentation in rst or md.\n├── environment.yaml <- The conda environment file for reproducibility.\n├── models <- Trained and serialized models, model predictions,\n│ or model summaries.\n├── notebooks <- Jupyter notebooks. Naming convention is a number (for\n│ ordering), the creator's initials and a description,\n│ e.g. `1.0-fw-initial-data-exploration`.\n├── references <- Data dictionaries, manuals, and all other materials.\n├── reports <- Generated analysis as HTML, PDF, LaTeX, etc.\n│ └── figures <- Generated plots and figures for reports.\n├── scripts <- Analysis and production scripts which import the\n│ actual PYTHON_PKG, e.g. train_model.\n├── setup.cfg <- Declarative configuration of your project.\n├── setup.py <- Use `python setup.py develop` to install for development or\n| or create a distribution with `python setup.py bdist_wheel`.\n├── src\n│ └── PYTHON_PKG <- Actual Python package where the main functionality goes.\n├── tests <- Unit tests which can be run with `py.test`.\n├── .coveragerc <- Configuration for coverage reports of unit tests.\n├── .isort.cfg <- Configuration for git hook that sorts imports.\n└── .pre-commit-config.yaml <- Configuration of pre-commit git hooks.\n```", "```py\ndvc run -d ../data/interim/ -o ../data/interim/01_generate_dataset -o ../reports/01_generate_dataset.ipynb papermill --progress-bar --log-output --cwd ../notebooks ../notebooks/01_generate_dataset.ipynb ../reports/01_generate_dataset.ipynb\n```", "```py\n#!/bin/bash\nset -eu\n​\nif [ $# -eq 0 ]; then\n echo \"Use:\"\n echo \"./dvc-run-notebook [data subdirectory] [notebook name] -d [your DVC dependencies]\"\n echo \"This script executes DVC on a notebook using papermill. Before running create ../data/[data subdirectory] if it does not exist and do not forget to specify your dependencies as multiple last arguments\"\n echo \"Example:\"\n echo \"./dvc-run-notebook interim ../notebooks/02_generate_dataset.ipynb -d ../data/interim/\"\n exit 1\nfi\n​\nNB_NAME=$(basename -- \"$2\")\n​\nCMD=\"dvc run ${*:3} -o ../data/$1 -o ../reports/$NB_NAME papermill --progress-bar --log-output --cwd ../notebooks ../notebooks/$NB_NAME ../reports/$NB_NAME\"\n​\necho \"Executing the following DVC command:\"\necho $CMD\n$CMD \n```", "```py\npip install -e .\n```"]