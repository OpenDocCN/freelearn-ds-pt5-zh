["```py\nPkg.update() \nPkg.add(\"Mocha\") \n\n```", "```py\ndata_layer = HDF5DataLayer(name=\"data\", source=\"data-list.txt\", batch_size=64, tops=[:data]) \nip_layer   = InnerProductLayer(name=\"ip\", output_dim=500, tops=[:ip], bottoms=[:data]) \n\n```", "```py\n# report training progress every 1000 iterations \nadd_coffee_break(solver, TrainingSummary(), every_n_iter=1000) \n\n# save snapshots every 5000 iterations \nadd_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000) \n\n```", "```py\nENV[\"MOCHA_USE_NATIVE_EXT\"] = \"true\" \n\nusing Mocha \n\nbackend = CPUBackend() \ninit(backend) \n\n```", "```py\ndata_layer  = HDF5DataLayer(name=\"train-data\", source=\"data/train.txt\", \nbatch_size=64, shuffle=true) \n\n```", "```py\nconv_layer = ConvolutionLayer(name=\"conv1\", n_filter=20, kernel=(5,5), \nbottoms=[:data], tops=[:conv1]) \n\n```", "```py\npool_layer = PoolingLayer(name=\"pool1\", kernel=(2,2), stride=(2,2), \n    bottoms=[:conv1], tops=[:pool1]) \nconv2_layer = ConvolutionLayer(name=\"conv2\", n_filter=50, kernel=(5,5), \n    bottoms=[:pool1], tops=[:conv2]) \npool2_layer = PoolingLayer(name=\"pool2\", kernel=(2,2), stride=(2,2), \n    bottoms=[:conv2], tops=[:pool2]) \n\n```", "```py\nfc1_layer  = InnerProductLayer(name=\"ip1\", output_dim=500, \n    neuron=Neurons.ReLU(), bottoms=[:pool2], tops=[:ip1]) \nfc2_layer  = InnerProductLayer(name=\"ip2\", output_dim=10, \n    bottoms=[:ip1], tops=[:ip2]) \n\n```", "```py\nloss_layer = SoftmaxLossLayer(name=\"loss\", bottoms=[:ip2,:label]) \n\n```", "```py\ncommon_layers = [conv_layer, pool_layer, conv2_layer, pool2_layer, \n    fc1_layer, fc2_layer] \n\nnet = Net(\"MNIST-train\", backend, [data_layer, common_layers..., loss_layer]) \n\n```", "```py\nexp_dir = \"snapshots\" \nmethod = SGD() \n\nparams = make_solver_parameters(method, max_iter=10000, regu_coef=0.0005, \n    mom_policy=MomPolicy.Fixed(0.9), \n    lr_policy=LRPolicy.Inv(0.01, 0.0001, 0.75), \n    load_from=exp_dir) \n\nsolver = Solver(method, params) \n\n```", "```py\nsetup_coffee_lounge(solver, save_into=\"$exp_dir/statistics.hdf5\", every_n_iter=1000) \n\nadd_coffee_break(solver, TrainingSummary(), every_n_iter=100) \n\nadd_coffee_break(solver, Snapshot(exp_dir), every_n_iter=5000) \n\n```", "```py\ndata_layer_test = HDF5DataLayer(name=\"test-data\", source=\"data/test.txt\", batch_size=100) \n\nacc_layer = AccuracyLayer(name=\"test-accuracy\", bottoms=[:ip2, :label]) \n\ntest_net = Net(\"MNIST-test\", backend, [data_layer_test, common_layers..., acc_layer]) \n\n```", "```py\nadd_coffee_break(solver, ValidationPerformance(test_net), every_n_iter=1000) \n\n```", "```py\nsolve(solver, net) \n\ndestroy(net) \ndestroy(test_net) \nshutdown(backend)  \n\n```", "```py\nCorrect label index: 5\nLabel probability vector:\nFloat32[5.870685e-6\n0.00057068263\n1.5419962e-5\n8.387835e-7\n0.99935246\n5.5915066e-6\n4.284061e-5\n1.2896479e-6\n4.2869314e-7\n4.600691e-6]\n\n```"]