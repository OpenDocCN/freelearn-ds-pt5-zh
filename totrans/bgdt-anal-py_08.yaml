- en: '*Chapter 8*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Creating a Full Analysis Report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Read data from different sources in Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform SQL operations on a Spark DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate statistical measurements in a consistent way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate graphs and plots using Plotly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compile an analysis report incorporating all the previous steps and data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will read the data using Spark, aggregating it, and extract
    the statistical measurements. We will also use the Pandas to generate graphs from
    aggregated data and form an analysis report.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you have been part of the data industry for a while, you will understand
    the challenge of working with different data sources, analyzing them, and presenting
    them in consumable business reports. When using Spark on Python, you may have
    to read data from various sources, such as flat files, REST APIs in JSON format,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, getting data in the right format is always a challenge and
    several SQL operations are required to gather data. Thus, it is mandatory for
    any data scientist to know how to handle different file formats and different
    sources, and to carry out basic SQL operations and present them in a consumable
    format.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides common methods for reading different types of data, carrying
    out SQL operations on it, doing descriptive statistical analysis, and generating
    a full analysis report. We will start with understanding how to read different
    kinds of data into PySpark and will then generate various analyses and plots on
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data in Spark from Different Data Sources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the advantages of Spark is the ability to read data from various data
    sources. However, this is not consistent and keeps changing with each Spark version.
    This section of the chapter will explain how to read files in CSV and JSON.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 47: Reading Data from a CSV File Using the PySpark Object'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To read CSV data, you have to write the `spark.read.csv("the file name with
    .csv")` function. Here, we are reading the bank data that was used in the earlier
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The `sep` function is used here.
  prefs: []
  type: TYPE_NORMAL
- en: We have to ensure that the right `sep` function is used based on how the data
    is separated in the source data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let''s perform the following steps to read the data from the `bank.csv`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the required packages into the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, import all the required libraries, as illustrated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use the `tick` themes, which will make our dataset more visible and provide
    higher contrast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, change the working directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s import the libraries required for Spark to build the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s read the CSV data after creating the `df_csv` Spark object using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the schema using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.1: Bank schema](img/C12913_08_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.1: Bank schema'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Reading JSON Data Using the PySpark Object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To read JSON data, you have to write the `read.json("the file name with .json")`
    function after setting the SQL context:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2: Reading JSON file in PySpark](img/C12913_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Reading JSON file in PySpark'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: SQL Operations on a Spark DataFrame
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A DataFrame in Spark is a distributed collection of rows and columns. It is
    the same as a table in a relational database or an Excel sheet. A Spark RDD/DataFrame
    is efficient at processing large amounts of data and has the ability to handle
    petabytes of data, whether structured or unstructured.
  prefs: []
  type: TYPE_NORMAL
- en: Spark optimizes queries on data by organizing the DataFrame into columns, which
    helps Spark understand the schema. Some of the most frequently used SQL operations
    include subsetting the data, merging the data, filtering, selecting specific columns,
    dropping columns, dropping all null values, and adding new columns, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 48: Reading Data in PySpark and Carrying Out SQL Operations'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For summary statistics of data, we can use the `spark_df.describe().show()`
    function, which will provide information on `count`, `mean`, `standard deviation`,
    `max`, and `min` for all the columns in the DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the dataset that we have considered—the bank marketing dataset
    ([https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv))—the
    summary statistics data can be obtained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a new Jupyter notebook, import all the required packages, as
    illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, change the working directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import all the libraries required for Spark to build the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create and read the data from the CSV file using the Spark object, as illustrated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s print the first five rows from the Spark object using the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.3: Bank data of first five rows (Unstructured)](img/C12913_08_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.3: Bank data of first five rows (Unstructured)'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The previous output is unstructured. Let''s first identify the data types to
    proceed to get the structured data. Use the following command to print the datatype
    of each column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.4: Bank datatypes (Structured)](img/C12913_08_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.4: Bank datatypes (Structured)'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now let''s calculate the total number of rows and columns with names to get
    a clear idea of the data we have:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.5: Total number of rows and columns names](img/Image38437.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.5: Total number of rows and columns names'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the summary statistics for the DataFrame using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6: Summary statistics of numerical columns](img/C12913_08_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.6: Summary statistics of numerical columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To select multiple columns from a DataFrame, we can use the `spark_df.select(''col1'',
    ''col2'', ''col3'')` function. For example, let''s select the first five rows
    from the `balance` and `y` columns using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.7: Data of the balance and y columns](img/C12913_08_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.7: Data of the balance and y columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To identify the relation between two variables in terms of their frequency
    of levels, `crosstab` can be used. To derive crosstab between two columns, we
    can use the `spark_df.crosstab(''col1'', col2)` function. Crosstab is carried
    out between two categorical variables and not between numerical variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 8.8: Pair wise frequency of categorical columns](img/C12913_08_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.8: Pair wise frequency of categorical columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let''s add a new column to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.9: Data of newly added column](img/C12913_08_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.9: Data of newly added column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Drop the newly created column using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 49: Creating and Merging Two DataFrames'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will extract and use the bank marketing data ([https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing))
    from the UCI Machine Learning Repository. The objective is to carry out merge
    operations on a Spark DataFrame using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: The data is related to the direct marketing campaigns of a Portuguese banking
    institution. The marketing campaigns were based on phone calls. Often, more than
    one contact for the same client was required, in order to access whether the product
    (**bank term deposit**) would be (**yes**) or would not be (**no**) subscribed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create two DataFrames from the current bank marketing data and
    merge them on the basis of a primary key:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the required header files in the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, change the working directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import all the libraries required for Spark to build the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the data from the CSV files into a Spark object using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the first five rows from the Spark object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Image38484.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.10: Bank data of first five rows (Unstructured)'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, to merge the two DataFrames using the primary key (ID), first, we will
    have to split it into two DataFrames.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, add a new DataFrame with an `ID` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, create another column, `ID2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the DataFrame using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, change the ID column names of `train_with_id2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Merge `train_with_id1` and `train_with_id2` using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 50: Subsetting the DataFrame'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will extract and use the bank marketing data ([https://archive.ics.uci.edu/ml/datasets/bank+marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing))
    from the UCI Machine Learning Repository. The objective is to carry out filter/subsetting
    operations on the Spark DataFrame using PySpark.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s subset the DataFrame where the balance is greater than `0` in the bank
    marketing data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the required header files in the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, change the working directory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import all the libraries required for Spark to build the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, read the CSV data as a Spark object using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s run SQL queries to subset and filter the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/Image38496.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.11: Filtered DataFrame'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Generating Statistical Measurements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Python is a general-purpose language with statistical modules. A lot of statistical
    analysis, such as carrying out descriptive analysis, which includes identifying
    the distribution of data for numeric variables, generating a correlation matrix,
    the frequency of levels in categorical variables with identifying mode and so
    on, can be carried out in Python. The following is an example of correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12: Segment numeric data and correlation matrix output](img/C12913_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.12: Segment numeric data and correlation matrix output'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Identifying the distribution of data and normalizing it is important for parametric
    models such as `yeo-johnson` method to normalize the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13: Identifying the distribution of the data—Normality test](img/C12913_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.13: Identifying the distribution of the data—Normality test'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The identified variables are then normalized using `yeo-johnson` or the `box-cox`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating the importance of features is important in a data science project
    where predictive techniques are used. This broadly falls under statistical analysis
    as various statistical techniques are used to identify the important variables.
    One of the methods that''s used here is `Boruta`, which is a wrap-around `RandomForest`
    algorithm for variable importance analysis. For this, we will be using the `BorutaPy`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14: Feature importance](img/C12913_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14: Feature importance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Activity 15: Generating Visualization Using Plotly'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we will extract and use the bank marketing data from the UCI
    Machine Learning Repository. The objective is to generate visualizations using
    Plotly in Python.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Plotly's Python graphing library makes interactive, publication-quality graphs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following steps to generate the visualization using Plotly:'
  prefs: []
  type: TYPE_NORMAL
- en: Import the required libraries and packages into the Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the libraries required for Plotly to visualize the data visualization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Read the data from the `bank.csv` file into the Spark DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Check the Plotly version that you are running on your system. Make sure you
    are running the updated version. Use the `pip install plotly --upgrade` command
    and then run the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now import the required libraries to plot the graphs using Plotly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the Plotly credentials in the following command, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: To generate an API key for Plotly, sign up for an account and navigate to [https://plot.ly/settings#/](https://plot.ly/settings#/).
    Click on the **API Keys** option and then click on the **Regenerate Key** option.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, plot each of the following graphs using Plotly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bar graph:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.15: Bar graph of bank data](img/C12913_08_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8.15: Bar graph of bank data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Scatter plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16: Scatter plot of bank data](img/C12913_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16: Scatter plot of bank data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Boxplot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17: Boxplot of bank data](img/C12913_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17: Boxplot of bank data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 248.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we learned how to import data from various sources into a Spark
    environment as a Spark DataFrame. In addition, we learned how to carry out various
    SQL operations on that DataFrame, and how to generate various statistical measures,
    such as correlation analysis, identifying the distribution of data, building a
    feature importance model, and so on. We also looked into how to generate effective
    graphs using Plotly offline, where you can generate various plots to develop an
    analysis report.
  prefs: []
  type: TYPE_NORMAL
- en: 'This book has hopefully offered a stimulating journey through big data. We
    started with Python and covered several libraries that are part of the Python
    data science stack: NumPy and Pandas, We also looked at home we can use Jupyter
    notebooks. We then saw how to create informative data visualizations, with some
    guiding principles on what is a good graph, and used Matplotlib and Seaborn to
    materialize the figures. Then we made a start with the Big Data tools - Hadoop
    and Spark, thereby understanding the principles and the basic operations.'
  prefs: []
  type: TYPE_NORMAL
- en: We have seen how we can use DataFrames in Spark to manipulate data, and have
    about utilize concepts such as correlation and dimensionality reduction to better
    understand our data. The book has also covered reproducibility, so that the analysis
    created can be supported and better replicated when needed, and we finished our
    journey with a final report. We hope that the subjects covered, and the practical
    examples in this book will help you in all areas of your own data journey.
  prefs: []
  type: TYPE_NORMAL
