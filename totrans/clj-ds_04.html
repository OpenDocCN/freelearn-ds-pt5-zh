<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 4. Classification"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch04" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 4. Classification</h1></div></div></div><div class="calibre2"><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre17"><td class="calibre18"> </td><td class="calibre18"><p class="calibre19"><span class="strong1"><em class="calibre13">"It is a truth universally acknowledged, that a single man in possession of a good fortune, must be in want of a wife."</em></span></p></td><td class="calibre18"> </td></tr><tr class="calibre17"><td class="calibre18"> </td><td colspan="2" class="calibre20">--<span class="strong1"><span class="strong1"><em class="calibre13">Jane Austen, Pride and Prejudice</em></span></span></td></tr></table></div><p class="calibre11">In the previous chapter, we learned how to make numeric predictions using linear regression. The model we built was able to learn how the features of Olympic swimmers related to their weight and we were able to use the model to make a weight prediction for a new swimmer. As with all regression techniques, our output was a number.</p><p class="calibre11">Not all predictions demand a numeric solution, though—sometimes we want our predictions to be items. For <a id="id387" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example, we may want to predict which candidate a voter will back in an election. Or we may want to know which of several products a customer is likely to buy. In these cases, the outcome is a selection from one of a number of possible discrete options. We call these options classes, and models we'll build in this chapter are classifiers.</p><p class="calibre11">We'll learn about several different types of classifier and compare their performance on a sample dataset—the list of passengers from the Titanic. Prediction and classification are intimately connected to theories of probability and information, and so we'll cover these in more detail too. We'll begin the chapter with ways of measuring relative probabilities between groups and move then on to applying statistical significance testing to the groups themselves.</p><div class="calibre2" title="About the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec78" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>About the data</h1></div></div></div><p class="calibre11">This chapter will make use of <a id="id388" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data about the passengers on the Titanic, which famously sank on her maiden voyage in 1912 after hitting an iceberg. The survival rates of passengers were strongly affected by a variety of factors, including class and sex.</p><p class="calibre11">The dataset is derived from a painstakingly compiled dataset produced by Michael A. Findlay. For more information about how the data was derived, including links to original sources, consult the <a id="id389" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>book's wiki at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://wiki.clojuredatascience.com">http://wiki.clojuredatascience.com</a>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note36" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The example code for this chapter is available from Packt Publishing's website or from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/clojuredatascience/ch4-classification">https://github.com/clojuredatascience/ch4-classification</a>.</p></div></div><p class="calibre11">The data is small enough to have been included together with the source code in the data directory.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Inspecting the data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec79" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Inspecting the data</h1></div></div></div><p class="calibre11">We encountered <a id="id390" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>categorical variables in the previous chapter as the dichotomous variable "sex" in the athlete dataset. That dataset also contained many other categorical variables including "sport", "event", and "country".</p><p class="calibre11">Let's take a look at the Titanic dataset (using the <code class="literal">clojure.java.io</code> library to access the file resource and the <code class="literal">incanter.io</code> library to read it in):</p><div class="calibre2"><pre class="programlisting">(defn load-data [file]
  (-&gt; (io/resource file)
      (str)
      (iio/read-dataset :delim \tab :header true)))

(defn ex-4-1 []
  (i/view (load-data :titanic)))</pre></div><p class="calibre11">The preceding code generates the following table:</p><div class="mediaobject"><img src="Images/7180OS_04_100.jpg" alt="Inspecting the data" class="calibre157"/></div><p class="calibre11">The Titanic dataset includes categorical variables too. For example—<span class="strong1"><strong class="calibre12">:sex</strong></span>, <span class="strong1"><strong class="calibre12">:pclass</strong></span> (the passenger class), and <span class="strong1"><strong class="calibre12">:embarked</strong></span> (a letter signifying the port of boarding). These are all string values, taking categories such as <span class="strong1"><strong class="calibre12">female</strong></span>, <span class="strong1"><strong class="calibre12">first</strong></span>, and <span class="strong1"><strong class="calibre12">C</strong></span>, but classes don't always have to be string values. Columns such as <span class="strong1"><strong class="calibre12">:ticket</strong></span>, <span class="strong1"><strong class="calibre12">:boat</strong></span>, and <span class="strong1"><strong class="calibre12">:body</strong></span> can be thought of as containing categorical variables too. Despite having numeric values, they are simply labels that have been applied to things.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note37" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">A categorical variable is one that can take on only a discrete number of values. This is in contrast to a continuous variable that can take on any value within its range.</p></div></div><p class="calibre11">Other numbers <a id="id391" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>representing counts are not so easy to define. The field <span class="strong1"><strong class="calibre12">:sibsp</strong></span> reports how many companions (spouse or siblings) were traveling with a passenger. These are counts, and their units are people. But they could just as easily represent labels, with <span class="strong1"><strong class="calibre12">0</strong></span> standing for "a passenger with no companions" and <span class="strong1"><strong class="calibre12">1</strong></span> "a passenger with one companion", and so on. There are only a small set of labels, and so the field's representation as a number is largely convenience. In other words, we could choose to represent <span class="strong1"><strong class="calibre12">:sibsp</strong></span> (and <span class="strong1"><strong class="calibre12">:parch</strong></span>—a count of related parents and children) as either categorical or numerical features.</p><p class="calibre11">Since categorical variables don't make sense on the number line, we can't plot a chart showing how these numbers relate to each other. We can construct a frequency table, though, showing how the counts of passengers in each of the groups are distributed. Since there are two sets of two variables, there are four groups in total.</p><p class="calibre11">The data can be summarized using Incanter core's <code class="literal">$rollup</code> function:</p><div class="calibre2"><pre class="programlisting">(defn frequency-table [sum-column group-columns dataset]
  (-&gt;&gt; (i/$ group-columns dataset)
       (i/add-column sum-column (repeat 1))
       (i/$rollup :sum sum-column group-columns)))

(defn ex-4-2 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (frequency-table :count [:sex :survived])))</pre></div><p class="calibre11">Incanter's <code class="literal">$rollup</code> requires that we provide three arguments—a function with which to "roll up" a group of rows, a column to roll up, and the columns whose unique values define the groups of interest. Any function that reduces a sequence to a single value can be used as a rollup function, but some are so common we can supply the keywords <code class="literal">:min</code>, <code class="literal">:max</code>, <code class="literal">:sum</code>, <code class="literal">:count</code>, and <code class="literal">:mean</code> instead.</p><p class="calibre11">The example generates the following table:</p><div class="calibre2"><pre class="programlisting">| :survived |   :sex | :count |
|-----------+--------+--------|
|         n |   male |    682 |
|         n | female |    127 |
|         y |   male |    161 |
|         y | female |    339 |</pre></div><p class="calibre11">This chart represents <a id="id392" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the frequencies of passengers falling into the various groups "males who perished", "females who survived", and so on. There are several ways of making sense of frequency counts like this; let's start with the most common.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Comparisons with relative risk and odds"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec80" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Comparisons with relative risk and odds</h1></div></div></div><p class="calibre11">The preceding <a id="id393" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Incanter dataset is an easily comprehensible representation of our data, but to extract the numbers for each of the groups individually we'll want to store the data in a more readily accessible data structure. Let's write a function to convert the dataset to a series of nested maps:</p><div class="calibre2"><pre class="programlisting">(defn frequency-map [sum-column group-cols dataset]
  (let [f (fn [freq-map row]
            (let [groups (map row group-cols)]
              (-&gt;&gt; (get row sum-column)
                   (assoc-in freq-map groups))))]
    (-&gt;&gt; (frequency-table sum-column group-cols dataset)
         (:rows)
         (reduce f {}))))</pre></div><p class="calibre11">For example, we can use the <code class="literal">frequency-map</code> function as follows to calculate a nested map of <code class="literal">:sex</code> and <code class="literal">:survived</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-3 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (frequency-map :count [:sex :survived])))

;; =&gt; {"female" {"y" 339, "n" 127}, "male" {"y" 161, "n" 682}}</pre></div><p class="calibre11">More generally, given any dataset and sequence of columns, this will make it easier to pull out just the counts we're interested in. We're going to be comparing the survival rates of males and females, so let's use Clojure's <code class="literal">get-in</code> function to extract the number of fatalities for men and women as well as the overall counts of men and women:</p><div class="calibre2"><pre class="programlisting">(defn fatalities-by-sex [dataset]
  (let [totals (frequency-map :count [:sex] dataset)
        groups (frequency-map :count [:sex :survived] dataset)]
    {:male (/ (get-in groups ["male" "n"])
              (get totals "male"))
     :female (/ (get-in groups ["female" "n"])
                (get totals "female"))}))

(defn ex-4-4 []
  (-&gt; (load-data "titanic.tsv")
      (fatalities-by-sex)))

;; {:male 682/843, :female 127/466}</pre></div><p class="calibre11">From these <a id="id394" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>numbers, we can calculate simple ratios. Relative risk is a ratio of probabilities of an event occurring in two separate groups:</p><div class="mediaobject"><img src="Images/7180OS_04_01.jpg" alt="Comparisons with relative risk and odds" class="calibre158"/></div><div class="mediaobject"><img src="Images/7180OS_04_02.jpg" alt="Comparisons with relative risk and odds" class="calibre159"/></div><p class="calibre11">Where <span class="strong1"><em class="calibre13">P(event)</em></span> is the probability of the event occurring. The risk of perishing on the Titanic as a male was <span class="strong1"><em class="calibre13">682</em></span> divided by <span class="strong1"><em class="calibre13">843</em></span>; the risk of perishing on the Titanic as a female was <span class="strong1"><em class="calibre13">127</em></span> divided by <span class="strong1"><em class="calibre13">466</em></span>:</p><div class="calibre2"><pre class="programlisting">(defn relative-risk [p1 p2]
  (float (/ p1 p2)))

(defn ex-4-5 []
  (let [proportions (-&gt; (load-data "titanic.tsv")
                        (fatalities-by-sex))]
    (relative-risk (get proportions :male)
                   (get proportions :female))))
;; 2.9685</pre></div><p class="calibre11">In other words, the risk of perishing on the Titanic was almost three times higher if you were a man. The relative risk is often used in healthcare to show how one's chances of developing an illness are affected by some other factor. A relative risk of one means that there is no difference in risk between the groups.</p><p class="calibre11">In contrast, the <a id="id395" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>odds ratio can be either positive or negative and measures the extent to which being in a group raises your odds of some other attribute. As with any correlation, no causation is implied. Both attributes could of course be linked by a third property—their mutual cause:</p><div class="mediaobject"><img src="Images/7180OS_04_03.jpg" alt="Comparisons with relative risk and odds" class="calibre160"/></div><div class="mediaobject"><img src="Images/7180OS_04_04.jpg" alt="Comparisons with relative risk and odds" class="calibre161"/></div><p class="calibre11">The odds of perishing as a male are <span class="strong1"><em class="calibre13">682</em></span>:<span class="strong1"><em class="calibre13">161</em></span> and the odds of perishing as a female are <span class="strong1"><em class="calibre13">127</em></span>:<span class="strong1"><em class="calibre13">339</em></span>. The odds ratio is simply the ratio of the two:</p><div class="calibre2"><pre class="programlisting">(defn odds-ratio [p1 p2]
  (float
   (/ (* p1 (- 1 p2))
      (* p2 (- 1 p1)))))

(defn ex-4-6 []
  (let [proportions (-&gt; (load-data "titanic.tsv")
                        (fatalities-by-sex))]
    (odds-ratio (get proportions :male)
                (get proportions :female))))
;; 11.3072</pre></div><p class="calibre11">This example shows how the odds ratio is sensitive to stating relative positions, and can generate much larger numbers.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="tip06" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Tip</h3><p class="calibre22">When presented with ratios, make sure you're aware whether they're relative-risk or odds ratios. While the two approaches appear similar, they output results over very different ranges.</p></div></div><p class="calibre11">Compare the two equations for relative risk and odds ratio. The numerators are the same in each case but <a id="id396" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>for risk the denominator is all females, whereas with the odds ratio it is females who survived.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The standard error of a proportion"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec81" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The standard error of a proportion</h1></div></div></div><p class="calibre11">It's clear that the proportion of women surviving the Titanic is much greater than the proportion of men. But, as with the dwell time differences we encountered in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Inference">Chapter 2</a>, <span class="strong1"><em class="calibre13">Inference</em></span>, we should ask ourselves whether these differences could have occurred due to chance alone.</p><p class="calibre11">We have seen in <a id="id397" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>previous chapters how to construct confidence intervals around statistics based on the sample's standard error. The standard error is based on the sample's variance, but what is the variance of a proportion? No matter how many samples we take, only one proportion will be generated—the proportion in the overall sample.</p><p class="calibre11">Clearly a proportion is still subject to some sort of variance. When we flip a fair coin 10 times we would expect to get roughly five heads, but there's it's not impossible we'd get ten heads in a row.</p><div class="calibre2" title="Estimation using bootstrapping"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec62" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Estimation using bootstrapping</h2></div></div></div><p class="calibre11">In <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Inference">Chapter 2</a>, <span class="strong1"><em class="calibre13">Inference</em></span>, we learned about bootstrapping statistics such as the mean and we saw how bootstrapping can be a useful way of estimating parameters through simulation. Let's use bootstrapping to estimate the standard error of the proportion of female passengers surviving the Titanic.</p><p class="calibre11">We can <a id="id398" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represent the 466 female passengers as a sequence of zeros and ones. Zero could represent a passenger who perished, and one a passenger who survived. This is a convenient representation because it means the sum of the whole sequence equals the total number of passengers who survived. By taking repeated random samples of 466 elements from this sequence of 466 zeros and ones, and taking the sum each time, we can get an estimate of the variance in the proportion:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-7 []
  (let [passengers (concat (repeat 127 0)
                           (repeat 339 1))
        bootstrap (s/bootstrap passengers i/sum :size 10000)]
    (-&gt; (c/histogram bootstrap
                     :x-label "Female Survivors"
                     :nbins 20)
        (i/view))))</pre></div><p class="calibre11">The preceding <a id="id399" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>code generates the following histogram:</p><div class="mediaobject"><img src="Images/7180OS_04_110.jpg" alt="Estimation using bootstrapping" class="calibre45"/></div><p class="calibre11">The histogram appears to show a normal distribution with a mean of 339—the measured number of female <a id="id400" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>survivors. The standard deviation of this distribution is the standard error of the sampled survivors and we can calculate it simply from the bootstrapped samples like so:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-8 []
  (-&gt; (concat (repeat 127 0)
              (repeat 339 1))
      (s/bootstrap i/sum :size 10000)
      (s/sd)))

;; 9.57</pre></div><p class="calibre11">Your standard deviation may be slightly different, depending on chance variation in the bootstrapped sample. It should be very close, though.</p><p class="calibre11">The units of standard deviation are people—female passengers—so to figure out the standard error of the <a id="id401" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proportion we have to divide this through by the total number of passengers in our sample, 466. This yields a standard error of the proportion of 0.021.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="The binomial distribution"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec82" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The binomial distribution</h1></div></div></div><p class="calibre11">The preceding <a id="id402" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>histogram looks a great deal like a normal distribution, but in fact it is a binomial distribution. The two distributions are very similar, but the binomial distribution is used to model cases where we want to determine how many times a binary event is expected to occur.</p><p class="calibre11">Let's plot both the binomial and the normal distribution on a histogram to see how they compare:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-9 []
  (let [passengers (concat (repeat 127 0)
                           (repeat 339 1))
        bootstrap (s/bootstrap passengers i/sum :size 10000)
        binomial (fn [x]
                   (s/pdf-binomial x :size 466 :prob (/ 339 466)))
        normal (fn [x]
                 (s/pdf-normal x :mean 339 :sd 9.57))]
    (-&gt; (c/histogram bootstrap
                     :x-label "Female Survivors"
                     :series-label "Bootstrap"
                     :nbins 20
                     :density true
                     :legend true)
        (c/add-function binomial 300 380
                        :series-label "Biomial")
        (c/add-function normal 300 380
                        :series-label "Normal")
        (i/view))))</pre></div><p class="calibre11">The preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_04_120.jpg" alt="The binomial distribution" class="calibre45"/></div><p class="calibre11">Notice how in the <a id="id403" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding chart the line corresponding to the binomial distribution is jagged—it represents discrete counts of things rather than a continuous value such as the normal distribution.</p><div class="calibre2" title="The standard error of a proportion formula"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec63" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The standard error of a proportion formula</h2></div></div></div><p class="calibre11">We have <a id="id404" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated the standard error empirically and found it to equal 0.021, using only the proportion of female survivors and the total number of female passengers. Although it's been instructive to see what the standard error of the proportion is actually measuring, there is a formula that allows us to get there in one step:</p><div class="mediaobject"><img src="Images/7180OS_04_05.jpg" alt="The standard error of a proportion formula" class="calibre162"/></div><p class="calibre11">Substituting in the counts of female survivors gives us the following:</p><div class="mediaobject"><img src="Images/7180OS_04_06.jpg" alt="The standard error of a proportion formula" class="calibre163"/></div><p class="calibre11">Fortunately, this <a id="id405" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number closely matches the standard error we calculated through bootstrapping. It's not exact, of course, since our bootstrapping calculation has its own sampling error.</p><div class="calibre2"><pre class="programlisting">(defn standard-error-proportion [p n]
  (-&gt; (- 1 p)
      (* p)
      (/ n)
      (i/sqrt)))

(defn ex-4-10 []
  (let [survived (-&gt;&gt; (load-data "titanic.tsv")
                      (frequency-map :count [:sex :survived]))
        n (reduce + (vals (get survived "female")))
        p (/ (get-in survived ["female" "y"]) n)]
    (se-proportion p n)))

;; 0.0206</pre></div><p class="calibre11">The equation for the standard error of a proportion gives us an important insight—the value of <span class="strong1"><em class="calibre13">p(1 - p)</em></span> is greatest when <span class="strong1"><em class="calibre13">p</em></span> is close to 0.5. This means that the greatest standard error in a proportion is when the proportion is close to a half.</p><p class="calibre11">If this seems surprising to you, consider this—when the proportion is 50 percent, the variation in the sample is greatest. Like a fair coin toss, we have no way of predicting what the next value will be. As the proportion increases (or decreases) within the sample, the data becomes increasingly homogenous. As a result, the variation decreases, and so the standard error decreases accordingly.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Significance testing proportions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec83" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Significance testing proportions</h1></div></div></div><p class="calibre11">Let's return <a id="id406" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to the question of whether the measured differences in male or female fatality rates could be due to chance alone. As in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Inference">Chapter 2</a>, <span class="strong1"><em class="calibre13">Inference</em></span>, our <span class="strong1"><em class="calibre13">z</em></span>-test is simply the difference in proportions divided by the pooled standard error:</p><div class="mediaobject"><img src="Images/7180OS_04_07.jpg" alt="Significance testing proportions" class="calibre164"/></div><p class="calibre11">In the preceding formula, <span class="strong1"><em class="calibre13">p</em></span><sub class="calibre25">1</sub> denotes the proportion of women who survived, that is, <span class="strong1"><em class="calibre13">339/466 = 0.73</em></span>. And <span class="strong1"><em class="calibre13">p</em></span><sub class="calibre25">2</sub> denotes the proportion of men who survived, that is, <span class="strong1"><em class="calibre13">161/843 = 0.19</em></span>.</p><p class="calibre11">To calculate the <span class="strong1"><em class="calibre13">z</em></span>-statistic, we need to pool our standard errors for the two proportions. Our proportions measure the survival rates of males and females respectively, so the pooled standard error is simply the standard error of the males and females combined, or the total survival rate overall, as follows:</p><div class="mediaobject"><img src="Images/7180OS_04_08.jpg" alt="Significance testing proportions" class="calibre165"/></div><p class="calibre11">Substituting the values into the equation for the <span class="strong1"><em class="calibre13">z</em></span>-statistic:</p><div class="mediaobject"><img src="Images/7180OS_04_09.jpg" alt="Significance testing proportions" class="calibre166"/></div><p class="calibre11">Using a <span class="strong1"><em class="calibre13">z</em></span>-score means we'll use the normal distribution to look up the <span class="strong1"><em class="calibre13">p</em></span>-value:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-11 []
  (let [dataset     (load-data "titanic.tsv")
        proportions (fatalities-by-sex dataset)
        survived    (frequency-map :count [:survived] dataset)
        total  (reduce + (vals survived))
        pooled (/ (get survived "n") total)
        p-diff (- (get proportions :male)
                  (get proportions :female))
        z-stat (/ p-diff (se-proportion pooled total))]
    (- 1 (s/cdf-normal (i/abs z-stat)))))

;; 0.0</pre></div><p class="calibre11">As we have a one-tailed test, the <span class="strong1"><em class="calibre13">p</em></span>-value is the probability that the <span class="strong1"><em class="calibre13">z</em></span>-score is less than 39.95. The response is zero, corresponding to a very, very significant result. This allows us to reject the <a id="id407" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>null hypothesis and conclude that the difference between survival rates between men and women was certainly not down to chance alone.</p><div class="calibre2" title="Adjusting standard errors for large samples"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec64" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Adjusting standard errors for large samples</h2></div></div></div><p class="calibre11">You may <a id="id408" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>be wondering why we're talking about standard errors at all. The data we have on passengers on the Titanic is not a sample of a wider population. It is the population. There was only one Titanic and only one fateful journey.</p><p class="calibre11">While this is true in one sense, there are many ways in which the Titanic disaster could have occurred. If the "women and children first" instructions had not been followed or had been followed more universally, a different set of results would have been obtained. If there had been enough lifeboats for everyone, or the evacuation process had run more smoothly, then this would have been represented in the outcome too.</p><p class="calibre11">Standard error and significance testing allows us to treat the disaster as one of an infinite number of potential similar disasters and determine whether the observed differences are likely to have been systemic or purely coincidental.</p><p class="calibre11">That said, sometimes we are more interested in how confident we can be that our samples are representative of a finite, quantified population. Where samples begin to measure more than about 10 percent of the population, we can adjust the standard error downwards to account for the decreased uncertainty:</p><div class="mediaobject"><img src="Images/7180OS_04_10.jpg" alt="Adjusting standard errors for large samples" class="calibre167"/></div><p class="calibre11">This can be written in Clojure as:</p><div class="calibre2"><pre class="programlisting">(defn se-large-proportion [p n N]
  (* (se-proportion p n)
     (i/sqrt (/ (- N n)
                (- n 1)))))</pre></div><p class="calibre11">Where <span class="strong1"><em class="calibre13">N</em></span> is the size of the overall population. As the sample size increases relative to the size of the <a id="id409" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>population, <span class="strong1"><em class="calibre13">(N - n)</em></span> tends towards zero. If you sample the entire population, then any difference in proportion—however small—is going to be judged significant.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chi-squared multiple significance testing"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec84" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chi-squared multiple significance testing</h1></div></div></div><p class="calibre11">Not all <a id="id410" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>categories are dichotomous (such as male and female, survived and perished). Although we would expect categorical variables to have a finite number of categories, there is no hard upper limit on the number of categories a particular attribute can have.</p><p class="calibre11">We could use other categorical variables to separate out the passengers on the Titanic, such as the class in which they were traveling. There were three class levels on the Titanic, and the <code class="literal">frequency-table</code> function we constructed at the beginning of this chapter is already able to handle multiple classes.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-12 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (frequency-table :count [:survived :pclass])))</pre></div><p class="calibre11">This code generates the following frequency table:</p><div class="calibre2"><pre class="programlisting">| :pclass | :survived | :count |
|---------+-----------+--------|
|   third |         y |    181 |
|   third |         n |    528 |
|  second |         y |    119 |
|  second |         n |    158 |
|   first |         n |    123 |
|   first |         y |    200 |</pre></div><p class="calibre11">These three classes give us an additional way to cut our data on survival rates. As the number of classes increases, it becomes harder to read patterns in the frequency table, so let's visualize it.</p><div class="calibre2" title="Visualizing the categories"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec65" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Visualizing the categories</h2></div></div></div><p class="calibre11">Although they <a id="id411" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>were originally devised to represent proportions, pie charts are generally not a good way to represent parts of a whole. People have a difficult time visually comparing the areas of slices of a circle. Representing quantities linearly, as with a stacked bar chart, is nearly always a better approach. Not only are the areas easier to interpret but they're easier to compare side by side.</p><p class="calibre11">We can visualize our data as a stacked bar chart:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-13 []
  (let [data (-&gt;&gt; (load-data "titanic.tsv")
                  (frequency-table :count [:survived :pclass]))]
    (-&gt; (c/stacked-bar-chart :pclass :count
                             :group-by :survived
                             :legend true
                             :x-label "Class"
                             :y-label "Passengers"
                             :data data)
        (i/view))))</pre></div><p class="calibre11">The <a id="id412" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding code generates the following chart:</p><div class="mediaobject"><img src="Images/7180OS_04_130.jpg" alt="Visualizing the categories" class="calibre45"/></div><p class="calibre11">The data clearly shows a difference in both the number of passengers who perished, and the proportion of passengers who perished, most visible between first and third class. We'd like to determine if this difference is significant.</p><p class="calibre11">We could <a id="id413" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>perform a <span class="strong1"><em class="calibre13">z</em></span>-test between each pair of proportions but, as we learned in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch02.xhtml" title="Chapter 2. Inference">Chapter 2</a>, <span class="strong1"><em class="calibre13">Inference</em></span>, this is much more likely to lead to Type I errors and cause us to find a significant result where, in fact, there is none.</p><p class="calibre11">The problem of multiple-category significance testing may seem to call for the <span class="strong1"><em class="calibre13">F</em></span>-test but the <span class="strong1"><em class="calibre13">F</em></span>-test is based on the ratio of variance of some continuous variable within and between groups. What we'd like, therefore, is a similar test that cares only about the relative proportion between groups. This is the premise on which the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test is based.</p></div><div class="calibre2" title="The chi-squared test"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec66" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The chi-squared test</h2></div></div></div><p class="calibre11">Pronounced <span class="strong1"><em class="calibre13">kai square</em></span>, the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test is a statistical test applied to sets of categorical data to evaluate <a id="id414" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how likely it is that any observed difference between proportions of those categories in the sets arose by chance.</p><p class="calibre11">When performing <a id="id415" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test, therefore, our null hypothesis is that the observed difference in proportions between groups is simply the result of chance variation. We can think of this as an independence test between two categorical variables. If category <span class="strong1"><em class="calibre13">A</em></span> is the passenger class and category <span class="strong1"><em class="calibre13">B</em></span> is whether they survived or not, the null hypothesis is that passenger class and survival rate are independent of each other. The alternate hypothesis is that the categories are not independent—that the passenger class and survival are related to each other in some way.</p><p class="calibre11">The <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> statistic is calculated by comparing the observed frequency counts from the sample to a table of frequencies calculated under the assumption of independence. This frequency table is an estimation of what the data would have looked like had the categories been independent. We can calculate the frequency table assuming independence in the following way, using the row, column, and grand totals:</p><div class="informaltable"><table border="1" class="calibre27"><colgroup class="calibre28"><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/><col class="calibre29"/></colgroup><thead class="calibre30"><tr class="calibre31"><th valign="bottom" class="calibre32"> </th><th valign="bottom" class="calibre32">
<p class="calibre19">Survived</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Perished</p>
</th><th valign="bottom" class="calibre32">
<p class="calibre19">Total</p>
</th></tr></thead><tbody class="calibre33"><tr class="calibre34"><td class="calibre35">
<p class="calibre19">First Class</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">323*500/1309 = 123.4</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">323*809/1309 = 199.6</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">323</em></span>
</p>
</td></tr><tr class="calibre36"><td class="calibre35">
<p class="calibre19">Second Class</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">277*500/1309 = 105.8</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">277*809/1309 = 171.2</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">277</em></span>
</p>
</td></tr><tr class="calibre34"><td class="calibre35">
<p class="calibre19">Third Class</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">709*500/1309 = 270.8</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">709*809/1309 = 438.2</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">709</em></span>
</p>
</td></tr><tr class="calibre51"><td class="calibre35">
<p class="calibre19">Total</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">500</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">809</em></span>
</p>
</td><td class="calibre35">
<p class="calibre19">
<span class="strong1"><em class="calibre13">1,309</em></span>
</p>
</td></tr></tbody></table></div><p class="calibre11">A simple formula calculates each cell value using only the totals for each row and <a id="id416" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>column, and assumes an even <a id="id417" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distribution amongst cells. This is our table of expected frequencies.</p><div class="calibre2"><pre class="programlisting">(defn expected-frequencies [data]
  (let [as (vals (frequency-map :count [:survived] data))
        bs (vals (frequency-map :count [:pclass] data))
        total (-&gt; data :rows count)]
    (for [a as
          b bs]
      (* a (/ b total)))))

(defn ex-4-14 []
  (-&gt; (load-data "titanic.tsv")
      (expected-frequencies)))

;; =&gt; (354500/1309 138500/1309 9500/77 573581/1309 224093/1309 15371/77)</pre></div><p class="calibre11">To demonstrate a statistically significant difference between the survival rates by class, we'll need to show that the difference between the frequencies assuming independence and the observed frequencies is unlikely to have arisen through chance alone.</p></div><div class="calibre2" title="The chi-squared statistic"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec67" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The chi-squared statistic</h2></div></div></div><p class="calibre11">The <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup><a id="id418" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>statistic simply measures <a id="id419" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>how far the actual frequencies differ from those calculated under the assumption of independence:</p><div class="mediaobject"><img src="Images/7180OS_04_11.jpg" alt="The chi-squared statistic" class="calibre168"/></div><p class="calibre11">
<span class="strong1"><em class="calibre13">F</em></span><sub class="calibre25">ij</sub> is the expected frequency assuming independence for categories <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">j</em></span>, and <span class="strong1"><em class="calibre13">f</em></span><sub class="calibre25">ij</sub> is the observed frequency for categories <span class="strong1"><em class="calibre13">i</em></span> and <span class="strong1"><em class="calibre13">j</em></span>. We therefore need to fetch the observed frequencies for our data. We can calculate this in Clojure as follows:</p><div class="calibre2"><pre class="programlisting">(defn observed-frequencies [data]
  (let [as (-&gt;&gt; (i/$rollup :sum :count :survived data)
                (summary :count [:survived]))
        bs (-&gt;&gt; (i/$rollup :sum :count :pclass data)
                (summary :count [:pclass]))
        actual (summary :count [:survived :pclass] data)]
    (for [a (keys as)
          b (keys bs)]
      (get-in actual [a b]))))</pre></div><p class="calibre11">As with the <code class="literal">expected-frequencies</code> function earlier, the <code class="literal">observed-frequencies</code> function returns a sequence of frequency counts for each combination of categories.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-15 []
  (-&gt; (load-data "titanic.tsv")
      (observed-frequencies)))

;; (200 119 181 123 158 528)</pre></div><p class="calibre11">This <a id="id420" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sequence—and the sequence <a id="id421" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of expected values from the previous example—give us all we need to calculate the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> statistic:</p><div class="calibre2"><pre class="programlisting">(defn chisq-stat [observed expected]
  (let [f (fn [observed expected]
            (/ (i/sq (- observed expected)) expected))]
    (reduce + (map f observed expected))))

(defn ex-4-16 []
  (let [data (load-data "titanic.tsv")
        observed (observed-frequencies data)
        expected (expected-frequencies data)]
    (float (chisq-stat observed expected))))

;; 127.86</pre></div><p class="calibre11">Now that we have our test statistic, we'll need to look this up in the relevant distribution to determine if the result is significant. Unsurprisingly, the distribution we refer to is the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2 </sup>distribution.</p></div><div class="calibre2" title="The chi-squared test"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec68" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The chi-squared test</h2></div></div></div><p class="calibre11">The <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup><a id="id422" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distribution is paramaterized by <a id="id423" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>one degree of freedom: the product of each of the category counts less one:</p><div class="mediaobject"><img src="Images/7180OS_04_12.jpg" alt="The chi-squared test" class="calibre169"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">a</em></span> is the number of categories for attribute <span class="strong1"><em class="calibre13">A</em></span> and <span class="strong1"><em class="calibre13">b</em></span> is the number of categories for attribute <span class="strong1"><em class="calibre13">B</em></span>. For our Titanic data, <span class="strong1"><em class="calibre13">a</em></span> is <span class="strong1"><em class="calibre13">3</em></span> and <span class="strong1"><em class="calibre13">b</em></span> is <span class="strong1"><em class="calibre13">2</em></span>, so our degrees of freedom parameter is <span class="strong1"><em class="calibre13">2</em></span>.</p><p class="calibre11">Our <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test <a id="id424" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simply needs to view our <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> statistic against the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> cumulative distribution function (CDF). Let's do this now:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-17 []
  (let [data (load-data "titanic.tsv")
        observed (observed-frequencies data)
        expected (expected-frequencies data)
        x2-stat  (chisq-stat observed expected)]
    (s/cdf-chisq x2-stat :df 2 :lower-tail? false)))

;; 1.721E-28</pre></div><p class="calibre11">This is an absolutely tiny number, and is as close to zero as makes no difference so we can comfortably reject the null hypothesis at any significance level. In other words, we can be absolutely certain that the observed difference is not the result of a chance sampling error.</p><p class="calibre11">Although it is useful to see the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> conducted by hand, the Incanter stats namespace has a function, <code class="literal">chisq-test</code>, for conducting the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test in one step. To use it we simply need to supply our original table of observations as a matrix:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-18 []
  (let [table  (-&gt;&gt; (load-data "titanic.tsv")
                    (frequency-table :count [:pclass :survived])
                    (i/$order [:survived :pclass] :asc))
        frequencies (i/$ :count table)
        matrix      (i/matrix frequencies 3)]
    (println "Observed:"     table)
    (println "Frequencies:"  frequencies)
    (println "Observations:" matrix)
    (println "Chi-Squared test:")
    (-&gt; (s/chisq-test :table matrix)
        (clojure.pprint/pprint))))</pre></div><p class="calibre11">In preceding the code, we calculated a frequency-table from the Titanic data and then ordered the contents, using <code class="literal">i/$order</code>, so that we get a table like this:</p><div class="calibre2"><pre class="programlisting">| :survived | :pclass | :count |
|-----------+---------+--------|
|         n |   first |    123 |
|         n |  second |    158 |
|         n |   third |    528 |
|         y |   first |    200 |
|         y |  second |    119 |
|         y |   third |    181 |</pre></div><p class="calibre11">We take the count column and convert it into a matrix of three columns using <code class="literal">(i/matrix frequencies 3)</code>:</p><div class="calibre2"><pre class="programlisting">A 2x3 matrix
 -------------
 1.23e+02  1.58e+02  5.28e+02
 2.00e+02  1.19e+02  1.81e+02</pre></div><p class="calibre11">This matrix is the <a id="id425" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>only input required by <a id="id426" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Incanter's <code class="literal">s/chisq-test</code> function. Run the example and you'll see the response is a map containing keys <code class="literal">:X-sq</code>, the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> statistic, and <code class="literal">:p-value</code>, the result of the test, amongst many others.</p><p class="calibre11">We have established that the categories of class and survived, and gender and survived are certainly not independent. This is analogous to discovering a correlation between variables—height, sex, and weight—in the previous chapter.</p><p class="calibre11">Now, as then, the next step is to use the dependence between the variables to make predictions. Whereas in the previous chapter our output was a predicted number—the weight—in this chapter our output will be a class—a prediction about whether the passenger survived or not. Assigning items to their expected class based on other attributes is the process of classification.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Classification with logistic regression"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec85" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Classification with logistic regression</h1></div></div></div><p class="calibre11">In the previous <a id="id427" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>chapter, we saw how linear regression produces a predicted value, <span class="strong1"><em class="calibre13">ŷ</em></span>, from an input vector <span class="strong1"><em class="calibre13">x</em></span> and a vector of coefficients <span class="strong1"><em class="calibre13">β</em></span>:</p><div class="mediaobject"><img src="Images/7180OS_04_13.jpg" alt="Classification with logistic regression" class="calibre148"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">ŷ</em></span> can be any real number. Logistic regression proceeds in a very similar way, but adjusts the prediction to guarantee an answer only between zero and one:</p><div class="mediaobject"><img src="Images/7180OS_04_14.jpg" alt="Classification with logistic regression" class="calibre170"/></div><p class="calibre11">Zero and one represent two different classes. The change is a simple one; we simply wrap the prediction in a function <span class="strong1"><em class="calibre13">g</em></span> that constrains the output between zero and one:</p><div class="mediaobject"><img src="Images/7180OS_04_15.jpg" alt="Classification with logistic regression" class="calibre171"/></div><p class="calibre11">Where <span class="strong1"><em class="calibre13">g</em></span> is called the <span class="strong1"><strong class="calibre12">sigmoid </strong></span><a id="id428" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">function</strong></span>. This seemingly minor change is enough to transform linear <a id="id429" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression into logistic regression and turn real-valued predictions into classes.</p><div class="calibre2" title="The sigmoid function"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec69" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The sigmoid function</h2></div></div></div><p class="calibre11">The sigmoid <a id="id430" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function is also referred to as <a id="id431" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <span class="strong1"><em class="calibre13">logistic function</em></span> and is shown next:</p><div class="mediaobject"><img src="Images/7180OS_04_16.jpg" alt="The sigmoid function" class="calibre172"/></div><p class="calibre11">For positive inputs, the logistic function rises quickly to one while, for negative inputs, it falls quickly to zero. These outputs correspond to the predicted classes. For values close to zero, the logistic function returns values close to <span class="strong1"><strong class="calibre12">0.5</strong></span>. This corresponds to increased uncertainty about the correct output class.</p><div class="mediaobject"><img src="Images/7180OS_04_140.jpg" alt="The sigmoid function" class="calibre173"/></div><p class="calibre11">Combining the formulae we have seen already gives rise to the following complete definition of the <a id="id432" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>logistic hypothesis:</p><div class="mediaobject"><img src="Images/7180OS_04_17.jpg" alt="The sigmoid function" class="calibre174"/></div><p class="calibre11">As with linear regression, the parameter vector <span class="strong1"><em class="calibre13">β</em></span> contains the coefficients that we're seeking to learn, and <span class="strong1"><em class="calibre13">x</em></span> is <a id="id433" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our vector of input features. We can express this in Clojure with the following higher-order function. Given a vector of coefficients, this function returns a function that will calculate <span class="strong1"><em class="calibre13">ŷ</em></span> for a given <span class="strong1"><em class="calibre13">x</em></span>:</p><div class="calibre2"><pre class="programlisting">(defn sigmoid-function [coefs]
  (let [bt (i/trans coefs)
        z  (fn [x] (- (first (i/mmult bt x))))]
    (fn [x]
      (/ 1
         (+ 1
            (i/exp (z x)))))))</pre></div><p class="calibre11">If the logistic function is given a <span class="strong1"><em class="calibre13">β</em></span> of <code class="literal">[0]</code>, then the feature is discounted as having any predictive power. The function will output <code class="literal">0.5</code>, corresponding to complete uncertainty, for any input <span class="strong1"><em class="calibre13">x</em></span>:</p><div class="calibre2"><pre class="programlisting">(let [f (sigmoid-function [0])]
  (f [1])
  ;; =&gt; 0.5

  (f [-1])
  ;; =&gt; 0.5

  (f [42])
  ;; =&gt; 0.5
  )</pre></div><p class="calibre11">However, if values other than zero are provided as coefficients, the sigmoid function can return values other than <code class="literal">0.5</code>. A positive <span class="strong1"><em class="calibre13">β</em></span> will result in a greater probability of a positive class given a positive <span class="strong1"><em class="calibre13">x</em></span>, whereas a negative <span class="strong1"><em class="calibre13">β</em></span> will correspond to a greater probability of a negative class given a <a id="id434" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>positive <span class="strong1"><em class="calibre13">x</em></span>.</p><div class="calibre2"><pre class="programlisting">(let [f (sigmoid-function [0.2])
      g (sigmoid-function [-0.2])]
  (f [5])
  ;; =&gt; 0.73

  (g [5])
  ;; =&gt; 0.27
  )</pre></div><p class="calibre11">Since values above <code class="literal">0.5</code> correspond to a positive class and values less than <code class="literal">0.5</code> correspond to a negative class, the sigmoid function output can simply be rounded to the nearest integer to get the output class. This would result in values of exactly <code class="literal">0.5</code> being classified as the positive class.</p><p class="calibre11">Now that we have a <code class="literal">sigmoid-function</code> that can return class predictions, we need to learn the parameters <span class="strong1"><em class="calibre13">β</em></span> which yield the best predictions <span class="strong1"><em class="calibre13">ŷ</em></span>. In the previous chapter, we saw two methods for calculating the coefficients for a linear model—calculating the slope and intercept using covariance, and the normal equation using matrices. In both cases the equations were able to find a linear solution that minimized the least-squares estimates of our model.</p><p class="calibre11">The squared error was an <a id="id435" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>appropriate function to use for our linear model, but it doesn't translate well to classification where classes are measured only between zero and one. We need an alternative method of determining how incorrect our predictions are.</p></div><div class="calibre2" title="The logistic regression cost function"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec70" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The logistic regression cost function</h2></div></div></div><p class="calibre11">As with <a id="id436" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>linear regression, the <a id="id437" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>logistic regression algorithm must learn from data. The <code class="literal">cost</code> function is a way to let the algorithm know how well, or poorly, it's doing.</p><p class="calibre11">The following is the <code class="literal">cost</code> function for logistic regression, which imposes a different cost depending on whether the output class is supposed to be zero or one. The cost for a single training example is calculated like so:</p><div class="mediaobject"><img src="Images/7180OS_04_18.jpg" alt="The logistic regression cost function" class="calibre175"/></div><p class="calibre11">This pair of functions captures the intuition that, if <span class="strong1"><em class="calibre13">ŷ</em></span> = 0 but <span class="strong1"><em class="calibre13">y</em></span> = 1, then the model should be penalized by a very large cost. Symmetrically, the model should also be heavily penalized if <span class="strong1"><em class="calibre13">ŷ</em></span> = 1 and <span class="strong1"><em class="calibre13">y</em></span> = 0. Where the model closely agrees with the data, the cost falls steeply towards zero.</p><p class="calibre11">This is the cost for an individual training point. To combine the individual costs and calculate an overall cost for a given vector of coefficients and a set of training data, we can simply take the average across all the training examples:</p><div class="mediaobject"><img src="Images/7180OS_04_19.jpg" alt="The logistic regression cost function" class="calibre176"/></div><p class="calibre11">This can be <a id="id438" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>represented in Clojure as follows:</p><div class="calibre2"><pre class="programlisting">(defn logistic-cost [ys y-hats]
  (let [cost (fn [y y-hat]
               (if (zero? y)
                 (- (i/log (- 1 y-hat)))
                 (- (i/log y-hat))))]
    (s/mean (map cost ys y-hats))))</pre></div><p class="calibre11">Now that we have a <code class="literal">cost</code> function that can quantify how incorrect our predictions are, the next step is to make use of this information to figure out better predictions. The very best classifier will be the one with the lowest overall cost, since by definition its predicted classes will be closest to the true classes. The method by which we can incrementally improve our cost is called <a id="id439" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">gradient descent</strong></span>.</p></div><div class="calibre2" title="Parameter optimization with gradient descent"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec71" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Parameter optimization with gradient descent</h2></div></div></div><p class="calibre11">The cost <a id="id440" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function, also called the <a id="id441" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">loss function</strong></span>, is the function that calculates the error of the model based on our coefficients. Different parameters will generate different costs for the same dataset, and we can visualize how the cost function changes with respect to the parameters on a graph.</p><div class="mediaobject"><img src="Images/7180OS_04_150.jpg" alt="Parameter optimization with gradient descent" class="calibre177"/></div><p class="calibre11">The preceding chart shows a representation of a cost function for a two-parameter model. The cost is plotted on the <span class="strong1"><em class="calibre13">y</em></span> axis (higher values correspond to a higher cost) and the two parameters are plotted on the <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">z</em></span> axes, respectively.</p><p class="calibre11">The best <a id="id442" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameters are the ones that minimize the cost function, corresponding to the parameters at the point identified as the "Global minimum". We don't know ahead of time what these parameters will be, but we can make an initial, arbitrary guess. These parameters are the ones identified by the point "P".</p><p class="calibre11">Gradient descent is an algorithm that iteratively improves on the initial condition by following the gradient downhill towards the minimum value. When the algorithm can't descend any <a id="id443" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>further, the minimum cost has been found. The parameters at this point correspond to our best estimate for the parameters that minimize the cost function.</p></div><div class="calibre2" title="Gradient descent with Incanter"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec72" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Gradient descent with Incanter</h2></div></div></div><p class="calibre11">Incanter <a id="id444" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>provides the ability to run gradient descent with the function <code class="literal">minimize</code> in the <code class="literal">incanter.optimize</code> namespace. Mathematical optimization is the general term for a series of techniques <a id="id445" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that aim to find the best available solution to some set of constraints. The <code class="literal">incanter.optimize</code> namespace contains functions for calculating the parameters that will minimize or maximize the value of any arbitrary function.</p><p class="calibre11">For example, the following code finds the minimum value of <code class="literal">f</code> given a starting position of <code class="literal">10</code>. Since <code class="literal">f</code> is <span class="strong1"><em class="calibre13">x</em></span><sup class="calibre42">2</sup>, the input that will produce the minimum value is <code class="literal">0</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-19 []
  (let [f (fn [[x]]
            (i/sq x))
        init [10]]
    (o/minimize f init)))</pre></div><p class="calibre11">Indeed, if you run the example you should get an answer very close to zero. You are very unlikely to get exactly zero though because gradient descent tends to provide only approximate answers—Incanter's <code class="literal">minimize</code> function accepts a tolerance argument <code class="literal">:tol</code> that defaults to 0.00001. If the result differs by less than this amount between iterations, then the equation is said to have converged. The function also accepts a <code class="literal">:max-iter</code> argument, the maximum number of steps to take before returning an answer, irrespective of convergence.</p></div><div class="calibre2" title="Convexity"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec73" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Convexity</h2></div></div></div><p class="calibre11">Gradient descent is <a id="id446" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>not always guaranteed to find the lowest possible cost for all equations. For example, the answer may find what is called a "local minimum", which represents the lowest cost in the vicinity of the initial guess but doesn't represent the best overall solution to the problem. This is illustrated in the following illustration:</p><div class="mediaobject"><img src="Images/7180OS_04_160.jpg" alt="Convexity" class="calibre178"/></div><p class="calibre11">If the initial position corresponds to either of the points labeled <span class="strong1"><strong class="calibre12">C</strong></span> on the graph, then the algorithm will converge to a local minimum. Gradient descent will have found a minimum, but it is not the best overall solution. Only initial guesses within the range <span class="strong1"><strong class="calibre12">A</strong></span> to <span class="strong1"><strong class="calibre12">B</strong></span> will converge on the global minimum.</p><p class="calibre11">It is therefore <a id="id447" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>possible that gradient descent will converge to different answers depending on its initialization. For gradient descent to guarantee the optimal solution, the equation to optimize needs to be a convex equation. This means that there is a single global minimum and no local minima.</p><p class="calibre11">For example, there is no global minimum of the <code class="literal">sin</code> function. The result we calculate for the minimum will depend strongly on our starting conditions:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-20 []
  (let [f (fn [[x]]
            (i/sin x))]
    (println (:value (o/minimize f [1])))
    (println (:value (o/minimize f [10])))
    (println (:value (o/minimize f [100])))))

A 1x1 matrix
 -------------
-2.14e+05

 A 1x1 matrix
 -------------
 1.10e+01

 A 1x1 matrix
 -------------
 9.90e+01</pre></div><p class="calibre11">Fortunately, logistic <a id="id448" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression is a convex function. This means that gradient descent will be able to determine the values of our coefficients corresponding to the global minimum irrespective of our starting position.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Implementing logistic regression with Incanter"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec86" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Implementing logistic regression with Incanter</h1></div></div></div><p class="calibre11">We can <a id="id449" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>define a logistic regression <a id="id450" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function with Incanter's <code class="literal">minimize</code> function as follows:</p><div class="calibre2"><pre class="programlisting">(defn logistic-regression [ys xs]
  (let [cost-fn (fn [coefs]
                  (let [classify (sigmoid-function coefs)
                        y-hats   (map (comp classify i/trans) xs)]
                    (logistic-cost ys y-hats)))
        init-coefs (repeat (i/ncol xs) 0.0)]
    (o/minimize cost-fn init-coefs)))</pre></div><p class="calibre11">The <code class="literal">cost-fn</code> accepts a matrix of coefficients. We create a classifier from the coefficients using the <code class="literal">sigmoid-function</code> previously defined, and a sequence of predictions, <code class="literal">y-hats</code>, based on the input data. Finally, we can calculate and return the <code class="literal">logistic-cost</code> value based on the provided coefficients.</p><p class="calibre11">To perform logistic regression, we minimize the logistic <code class="literal">cost-fn</code> by selecting the optimal parameters to the <code class="literal">sigmoid-function</code>. Since we have to start somewhere, our initial coefficients are simply <code class="literal">0.0</code> for each parameter.</p><p class="calibre11">The <code class="literal">minimize</code> function expects to receive an input in numeric form. Like the athlete data in the previous chapter, we have to convert our Titanic data into a feature matrix and create dummy variables for our categorical data.</p><div class="calibre2" title="Creating a feature matrix"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec74" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Creating a feature matrix</h2></div></div></div><p class="calibre11">Let's <a id="id451" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>define a <a id="id452" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function, <code class="literal">add-dummy</code>, that will create a dummy variable for a given column. Where the value in the input column equals a particular value, the dummy column will contain a <code class="literal">1</code>. Where the value in the input column does not contain that value, the dummy column will be <code class="literal">0</code>.</p><div class="calibre2"><pre class="programlisting">(defn add-dummy [column-name from-column value dataset]
  (i/add-derived-column column-name
                        [from-column]
                        #(if (= % value) 1 0)
                        dataset))</pre></div><p class="calibre11">This simple <a id="id453" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function makes it very straightforward to convert our Titanic data to a feature matrix:</p><div class="calibre2"><pre class="programlisting">(defn matrix-dataset []
  (-&gt;&gt; (load-data "titanic.tsv")
       (add-dummy :dummy-survived :survived "y")
       (i/add-column :bias (repeat 1.0))
       (add-dummy :dummy-mf :sex "male")
       (add-dummy :dummy-1 :pclass "first")
       (add-dummy :dummy-2 :pclass "second")
       (add-dummy :dummy-3 :pclass "third")
       (i/$ [:dummy-survived :bias :dummy-mf
             :dummy-1 :dummy-2 :dummy-3])
       (i/to-matrix)))</pre></div><p class="calibre11">Our output matrix will entirely consist of zeros and ones. The first element in the feature matrix is the dummy variable determining survival. This is our class label. <code class="literal">0</code> corresponds to perishing and <code class="literal">1</code> corresponds to survival. The second is a <code class="literal">bias</code> term, which always contains the value <code class="literal">1.0</code>.</p><p class="calibre11">With our <code class="literal">matrix-dataset</code> and <code class="literal">logistic-regression</code> functions defined, running logistic regression <a id="id454" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is as simple as this:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-21 []
  (let [data (matrix-dataset)
        ys (i/$ 0 data)
        xs (i/$ [:not 0] data)]
    (logistic-regression ys xs)))</pre></div><p class="calibre11">We're providing <code class="literal">0</code> to Incanter's <code class="literal">i/$</code> function to select the first column of the matrix (the classes), and [<code class="literal">:not 0</code>] to select everything else (the features):</p><div class="calibre2"><pre class="programlisting">;; [0.9308681940090573 -2.5150078795265753 1.1782368822555778
;;  0.29749924127081434 -0.5448679293359383]</pre></div><p class="calibre11">If you run this example, you'll find that it returns a vector of numbers. This vector corresponds to the best estimates for the coefficients of the logistic model.</p></div><div class="calibre2" title="Evaluating the logistic regression classifier"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec75" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Evaluating the logistic regression classifier</h2></div></div></div><p class="calibre11">The vector <a id="id455" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated in the previous section contains the coefficients of our logistic model. We can make predictions with them by passing them to our <code class="literal">sigmoid-function</code> like this:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-22 []
  (let [data (matrix-dataset)
        ys (i/$ 0 data)
        xs (i/$ [:not 0] data)
        coefs (logistic-regression ys xs)
        classifier (comp logistic-class
                      (sigmoid-function coefs)
                      i/trans)]
    (println "Observed: " (map int (take 10 ys)))
    (println "Predicted:" (map classifier (take 10 xs)))))

;; Observed:  (1 1 0 0 0 1 1 0 1 0)
;; Predicted: (1 0 1 0 1 0 1 0 1 0)</pre></div><p class="calibre11">You can see that the classifier is not doing a perfect job—it's confused by some of the classes. In the first <a id="id456" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>ten results, it's getting four classes incorrect, which is only just better <a id="id457" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>than chance. Let's see what proportion of classes was correctly identified over the entire dataset:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-23 []
  (let [data (matrix-dataset)
        ys (i/$ 0 data)
        xs (i/$ [:not 0] data)
        coefs (logistic-regression ys xs)
        classifier (comp logistic-class
                      (sigmoid-function coefs)
                      i/trans)
        y-hats (map classifier xs)]
    (frequencies (map = y-hats (map int ys)))))

;; {true 1021, false 288}</pre></div><p class="calibre11">In the preceding code we train a classifier as before, and simply map over the entire dataset looking for predictions that equal observed classes. We use Clojure core's <code class="literal">frequencies</code> function to provide a simple count of the number of times the classes are equal.</p><p class="calibre11">Predicting the correct outcome 1,021 times out of 1,309 equates to 78 percent correct. Our classifier is definitely performing better than chance.</p></div><div class="calibre2" title="The confusion matrix"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec76" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The confusion matrix</h2></div></div></div><p class="calibre11">While <a id="id458" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>percent correct is a simple measure to calculate and comprehend, it's vulnerable to situations where a classifier <a id="id459" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>systematically under- or over-represents a given class. As an extreme example, consider a classifier that always classifies passengers as having perished. On our Titanic dataset such a classifier would appear to be 68 percent correct, but it would perform terribly on an alternative dataset where most of the passengers survived.</p><p class="calibre11">A <code class="literal">confusion-matrix</code> function shows how many misclassified items there are in the training set, split into true positives, true negatives, false positives, and false negatives. The confusion matrix has a row for each category of the input and a column for each category of the model. We can create one like this in Clojure:</p><div class="calibre2"><pre class="programlisting">(defn confusion-matrix [ys y-hats]
  (let [classes   (into #{} (concat ys y-hats))
        confusion (frequencies (map vector ys y-hats))]
    (i/dataset (cons nil classes)
               (for [x classes]
                 (cons x
                       (for [y classes]
                         (get confusion [x y])))))))</pre></div><p class="calibre11">We can then run our confusion matrix on the results of our logistic regression like so:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-24 []
  (let [data (matrix-dataset)
        ys (i/$ 0 data)
        xs (i/$ [:not 0] data)
        coefs (logistic-regression ys xs)
        classifier (comp logistic-class
                      (sigmoid-function coefs)
                      i/trans)
        y-hats (map classifier xs)]
    (confusion-matrix (map int ys) y-hats)))</pre></div><p class="calibre11">which returns the following matrix:</p><div class="calibre2"><pre class="programlisting">|   |   0 |   1 |
|---+-----+-----|
| 0 | 682 | 127 |
| 1 | 161 | 339 |</pre></div><p class="calibre11">We can see how the model returned <code class="literal">682</code> true negatives and <code class="literal">339</code> true positives, adding up to the 1,021 correctly predicted results. The confusion matrix for a good model will be dominated by counts along the diagonal, with much smaller numbers in the off-diagonal positions. A <a id="id460" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>perfect classifier <a id="id461" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>would have zero in all off-diagonal cells.</p></div><div class="calibre2" title="The kappa statistic"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec77" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>The kappa statistic</h2></div></div></div><p class="calibre11">The <a id="id462" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kappa statistic can be used for comparing two pairs of classes to see how well the classes agree. It is more robust that simply looking at percentage agreement because the equation aims to account for the possibility that some of the agreement has occurred simply due to chance alone.</p><p class="calibre11">The kappa statistic <a id="id463" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>models how often each class occurs in each sequence and factors this into the calculation. For example, if I correctly guess the result of a coin toss 50 percent of the time, but I always guess heads, the kappa statistic will be zero. This is because the agreement is no more than could be expected by chance.</p><p class="calibre11">To calculate the kappa statistic we need to know two things:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong1"><em class="calibre13">p(a)</em></span>: This is the probability of actual observed agreement</li><li class="listitem"><span class="strong1"><em class="calibre13">p(e)</em></span>: This is the probability of expected agreement</li></ul></div><p class="calibre11">The value of <span class="strong1"><em class="calibre13">p(a)</em></span> is the percentage agreement we calculated previously to be 78 percent. It's the sum of true positives and true negatives divided by the size of the sample.</p><p class="calibre11">To calculate the value of <span class="strong1"><em class="calibre13">p(e) </em></span>we need to know both the proportion of negative classes present in the data, and the proportion of negative classes predicted by our model. The proportion of negative classes in our data is <span class="inlinemediaobject"><img src="Images/7180OS_04_20.jpg" alt="The kappa statistic" class="calibre179"/></span>, or 62 percent. This is the probability of perishing in the Titanic disaster overall. The proportion of negative classes in our model can be calculated from the confusion matrix as <span class="inlinemediaobject"><img src="Images/7180OS_04_21.jpg" alt="The kappa statistic" class="calibre179"/></span>, or 64 percent.</p><p class="calibre11">The probability that the data and model might agree by chance, <span class="strong1"><em class="calibre13">p(e)</em></span>, is the probability that the model and the data both have a negative class <span class="inlinemediaobject"><img src="Images/7180OS_04_22.jpg" alt="The kappa statistic" class="calibre180"/></span> plus the probability that both the data and the model have a positive class <span class="inlinemediaobject"><img src="Images/7180OS_04_23.jpg" alt="The kappa statistic" class="calibre180"/></span>. Therefore the probability of random agreement <span class="strong1"><em class="calibre13">p(e)</em></span> is about 53 percent.</p><p class="calibre11">The preceding information is all we need to calculate the kappa statistic:</p><div class="mediaobject"><img src="Images/7180OS_04_24.jpg" alt="The kappa statistic" class="calibre181"/></div><p class="calibre11">Substituting in <a id="id464" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the values we just <a id="id465" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculated yields:</p><div class="mediaobject"><img src="Images/7180OS_04_25.jpg" alt="The kappa statistic" class="calibre182"/></div><p class="calibre11">We can calculate this in Clojure as follows:</p><div class="calibre2"><pre class="programlisting">(defn kappa-statistic [ys y-hats]
  (let [n (count ys)
        pa (/ (count (filter true? (map = ys y-hats))) n)
        ey (/ (count (filter zero? ys)) n)
        eyh (/ (count (filter zero? y-hats)) n)
        pe (+ (* ey eyh)
              (* (- 1 ey)
                 (- 1 eyh)))]
    (/ (- pa pe)
       (- 1 pe))))

(defn ex-4-25 []
   (let [data (matrix-dataset)
         ys (i/$ 0 data)
         xs (i/$ [:not 0] data)
         coefs (logistic-regression ys xs)
         classifier (comp logistic-class
                       (sigmoid-function coefs)
                       i/trans)
         y-hats (map classifier xs)]
     (float (kappa-statistic (map int ys) y-hats))))

;; 0.527</pre></div><p class="calibre11">Values of kappa range between 0 and 1, with 1 corresponding to complete agreement across both output classes. Complete agreement for only one output class is undefined with kappa—if I <a id="id466" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>guess the result of a <a id="id467" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>coin toss correctly 100 percent of the time, but the coin always comes up heads, there is no way of knowing that the coin was a fair coin.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Probability"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec87" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Probability</h1></div></div></div><p class="calibre11">We have <a id="id468" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>encountered probability in several guises so far in this book: as <span class="strong1"><em class="calibre13">p</em></span>-values, confidence intervals, and most recently as the output of logistic regression where the result can be considered as the probability of the output class being positive. The probabilities we calculated for the kappa statistic were the result of adding up counts and dividing by totals. The probability of agreement, for example, was calculated as the <a id="id469" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of times the model and the data agreed divided by the number of samples. This way of calculating probabilities is referred to as <span class="strong1"><strong class="calibre12">frequentist</strong></span>, because it is concerned with the rates at which things happen.</p><p class="calibre11">An output of <code class="literal">1.0</code> from logistic regression (pre-rounding) corresponds to the certainty that the input is in the positive class; an output of <code class="literal">0.0</code> corresponds to the certainty that the input isn't in the positive class. An output of <code class="literal">0.5</code> corresponds to complete uncertainty about the output class. For example, if <span class="strong1"><em class="calibre13">ŷ = 0.7</em></span> the probability of <span class="strong1"><em class="calibre13">y = 1</em></span> is 70 percent. We can write this in the following way:</p><div class="mediaobject"><img src="Images/7180OS_04_26.jpg" alt="Probability" class="calibre183"/></div><p class="calibre11">We say <span class="strong1"><em class="calibre13">y-hat equals the probability that y equals one given x, parameterized by beta</em></span>. This new notation expresses the fact that our prediction, <span class="strong1"><em class="calibre13">ŷ</em></span>, is informed by inputs including <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">β</em></span>. The values contained in these vectors affect our calculation of the output probability, and correspondingly our prediction for <span class="strong1"><em class="calibre13">y</em></span>.</p><p class="calibre11">An alternative to the <a id="id470" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>frequentist view of probability is <span class="strong1"><strong class="calibre12">Bayesian view</strong></span>. The Bayesian conception of probability incorporates a prior belief into the probability calculation. To illustrate the difference, let's look again at the example of tossing a coin.</p><p class="calibre11">Let's imagine that a coin is tossed 14 times in a row and comes up as heads 10 times. You're asked to bet whether it will land heads on the next two throws. Would you take the bet?</p><p class="calibre11">To a frequentist, the probability of the coin landing heads for two consecutive further throws is <span class="inlinemediaobject"><img src="Images/7180OS_04_27.jpg" alt="Probability" class="calibre184"/></span>. This is marginally better than 50 percent, so it makes sense to take the bet.</p><p class="calibre11">A Bayesian would frame the problem differently. With a prior belief that the coin is fair, how well does the data fit this belief? The standard error of the proportion over 14 throws is 0.12. The difference between <span class="inlinemediaobject"><img src="Images/7180OS_04_28.jpg" alt="Probability" class="calibre185"/></span> and <span class="inlinemediaobject"><img src="Images/7180OS_04_29.jpg" alt="Probability" class="calibre26"/></span> divided by the standard error is approximately 1.77, corresponding <a id="id471" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to a <span class="strong1"><em class="calibre13">p</em></span>-value of about 0.08. There's simply not enough evidence to reject the theory that the coin is fair. If the coin were fair, then the probability of getting two consecutive heads is <span class="inlinemediaobject"><img src="Images/7180OS_04_30.jpg" alt="Probability" class="calibre186"/></span> and we would likely lose the bet.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note38" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">In the 18<sup class="calibre112">th</sup> Century, Pierre-Simon Laplace posited "What is the probability the sun will rise tomorrow?" to illustrate the difficulty of using probability theory to evaluate the plausibility of statements.</p></div></div><p class="calibre11">The Bayesian view of probability gives rise to a very useful theorem called <span class="strong1"><strong class="calibre12">Bayes theorem</strong></span>.</p><div class="calibre2" title="Bayes theorem"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec78" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bayes theorem</h2></div></div></div><p class="calibre11">The logistic <a id="id472" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>regression equation we presented in the previous section <a id="id473" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is an example of conditional probability:</p><div class="mediaobject"><img src="Images/7180OS_04_26.jpg" alt="Bayes theorem" class="calibre183"/></div><p class="calibre11">The probability of our prediction <span class="strong1"><em class="calibre13">ŷ</em></span> is determined by the values <span class="strong1"><em class="calibre13">x</em></span> and <span class="strong1"><em class="calibre13">β</em></span>. A conditional probability is the likelihood of one thing given another thing we already know about. For example, we have already considered questions such as the "probability of survival given that the passenger was female".</p><p class="calibre11">Assuming we are interested in <span class="strong1"><em class="calibre13">x</em></span>, <span class="strong1"><em class="calibre13">y</em></span>, and <span class="strong1"><em class="calibre13">z</em></span>, the basic notation for probability is as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="inlinemediaobject"><img src="Images/7180OS_04_31.jpg" alt="Bayes theorem" class="calibre187"/></span>: This is the probability of <span class="strong1"><em class="calibre13">A</em></span> occurring
 </li><li class="listitem"><span class="inlinemediaobject"><img src="Images/7180OS_04_32.jpg" alt="Bayes theorem" class="calibre188"/></span>: This is the joint probability of both <span class="strong1"><em class="calibre13">A</em></span> and <span class="strong1"><em class="calibre13">B</em></span> occurring
</li><li class="listitem"><span class="inlinemediaobject"><img src="Images/7180OS_04_33.jpg" alt="Bayes theorem" class="calibre188"/></span>: This is the probability of <span class="strong1"><em class="calibre13">A</em></span> or <span class="strong1"><em class="calibre13">B</em></span> occurring
</li><li class="listitem"><span class="inlinemediaobject"><img src="Images/7180OS_04_80.jpg" alt="Bayes theorem" class="calibre189"/></span>: This is the probability of <span class="strong1"><em class="calibre13">A</em></span> occurring given <span class="strong1"><em class="calibre13">B</em></span> has occurred
</li><li class="listitem"><span class="inlinemediaobject"><img src="Images/7180OS_04_81.jpg" alt="Bayes theorem" class="calibre190"/></span>: This <a id="id474" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is the probability of both <span class="strong1"><em class="calibre13">A</em></span> and <span class="strong1"><em class="calibre13">B</em></span> occurring given that <span class="strong1"><em class="calibre13">C</em></span> has occurred
 </li></ul></div><p class="calibre11">The relationship <a id="id475" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>between the preceding variables is expressed in the following formula:</p><div class="mediaobject"><img src="Images/7180OS_04_34.jpg" alt="Bayes theorem" class="calibre191"/></div><p class="calibre11">Using this, we can solve for <span class="inlinemediaobject"><img src="Images/7180OS_04_80.jpg" alt="Bayes theorem" class="calibre189"/></span> assuming <span class="inlinemediaobject"><img src="Images/7180OS_04_35.jpg" alt="Bayes theorem" class="calibre192"/></span> to get what is called Bayes theorem:</p><div class="mediaobject"><img src="Images/7180OS_04_36.jpg" alt="Bayes theorem" class="calibre193"/></div><p class="calibre11">We read this as "the probability of <span class="strong1"><em class="calibre13">A</em></span> given <span class="strong1"><em class="calibre13">B</em></span> is equal to the probability of <span class="strong1"><em class="calibre13">B</em></span>, given <span class="strong1"><em class="calibre13">A</em></span>, times the probability of <span class="strong1"><em class="calibre13">A</em></span> all over the probability of <span class="strong1"><em class="calibre13">B</em></span>".</p><p class="calibre11"><span class="inlinemediaobject"><img src="Images/7180OS_04_31.jpg" alt="Bayes theorem" class="calibre187"/></span> is the prior probability: the initial degree of belief in <span class="strong1"><em class="calibre13">A</em></span>.</p><p class="calibre11"><span class="inlinemediaobject"><img src="Images/7180OS_04_80.jpg" alt="Bayes theorem" class="calibre189"/></span> is the conditional probability—the degree of belief in <span class="strong1"><em class="calibre13">A</em></span> having taken <span class="strong1"><em class="calibre13">B</em></span> into account.</p><p class="calibre11">The quotient <span class="inlinemediaobject"><img src="Images/7180OS_04_37.jpg" alt="Bayes theorem" class="calibre194"/></span> represents the support that <span class="strong1"><em class="calibre13">B</em></span> provides for <span class="strong1"><em class="calibre13">A</em></span>.</p><p class="calibre11">Bayes theorem can appear intimidating and abstract, so let's see an example of why it's useful. Let's say we're testing for disease that has infected 1 percent of the population. We have a highly sensitive and specific test that is not quite perfect:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">99 percent of sick patients test positive</li><li class="listitem">99 percent of healthy patients test negative</li></ul></div><p class="calibre11">Given that a patient <a id="id476" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tests positive, what is the probability that the <a id="id477" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>patient is actually sick?</p><p class="calibre11">The preceding bullet points appear to imply that a positive test means a 99 percent chance of being sick, but this fails to take into account how rare the disease is in the population. Since the probability of being infected (the prior) is so small, this hugely decreases your chances of actually having the disease even if you test positive.</p><p class="calibre11">Let's work through the numbers with 10,000 representative people. That would mean that 100 are sick, but 9,900 are healthy. If we applied the test to all 10,000 people we would find 99 sick people testing sick (true positives), but 99 healthy people, testing sick (false positives) as well. If you test positive, the chances of actually having the disease are <span class="inlinemediaobject"><img src="Images/7180OS_04_38.jpg" alt="Bayes theorem" class="calibre195"/></span>, or 50 percent:</p><div class="mediaobject"><img src="Images/7180OS_04_170.jpg" alt="Bayes theorem" class="calibre196"/></div><p class="calibre11">We can calculate the same example using Bayes rule. Let <span class="strong1"><em class="calibre13">y</em></span> to refer to "sick" and <span class="strong1"><em class="calibre13">x</em></span> refer to the event "+" for a positive result:</p><div class="mediaobject"><img src="Images/7180OS_04_40.jpg" alt="Bayes theorem" class="calibre197"/></div><p class="calibre11">In other words, although <a id="id478" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a positive test has vastly increased your chances of having the disease (up from 1 percent in the population), you still only have even odds <a id="id479" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of actually being sick—nowhere near the 99 percent implied by the test accuracy alone.</p><p class="calibre11">The previous example provides neat numbers for us to work through, let's run the example on the Titanic data now.</p><p class="calibre11">The probability of surviving given you are female is equal to the probability of being female given you survived multiplied by the probability of surviving all divided by the probability of being a woman on the Titanic:</p><div class="mediaobject"><img src="Images/7180OS_04_41.jpg" alt="Bayes theorem" class="calibre198"/></div><p class="calibre11">Let's remind ourselves of the contingency table from earlier:</p><div class="calibre2"><pre class="programlisting">| :survived |   :sex | :count |
|-----------+--------+--------|
|         n |   male |    682 |
|         n | female |    127 |
|         y |   male |    161 |
|         y | female |    339 |</pre></div><p class="calibre11">
<span class="strong1"><em class="calibre13">P(survival|female)</em></span>is the <a id="id480" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>posterior, the degree of belief in survival given the <a id="id481" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>evidence. This is the value we are trying to calculate.</p><p class="calibre11">
<span class="strong1"><em class="calibre13">P(female|survival)</em></span> is the conditional probability of being female, given survival:</p><div class="mediaobject"><img src="Images/7180OS_04_42.jpg" alt="Bayes theorem" class="calibre199"/></div><p class="calibre11">
<span class="strong1"><em class="calibre13">P(survival)</em></span> is the prior, the initial degree of belief in survival:</p><div class="mediaobject"><img src="Images/7180OS_04_43.jpg" alt="Bayes theorem" class="calibre200"/></div><p class="calibre11">
<span class="strong1"><em class="calibre13">P(female)</em></span> is the evidence:</p><div class="mediaobject"><img src="Images/7180OS_04_44.jpg" alt="Bayes theorem" class="calibre201"/></div><p class="calibre11">Substituting these proportions into Bayes rule:</p><div class="mediaobject"><img src="Images/7180OS_04_45.jpg" alt="Bayes theorem" class="calibre202"/></div><p class="calibre11">Using Bayes rule we have calculated that the probability of survival, given being female, is <span class="inlinemediaobject"><img src="Images/7180OS_04_46.jpg" alt="Bayes theorem" class="calibre203"/></span> or 76 percent.</p><p class="calibre11">Notice that we could have calculated this value from the contingency table too, by looking up the proportion <a id="id482" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of survivors out of the total females: <span class="inlinemediaobject"><img src="Images/7180OS_04_47.jpg" alt="Bayes theorem" class="calibre204"/></span>. The reason for the popularity of Bayes rule is that it gives us a way of calculating this probability <a id="id483" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>where no such contingency table exists.</p></div><div class="calibre2" title="Bayes theorem with multiple predictors"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec79" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bayes theorem with multiple predictors</h2></div></div></div><p class="calibre11">As an <a id="id484" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>example of how we can use Bayes rule <a id="id485" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>without a full contingency table, let's use the example of a third-class male. What's the probability of survival for third-class male passengers?</p><p class="calibre11">Let's write out Bayes rule for this new question:</p><div class="mediaobject"><img src="Images/7180OS_04_48.jpg" alt="Bayes theorem with multiple predictors" class="calibre205"/></div><p class="calibre11">Next, we have two contingency tables:</p><div class="calibre2"><pre class="programlisting">| :survived | :pclass | :count |
|-----------+---------+--------|
|         n |   first |    123 |
|         n |  second |    158 |
|         n |   third |    528 |
|         y |   first |    200 |
|         y |  second |    119 |
|         y |   third |    181 |

| :survived |   :sex | :count |
|-----------+--------+--------|
|         n | female |    127 |
|         n |   male |    682 |
|         y | female |    339 |
|         y |   male |    161 |</pre></div><p class="calibre11">"Third-class male" is not a category in any of our contingency tables that we can simply look up. However, by using Bayes theorem we can calculate it like this:</p><p class="calibre11">The posterior probability we're seeking is <span class="strong1"><em class="calibre13">P(survive|male,third)</em></span>.</p><p class="calibre11">The prior probability of survival is the same as before:<span class="inlinemediaobject"><img src="Images/7180OS_04_49.jpg" alt="Bayes theorem with multiple predictors" class="calibre206"/></span> or about 0.38.</p><p class="calibre11">The <a id="id486" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>conditional probability is <span class="inlinemediaobject"><img src="Images/7180OS_04_50.jpg" alt="Bayes theorem with multiple predictors" class="calibre207"/></span>. This is the same as <span class="inlinemediaobject"><img src="Images/7180OS_04_51.jpg" alt="Bayes theorem with multiple predictors" class="calibre208"/></span>. In other words, we can multiply the two probabilities together:</p><div class="mediaobject"><img src="Images/7180OS_04_52.jpg" alt="Bayes theorem with multiple predictors" class="calibre209"/></div><div class="mediaobject"><img src="Images/7180OS_04_53.jpg" alt="Bayes theorem with multiple predictors" class="calibre210"/></div><p class="calibre11">The evidence is the <a id="id487" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>probability of being both male and in third class <span class="inlinemediaobject"><img src="Images/7180OS_04_54.jpg" alt="Bayes theorem with multiple predictors" class="calibre211"/></span>:</p><div class="mediaobject"><img src="Images/7180OS_04_55.jpg" alt="Bayes theorem with multiple predictors" class="calibre212"/></div><div class="mediaobject"><img src="Images/7180OS_04_56.jpg" alt="Bayes theorem with multiple predictors" class="calibre210"/></div><p class="calibre11">Putting this all together:</p><div class="mediaobject"><img src="Images/7180OS_04_57.jpg" alt="Bayes theorem with multiple predictors" class="calibre213"/></div><p class="calibre11">In actual <a id="id488" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>fact, there were 75 surviving third class males out of 493 in total, giving a true survival rate of 15 percent. Bayes Theorem has <a id="id489" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>allowed us to calculate the true answer very closely, without the use of a complete contingency table.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Naive Bayes classification"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec88" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Naive Bayes classification</h1></div></div></div><p class="calibre11">The reason that <a id="id490" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the answer we arrived at using Bayes theorem and the actual result differ slightly is that by using Bayes rule we made an assumption when calculating <span class="inlinemediaobject"><img src="Images/7180OS_04_54.jpg" alt="Naive Bayes classification" class="calibre211"/></span> that the probability of being male, and the probability of being in third class, are independent. In the next section, we'll use Bayes theorem to produce a naive Bayes classifier.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note39" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The reason this algorithm is called naive is because it assumes all variables are independent. We know this is often not the case, and there are interaction effects between variables. For example, we might know that combinations of parameters make a certain class very much more likely—for example, being both male and in third class.</p></div></div><p class="calibre11">Let's look at how we might use Bayes rule for a classifier. The Bayes theorem for two possible classes, survive and perish, are shown as follows for a male in third class:</p><div class="mediaobject"><img src="Images/7180OS_04_58.jpg" alt="Naive Bayes classification" class="calibre214"/></div><div class="mediaobject"><img src="Images/7180OS_04_59.jpg" alt="Naive Bayes classification" class="calibre215"/></div><p class="calibre11">The most likely class will be the one with the greatest posterior probability.</p><p class="calibre11"><span class="inlinemediaobject"><img src="Images/7180OS_04_54.jpg" alt="Naive Bayes classification" class="calibre211"/></span> appears as the common factor for both classes. If we were to relax the requirements of Bayes theorem a little so that it didn't have to return probabilities, we could remove the common factor to arrive at the following:</p><div class="mediaobject"><img src="Images/7180OS_04_61.jpg" alt="Naive Bayes classification" class="calibre216"/></div><div class="mediaobject"><img src="Images/7180OS_04_62.jpg" alt="Naive Bayes classification" class="calibre217"/></div><p class="calibre11">We have simply removed the denominator from the right hand side of both equations. Since we are no <a id="id491" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>longer calculating probabilities, the equals sign has become <span class="inlinemediaobject"><img src="Images/7180OS_04_63.jpg" alt="Naive Bayes classification" class="calibre218"/></span>, meaning "is proportional to".</p><p class="calibre11">Putting the values from our previous table of data into the equations yields:</p><div class="mediaobject"><img src="Images/7180OS_04_64.jpg" alt="Naive Bayes classification" class="calibre219"/></div><div class="mediaobject"><img src="Images/7180OS_04_65.jpg" alt="Naive Bayes classification" class="calibre220"/></div><p class="calibre11">We can instantly see that we are not calculating probabilities because the two classes do not add up to one. This doesn't matter for our classifier since we were only going to select the class associated with the highest value anyway. Unfortunately for our third-class male, our naive Bayes model predicts that he will perish.</p><p class="calibre11">Let's do the equivalent calculation for a first class female:</p><div class="mediaobject"><img src="Images/7180OS_04_66.jpg" alt="Naive Bayes classification" class="calibre221"/></div><div class="mediaobject"><img src="Images/7180OS_04_67.jpg" alt="Naive Bayes classification" class="calibre222"/></div><p class="calibre11">Fortunately for our first class female, the model predicts that she will survive.</p><p class="calibre11">A Bayes classifier is a <a id="id492" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>combination of the Bayes probability <a id="id493" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model combined with a decision rule (which class to choose). The decision rule described earlier is the maximum a posteriori rule, or MAP rule.</p><div class="calibre2" title="Implementing a naive Bayes classifier"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec80" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Implementing a naive Bayes classifier</h2></div></div></div><p class="calibre11">Fortunately, implementing a naive Bayes model in code is much easier than understanding <a id="id494" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the mathematics. The first step is simply to calculate the number of examples corresponding to each value of each feature for each class. The following code keeps a count of the number of times each parameter is seen for each class label:</p><div class="calibre2"><pre class="programlisting">(defn inc-class-total [model class]
  (update-in model [class :total] (fnil inc 0)))

(defn inc-predictors-count-fn [row class]
  (fn [model attr]
    (let [val (get row attr)]
      (update-in model [class attr val] (fnil inc 0)))))

(defn assoc-row-fn [class-attr predictors]
  (fn [model row]
    (let [class (get row class-attr)]
      (reduce (inc-predictors-count-fn row class)
              (inc-class-total model class)
              predictors))))

(defn bayes-classifier [data class-attr predictors]
  (reduce (assoc-row-fn class-attr predictors) {} data))</pre></div><p class="calibre11">The label is the attribute corresponding to the class (for example, in our Titanic data "survived" is the label corresponding to our classes true and false), and parameters are the sequence of attributes corresponding to the features (sex and class).</p><p class="calibre11">It can be used like so:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-26 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (bayes-classifier :survived [:sex :pclass])
       (clojure.pprint/pprint)))</pre></div><p class="calibre11">This example yields the following Bayes model:</p><div class="calibre2"><pre class="programlisting">{:classes
 {"n"
  {:predictors
   {:pclass {"third" 528, "second" 158, "first" 123},
    :sex {"male" 682, "female" 127}},
   :n 809},
  "y"
  {:predictors
   {:pclass {"third" 181, "second" 119, "first" 200},
    :sex {"male" 161, "female" 339}},
   :n 500}},
 :n 1309}</pre></div><p class="calibre11">The model is simply a two-level hierarchy implemented as nested maps. At the top level are our two classes—<code class="literal">"n"</code> and <code class="literal">"y"</code>, corresponding to "perished" and "survived", respectively. For each class we have a map of predictors—<code class="literal">:pclass</code> and <code class="literal">:sex</code>. Each key corresponds to a <a id="id495" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>map of possible values and counts. As well as a map of predictors, each class has a count <code class="literal">:n</code>.</p><p class="calibre11">Now that we have calculated our Bayes model, we can implement our MAP decision rule. The following is a function that calculates the conditional probability of a provided class. For example, <span class="inlinemediaobject"><img src="Images/7180OS_04_68.jpg" alt="Implementing a naive Bayes classifier" class="calibre223"/></span>:</p><div class="calibre2"><pre class="programlisting">(defn posterior-probability [model test class-attr]
  (let [observed (get-in model [:classes class-attr])
        prior (/ (:n observed)
                 (:n model))]
    (apply * prior
           (for [[predictor value] test]
             (/ (get-in observed [:predictors predictor value])
                (:n observed))))))</pre></div><p class="calibre11">Given a particular <code class="literal">class-attr</code>, the preceding code will calculate the posterior probability of the class, given the observations. Having implemented the earlier code, the classifier simply needs to return the class corresponding to the maximum posterior probability:</p><div class="calibre2"><pre class="programlisting">(defn bayes-classify [model test]
  (let [probability (partial posterior-probability model test)
        classes     (keys (:classes model))]
    (apply max-key probability classes)))</pre></div><p class="calibre11">The preceding code calculates the probability of the test input against each of the model's classes. The returned <a id="id496" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>class is simply the one with the highest posterior probability.</p></div><div class="calibre2" title="Evaluating the naive Bayes classifier"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec81" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Evaluating the naive Bayes classifier</h2></div></div></div><p class="calibre11">Now that we <a id="id497" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have written two complementary functions, <code class="literal">bayes-classifier</code> and <code class="literal">bayes-classify</code>, we can use our model to make predictions. Let's train our model on the Titanic dataset and check its predictions for the third-class male and first-class female that we've already calculated:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-27 []
  (let [model (-&gt;&gt; (load-data "titanic.tsv")
                   (:rows)
                   (naive-bayes :survived [:sex :pclass]))]
    (println "Third class male:"
             (bayes-classify model {:sex "male" :pclass "third"}))
    (println "First class female:"
             (bayes-classify model {:sex "female" :pclass "first"}))))

;; Third class male: n
;; First class female: y</pre></div><p class="calibre11">It's a good start—our classifier is in agreement with the outcomes we've calculated by hand. Let's take a look at the percent correct for the naive Bayes classifier:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-28 []
   (let [data (:rows (load-data "titanic.tsv"))
         model (bayes-classifier :survived [:sex :pclass] data)
         test (fn [test]
                (= (:survived test)
                   (bayes-classify model
                            (select-keys test [:sex :class]))))
         results (frequencies (map test data))]
     (/ (get results true)
        (apply + (vals results)))))

;; 1021/1309</pre></div><p class="calibre11">By replicating our test over the entire dataset and comparing outputs, we can see how often our classifier got the correct answer. 78 percent is the same percent correct we got using our logistic regression classifier. For such a simple model, naive Bayes is performing remarkably well.</p><p class="calibre11">We can calculate a confusion matrix:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-195 []
    (let [data (:rows (load-data "titanic.tsv"))
          model (bayes-classifier :survived [:sex :pclass] data)
          classify (fn [test]
                     (-&gt;&gt; (select-keys test [:sex :pclass])
                          (bayes-classify model)))
          ys      (map :survived data)
          y-hats (map classify data)]
      (confusion-matrix ys y-hats)))</pre></div><p class="calibre11">The preceding code generates the following matrix:</p><div class="calibre2"><pre class="programlisting">|   |   n |   y |
|---+-----+-----|
| n | 682 | 127 |
| y | 161 | 339 |</pre></div><p class="calibre11">This confusion <a id="id498" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix is identical to the one we obtained previously from logistic regression. Despite taking very different approaches, they have both been able to classify the dataset to the same degree of accuracy.</p><div class="calibre2" title="Comparing the logistic regression and naive Bayes approaches"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title7"><a id="ch04lvl3sec02" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Comparing the logistic regression and naive Bayes approaches</h3></div></div></div><p class="calibre11">Although <a id="id499" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>they have performed equally well on our small Titanic dataset, the two methods of classification are generally suited to different tasks.</p><p class="calibre11">In spite of being conceptually a simpler classifier as compared to logistic regression, naive Bayes can often outperform it in cases where either data is scarce or the number of parameters is very large. Because of naive Bayes' ability to deal with a very large number of features, it is often employed for problems such as automatic medical diagnosis or in spam classification. In spam classification, features could run into the tens or hundreds of thousands, with each word representing a feature that can help identify whether the message is spam or not.</p><p class="calibre11">However, a drawback of naive Bayes is its assumption of independence—in problem domains where this assumption is not valid, other classifiers can outperform naive Bayes. With a lot of data, logistic regression is able to learn more sophisticated models and classify potentially more accurately than naive Bayes is able to.</p><p class="calibre11">There is <a id="id500" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>another method that—while simple and relatively straightforward to model—is able to learn more sophisticated relationships amongst parameters. This method is the decision tree.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Decision trees"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec89" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Decision trees</h1></div></div></div><p class="calibre11">The third <a id="id501" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method of classification we'll look at in this chapter is the decision tree. A decision tree models the process of classification as a series of tests that checks the value of a particular attribute or attributes of the item to be classified. It can be thought of as similar to a flowchart, with each test being a branch in the flow. The process continues, testing and branching, until a leaf node is reached. The leaf node will represent the most likely class for the item.</p><p class="calibre11">Decision trees share some similarities with both logistic regression and naive Bayes. Although the classifier can support categorical variables without dummy coding, it is also able to model complex dependencies between variables through repeated branching.</p><p class="calibre11">In the old-fashioned parlor game <span class="strong1"><em class="calibre13">Twenty Questions</em></span>, one person, the "answerer", chooses an object but does not reveal their choice to the others. All other players are "questioners" and take turns to ask questions that aim to guess the object the answerer has thought of. Each question can only be answered with a simple "yes" or "no". The challenge for the questioners is to guess the object the answerer was thinking of in only 20 questions, and to pick questions that reveal the most amount of information about the object the answerer is thinking of. This is not an easy task—ask questions that are too broad and you do not gain much information through the answer. Ask questions that are too specific and you will not reach an answer in only 20 steps.</p><p class="calibre11">Unsurprisingly, these concerns also appear in decision tree classification. Information is something that is quantifiable, and decision trees aim to ask questions that are likely to yield the biggest information gain.</p><div class="calibre2" title="Information"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec82" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Information</h2></div></div></div><p class="calibre11">Imagine that I <a id="id502" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>pick a random card from a normal deck of 52 playing cards. Your challenge is to guess what card I have picked. But first, I offer to answer one question with a "yes" or a "no". Which question would you rather ask?</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Is it red? (a Heart or a Diamond)</li><li class="listitem">Is it a picture card? (a Jack, Queen, or King)</li></ul></div><p class="calibre11">We will explore this challenge in detail over the coming pages. Take a moment to consider your question.</p><p class="calibre11">There are 26 red cards in a deck, so the probability of a random red card being chosen is <span class="inlinemediaobject"><img src="Images/7180OS_04_29.jpg" alt="Information" class="calibre26"/></span>. There are 12 picture <a id="id503" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cards in a deck so the probability of a picture card being randomly chosen is <span class="inlinemediaobject"><img src="Images/7180OS_04_69.jpg" alt="Information" class="calibre185"/></span>.</p><p class="calibre11">The information <span class="strong1"><em class="calibre13">I</em></span> associated with a single event is:</p><div class="mediaobject"><img src="Images/7180OS_04_70.jpg" alt="Information" class="calibre224"/></div><p class="calibre11">Incanter has a <code class="literal">log2</code> function that enables us to calculate information like this:</p><div class="calibre2"><pre class="programlisting">(defn information [p]
  (- (i/log2 p)))</pre></div><p class="calibre11">Here, <code class="literal">log2</code> is the log to base 2. Therefore:</p><div class="mediaobject"><img src="Images/7180OS_04_71.jpg" alt="Information" class="calibre225"/></div><div class="mediaobject"><img src="Images/7180OS_04_72.jpg" alt="Information" class="calibre226"/></div><p class="calibre11">Since a picture card has the lower probability, it also carries the highest information value. If we know the card is a picture card, there are only 12 cards it could possibly be. If we know the card is red, then 26 possibilities still remain.</p><p class="calibre11">Information is usually measured in bits. The information content of knowing the card is red carries only one bit of information. A computer bit can only represent a zero or a one. One bit is enough to contain a simple 50/50 split. Knowing that the card is a picture card offers two bits of information. This appears to suggest that the best question to ask therefore is "Is it a picture card?". An affirmative answer will carry with it a lot of information.</p><p class="calibre11">But look what happens if <a id="id504" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>we find out the answer to the question is "no". What's the information content of finding out that the card I've chosen is not a picture card?</p><div class="mediaobject"><img src="Images/7180OS_04_73.jpg" alt="Information" class="calibre227"/></div><div class="mediaobject"><img src="Images/7180OS_04_74.jpg" alt="Information" class="calibre228"/></div><p class="calibre11">It appears that now we could be better off asking whether the card is red, since the information content is greater. Finding out our card is not a picture card still leaves 36 possibilities remaining. We clearly don't know in advance whether the answer will be "yes" or "no", so how can we go about choosing the best question?</p></div><div class="calibre2" title="Entropy"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec83" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Entropy</h2></div></div></div><p class="calibre11">Entropy is a measure of <a id="id505" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>uncertainty. By calculating the entropy we can strike a balance between information content over all possible responses.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note40" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The concept of <a id="id506" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>entropy was introduced by Rudolf Clausius in the mid-nineteenth century as part of the emerging science of thermodynamics to help explain how part of the functional energy of combustion engines was lost due to heat dissipation. In this chapter we talk about Shannon Entropy, which comes from Claude Shannon's work on information theory in the mid-twentieth century. The two concepts are closely related, despite hailing from different corners of science in very different contexts.</p></div></div><p class="calibre11">Entropy, <span class="strong1"><em class="calibre13">H</em></span>, can be calculated in the following way:</p><div class="mediaobject"><img src="Images/7180OS_04_75.jpg" alt="Entropy" class="calibre229"/></div><p class="calibre11">Here, <span class="strong1"><em class="calibre13">P(x)</em></span>is the probability of <span class="strong1"><em class="calibre13">x</em></span> occurring and <span class="strong1"><em class="calibre13">I(P(x))</em></span>is the information content of <span class="strong1"><em class="calibre13">x</em></span>.</p><p class="calibre11">For example, let's compare the entropy of a pack of cards where each class is simply "red" and "not red". We know the information content of "red" is 1 and the probability is <span class="inlinemediaobject"><img src="Images/7180OS_04_29.jpg" alt="Entropy" class="calibre26"/></span>. The same is true for "not red", so the entropy is the following sum:</p><div class="mediaobject"><img src="Images/7180OS_04_76.jpg" alt="Entropy" class="calibre230"/></div><p class="calibre11">Splitting the pack in this way yields an entropy of 1. What about splitting the pack into "picture" and "not picture" cards? The information content of "picture" is 2.12 and the probability is <span class="inlinemediaobject"><img src="Images/7180OS_04_69.jpg" alt="Entropy" class="calibre185"/></span>. The <a id="id507" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>information content of "not picture" is 0.38 and the <a id="id508" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>probability is <span class="inlinemediaobject"><img src="Images/7180OS_04_77.jpg" alt="Entropy" class="calibre185"/></span>:</p><div class="mediaobject"><img src="Images/7180OS_04_78.jpg" alt="Entropy" class="calibre231"/></div><p class="calibre11">If we imagine the deck of cards as a sequence of classes, positive and negative, we can calculate the entropy for our two decks using Clojure:</p><div class="calibre2"><pre class="programlisting">(defn entropy [xs]
  (let [n (count xs)
        f (fn [x]
            (let [p (/ x n)]
              (* p (information p))))]
    (-&gt;&gt; (frequencies xs)
         (vals)
         (map f)
         (reduce +))))

(defn ex-4-30 []
  (let [red-black (concat (repeat 26 1)
                          (repeat 26 0))]
    (entropy red-black)))

;; 1.0

(defn ex-4-202 []
  (let [picture-not-picture (concat (repeat 12 1)
                                    (repeat 40 0))]
    (entropy picture-not-picture)))

;; 0.779</pre></div><p class="calibre11">Entropy is a measure of uncertainty. The lower entropy by splitting the deck into "picture" and "not picture" groups shows us that asking whether or not the card is a picture is the best question <a id="id509" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to ask. It remains the best question to have asked even <a id="id510" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>if we discover that my card is not a picture card, because the amount of uncertainty remaining in the deck is lower. Entropy does not just apply to sequences of numbers, but to any sequence.</p><div class="calibre2"><pre class="programlisting">(entropy "mississippi")
;; 1.82</pre></div><p class="calibre11">is lower than</p><div class="calibre2"><pre class="programlisting">(entropy "yellowstone")
;; 2.91</pre></div><p class="calibre11">This in spite of their equal length, because there is more consistency amongst the letters.</p></div><div class="calibre2" title="Information gain"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec84" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Information gain</h2></div></div></div><p class="calibre11">Entropy has <a id="id511" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>indicated to us that the best question to ask—the one that will decrease the entropy of our deck of cards most—is whether or not the card is a picture card.</p><p class="calibre11">In general, we can use <a id="id512" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>entropy to tell us whether a grouping is a good grouping or not using the theory of information gain. To illustrate this, let's return to our Titanic survivors. Let's assume that I've picked a passenger at random and you have to guess whether or not they survived. This time, before you answer, I offer to tell you one of two things:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Their sex (male or female)</li><li class="listitem">The class they were traveling in (first, second, or third)</li></ul></div><p class="calibre11">Which would you rather know?</p><p class="calibre11">It might appear at first that the best question to ask is which class they were travelling in. This will divide the passengers into three groups and, as we saw with the playing cards, smaller groups are better. Don't forget, though, that the objective is to guess the survival of the passenger. To determine the best question to ask we need to know which question gives us the highest information gain.</p><p class="calibre11">Information gain is <a id="id513" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>measured as the difference between entropy before <a id="id514" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and after we learn the new information. Let's calculate the information gain when we learn that the passenger is male. First, let's calculate the baseline entropy of the survival rates for all passengers.</p><p class="calibre11">We can use our existing entropy calculation and pass it the sequence of survival classes:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-32 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (map :survived)
       (entropy)))

;; 0.959</pre></div><p class="calibre11">This is a high entropy. We already know that an entropy of 1.0 indicates a 50/50 split, yet we also know that survival on the Titanic was around 38 percent. The reason for this apparent discrepancy is that entropy does not change linearly, but rises quickly towards 1 as illustrated in the following graph:</p><div class="mediaobject"><img src="Images/7180OS_04_180.jpg" alt="Information gain" class="calibre232"/></div><p class="calibre11">Next, let's consider the entropy of survival when split by sex. Now we have two groups to calculate entropy for: males and females. The combined entropy is the weighted average of the two groups. We <a id="id515" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can calculate the weighted average for an arbitrary <a id="id516" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>number of groups in Clojure by using the following function:</p><div class="calibre2"><pre class="programlisting">(defn weighted-entropy [groups]
  (let [n (count (apply concat groups))
        e (fn [group]
            (* (entropy group)
               (/ (count group) n)))]
    (-&gt;&gt; (map e groups)
         (reduce +))))

(defn ex-4-33 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (group-by :sex)
       (vals)
       (map (partial map :survived))
       (weighted-entropy)))

;; 0.754</pre></div><p class="calibre11">We can see that the weighted entropy for the survival classes that have been grouped by sex is lower than the 0.96 we obtained from the passengers as a whole. Therefore our information gain is <span class="strong1"><em class="calibre13">0.96 - 0.75 = 0.21</em></span> bits.</p><p class="calibre11">We can easily express the gain as a Clojure function based on the <code class="literal">entropy</code> and <code class="literal">weighted-entropy</code> functions that we've just defined:</p><div class="calibre2"><pre class="programlisting">(defn information-gain [groups]
  (- (entropy (apply concat groups))
     (weighted-entropy groups)))</pre></div><p class="calibre11">Let's use this to <a id="id517" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the gain if we group the <a id="id518" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>passengers by their class, instead:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-205 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (group-by :pclass)
       (vals)
       (map (partial map :survived))
       (information-gain)))

;; 0.07</pre></div><p class="calibre11">The information gain for passenger class is 0.07, and for sex is 0.21. Therefore, when classifying survival rates, knowing the passenger's sex is much more useful than the class they were traveling in.</p></div><div class="calibre2" title="Using information gain to identify the best predictor"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec85" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using information gain to identify the best predictor</h2></div></div></div><p class="calibre11">Using <a id="id519" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the functions we have <a id="id520" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>just defined, we can construct an effective tree classifier. We'll want a general purpose way to calculate the information gain for a specific predictor attribute, given an output class. In the preceding example, the predictor was <code class="literal">:pclass</code> and the class attribute was <code class="literal">:survived</code>, but we can make a generic function that will accept these keywords as the arguments <code class="literal">class-attr</code> and <code class="literal">predictor</code>:</p><div class="calibre2"><pre class="programlisting">(defn gain-for-predictor [class-attr xs predictor]
  (let [grouped-classes (-&gt;&gt; (group-by predictor xs)
                             (vals)
                             (map (partial map class-attr)))]
    (information-gain grouped-classes)))</pre></div><p class="calibre11">Next, we'll want a way to calculate the best predictor for a given set of rows. We can simply map the preceding function over all the desired predictors and return the predictor corresponding to the highest gain:</p><div class="calibre2"><pre class="programlisting">(defn best-predictor [class-attr xs predictors]
  (let [gain (partial gain-for-predictor class-attr xs)]
    (when (seq predictors)
      (apply max-key gain predictors))))</pre></div><p class="calibre11">Let's test this <a id="id521" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function by <a id="id522" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>asking which of the predictors <code class="literal">:sex</code> and <code class="literal">:pclass</code> is the best predictor:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-35 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (best-predictor :survived [:sex :pclass])))

;; :sex</pre></div><p class="calibre11">Reassuringly, we're getting the same answer as before. Decision trees allow us to apply this logic recursively to build a tree structure that chooses the best question to ask at each branch, based solely on the data in that branch.</p></div><div class="calibre2" title="Recursively building a decision tree"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec86" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Recursively building a decision tree</h2></div></div></div><p class="calibre11">By <a id="id523" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>applying the functions we have written recursively to the data, we can build up a data structure that represents the best category split at each level of the tree. First, let's define a function that will return the <span class="strong1"><strong class="calibre12">modal</strong></span> (most common) class, given a sequence of data. When our decision tree reaches a point at which it can't split the data any more (either because the entropy is zero or because there are no remaining predictors left on which to split), we'll return the modal class.</p><div class="calibre2"><pre class="programlisting">(defn modal-class [classes]
  (-&gt;&gt; (frequencies classes)
       (apply max-key val)
       (key)))</pre></div><p class="calibre11">With that simple function in place, we're ready to construct the decision tree. This is implemented as a recursive function. Given a class attribute, a sequence of predictors, and a sequence of values, we build a sequence of available classes by mapping the <code class="literal">class-attr</code> over our <code class="literal">xs</code>. If the entropy is zero, then all the classes are the same, so we simply return the first.</p><p class="calibre11">If the classes are not identical in our group, then we need to pick a predictor to branch on. We use our <code class="literal">best-predictor</code> function to select the predictor associated with the highest information gain. We remove this from our list of predictors (there's no point in trying to use the same predictor twice), and construct a <code class="literal">tree-branch</code> function. This is a partial recursive call to <code class="literal">decision-tree</code> with the remaining predictors.</p><p class="calibre11">Finally, we group our data on the <code class="literal">best-predictor</code>, and call our partially applied <code class="literal">tree-branch</code> function on each group. This causes the whole process to repeat again, but this time only on the <a id="id524" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>subset of data defined by <code class="literal">group-by</code>. The return value is wrapped in a vector, together with the predictor:</p><div class="calibre2"><pre class="programlisting">(defn decision-tree [class-attr predictors xs]
  (let [classes (map class-attr xs)]
    (if (zero? (entropy classes))
      (first classes)
      (if-let [predictor (best-predictor class-attr
                                         predictors xs)]
        (let [predictors  (remove #{predictor} predictors)
              tree-branch (partial decision-tree
                                   class-attr predictors)]
          (-&gt;&gt; (group-by predictor xs)
               (map-vals tree-branch)
               (vector predictor)))
        (modal-class classes)))))</pre></div><p class="calibre11">Let's visualize the output of this function for the predictors <code class="literal">:sex</code> and <code class="literal">:pclass</code>.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-36 []
  (-&gt;&gt; (load-data "titanic.tsv")
       (:rows)
       (decision-tree :survived [:pclass :sex])
       (clojure.pprint/pprint)))

;; [:sex
;;  {"female" [:pclass {"first" "y", "second" "y", "third" "n"}],
;;   "male" [:pclass {"first" "n", "second" "n", "third" "n"}]}]</pre></div><p class="calibre11">We can see how the decision tree is represented as a vector. The first element of the vector is the predictor that's being used to branch the tree. The second element is a map containing the attributes of this predictor as keys <code class="literal">"male"</code> and <code class="literal">"female"</code> with values corresponding to a further branch on <code class="literal">:pclass</code>.</p><p class="calibre11">To see how we can build up arbitrarily deep trees using this function, let's add a further predictor <code class="literal">:age</code>. Unfortunately, the tree classifier we've built is only able to deal with categorical data, so let's split the age continuous variable into three simple categories: <code class="literal">unknown</code>, <code class="literal">child</code>, and <code class="literal">adult</code>.</p><div class="calibre2"><pre class="programlisting">(defn age-categories [age]
  (cond
   (nil? age) "unknown"
   (&lt; age 13) "child"
   :default   "adult"))

(defn ex-4-37 []
  (let [data (load-data "titanic.tsv")]
    (-&gt;&gt; (i/transform-col data :age age-categories)
         (:rows)
         (decision-tree :survived [:pclass :sex :age])
         (clojure.pprint/pprint))))</pre></div><p class="calibre11">This code <a id="id525" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>yields the following tree:</p><div class="calibre2"><pre class="programlisting">[:sex
 {"female"
  [:pclass
   {"first" [:age {"adult" "y", "child" "n", "unknown" "y"}],
    "second" [:age {"adult" "y", "child" "y", "unknown" "y"}],
    "third" [:age {"adult" "n", "child" "n", "unknown" "y"}]}],
  "male"
  [:age
   {"unknown" [:pclass {"first" "n", "second" "n", "third" "n"}],
    "adult" [:pclass {"first" "n", "second" "n", "third" "n"}],
    "child" [:pclass {"first" "y", "second" "y", "third" "n"}]}]}]</pre></div><p class="calibre11">Notice how the best overall predictor is still the sex of the passenger, as before. However, if the sex is male, age is the next most informative predictor. On the other hand, if the sex is female, passenger class is the most informative predictor. Because of the recursive nature of the tree, each branch is able to determine the best predictor only for the data in that particular branch of the tree.</p></div><div class="calibre2" title="Using the decision tree for classification"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec87" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using the decision tree for classification</h2></div></div></div><p class="calibre11">With the <a id="id526" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data structure returned from the decision-tree function, we have all the information we require to classify passengers into their most likely class. Our classifier will also be implemented recursively. If a vector has been passed in as the model, we know it will contain two elements—the predictor and the branches. We destructure the predictor and branches from the model and then determine the branch our test is on. To do this, we simply get the value of the predictor from the test with <code class="literal">(get test predictor)</code>. The branch we want will be the one corresponding to this value.</p><p class="calibre11">Once we have the branch, we need to call <code class="literal">tree-classify</code> again on the branch. Because we're in the tail position (no further logic is applied after the <code class="literal">if</code>) we can call <code class="literal">recur</code>, allowing the Clojure compiler to optimize our recursive function call:</p><div class="calibre2"><pre class="programlisting">(defn tree-classify [model test]
  (if (vector? model)
    (let [[predictor branches] model
          branch (get branches (get test predictor))]
      (recur branch test))
    model))</pre></div><p class="calibre11">We continue to <a id="id527" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>call tree-classify recursively until <code class="literal">(vector? model)</code> returns false. At this point we will have traversed the full depth of the decision tree and reached a leaf node. At this point the <code class="literal">model</code> argument contains the predicted class, so we simply return it.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-38 []
  (let [data (load-data "titanic.tsv")
        tree (-&gt;&gt; (i/transform-col data :age age-categories)
                  (:rows)
                  (decision-tree :survived [:pclass :sex :age]))
        test {:sex "male" :pclass "second" :age "child"}]
    (tree-classify tree test)))

;; "y"</pre></div><p class="calibre11">The decision tree predicts that the young male from second class will survive.</p></div><div class="calibre2" title="Evaluating the decision tree classifier"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec88" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Evaluating the decision tree classifier</h2></div></div></div><p class="calibre11">As before, we <a id="id528" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can calculate our confusion matrix and kappa statistic:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-39 []
  (let [data (-&gt; (load-data "titanic.tsv")
                 (i/transform-col :age age-categories)
                 (:rows))
        tree (decision-tree :survived [:pclass :sex :age] data)]
    (confusion-matrix (map :survived data)
                      (map (partial tree-classify tree) data))))</pre></div><p class="calibre11">The confusion matrix looks like this:</p><div class="calibre2"><pre class="programlisting">|   |   n |   y |
|---+-----+-----|
| n | 763 |  46 |
| y | 219 | 281 |</pre></div><p class="calibre11">We can immediately see that the classifier is generating a lot of false negatives: <code class="literal">219</code>. Let's calculate the kappa statistic:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-40 []
   (let [data (-&gt; (load-data "titanic.tsv")
                  (i/transform-col :age age-categories)
                  (:rows))
         tree (decision-tree :survived [:pclass :sex :age] data)
         ys     (map :survived data)
         y-hats (map (partial tree-classify tree) data)]
     (float (kappa-statistic ys y-hats))))

;; 0.541</pre></div><p class="calibre11">Our tree classifier isn't <a id="id529" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>performing nearly as well as others we have tried. One way we could try to improve the accuracy is to increase the number of predictors we're using. Rather than use crude categories for age, let's use the actual data for age as a feature. This will allow our classifier to better distinguish between our passengers. While we're at it, let's add the fare too:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-41 []
   (let [data (-&gt; (load-data "titanic.tsv")
                  (:rows))
         tree (decision-tree :survived
                             [:pclass :sex :age :fare] data)
         ys     (map :survived data)
         y-hats (map (partial tree-classify tree) data)]
     (float (kappa-statistic ys y-hats))))

;; 0.925</pre></div><p class="calibre11">Great! We've made fantastic progress; our new model is the best yet. By adding more granular predictors, we've built a model that's able to predict with a very high degree of accuracy.</p><p class="calibre11">Before we celebrate too much, though, we should think carefully about how general our model is. The purpose of building a classifier is usually to make predictions about new data. This means that it should perform well on data that it's never seen before. The model we've just built has a significant problem. To understand what it is, we'll turn to the library clj-ml, which contains a variety of functions for training and testing classifiers.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Classification with clj-ml"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec90" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Classification with clj-ml</h1></div></div></div><p class="calibre11">While building <a id="id530" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>our own versions of logistic regression, naive Bayes, and decision trees has provided a valuable opportunity to talk about the theory behind them, Clojure gives us several libraries for building classifiers. One of the better supported is the clj-ml library.</p><p class="calibre11">The clj-ml library is currently maintained by Josua Eckroth and is documented on his GitHub page at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/joshuaeckroth/clj-ml">https://github.com/joshuaeckroth/clj-ml</a>. The library provides Clojure interfaces for <a id="id531" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>running linear regression described in the previous chapter, as well as classification with logistic regression, naive Bayes, decision trees, and other algorithms.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note41" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">The underlying implementation for most machine learning functionality in clj-ml is provided by the Java machine learning library <code class="literal">Weka</code>. <span class="strong1"><strong class="calibre12">Waikato Environment for Knowledge Analysis</strong></span> (<span class="strong1"><strong class="calibre12">Weka</strong></span>), an open source machine learning project released and maintained primarily by the Machine Learning Group at the <a id="id532" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>University of Waikato, New Zealand (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.cs.waikato.ac.nz/ml/">http://www.cs.waikato.ac.nz/ml/</a>).</p></div></div><div class="calibre2" title="Loading data with clj-ml"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec89" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Loading data with clj-ml</h2></div></div></div><p class="calibre11">Because of <a id="id533" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>its specialized support for machine learning algorithms, clj-ml provides functions for creating datasets that identify the classes and attributes of a dataset. The function <code class="literal">clj-ml.data/make-dataset</code> allows us to create a dataset that can be passed to Weka's classifiers. In the following code, we include <code class="literal">clj-ml.data</code> as <code class="literal">mld</code>:</p><div class="calibre2"><pre class="programlisting">(defn to-weka [dataset]
  (let [attributes [{:survived ["y" "n"]}
                    {:pclass ["first" "second" "third"]}
                    {:sex ["male" "female"]}
                    :age
                    :fare]
        vectors (-&gt;&gt; dataset
                     (i/$ [:survived :pclass :sex :age :fare])
                     (i/to-vect))]
    (mld/make-dataset :titanic-weka attributes vectors
                      {:class :survived})))</pre></div><p class="calibre11">
<code class="literal">mld/make-dataset</code> expects to receive the name of the dataset, a vector of attributes, a dataset as a sequence of row vectors, and an optional map of further settings. The attributes identify the column names and, in the case of categorical variables, also enumerate all the possible categories. Categorical variables, for example <code class="literal">:survived</code>, are passed as a map <code class="literal">{:survived ["y" "n"]}</code>, whereas continuous variables such as <code class="literal">:age</code> and <code class="literal">:fare</code> are passed as straightforward keywords. The dataset must be provided as a sequence of row <a id="id534" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vectors. To construct this, we're simply using Incanter's <code class="literal">i/$</code> function and calling <code class="literal">i/to-vect</code> on the results.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note42" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">While <code class="literal">make-dataset</code> is a flexible way to create datasets from arbitrary data sources, <code class="literal">clj-ml.io</code> provides a <code class="literal">load-instances</code> function that loads data from a variety of sources such as CSV or Attribute-Relation File Format (ARFF) files and the MongoDB database.</p></div></div><p class="calibre11">With our dataset in a format that clj-ml understands, it's time to train a classifier.</p></div><div class="calibre2" title="Building a decision tree in clj-ml"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec90" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Building a decision tree in clj-ml</h2></div></div></div><p class="calibre11">Clj-ml <a id="id535" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implements a large variety of classifiers, and all are <a id="id536" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>accessible through the <code class="literal">cl/make-classifier</code> function. We pass two keyword arguments to the constructor: the classifier type and an algorithm to use. For example, let's look at the <code class="literal">:decision-tree</code>, <code class="literal">:c45</code> algorithm. The <span class="strong1"><strong class="calibre12">C4.5 algorithm</strong></span> <a id="id537" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>was devised by Ross Quinlan and builds a tree classifier based on information entropy in the same way as our very own <code class="literal">tree-classifier</code> function from earlier in the chapter. C4.5 extends the classifier we built in a couple of ways:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Where none of the predictors provide any information gain, C4.5 creates a decision node higher up the tree using the expected value of the class</li><li class="listitem">If a previously-unseen class is encountered, C4.5 will create a decision node higher up the tree with the expected value of the class</li></ul></div><p class="calibre11">We can create a decision tree in clj-ml with the following code:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-42 []
   (let [dataset (to-weka (load-data "titanic.tsv"))
         classifier (-&gt; (cl/make-classifier :decision-tree :c45)
                        (cl/classifier-train dataset))
         classify (partial cl/classifier-classify classifier)
         ys     (map str  (mld/dataset-class-values dataset))
         y-hats (map name (map classify dataset))]
     (println "Confusion:" (confusion-matrix ys y-hats))
     (println "Kappa:" (kappa-statistic ys y-hats))))</pre></div><p class="calibre11">The preceding code returns the following information:</p><div class="calibre2"><pre class="programlisting">;; Confusion:
;; |   |   n |   y |
;; |---+-----+-----|
;; | n | 712 |  97 |
;; | y | 153 | 347 |
;;
;; Kappa: 0.587</pre></div><p class="calibre11">Notice how we don't need to explicitly provide the class and predictor attributes while training our <a id="id538" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>classifier or using it for prediction. The Weka dataset already <a id="id539" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>contains the information about the class attribute of each instance, and the classifier will use all the attributes it can to arrive at a prediction. In spite of this, the results still aren't as good as we were getting before. The reason is that Weka's implementation of decision trees is refusing to over-fit the data.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Bias and variance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec91" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bias and variance</h1></div></div></div><p class="calibre11">Overfitting is a <a id="id540" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>problem that occurs with machine learning algorithms that are able to generate very accurate results on a training dataset but fail to generalize very well from what they've learned. We say that models which have overfit the data have very high variance. When we trained our decision tree on data that included the numeric age of passengers, we were overfitting the data.</p><p class="calibre11">Conversely, certain models may have very high bias. This is a situation where the model has a strong tendency towards a certain outcome irrespective of the training examples to the contrary. Recall our example of a classifier that always predicts that a survivor will perish. This classifier would perform well on dataset with low survivor rates, but very poorly otherwise.</p><p class="calibre11">In the case of high bias, the model is unlikely to perform well on diverse inputs at the training stage. In the case of high variance, the model is unlikely to perform well on data that differs from that which it was trained on.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title4"><a id="note43" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">Like the balance to be struck between Type I and Type II errors in hypothesis testing, balancing bias and variance is critical for producing good results from machine learning.</p></div></div><p class="calibre11">If we have too many features, the learned hypothesis may fit the training set very well but fail to generalize to new examples very well.</p><div class="calibre2" title="Overfitting"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec91" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Overfitting</h2></div></div></div><p class="calibre11">The secret to identifying overfitting, then, is to test the classifier on examples that it has not been trained on. If the classifier performs poorly on these examples then there is a possibility that the model is overfitting.</p><p class="calibre11">The usual approach is to <a id="id541" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>divide the dataset into two groups: a training set and a test set. The training set is used to train the classifier, and the test set is used to determine whether the classifier is able to generalize well from what it has learned.</p><p class="calibre11">The test set should be large enough that it will be a representative sample from the dataset, but should still leave the majority of records for training. Test sets are often around 10-30 percent of the overall dataset. Let's use <code class="literal">clj-ml.data/do-split-dataset</code> to return two sets of instances. The smaller will be our test set and the larger will be our training set:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-43 []
  (let [[test-set train-set] (-&gt; (load-data "titanic.tsv")
                                 (to-weka)
                                 (mld/do-split-dataset :percentage
                                                       30))
        classifier (-&gt; (cl/make-classifier :decision-tree :c45)
                       (cl/classifier-train train-set))
        classify (partial cl/classifier-classify classifier)
        ys     (map str  (mld/dataset-class-values test-set))
        y-hats (map name (map classify test-set))]
    (println "Confusion:" (confusion-matrix ys y-hats))
    (println "Kappa:" (kappa-statistic ys y-hats))))

;; Confusion:
;; |   |   n |   y |
;; |---+-----+-----|
;; | n | 152 |   9 |
;; | y |  65 | 167 |
;;
;; Kappa: 0.630</pre></div><p class="calibre11">If you compare this kappa statistic to the previous one, you'll see that actually our accuracy has improved on unseen data. Whilst this appears to suggest our classifier is not overfitting our training set, it doesn't seem very realistic that our classifier should be able to make better predictions for new data than the data we've actually told it about.</p><p class="calibre11">This suggests that we may have been fortunate with the values that were returned in our test set. Perhaps this just happened to contain some of the easier-to-classify passengers compared to the training set. Let's <a id="id542" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see what happens if we take the test set from the final 30 percent instead:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-44 []
  (let [[train-set test-set] (-&gt; (load-data "titanic.tsv")
                                 (to-weka)
                                 (mld/do-split-dataset :percentage
                                                       70))
        classifier (-&gt; (cl/make-classifier :decision-tree :c45)
                       (cl/classifier-train train-set))
        classify (partial cl/classifier-classify classifier)
        ys     (map str  (mld/dataset-class-values test-set))
        y-hats (map name (map classify test-set))]
    (println "Kappa:" (kappa-statistic ys y-hats))))

;; Kappa: 0.092</pre></div><p class="calibre11">The classifier is struggling on test data from the final 30 percent of the dataset. To get a fair reflection of the actual performance of the classifier overall, therefore, we'll want to make sure we test it on several random subsets of the data to even out the classifier's performance.</p></div><div class="calibre2" title="Cross-validation"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec92" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Cross-validation</h2></div></div></div><p class="calibre11">The process of splitting <a id="id543" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a dataset into complementary subsets of training and test data is called cross-validation. To reduce the variability in output we've just seen, with a lower error rate on the test set compared to the training set, it's usual to run multiple rounds of cross-validation on different partitions of the data. By averaging the results of all runs we get a much more accurate picture of the model's true accuracy. This is such a common practice that clj-ml includes a function for just this purpose:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-45 []
  (let [dataset (-&gt; (load-data "titanic.tsv")
                    (to-weka))
         classifier (-&gt; (cl/make-classifier :decision-tree :c45)
                        (cl/classifier-train dataset))
         evaluation (cl/classifier-evaluate classifier
                                            :cross-validation
                                            dataset 10)]
     (println (:confusion-matrix evaluation))
     (println (:summary evaluation))))</pre></div><p class="calibre11">In the preceding code, we make use of <code class="literal">cl/classifier-evaluate</code> to run 10 cross-validations on our dataset. The result is returned as a map with useful information about the model performance—for example, a confusion matrix and a list of summary statistics—including the <a id="id544" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kappa statistic we've been tracking so far. We print out the confusion matrix and the summary string that clj-ml provides, as follows:</p><div class="calibre2"><pre class="programlisting">;; === Confusion Matrix ===
;;
;;    a   b   &lt;-- classified as
;;  338 162 |   a = y
;;   99 710 |   b = n
;;
;;
;; Correctly Classified Instances        1048            80.0611 %
;; Incorrectly Classified Instances       261            19.9389 %
;; Kappa statistic                          0.5673
;; Mean absolute error                      0.284
;; Root mean squared error                  0.3798
;; Relative absolute error                 60.1444 %
;; Root relative squared error             78.171  %
;; Coverage of cases (0.95 level)          99.3888 %
;; Mean rel. region size (0.95 level)      94.2704 %
;; Total Number of Instances             1309    </pre></div><p class="calibre11">The kappa after 10 cross-validations is 0.56, only slightly lower than our model validated against the training data. This seems about as high as we will be able to get.</p></div><div class="calibre2" title="Addressing high bias"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec93" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Addressing high bias</h2></div></div></div><p class="calibre11">Whereas overfitting can be caused by including too many features in our model—such as when we included age as a categorical variable in our decision tree—high bias can be caused by other factors including not having enough data.</p><p class="calibre11">One simple way of <a id="id545" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>increasing the accuracy of the model is to ensure that there are no missing values in the training set. Missing values are necessarily discarded by the model, limiting the number of training examples from which the model can learn. With a relatively small dataset such as this, each example can have a material effect on the outcome, and there are numerous age values and one fare value missing from the dataset.</p><p class="calibre11">We could simply substitute the mean value for a missing value in numeric columns. This is a reasonable default value and a fair tradeoff—in return for slightly lowering the variance of the field, we are potentially gaining several more training examples.</p><p class="calibre11">Clj-ml contains numerous filters in the <code class="literal">clj-ml.filters</code> namespace that are able to alter the dataset in some way. A useful filter is <code class="literal">:replace-missing-values</code>, which will substitute any missing <a id="id546" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>numeric values with the means from the dataset. For categorical data, the modal category is substituted.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-46 []
  (let [dataset (-&gt;&gt; (load-data "titanic.tsv")
                     (to-weka)
                     (mlf/make-apply-filter
                      :replace-missing-values {}))
        classifier (-&gt; (cl/make-classifier :decision-tree :c45)
                       (cl/classifier-train dataset))
        evaluation (cl/classifier-evaluate classifier
                                           :cross-validation
                                           dataset 10)]
    (println (:kappa evaluation))))

;; 0.576</pre></div><p class="calibre11">Simply plugging the missing values in the age column has nudged our kappa statistic upwards. Our model is currently struggling to distinguish between passengers with different survival outcomes and more information may help the algorithm determine the correct class. Whilst we could return to the data and pull in all of the remaining fields, it's also possible to construct new features out of existing features.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note44" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">For numeric values, another way of increasing the number of parameters is to include polynomial versions of the values as features. For example we could create features for age<sup class="calibre112">2</sup> and age<sup class="calibre112">3</sup> simply by squaring or cubing the existing age value. While these may appear to add no new information to the model, polynomials scale differently and provide alternative features for the model to learn from.</p></div></div><p class="calibre11">The final way we'll look at for balancing bias and variance is to combine the output from multiple models.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Ensemble learning and random forests"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec92" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Ensemble learning and random forests</h1></div></div></div><p class="calibre11">Ensemble learning <a id="id547" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>combines the output from multiple models to obtain a better prediction than could be obtained with any of the models individually. The principle is that the combined accuracy of many weak learners is greater than any of the weak learners taken individually.</p><p class="calibre11">Random forests is <a id="id548" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>an ensemble learning algorithm devised and trademarked by Leo Breiman and Adele Cutler. It combines multiple decision trees into one large forest learner. Each tree is trained on the data using a subset of the available features, meaning that each tree will have a slightly different view of the data and is capable of generating a different prediction from that of its peers.</p><p class="calibre11">Creating a Random Forest in clj-ml simply requires that we alter the arguments to <code class="literal">cl/make-classifier</code> to <code class="literal">:decision-tree</code>, <code class="literal">:random-forest</code>.</p><div class="calibre2" title="Bagging and boosting"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title5"><a id="ch04lvl2sec94" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Bagging and boosting</h2></div></div></div><p class="calibre11">Bagging and <a id="id549" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>boosting are two opposing techniques for creating ensemble models. Boosting is the name for a general technique of building an ensemble by training each new <a id="id550" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>model to emphasize correct the classification of training examples that previous models weren't able to correctly classify. It is a <a id="id551" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><span class="strong1"><strong class="calibre12">meta-algorithm</strong></span>.</p><div class="sidebar" title="Note"><div class="inner"><h3 class="title6"><a id="note45" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre22">One of the most <a id="id552" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>popular boosting algorithms is <span class="strong1"><strong class="calibre12">AdaBoost</strong></span>, a portmanteau of "adaptive boosting". As long as each model performs slightly better than random guessing, the combined output can be shown to converge to a strong learner.</p></div></div><p class="calibre11">Bagging is a portmanteau of "bootstrap aggregating" and is the name of another meta-algorithm that is usually applied to decision tree learners but can be applied to other learners too. In cases where a single tree might overfit the training data, bagging helps reduce the variance of the combined model. It does this by sampling the training data with replacement, just as with our bootstrapped standard error at the beginning of the chapter. As a result, each model in the ensemble has a differently incomplete view of the world, making it less likely that the combined model will learn an overly specific hypothesis on the training data. Random forests is an example of a bagging algorithm.</p><div class="calibre2"><pre class="programlisting">(defn ex-4-47 []
  (let [dataset (-&gt;&gt; (load-data "titanic.tsv")
                     (to-weka)
                     (mlf/make-apply-filter
                      :replace-missing-values {}))
        classifier (cl/make-classifier :decision-tree
                                       :random-forest)
        evaluation (cl/classifier-evaluate classifier
                                           :cross-validation
                                           dataset 10)]
    (println (:confusion-matrix evaluation))
    (println (:summary evaluation))))</pre></div><p class="calibre11">With the random forests classifier, you should observe a kappa of around 0.55, slightly lower than the decision tree we have been optimizing. The random forest implementation has sacrificed some of the variance of the model.</p><p class="calibre11">Whilst this might seem <a id="id553" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>disappointing, it is actually part of the reason for random forests' <a id="id554" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>appeal. Their ability to strike a balance between bias and variance makes them flexible and general-purpose classifiers suitable for a wide variety of problems.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Saving the classifier to a file"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec93" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Saving the classifier to a file</h1></div></div></div><p class="calibre11">Finally, we can write <a id="id555" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>out our classifier to a file using <code class="literal">clj-ml.utils/serialize-to-file</code>:</p><div class="calibre2"><pre class="programlisting">(defn ex-4-48 []
  (let [dataset (-&gt;&gt; (load-data "titanic.tsv")
                     (to-weka)
                     (mlf/make-apply-filter
                      :replace-missing-values {}))
        classifier (cl/make-classifier :decision-tree
                                       :random-forest)
        file (io/file (io/resource "classifier.bin"))]
    (clu/serialize-to-file classifier file)))</pre></div><p class="calibre11">At some point later, we can load up our trained classifier using the <code class="literal">clj-ml.utils/deserialize-from-file</code> and immediately begin classifying new data.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title2"><a id="ch04lvl1sec94" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we've learned about how to make use of categorical variables to group data into classes.</p><p class="calibre11">We've seen how quantify the difference between groups using the odds ratio and relative risk, and how to perform statistical significance tests on groups using the <span class="strong1"><em class="calibre13">X</em></span><sup class="calibre42">2</sup> test. We've learned about how to build machine learning models suitable for the task of classification with a variety of techniques: logistic regression, naive Bayes, decision trees, and random forests, and several methods of evaluating them; the confusion matrix and the kappa statistic. We also learned about the opposing dangers of high bias and of overfitting in machine learning, and how to ensure that your model is not overfitting by making use of cross-validation. Finally, we've seen how the clj-ml library can help to prepare data and to build many different types of classifiers and save them for future use.</p><p class="calibre11">In the next chapter, we'll learn about how to adapt some of the techniques we've learned about so far to the task of processing very large datasets that exceed the storage and processing capabilities of any single computer—so-called <span class="strong1"><strong class="calibre12">Big Data</strong></span>. We'll see how one of the techniques we encountered in this chapter, gradient descent, turns out to be particularly amenable to parameter optimization on a very large scale.</p></div></div>



  </body></html>