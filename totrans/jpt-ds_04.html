<html><head></head><body><div><h1 class="header-title">Data Mining and SQL Queries</h1>
                
            
            
                
<p class="calibre5">PySpark exposes the Spark programming model to Python. Spark is a fast, general engine for large-scale data processing. We can use Python under Jupyter. So, we can use Spark in Jupyter.</p>
<p class="calibre5">Installing Spark requires the following components to be installed on your machine:</p>
<ul class="calibre19">
<li class="calibre20">Java JDK.</li>
<li class="calibre20">Scala from <a href="http://www.scala-lang.org/download/" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9">http://www.scala-lang.org/download/</a>.</li>
<li class="calibre20">Python recommend downloading Anaconda with Python (from <a href="http://continuum.io" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9">http://continuum.io</a>).</li>
<li class="calibre20">Spark from <a href="https://spark.apache.org/downloads.html" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9">https://spark.apache.org/downloads.html</a>.</li>
<li class="calibre20"><kbd class="calibre21">winutils</kbd>: This is a command-line utility that exposes Linux commands to Windows. There are 32-bit and 64-bit versions available at:
<ul class="calibre19">
<li class="calibre20">32-bit <kbd class="calibre21">winutils.exe</kbd> at <a href="https://code.google.com/p/rrd-hadoop-win32/source/checkout" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9">https://code.google.com/p/rrd-hadoop-win32/source/checkout</a></li>
<li class="calibre20">64-bit <kbd class="calibre21">winutils.exe</kbd> at <a href="https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9">https://github.com/steveloughran/winutils/tree/master/hadoop-2.6.0/bin</a></li>
</ul>
</li>
</ul>
<p class="calibre5">Then set environment variables that show the position of the preceding components:</p>
<ul class="calibre19">
<li class="calibre20"><kbd class="calibre21">JAVA_HOME</kbd>: The bin directory where you installed JDK</li>
<li class="calibre20"><kbd class="calibre21">PYTHONPATH</kbd>: Directory where Python was installed</li>
<li class="calibre20"><kbd class="calibre21">HADOOP_HOME</kbd>: Directory where <kbd class="calibre21">winutils</kbd> resides</li>
<li class="calibre20"><kbd class="calibre21">SPARK_HOME</kbd>: Where Spark is installed</li>
</ul>
<p class="calibre5">These components are readily available over the internet for a variety of operating systems. I have successfully installed these previous components in a Windows environment and a Mac environment.</p>
<p class="calibre5">Once you have these installed you should be able to run the command, <kbd class="calibre21">pyspark</kbd>, from a command line window and a Jupyter Notebook with Python (with access to Spark) can be used. In my installation I used the command:</p>
<pre class="commandlinepackt"><strong class="calibre3">pyspark</strong>  </pre>
<p class="calibre5">As I had installed Spark in the root with the <kbd class="calibre21">\spark</kbd> directory. Yes, <kbd class="calibre21">pyspark</kbd> is a built-in tool for use by Spark.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Special note for Windows installation</h1>
                
            
            
                
<p class="calibre5">Spark (really Hadoop) needs a temporary storage location for its working set of data. Under Windows this defaults to the <kbd class="calibre21">\tmp\hive</kbd> location. If the directory does not exist when Spark/Hadoop starts it will create it. Unfortunately, under Windows, the installation does not have the correct tools built-in to set the access privileges to the directory.</p>
<p class="calibre5">You should be able to run <kbd class="calibre21">chmod</kbd> under <kbd class="calibre21">winutils</kbd> to set the access privileges for the <kbd class="calibre21">hive</kbd> directory. However, I have found that the <kbd class="calibre21">chmod</kbd> function does not work correctly.</p>
<p class="calibre5">A better idea has been to create the <kbd class="calibre21">tmp\hive</kbd> directory yourself in admin mode. And then grant full privileges to the hive directory to all users, again in admin mode.</p>
<p class="calibre5">Without this change, Hadoop fails right away. When you start <kbd class="calibre21">pyspark</kbd>, the output (including any errors) are displayed in the command line window. One of the errors will be insufficient access to this directory.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using Spark to analyze data</h1>
                
            
            
                
<p class="calibre5">The first thing to do in order to access Spark is to create a <kbd class="calibre21">SparkContext</kbd>. The <kbd class="calibre21">SparkContext</kbd> initializes all of Spark and sets up any access that may be needed to Hadoop, if you are using that as well.</p>
<p class="calibre5">The initial object used to be a <kbd class="calibre21">SQLContext</kbd>, but that has been deprecated recently in favor of <kbd class="calibre21">SparkContext</kbd>, which is more open-ended.</p>
<p class="calibre5">We could use a simple example to just read through a text file as follows:</p>
<pre class="commandlinepackt"><strong class="calibre3">from pyspark import SparkContext</strong>
<strong class="calibre3">sc = SparkContext.getOrCreate()</strong>

<strong class="calibre3">lines = sc.textFile("B05238_04 Spark Total Line Lengths.ipynb")</strong>
<strong class="calibre3">lineLengths = lines.map(lambda s: len(s))</strong>
<strong class="calibre3">totalLength = lineLengths.reduce(lambda a, b: a + b)</strong>
<strong class="calibre3">print(totalLength)</strong>  </pre>
<p class="calibre5">In this example:</p>
<ul class="calibre19">
<li class="calibre20">We obtain a <kbd class="calibre21">SparkContext</kbd></li>
<li class="calibre20">With the context, read in a file (the Jupyter file for this example)</li>
<li class="calibre20">We use a Hadoop <kbd class="calibre21">map</kbd> function to split up the text file into different lines and gather the lengths</li>
<li class="calibre20">We use a Hadoop <kbd class="calibre21">reduce</kbd> function to calculate the length of all the lines</li>
<li class="calibre20">We display our results</li>
</ul>
<p class="calibre5">Under Jupyter this looks like the following:</p>
<div><div><img class="image-border51" src="img/bbaf558a-ad63-46c7-ae00-83fe90b04571.png"/></div>
</div>


            

            
        
    </div>



  
<div><h1 class="header-title">Another MapReduce example</h1>
                
            
            
                
<p class="calibre5">We can use MapReduce in another example where we get the word counts from a file. A standard problem, but we use MapReduce to do most of the heavy lifting. We can use the source code for this example. We can use a script similar to this to count the word occurrences in a file:</p>
<pre class="commandlinepackt"><strong class="calibre3">import pyspark</strong>
<strong class="calibre3">if not 'sc' in globals():</strong>
<strong class="calibre3">    sc = pyspark.SparkContext()</strong>

<strong class="calibre3">text_file = sc.textFile("Spark File Words.ipynb")</strong>
<strong class="calibre3">counts = text_file.flatMap(lambda line: line.split(" ")) \</strong>
<strong class="calibre3">             .map(lambda word: (word, 1)) \</strong>
<strong class="calibre3">             .reduceByKey(lambda a, b: a + b)</strong>
<strong class="calibre3">for x in counts.collect():</strong>
<strong class="calibre3">    print x</strong>  </pre>
<p>We have the same preamble to the coding.</p>
<p class="calibre5">Then we load the text file into memory.</p>
<div><kbd class="calibre21">text_file</kbd> is a Spark <strong class="calibre3">RDD</strong> (<strong class="calibre3">Resilient Distributed Dataset</strong>), not a data frame.</div>
<p class="calibre5">It is assumed to be massive and the contents distributed over many handlers.</p>
<p class="calibre5">Once the file is loaded we split each line into words, and then use a <kbd class="calibre21">lambda</kbd> function to tick off each occurrence of a word. The code is truly creating a new record for each word occurrence, such as <em class="calibre18">at appears 1</em>, <em class="calibre18">at appears 1</em>. For example, if the word <em class="calibre18">at</em> appears twice each occurrence would have a record added like <em class="calibre18">at</em> <em class="calibre18">appears 1</em>. The idea is to not aggregate results yet, just record the appearances that we see. The idea is that this process could be split over multiple processors where each processor generates these low-level information bits. We are not concerned with optimizing this process at all.</p>
<p class="calibre5">Once we have all of these records we reduce/summarize the record set according to the word occurrences mentioned.</p>
<p class="calibre5">The <kbd class="calibre21">counts</kbd> object is also RDD in Spark. The last <kbd class="calibre21">for</kbd> loop runs a <kbd class="calibre21">collect()</kbd> against the RDD. As mentioned, this RDD could be distributed among many nodes. The <kbd class="calibre21">collect()</kbd> function pulls in all copies of the RDD into one location. Then we loop through each record.</p>
<p class="calibre5">When we run this in Jupyter we see something akin to this display:</p>
<div><div><img class="image-border52" src="img/f0c88072-d9fa-4076-a14c-502f89c2dca1.png"/></div>
</div>
<p class="calibre5">The listing is abbreviated as the list of words continues for some time.</p>
<p>The previous example doesn't work well with Python 3. There is a workaround when coding directly in Python, but not for Jupyter.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using SparkSession and SQL</h1>
                
            
            
                
<p class="calibre5">Spark exposes many SQL-like actions that can be taken upon a data frame. For example, we could load a data frame with product sales information in a CSV file:</p>
<pre class="commandlinepackt"><strong class="calibre3">from pyspark.sql import SparkSession </strong>
<strong class="calibre3">spark = SparkSession(sc) </strong>

<strong class="calibre3">df = spark.read.format("csv") \</strong>
<strong class="calibre3">        .option("header", "true") \</strong>
<strong class="calibre3">        .load("productsales.csv");</strong>
<strong class="calibre3">df.show()</strong>  </pre>
<p class="calibre5">The example:</p>
<ul class="calibre19">
<li class="calibre20">Starts a <kbd class="calibre21">SparkSession</kbd> (needed for most data access)</li>
<li class="calibre20">Uses the session to read a CSV formatted file, that contains a header record</li>
<li class="calibre20">Displays initial rows</li>
</ul>
<div><div><img class="image-border53" src="img/3b841e8d-1388-4d3e-b9e1-295886ea97fa.png"/></div>
</div>
<p class="calibre5">We have a few interesting columns in the sales data:</p>
<ul class="calibre19">
<li class="calibre20">Actual sales for the products by division</li>
<li class="calibre20">Predicted sales for the products by division</li>
</ul>
<p class="calibre5">If this were a bigger file, we could use SQL to determine the extent of the product list. Then the following is the Spark SQL to determine the product list:</p>
<pre class="commandlinepackt"><strong class="calibre3">df.groupBy("PRODUCT").count().show()</strong>  </pre>
<p class="calibre5">The data frame <kbd class="calibre21">groupBy</kbd> function works very similar to the SQL <kbd class="calibre21">Group By</kbd> clause. <kbd class="calibre21">Group By</kbd> collects the items in the dataset according to the values in the column specified. In this case the <kbd class="calibre21">PRODUCT</kbd> column. The <kbd class="calibre21">Group By</kbd> results in a dataset being established with the results. As a dataset, we can query how many rows are in each with the <kbd class="calibre21">count()</kbd> function.</p>
<p class="calibre5">So, the result of the <kbd class="calibre21">groupBy</kbd> is a count of the number of items that correspond to the grouping element. For example, we grouped the items by <kbd class="calibre21">CHAIR</kbd> and found 288 of them:</p>
<div><div><img class="image-border54" src="img/3da579b9-b3d4-4abd-b655-a021a33f727a.png"/></div>
</div>
<p class="calibre5">So, we obviously do not have real product data. It is unlikely that any company has the exact same number of products in each line.</p>
<p class="calibre5">We can look into the dataset to determine how the different divisions performed in actual versus predicted sales using the <kbd class="calibre21">filter()</kbd> command in this example:</p>
<pre class="commandlinepackt"><strong class="calibre3">df.filter(df['ACTUAL'] &gt; df['PREDICT']).show()</strong>  </pre>
<p class="calibre5">We pass a logical test to the <kbd class="calibre21">filter</kbd> command that will be operated against every row in the dataset. If the data in that row passes the test then the row is returned. Otherwise, the row is dropped from the results.</p>
<p class="calibre5">Our test is only interested in sales where the actual sales figure exceeds the predicted values.</p>
<p class="calibre5">Under Jupyter this looks as like the following:</p>
<div><div><img class="image-border55" src="img/9ffb9eb1-9a13-497c-992c-63465c614db1.png"/></div>
</div>
<p class="calibre5">So, we get a reduced result set. Again, this was produced by the <kbd class="calibre21">filter</kbd> function as a data frame and can then be called upon to <kbd class="calibre21">show</kbd> as any other data frame. Notice the third record from the previous display is not present as its actual sales were less than predicted. It is always a good idea to use a quick survey to make sure you have the correct results.</p>
<p class="calibre5">What if we wanted to pursue this further and determine which were the best performing areas within the company?</p>
<p class="calibre5">If this were a database table we could create another column that stored the difference between actual and predicted sales and then sort our display on that column. We can perform very similar steps in Spark.</p>
<p class="calibre5">Using a data frame we could use coding like this:</p>
<pre class="commandlinepackt"><strong class="calibre3">#register dataframe as temporary SQL table</strong>
<strong class="calibre3">df.createOrReplaceTempView("sales")</strong>
<strong class="calibre3"># pull the values by the difference calculated</strong>
<strong class="calibre3">sqlDF = spark.sql("SELECT *, ACTUAL-PREDICT as DIFF FROM sales ORDER BY DIFF desc")</strong>
<strong class="calibre3">sqlDF.show()</strong>  </pre>
<p class="calibre5">The first statement is creating a view/data frame within the context for further manipulation. This view is lazy evaluated, will not persist unless specific steps are taken, and most importantly can be accessed as a hive view. The view is available directly from the <kbd class="calibre21">SparkContext</kbd>.</p>
<p class="calibre5">We then create a new data frame with the computed new column using the new sales view that we created. Under Jupyter this looks as follows:</p>
<div><div><img class="image-border56" src="img/17685618-ddb1-4d13-8423-b1176f2f9b41.png"/></div>
</div>
<p class="calibre5">Again, I don't think we have realistic values as the differences are very far off from predicted values.</p>
<p>The data frames created are immutable, unlike database tables.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Combining datasets</h1>
                
            
            
                
<p class="calibre5">So, we have seen moving a data frame into Spark for analysis. This appears to be very close to SQL tables. Under SQL it is standard practice not to reproduce items in different tables. For example, a product table might have the price and an order table would just reference the product table by product identifier, so as not to duplicate data. So, then another SQL practice is to join or combine the tables to come up with the full set of information needed. Keeping with the order analogy, we combine all of the tables involved as each table has pieces of data that are needed for the order to be complete.</p>
<p class="calibre5">How difficult would it be to create a set of tables and join them using Spark? We will use example tables of <kbd class="calibre21">Product</kbd>, <kbd class="calibre21">Order</kbd>, and <kbd class="calibre21">ProductOrder</kbd>:</p>
<table class="msotablegrid">
<tbody class="calibre11">
<tr class="calibre12">
<td class="calibre13">
<p class="calibre15"><strong class="calibre7">Table</strong></p>
</td>
<td class="calibre13">
<p class="calibre15"><strong class="calibre7">Columns</strong></p>
</td>
</tr>
<tr class="calibre16">
<td class="calibre13">
<p class="calibre15">Product</p>
</td>
<td class="calibre13">
<p class="calibre15">Product ID,</p>
<p class="calibre15">Description,</p>
<p class="calibre15">Price</p>
</td>
</tr>
<tr class="calibre12">
<td class="calibre13">
<p class="calibre15">Order</p>
</td>
<td class="calibre13">
<p class="calibre15">Order ID,</p>
<p class="calibre15">Order Date</p>
</td>
</tr>
<tr class="calibre17">
<td class="calibre13">
<p class="calibre15">ProductOrder</p>
</td>
<td class="calibre13">
<p class="calibre15">Order ID,</p>
<p class="calibre15">Product ID,</p>
<p class="calibre15">Quantity</p>
</td>
</tr>
</tbody>
</table>
<p class="calibre5"> </p>
<p class="calibre5">So, an <kbd class="calibre21">Order</kbd> has a list of <kbd class="calibre21">Product</kbd>/<kbd class="calibre21">Quantity</kbd> values associated.</p>
<p class="calibre5">We can populate the data frames and move them into Spark:</p>
<pre class="commandlinepackt"><strong class="calibre3">from pyspark import SparkContext</strong>
<strong class="calibre3">from pyspark.sql import SparkSession</strong>

<strong class="calibre3">sc = SparkContext.getOrCreate()</strong>
<strong class="calibre3">spark = SparkSession(sc) </strong>

<strong class="calibre3"># load product set</strong>
<strong class="calibre3">productDF = spark.read.format("csv") \</strong>
<strong class="calibre3">        .option("header", "true") \</strong>
<strong class="calibre3">        .load("product.csv");</strong>
<strong class="calibre3">productDF.show()</strong>
<strong class="calibre3">productDF.createOrReplaceTempView("product")</strong>

<strong class="calibre3"># load order set</strong>
<strong class="calibre3">orderDF = spark.read.format("csv") \</strong>
<strong class="calibre3">        .option("header", "true") \</strong>
<strong class="calibre3">        .load("order.csv");</strong>
<strong class="calibre3">orderDF.show()</strong>
<strong class="calibre3">orderDF.createOrReplaceTempView("order")</strong>

<strong class="calibre3"># load order/product set</strong>
<strong class="calibre3">orderproductDF = spark.read.format("csv") \</strong>
<strong class="calibre3">        .option("header", "true") \</strong>
<strong class="calibre3">        .load("orderproduct.csv");</strong>
<strong class="calibre3">orderproductDF.show()</strong>
<strong class="calibre3">orderproductDF.createOrReplaceTempView("orderproduct")</strong>  </pre>
<p class="calibre5">Now, we can attempt to perform an SQL-like <kbd class="calibre21">JOIN</kbd> operation among them:</p>
<pre class="commandlinepackt"><strong class="calibre3"># join the tables</strong>
<strong class="calibre3">joinedDF = spark.sql("SELECT * " \</strong>
<strong class="calibre3">      "FROM orderproduct " \</strong>
<strong class="calibre3">      "JOIN order ON order.orderid = orderproduct.orderid " \</strong>
<strong class="calibre3">      "ORDER BY order.orderid")</strong>
<strong class="calibre3">joinedDF.show()</strong>  </pre>
<p class="calibre5">Doing all of this in Jupyter results in the display as follows:</p>
<div><div><img class="image-border57" src="img/b653416b-1c6d-42df-9606-7d9883e10a3e.png"/></div>
</div>
<p class="calibre5">Our standard imports obtain a <kbd class="calibre21">SparkContext</kbd> and initialize a <kbd class="calibre21">SparkSession</kbd>. Note, the <kbd class="calibre21">getOrCreate</kbd> of the <kbd class="calibre21">SparkContext</kbd>. If you were to run this code outside of Jupyter there would be no context and a new one would be created. Under Jupyter, the startup for Spark in Jupyter initializes a context for all scripts. We can use that context at will with any Spark script, rather than have to create one ourselves.</p>
<p class="calibre5">Load our <kbd class="calibre21">product</kbd> table:</p>
<div><div><img class="image-border58" src="img/f2e2fb89-1932-4714-9c4b-104974f5b136.png"/></div>
</div>
<p class="calibre5">Load the <kbd class="calibre21">order</kbd> table:</p>
<div><img class="image-border59" src="img/d26f6dd2-656c-4fa2-b545-904e59e648d0.png"/></div>
<p class="calibre5">Load the <kbd class="calibre21">orderproduct</kbd> table. Note that at least one of the orders has multiple products:</p>
<div><img class="image-border60" src="img/3909e3fb-6f95-4072-9f1e-c63225e75526.png"/></div>
<p class="calibre5">We have the <kbd class="calibre21">orderid</kbd> column from <kbd class="calibre21">order</kbd> and <kbd class="calibre21">orderproduct</kbd> in the result set. We could be more selective in our query and specify the exact columns we want to be returned:</p>
<div><div><img class="image-border61" src="img/7253a504-d37c-47f7-85e8-3c10b30a2ba3.png"/></div>
</div>
<p>I had tried to use the Spark <kbd class="calibre21">join()</kbd> command with no luck.</p>
<p class="calibre5">The documentation and examples I found on the internet are old, sparse, and incorrect.</p>
<p class="calibre5">Using the command also presented the persistent error of a task not returning results in time. From the underlying Hadoop, I expect that processing tasks are normally broken up into separate tasks. I assume that Spark is breaking up functions into separate threads for completion similarly. It is not clear why such minor tasks are not completing as I was not asking it to perform anything extraordinary.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Loading JSON into Spark</h1>
                
            
            
                
<p class="calibre5">Spark can also access JSON data for manipulation. Here we have an example that:</p>
<ul class="calibre19">
<li class="calibre20">Loads a JSON file into a Spark data frame</li>
<li class="calibre20">Examines the contents of the data frame and displays the apparent schema</li>
<li class="calibre20">Like the other preceding data frames, moves the data frame into the context for direct access by the Spark session</li>
<li class="calibre20">Shows an example of accessing the data frame in the Spark context</li>
</ul>
<p class="calibre5">The listing is as follows:</p>
<p class="calibre5">Our standard includes for Spark:</p>
<pre class="commandlinepackt"><strong class="calibre3">from pyspark import SparkContext</strong>
<strong class="calibre3">from pyspark.sql import SparkSession </strong>
<strong class="calibre3">sc = SparkContext.getOrCreate()</strong>
<strong class="calibre3">spark = SparkSession(sc)</strong>  </pre>
<p class="calibre5">Read in the JSON and display what we found:</p>
<pre class="commandlinepackt"><strong class="calibre3">#using some data from file from https://gist.github.com/marktyers/678711152b8dd33f6346</strong>
<strong class="calibre3">df = spark.read.json("people.json")</strong>
<strong class="calibre3">df.show()</strong>  </pre>
<div><div><img class="image-border62" src="img/3248ff85-7185-4ba5-8b38-1facfcdc429b.png"/></div>
</div>
<p class="calibre5">I had a difficult time getting a standard JSON to load into Spark. Spark appears to expect one record of data per list of the JSON file versus most JSON I have seen pretty much formats the record layouts with indentation and the like.</p>
<p>Notice the use of null values where an attribute was not specified in an instance.</p>
<p class="calibre5">Display the interpreted schema for the data:</p>
<pre class="commandlinepackt"><strong class="calibre3">df.printSchema()</strong>  </pre>
<div><img class="image-border63" src="img/78bf59a2-f9ee-4ccd-89f1-2a5455b9355e.png"/></div>
<p class="calibre5">The default for all columns is <kbd class="calibre21">nullable</kbd>. You can change an attribute of a column, but you cannot change the value of a column as the data values are immutable.</p>
<p class="calibre5">Move the data frame into the context and access it from there:</p>
<pre class="commandlinepackt"><strong class="calibre3">df.registerTempTable("people")</strong>
<strong class="calibre3">spark.sql("select name from people"</strong><strong class="calibre3">).show()</strong>  </pre>
<div><div><img class="image-border64" src="img/269c578a-deec-424a-bd72-3a7f60ab54be.png"/></div>
</div>
<p class="calibre5">At this point, the <kbd class="calibre21">people</kbd> table works like any other temporary SQL table in Spark.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Using Spark pivot</h1>
                
            
            
                
<p class="calibre5">The <kbd class="calibre21">pivot()</kbd> function allows you to translate rows into columns while performing aggregation on some of the columns. If you think about it you are physically adjusting the axes of a table about a pivot point.</p>
<p class="calibre5">I thought of an easy example to show how this all works. I think it is one of those features that once you see it in action you realize the number of areas that you could apply it.</p>
<p class="calibre5">In our example, we have some raw price points for stocks and we want to convert that table about a pivot to produce average prices per year per stock.</p>
<p class="calibre5">The code in our example is:</p>
<pre class="commandlinepackt"><strong class="calibre3">from pyspark import SparkContext</strong>
<strong class="calibre3">from pyspark.sql import SparkSession</strong>
<strong class="calibre3">from pyspark.sql import functions as func</strong>

<strong class="calibre3">sc = SparkContext.getOrCreate()</strong>
<strong class="calibre3">spark = SparkSession(sc)</strong>

<strong class="calibre3"># load product set</strong>
<strong class="calibre3">pivotDF = spark.read.format("csv") \</strong>
<strong class="calibre3">        .option("header", "true") \</strong>
<strong class="calibre3">        .load("pivot.csv");</strong>
<strong class="calibre3">pivotDF.show()</strong>
<strong class="calibre3">pivotDF.createOrReplaceTempView("pivot")</strong>

<strong class="calibre3"># pivot data per the year to get average prices per stock per year</strong>
<strong class="calibre3">pivotDF \</strong>
<strong class="calibre3">    .groupBy("stock") \</strong>
<strong class="calibre3">    .pivot("year",[2012,2013]) \</strong>
<strong class="calibre3">    .agg(func.avg("price")) \</strong>
<strong class="calibre3">    .show()</strong>  </pre>
<p class="calibre5">This looks as follows in Jupyter:</p>
<div><img class="image-border65" src="img/592144ef-0026-42d2-9cdd-11e153424779.png"/></div>
<p class="calibre5">All the standard includes what we need for Spark to initialize the <kbd class="calibre21">SparkContext</kbd> and the <kbd class="calibre21">SparkSession</kbd>:</p>
<div><img class="image-border66" src="img/7f5525e2-f9d0-415e-b807-073728a1eb17.png"/></div>
<p class="calibre5">We load the stock price information from a CSV file. It is important that at least one of the stocks have more than one price for the same year:</p>
<div><img class="image-border67" src="img/fd3422fc-e60a-4d88-8a36-3ec0c706b61e.png"/></div>
<p class="calibre5">We are grouping the information by stock symbol. The pivot is on the year that has two values, 2012 and 2013, in our dataset. We are computing an average price for each year.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p class="calibre5">In this chapter, we got familiar with obtaining a <kbd class="calibre21">SparkContext</kbd>. We saw examples of using Hadoop MapReduce. We used SQL with Spark data. We combined data frames and operated on the resulting set. We imported JSON data and manipulated it with Spark. Lastly, we looked at using a pivot to gather information about a data frame.</p>
<p class="calibre5">In the next chapter, we will look at using R programming under Jupyter.</p>
<p class="calibre5"/>


            

            
        
    </div>



  </body></html>