<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch10" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 10. Large-Scale Machine Learning – Online Learning</h1></div></div></div><p class="calibre11">In this chapter, we will see the following recipes:</p><div><ul class="itemizedlist"><li class="listitem">Using perceptron as an online linear algorithm</li><li class="listitem">Using stochastic gradient descent for regression</li><li class="listitem">Using stochastic gradient descent for classification</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch10lvl1sec85" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we will concentrate on large-scale machine learning and the algorithms suited to tackle such large-scale problems. Till now, when we trained all our models, we assumed that our training set can fit into our computer's memory. In this chapter, we will see how to go about building models when this assumption is no longer satisfied. Our training records are of a huge size and so we cannot fit them completely into our memory. We may have to load them piecewise and still produce a model with a good accuracy. The argument of a training set not fitting into our computer memory can be extrapolated to streaming data. With streaming data, we don't see all the data at once. We should be able to make decisions based on whatever data we are exposed to and also have a mechanism for continuously improving our model as new data arrives.</p><p class="calibre11">We will introduce the framework of the stochastic gradient descent-based algorithms. This is a versatile framework to handle very large-scale datasets that will not fit completely into our memory. Several types of linear algorithms, including logistic regression, linear regression, and linear SVM, can be accommodated using this framework. The kernel trick, which we introduced in our previous chapter, can be included in this framework in order to deal with datasets with nonlinear relationships.</p><p class="calibre11">We will begin our list of recipes with the perceptron algorithm, the oldest machine learning algorithm. Perceptron is easy to understand and implement. However, Perceptron is limited to solving only linear problems. A kernel-based perceptron can be used to solve nonlinear datasets.</p><p class="calibre11">In our second recipe, we will formally introduce the framework of gradient descent-based methods and how it can be used to perform regression-based tasks. We will look at different loss functions to see how different types of linear models can be built using these functions. We will also see how perceptron belongs to the family of stochastic gradient descent.</p><p class="calibre11">In our final recipe, we will see how classification algorithms can be built using the stochastic gradient descent framework.</p><p class="calibre11">Even though we don't have a direct example of streaming data, with our existing datasets, we will see how the streaming data use cases can be addressed. Online learning algorithms are not limited to streaming data, they can be applied to batch data also, except that they process only one instance at a time.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch10lvl1sec86" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using perceptron as an online learning algorithm</h1></div></div></div><p class="calibre11">As mentioned earlier, perceptron is one of the oldest machine learning algorithms. It was first mentioned in a 1943 paper:</p><p class="calibre11">
<em class="calibre15">A LOGICAL CALCULUS OF THE IDEAS IMMANENT IN NERVOUS ACTIVITY</em>. WARREN S. MCCULLOCH AND WALTER PITTS University of Illinois, College of Medicine, Department of Psychiatry at the Illinois Neuropsychiatric Institute, University of Chicago, Chicago, U.S.A.</p><p class="calibre11">Let's revisit our definition of a classification problem. Each record or instance can be written as a set (X,y), where X is a set of attributes and y is a corresponding class label.</p><p class="calibre11">Learning a <a id="id745" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>target function, F, that maps each record's attribute set to one of the predefined class label, y, is the job of a classification <a id="id746" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>algorithm.</p><p class="calibre11">The difference in our case is that we have a large-scale learning problem. All our data will not fit into our main memory. So, we need to keep our data on a disk and use only a portion of it at a time in order to build our perceptron model.</p><p class="calibre11">Let's proceed to outline the perceptron algorithm:</p><div><ol class="orderedlist"><li class="listitem1">Initialize the weights of the model to a small random number.</li><li class="listitem1">Center the input data, <code class="literal">x</code>, with its mean.</li><li class="listitem1">At each time step t (also called epoch):<div><ul class="itemizedlist1"><li class="listitem">Shuffle the dataset</li><li class="listitem">Pick a single instance of the record and make a prediction</li><li class="listitem">Observe the deviation of the prediction from the true label output</li><li class="listitem">Update the weights if the prediction is different from the true label</li></ul></div></li></ol></div><p class="calibre11">Let's consider the following scenario. We have the complete dataset on our disk. In a single<a id="id747" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> epoch, that is, in step 3, all the steps <a id="id748" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mentioned are performed on all the data on our disk. In an online learning scenario, a bunch of instances based on a windowing function will be available to us at any point in time. We can update the weights as many times as the number of instances in our window in a single epoch.</p><p class="calibre11">Let's see how to go about updating our weights.</p><p class="calibre11">Let's say our input X is as follows:</p><div><img src="img/B04041_10_01.jpg" alt="Using perceptron as an online learning algorithm" class="calibre216"/></div><p class="calibre11">Our <code class="literal">Y</code> is as follows:</p><div><img src="img/B04041_10_02.jpg" alt="Using perceptron as an online learning algorithm" class="calibre217"/></div><p class="calibre11">We will define our weights as the following equation:</p><div><img src="img/B04041_10_03.jpg" alt="Using perceptron as an online learning algorithm" class="calibre218"/></div><p class="calibre11">Our prediction after we see each record is defined as follows:</p><div><img src="img/B04041_10_04.jpg" alt="Using perceptron as an online learning algorithm" class="calibre219"/></div><p class="calibre11">The sign function returns +1 if the product of the weight and attributes is positive, or -1 if the product is negative.</p><p class="calibre11">Perceptron <a id="id749" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proceeds to compare the predicted y <a id="id750" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with the actual y. If the predicted y is correct, it moves on to the next record. If the prediction is incorrect, there are two scenarios. If the predicted y is +1 and the actual y is -1, it decrements the weight with an x value, and vice versa. If the actual y is +1 and the predicted y is -1, it increments the weights. Let's see this as an equation for more clarity:</p><div><img src="img/B04041_10_05.jpg" alt="Using perceptron as an online learning algorithm" class="calibre220"/></div><p class="calibre11">Typically, a learning rate alpha is provided so that the weights are updated in a controlled manner. With the presence of noise in the data, a full increment of decrements will lead to the weights not converging:</p><div><img src="img/B04041_10_06.jpg" alt="Using perceptron as an online learning algorithm" class="calibre221"/></div><p class="calibre11">Alpha is a very small value ranging, between 0.1 and 0.4.</p><p class="calibre11">Let's jump into our recipe now.</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec307" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's generate data using <code class="literal">make_classification</code> in batches with a generator function to simulate large-scale data and data streaming, and proceed to write the perceptron algorithm.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec308" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's load the<a id="id751" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> necessary libraries. We will then <a id="id752" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>write a function, <code class="literal">get_data</code>, which is a generator:</p><div><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.metrics import  classification_report
from sklearn.preprocessing import scale
import numpy as np

def get_data(batch_size):
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    b_size = 0
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.8*no_features)
    repeated_features = int(0.1*no_features)

    while b_size &lt; batch_size:
        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)
        y_indx = y &lt; 1
        y[y_indx] = -1
        x = scale(x,with_mean=True,with_std=True)

        yield x,y
        b_size+=1</pre></div><p class="calibre11">We will <a id="id753" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proceed to write two functions, one<a id="id754" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to build our perceptron model and the other one to test the worthiness of our model:</p><div><pre class="programlisting">def build_model(x,y,weights,epochs,alpha=0.5):
    """
    Simple Perceptron
    """
  
    for i in range(epochs):
        
        # Shuffle the dataset
        shuff_index = np.random.shuffle(range(len(y)))
        x_train = x[shuff_index,:].reshape(x.shape)
        y_train = np.ravel(y[shuff_index,:])
        
        # Build weights one instance at a time
        for index in range(len(y)):
            prediction = np.sign( np.sum(x_train[index,:] * weights) ) 
            if prediction != y_train[index]:
                weights = weights + alpha * (y_train[index] * x_train[index,:])
            
    return weights
    

                    
def model_worth(x,y,weights):
    prediction = np.sign(np.sum(x * weights,axis=1))
print classification_report(y,prediction)</pre></div><p class="calibre11">Finally, we will write our main function to invoke all the preceding functions, to demonstrate the perceptron algorithm:</p><div><pre class="programlisting">if __name__ == "__main__":
    data = get_data(10)    
    x,y = data.next()
    weights = np.zeros(x.shape[1])    
    for i in range(10):
        epochs = 100
        weights = build_model(x,y,weights,epochs)
        print
        print "Model worth after receiving dataset batch %d"%(i+1)    
        model_worth(x,y,weights)
        print
        if i &lt; 9:
            x,y = data.next()</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec309" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start with our main function. We will ask our generator to send us 10 sets of data:</p><div><pre class="programlisting">    data = get_data(10)    </pre></div><p class="calibre11">Here, we want to simulate both large-scale data and data streaming. While building our model, we don't have access to all the data, just part of it:</p><div><pre class="programlisting">    x,y = data.next()</pre></div><p class="calibre11">We will use the <code class="literal">next()</code> function in the generator in order to get the next set of data. In the <code class="literal">get_data</code> function, we will use the <code class="literal">make_classification</code> function from scikit-learn:</p><div><pre class="programlisting">        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)</pre></div><p class="calibre11">Let's look at<a id="id755" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the parameters passed to <a id="id756" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <code class="literal">make_classification</code> method. The first parameter is the number of instances required, in this case, we need 1,000 instances. The second parameter is about how many attributes per instance are required. We will assume that we need 30. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce some noise in our data. The next parameter is about these 30 features and how many of them should be informative enough to be used in our classification. We specified that 60 percent of our features, that is, 18 out of 30, should be informative. The next parameter is about the redundant features. These are generated as a linear combination of the informative features in order to introduce correlation among the features. Finally, repeated features are duplicate features that are drawn randomly from both the informative features and the redundant features.</p><p class="calibre11">When we call <code class="literal">next()</code>, we will get 1,000 instances of this data. This function returns a y label as <code class="literal">{0,1}</code>; we want <code class="literal">{-1,+1}</code> and hence we will change all the zeros in y to <code class="literal">-1</code>:</p><div><pre class="programlisting">        y_indx = y &lt; 1
        y[y_indx] = -1</pre></div><p class="calibre11">Finally, we will center our data using the scale function from scikit-learn.</p><p class="calibre11">Let's proceed to build our model with the first batch of data. We will initialize our weights matrix with zeros:</p><div><pre class="programlisting">    weights = np.zeros(x.shape[1])    </pre></div><p class="calibre11">As we need 10 batches of data to simulate large-scale learning and data streaming, we will do the model building 10 times in the for loop:</p><div><pre class="programlisting">    for i in range(10):
        epochs = 100
        weights = build_model(x,y,weights,epochs)</pre></div><p class="calibre11">Our perceptron algorithm is built in <code class="literal">build_model</code>. A predictor x, response variable y, the weights matrix, and number of time steps or epochs are passed as parameters. In our case, we have<a id="id757" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> set the number of epochs to <code class="literal">100</code>. This<a id="id758" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function has one additional parameter, alpha value:</p><div><pre class="programlisting">def build_model(x,y,weights,epochs,alpha=0.5)</pre></div><p class="calibre11">By default, we have set our alpha value to <code class="literal">0.5</code>.</p><p class="calibre11">Let's see in our <code class="literal">build_model</code>. We will start with shuffling the data:</p><div><pre class="programlisting">        # Shuffle the dataset
        shuff_index = np.random.shuffle(range(len(y)))
        x_train = x[shuff_index,:].reshape(x.shape)
        y_train = np.ravel(y[shuff_index,:])</pre></div><p class="calibre11">We will go through each record in our dataset and start updating our weights:</p><div><pre class="programlisting">        
 # Build weights one instance at a time
        for index in range(len(y)):
            prediction = np.sign( np.sum(x_train[index,:] * weights) ) 
            if prediction != y_train[index]:
                weights = weights + alpha * (y_train[index] * x_train[index,:])</pre></div><p class="calibre11">In the for loop, you can see that we do the prediction:</p><div><pre class="programlisting">            prediction = np.sign( np.sum(x_train[index,:] * weights) ) </pre></div><p class="calibre11">We will multiply our training data with weights, and add them together. Finally, we will use the np.sign function to get our prediction. Now, based on the prediction, we will update our weights:</p><div><pre class="programlisting">                weights = weights + alpha * (y_train[index] * x_train[index,:])</pre></div><p class="calibre11">That is all. We will return the weights to the calling function.</p><p class="calibre11">In our main function, we will invoke the <code class="literal">model_worth</code> function to print the goodness of the model. Here, we will use the <code class="literal">classification_report</code> convienience function to print the accuracy score of the model:</p><div><pre class="programlisting">        print
        print "Model worth after receiving dataset batch %d"%(i+1)    
        model_worth(x,y,weights)</pre></div><p class="calibre11">We will then <a id="id759" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proceed to update our model for the<a id="id760" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> next batch of incoming data. Note that we have not altered the <code class="literal">weights</code> parameter. It gets updated with every batch of new data coming in.</p><p class="calibre11">Let's see what <code class="literal">model_worth</code> has printed:</p><div><img src="img/B04041_10_07.jpg" alt="How it works…" class="calibre222"/></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec310" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Scikit-learn provides us with an implementation of perceptron. Refer to the following URL for more details:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html</a>.</p><p class="calibre11">Another improvement<a id="id761" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that can be made in the perceptron algorithm is to use more features.</p><p class="calibre11">Remember the prediction equation, we can rewrite it as follows:</p><div><img src="img/B04041_10_08.jpg" alt="There's more…" class="calibre223"/></div><p class="calibre11">We replaced the<a id="id762" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> x values with a function. Here, we <a id="id763" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>can send a feature generator. For example, a polynomial feature generator can be added to our <code class="literal">get_data</code> function, as follows:</p><div><pre class="programlisting">def get_data(batch_size):
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    b_size = 0
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.8*no_features)
    repeated_features = int(0.1*no_features)
    poly = PolynomialFeatures(degree=2)

    while b_size &lt; batch_size:
        x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
                n_informative = informative_features, n_redundant = redundant_features \
                ,n_repeated = repeated_features, random_state=51)
        y_indx = y &lt; 1
        y[y_indx] = -1
        x = poly.fit_transform(x)
        yield x,y
        b_size+=1</pre></div><p class="calibre11">Finally, kernel-based <a id="id764" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>perceptron algorithms are <a id="id765" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>available to handle nonlinear datasets. Refer to the Wikipedia article for more information about kernel-based<a id="id766" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> perceptron:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://en.wikipedia.org/wiki/Kernel_perceptron">https://en.wikipedia.org/wiki/Kernel_perceptron</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec311" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Learning and using Kernels</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Data Mining – Needle in a Haystack">Chapter 5</a>, <em class="calibre15">Data Mining - Finding a needle in a haystack</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch10lvl1sec87" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using stochastic gradient descent for regression</h1></div></div></div><p class="calibre11">In a<a id="id767" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> typical regression setup, we have a<a id="id768" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> set of predictors (instances), as follows:</p><div><img src="img/B04041_10_09.jpg" alt="Using stochastic gradient descent for regression" class="calibre224"/></div><p class="calibre11">Each instance has m attributes, as follows:</p><div><img src="img/B04041_10_10.jpg" alt="Using stochastic gradient descent for regression" class="calibre225"/></div><p class="calibre11">The response variable, Y, is a vector of real-valued entries. The job of regression is to find a function such that when x is provided as an input to this function, it should return y:</p><div><img src="img/B04041_10_11.jpg" alt="Using stochastic gradient descent for regression" class="calibre137"/></div><p class="calibre11">The preceding function is parameterized by a weight vector, that is, a combination of the weight vector and input vector is used to predict <code class="literal">Y</code>, so rewriting the function with the weight vector will get the following:</p><div><img src="img/B04041_10_13.jpg" alt="Using stochastic gradient descent for regression" class="calibre226"/></div><p class="calibre11">So, the question now is how do we know that we have the right weight vectors? We will use a loss function, L, to get the right weight vectors. The loss function measures the cost of making a <a id="id769" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>wrong prediction. It empirically <a id="id770" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>measures the cost of predicting y when the actual value is y. The regression problem now becomes the problem of finding the right weight vector that will minimize the loss function. For our whole dataset of <code class="literal">n</code> elements, the overall loss function is as follows:</p><div><img src="img/B04041_10_14.jpg" alt="Using stochastic gradient descent for regression" class="calibre173"/></div><p class="calibre11">Our weight vectors should be those that minimize the preceding value.</p><p class="calibre11">Gradient descent is an optimization technique used to minimize the preceding equation. For this equation, we will find the gradient, that is, the first-order derivative with respect to W.</p><p class="calibre11">Unlike other optimization techniques such as the batch gradient descent, stochastic gradient descent operates on one instance at a time. The steps involved in stochastic gradient descent are as follows:</p><div><ol class="orderedlist"><li class="listitem1">For each epoch, shuffle the dataset.</li><li class="listitem1">Pick an instance and its response variable, y.</li><li class="listitem1">Calculate the loss function and its derivative, w.r.t weights.</li><li class="listitem1">Update the weights.</li></ol></div><p class="calibre11">Let's say:</p><div><img src="img/B04041_10_15.jpg" alt="Using stochastic gradient descent for regression" class="calibre227"/></div><p class="calibre11">This signifies the derivative, w.r.t w. The weights are updated as follows:</p><div><img src="img/B04041_10_16.jpg" alt="Using stochastic gradient descent for regression" class="calibre228"/></div><p class="calibre11">As you can see, the weights are moved in the opposite direction to the gradient, thus forcing a descent that will eventually give the weight vector values, which can reduce the objective <a id="id771" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>cost function.</p><p class="calibre11">A squared loss is a typical loss function used with regression. The squared loss of an instance is<a id="id772" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> defined in the following way:</p><div><img src="img/B04041_10_17.jpg" alt="Using stochastic gradient descent for regression" class="calibre229"/></div><p class="calibre11">The derivative of the preceding equation is substituted into the weight update equation. With this background knowledge, let's proceed to our recipe for stochastic gradient descent regression.</p><p class="calibre11">As explained in perceptron, a learning rate, eta, is added to the weight update equation in order to avoid the effect of noise:</p><div><img src="img/B04041_10_18.jpg" alt="Using stochastic gradient descent for regression" class="calibre230"/></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec312" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will be leveraging the scikit-learn's implementation of SGD regression. As in some of the previous recipes, we will use the <code class="literal">make_regression</code> function from scikit-learn to generate data for our recipe in order to demonstrate stochastic gradient descent regression.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec313" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's start with a very simple example demonstrating how to build a stochastic gradient descent regressor.</p><p class="calibre11">We will first load the required libraries. We will then write a function to generate predictors and response variables to demonstrate regression:</p><div><pre class="programlisting">from sklearn.datasets import make_regression
from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.cross_validation import train_test_split


def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30

    x,y = make_regression(n_samples=1000,n_features=no_features,\
             random_state=51)
    return x,y</pre></div><p class="calibre11">We will <a id="id773" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>proceed to write the functions <a id="id774" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that will help us build, validate, and inspect our model:</p><div><pre class="programlisting">def build_model(x,y):
    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='none')
    estimator.fit(x,y)
    
    return estimator
    
    
def model_worth(model,x,y):
    predicted_y = model.predict(x)
    print "\nMean absolute error = %0.2f"%mean_absolute_error(y,predicted_y)
    print "Mean squared error = %0.2f"%mean_squared_error(y,predicted_y)
    
def inspect_model(model):
    print "\nModel Itercept {0}".format(model.intercept_)
    print
    for i,coef in enumerate(model.coef_):
        print "Coefficient {0} = {1:.3f}".format(i+1,coef)</pre></div><p class="calibre11">Finally, we will write our main function to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()
    
    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    model = build_model(x_train,y_train)

    inspect_model(model)

    print "Model worth on train data"
    model_worth(model,x_train,y_train)
    print "Model worth on dev data"
    model_worth(model,x_dev,y_dev)
    
    # Building model with l2 regularization
    model = build_model_regularized(x_train,y_train)
    inspect_model(model)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec314" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start<a id="id775" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> with our main function. We<a id="id776" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> will invoke the <code class="literal">get_data</code> function to generate our predictor x and response variable y:</p><div><pre class="programlisting">    x,y = get_data()</pre></div><p class="calibre11">In the <code class="literal">get_data</code> function, we will leverage the convenient <code class="literal">make_regression</code> function from scikit-learn to generate a dataset for the regression problems:</p><div><pre class="programlisting">    no_features = 30
    x,y = make_regression(n_samples=1000,n_features=no_features,\
             random_state=51)	</pre></div><p class="calibre11">As you can see, we will generate a dataset with 1,000 instances specified by an <code class="literal">n_samples</code> parameter, and 30 features defined by an <code class="literal">n_features</code> parameter.</p><p class="calibre11">Let's split the data into training and testing sets using <code class="literal">train_test_split</code>. We will reserve 30 percent of our data to test:</p><div><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we will leverage <code class="literal">train_test_split</code> to split our test data into dev and test sets:</p><div><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">With the data divided to build, evaluate, and test the model, we will proceed to build our models.</p><p class="calibre11">We will invoke the <code class="literal">build_model</code> function with our training dataset:</p><div><pre class="programlisting">    model = build_model(x_train,y_train)</pre></div><p class="calibre11">In <code class="literal">build_model</code>, we will leverage scikit-learn's SGD regressor class to build our stochastic gradient descent method:</p><div><pre class="programlisting">    estimator = SGDRegressor(n_iter = 10, shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='none')
    estimator.fit(x,y)</pre></div><p class="calibre11">The SGD regressor is a vast method and can be used to fit a number of linear models with a lot of<a id="id777" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> parameters. We will first explain the<a id="id778" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> basic method of stochastic gradient descent and then proceed to explain the other details.</p><p class="calibre11">Let's look at the parameters that we used. The first parameter is the number of times that we want to go through our dataset in order to update the weights. Here, we will say that we want 10 iterations. As in perceptron, after going through all the records once, we need to shuffle our input records when we start the next iteration. A parameter shuffle is used for the same. The default value of shuffle is true, we have included it here for explanation purposes. Our loss function is the squared loss and we want to do a linear regression; hence, we will specify this using the loss parameter. </p><p class="calibre11">Our learning rate, eta, is a constant that we will specify with the <code class="literal">learning_rate</code> parameter. We will provide a value for our learning rate using the eta<code class="literal">0</code> parameter. We will then say that we need to fit the intercept as we have not centered our data by its mean. Finally, the penalty parameter controls the type of shrinkage required. In our case, we don't need any shrinkage using the none string.</p><p class="calibre11">We will proceed to build our model by invoking the fit function with our predictor and response variable. Finally we will return the model that we built to our calling function.</p><p class="calibre11">Let's now inspect our model and see the value of the intercept and coefficients:</p><div><pre class="programlisting">    inspect_model(model)</pre></div><p class="calibre11">In the inspect model, we will print the values of the model intercepts and coefficients:</p><div><img src="img/B04041_10_19.jpg" alt="How it works…" class="calibre231"/></div><p class="calibre11">Let's now<a id="id779" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> look at how our model has<a id="id780" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> performed in our training data:</p><div><pre class="programlisting">    print "Model worth on train data"
    model_worth(model,x_train,y_train)</pre></div><p class="calibre11">We will invoke the model_worth function to look at our model's performance. The model_worth function prints the mean absolute error and mean squared error values.</p><p class="calibre11">The mean squared error is defined as follows:</p><div><img src="img/B04041_10_22.jpg" alt="How it works…" class="calibre232"/></div><p class="calibre11">The mean absolute error is defined in the following way:</p><div><img src="img/B04041_10_23.jpg" alt="How it works…" class="calibre233"/></div><p class="calibre11">The mean<a id="id781" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> squared error is sensitive<a id="id782" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> to outliers. Hence, the mean absolute error is a more robust measure. Let's look at the model's performance using the training data:</p><div><img src="img/B04041_10_20.jpg" alt="How it works…" class="calibre234"/></div><p class="calibre11">Let's now look at the model's performance using our dev data:</p><div><img src="img/B04041_10_21.jpg" alt="How it works…" class="calibre235"/></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec315" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">We can include regularization in the stochastic gradient descent framework. Recall the following cost function of ridge regression from the previous chapter:</p><div><img src="img/B04041_10_24.jpg" alt="There's more…" class="calibre150"/></div><p class="calibre11">We included<a id="id783" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> an expanded version<a id="id784" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> of the square loss function here and added the regularization term—the sum of the square of the weights. We can include it in our gradient descent procedure. Let's say that we denote our regularization term as R(W). Our weight update is now as follows:</p><div><img src="img/B04041_10_25.jpg" alt="There's more…" class="calibre236"/></div><p class="calibre11">As you can see, now we have the derivative of the loss function with respect to the weight vector, w, and the derivative of the regularization term with respect to the weights is added to our weight update rule.</p><p class="calibre11">Let's write a new function to build our model to include regularization:</p><div><pre class="programlisting">def build_model_regularized(x,y):
    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='l2',alpha=0.01)
    estimator.fit(x,y)
    
    return estimator</pre></div><p class="calibre11">We can invoke this function from our main function as follows:</p><div><pre class="programlisting">model = build_model_regularized(x_train,y_train)
inspect_model(model)</pre></div><p class="calibre11">Let's see the new parameters that we passed compared with our previous build model method:</p><div><pre class="programlisting">    estimator = SGDRegressor(n_iter = 10,shuffle=True,loss = "squared_loss", \
            learning_rate='constant',eta0=0.01,fit_intercept=True, \
            penalty='l2',alpha=0.01)</pre></div><p class="calibre11">Earlier, we<a id="id785" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> mentioned our penalty as <a id="id786" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>none. Now, you can see that we mentioned that we need to add an L2 penalty to our model. Again, we will give an <code class="literal">alpha</code> value of <code class="literal">0.01</code> using the <code class="literal">alpha</code> parameter. Let's look at our coefficients:</p><div><img src="img/B04041_10_26.jpg" alt="There's more…" class="calibre237"/></div><p class="calibre11">You can see the effect of the L2 regularization: a lot of the coefficients have attained a zero value. Similarly, the L1 regularization and elastic net, which combines both the L1 and L2 regularization, can be included using the penalty parameter.</p><p class="calibre11">Remember<a id="id787" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> in our introduction, we mentioned<a id="id788" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> that stochastic gradient descent is more of a framework than a single method. Other linear models can be generated using this framework by changing the loss function.</p><p class="calibre11">SVM regression models can be built using the epsilon-insensitive loss function. This loss function is defined as follows:</p><div><img src="img/B04041_10_27.jpg" alt="There's more…" class="calibre238"/></div><p class="calibre11">Refer to the<a id="id789" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> following URL for the various<a id="id790" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> parameters that can be passed to the <a id="id791" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>SGD regressor in scikit-learn:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec316" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Predicting real valued numbers using regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li><li class="listitem"><em class="calibre15">Shrinkage using Ridge Regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch10lvl1sec88" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Using stochastic gradient descent for classification</h1></div></div></div><p class="calibre11">A classification problem setup is very similar to a regression setup except for the response <a id="id792" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>variable. In a classification <a id="id793" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>setup, the response is a categorical variable. Due to its nature, we have a different loss function to measure the cost of the wrong predictions. Let's assume a binary classifier for our discussion and recipe, and our target variable, Y, can take the values {<code class="literal">0</code>,<code class="literal">1</code>}.</p><p class="calibre11">We will use the derivative of this loss function in our weight update rule to arrive at our weight vectors.</p><p class="calibre11">The SGD classifier class from scikit-learn provides us with a variety of loss functions. However, in this recipe, we will see log loss, which will give us logistic regression.</p><p class="calibre11">Logistic regression fits a linear model to a data of the following form:</p><div><img src="img/B04041_10_29.jpg" alt="Using stochastic gradient descent for classification" class="calibre239"/></div><p class="calibre11">We have given a generalized notation. The intercept is assumed to be the first dimension of our weight vector. For a binary classification problem, a logit function is applied to get a prediction. as follows:</p><div><img src="img/B04041_10_30.jpg" alt="Using stochastic gradient descent for classification" class="calibre240"/></div><p class="calibre11">The preceding<a id="id794" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> function is also called<a id="id795" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> the sigmoid function. For very large positive values of x_i, this function will return a value close to one, and vice versa for large negative values close to zero. With this, we can define our log loss function as follows:</p><div><img src="img/B04041_10_31.jpg" alt="Using stochastic gradient descent for classification" class="calibre241"/></div><p class="calibre11">With the preceding loss function fitted into the weight update rule of the gradient descent, we can arrive at the appropriate weight vectors.</p><p class="calibre11">For the log loss function defined in scikit-learn, refer to the following URL:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html</a>.</p><p class="calibre11">With this knowledge, let's jump into our recipe for stochastic gradient descent-based classification.</p><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec317" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will leverage scikit-learn's implementation of the stochastic gradient descent classifier. As we did in some of the previous recipes, we will use the <code class="literal">make_classification</code> function from scikit-learn to generate data for our recipe in order to demonstrate the stochastic gradient descent classification.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec318" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's start <a id="id796" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>with a very simple example<a id="id797" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> demonstrating how to build a stochastic gradient descent regressor.</p><p class="calibre11">We will first load the required libraries. We will then write a function to generate the predictors and response variables:</p><div><pre class="programlisting">from sklearn.datasets import make_classification
from sklearn.metrics import  accuracy_score
from sklearn.cross_validation import train_test_split
from sklearn.linear_model import SGDClassifier

import numpy as np

def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=1000,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">We will proceed to write functions that will help us build and validate our model:</p><div><pre class="programlisting">def build_model(x,y,x_dev,y_dev):
    estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
                learning_rate = "constant",eta0=0.0001,fit_intercept=True, penalty="none")
    estimator.fit(x,y)
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print 
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)</pre></div><p class="calibre11">Finally, we<a id="id798" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> will write our main <a id="id799" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function to invoke all the preceding functions:</p><div><pre class="programlisting">if __name__ == "__main__":
    x,y = get_data()    

    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)
    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)
    
    build_model(x_train,y_train,x_dev,y_dev)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec319" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's start with our main function. We will invoke <code class="literal">get_data</code> to get our <code class="literal">x</code> predictor attributes and <code class="literal">y</code> response attributes. In <code class="literal">get_data</code>, we will leverage the <code class="literal">make_classification</code> dataset in order to generate our training data for the random forest method:</p><div><pre class="programlisting">def get_data():
    """
    Make a sample classification dataset
    Returns : Independent variable y, dependent variable x
    """
    no_features = 30
    redundant_features = int(0.1*no_features)
    informative_features = int(0.6*no_features)
    repeated_features = int(0.1*no_features)
    x,y = make_classification(n_samples=500,n_features=no_features,flip_y=0.03,\
            n_informative = informative_features, n_redundant = redundant_features \
            ,n_repeated = repeated_features,random_state=7)
    return x,y</pre></div><p class="calibre11">Let's look at the parameters passed to the <code class="literal">make_classification</code> method. The first parameter is the number of instances required. In this case, we need 500 instances. The second parameter is about how many attributes per instance are required. We say that we need 30. The third parameter, <code class="literal">flip_y</code>, randomly interchanges 3 percent of the instances. This is done to introduce noise in our data. The next parameter is about how many out of those 30 features should be informative enough to be used in our classification. We specified that 60 percent of our features, that is, 18 out of 30, should be informative. The next parameter is about redundant features. These are generated as a linear combination of the informative<a id="id800" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> features in order to introduce <a id="id801" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>correlation among the features. Finally, the repeated features are duplicate features that are drawn randomly from both the informative and redundant features.</p><p class="calibre11">Let's split the data into training and testing sets using <code class="literal">train_test_split</code>. We will reserve 30 percent of our data to test:</p><div><pre class="programlisting">    # Divide the data into Train, dev and test    
    x_train,x_test_all,y_train,y_test_all = train_test_split(x,y,test_size = 0.3,random_state=9)</pre></div><p class="calibre11">Once again, we will leverage <code class="literal">train_test_split</code> to split our test data into dev and test sets:</p><div><pre class="programlisting">    x_dev,x_test,y_dev,y_test = train_test_split(x_test_all,y_test_all,test_size=0.3,random_state=9)</pre></div><p class="calibre11">With the data divided to build, evaluate, and test the model, we will proceed to build our models:</p><div><pre class="programlisting">build_model(x_train,y_train,x_dev,y_dev)</pre></div><p class="calibre11">In <code class="literal">build_model</code>, we will leverage scikit-learn's <code class="literal">SGDClassifier</code> class to build our stochastic gradient descent method:</p><div><pre class="programlisting">    estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
                learning_rate = "constant",eta0=0.0001,fit_intercept=True, penalty="none")</pre></div><p class="calibre11">Let's look at the parameters that we used. The first parameter is the number of times we want to go through our dataset to update the weights. Here, we say that we want 50 iterations. As in perceptron, after going through all the records once, we need to shuffle our input records when we start the next iteration. The shuffle parameter is used for the same. The default value of shuffle is true, we have included it here for explanation purposes. Our loss function is log loss: we want to do a logistic regression and we will specify this using the loss parameter. Our learning rate, eta, is a constant that we will specify with the <code class="literal">learning_rate</code> parameter. We will provide the value for our learning rate using the eta<code class="literal">0</code> parameter. We will then proceed to say that we need to fit the intercept, as we have not centered our data by its mean. Finally, the penalty parameter controls the type of shrinkage required. In our case, we will say that we don't need any shrinkage using the none string.</p><p class="calibre11">We will proceed to build our model by invoking the fit function with our predictor and response variable,<a id="id802" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> and evaluate our model<a id="id803" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> with our training and dev dataset:</p><div><pre class="programlisting"> estimator.fit(x,y)
    train_predcited = estimator.predict(x)
    train_score = accuracy_score(y,train_predcited)
    dev_predicted = estimator.predict(x_dev)
    dev_score = accuracy_score(y_dev,dev_predicted)
    
    print 
    print "Training Accuracy = %0.2f Dev Accuracy = %0.2f"%(train_score,dev_score)</pre></div><p class="calibre11">Let's look at our accuracy scores:</p><div><img src="img/B04041_10_28.jpg" alt="How it works…" class="calibre242"/></div></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec320" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">Regularization, L1, L2, or elastic net can be applied for SGD classification. The procedure is the same as that of regression, and hence, we will not repeat it here. Refer to the previous recipe for this.</p><p class="calibre11">The learning rate, eta, was constant in our example. This need not be the case. With every iteration, the eta value can be reduced. The learning rate parameter, <code class="literal">learning_rate</code>, can be set to an optimal string or invscaling. Refer to the following scikit documentation:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/sgd.html">http://scikit-learn.org/stable/modules/sgd.html</a>.</p><p class="calibre11">The parameter is specified as follows:</p><div><pre class="programlisting">estimator = SGDClassifier(n_iter=50,shuffle=True,loss="log", \
learning_rate = "invscaling",eta0=0.001,fit_intercept=True, penalty="none")</pre></div><p class="calibre11">We used the fit method to build our model. As mentioned previously, in large-scale machine learning, we know that all the data will not be available to us at once. When we receive the data in batches, we<a id="id804" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/> need to use the <code class="literal">partial_fit</code> method, instead of <code class="literal">fit</code>. Using the <code class="literal">fit</code> method will reinitialize the weights and we will lose all the training information from the previous batch of data. Refer to the following link for <a id="id805" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>more information on <code class="literal">partial_fit</code>:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit">http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch10lvl2sec321" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Shrinkage using Ridge Regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7. Machine Learning 2">Chapter 7</a>, <em class="calibre15">Machine Learning II</em></li><li class="listitem"><em class="calibre15">Using stochastic gradient descent for regression</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch09.xhtml" title="Chapter 9. Growing Trees">Chapter 9</a>, <em class="calibre15">Machine Learning III</em></li></ul></div></div></div></div>



  </body></html>