- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The continued growth in data coupled with the need to make increasingly complex
    decisions against that data is creating massive hurdles that prevent organizations
    from deriving insights in a timely manner using traditional analytical approaches.
    The field of big data has become so related to these frameworks that its scope
    is defined by what these frameworks can handle. Whether you're scrutinizing the
    clickstream from millions of visitors to optimize online ad placements, or sifting
    through billions of transactions to identify signs of fraud, the need for advanced
    analytics, such as machine learning and graph processing, to automatically glean
    insights from enormous volumes of data is more evident than ever.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark, the de facto standard for big data processing, analytics, and
    data sciences across all academia and industries, provides both machine learning
    and graph processing libraries, allowing companies to tackle complex problems
    easily with the power of highly scalable and clustered computers. Spark's promise
    is to take this a little further to make writing distributed programs using Scala
    feel like writing regular programs for Spark. Spark will be great in giving ETL
    pipelines huge boosts in performance and easing some of the pain that feeds the
    MapReduce programmer's daily chant of despair to the Hadoop gods.
  prefs: []
  type: TYPE_NORMAL
- en: In this book, we used Spark and Scala for the endeavor to bring state-of-the-art
    advanced data analytics with machine learning, graph processing, streaming, and
    SQL to Spark, with their contributions to MLlib, ML, SQL, GraphX, and other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: We started with Scala and then moved to the Spark part, and finally, covered
    some advanced topics for big data analytics with Spark and Scala. In the appendix,
    we will see how to extend your Scala knowledge for SparkR, PySpark, Apache Zeppelin,
    and in-memory Alluxio. This book isn't meant to be read from cover to cover. Skip
    to a chapter that looks like something you're trying to accomplish or that simply
    ignites your interest.
  prefs: []
  type: TYPE_NORMAL
- en: Happy reading!
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 1](part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c), *Introduction
    to Scala*, will teach big data analytics using the Scala-based APIs of Spark.
    Spark itself is written with Scala and naturally, as a starting point, we will
    discuss a brief introduction to Scala, such as the basic aspects of its history,
    purposes, and how to install Scala on Windows, Linux, and Mac OS. After that,
    the Scala web framework will be discussed in brief. Then, we will provide a comparative
    analysis of Java and Scala. Finally, we will dive into Scala programming to get
    started with Scala.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 2](part0058.html#1NA0K1-21aec46d8593429cacea59dbdcd64e1c), *Object-Oriented
    Scala*, says that the object-oriented programming (OOP) paradigm provides a whole
    new layer of abstraction. In short, this chapter discusses some of the greatest
    strengths of OOP languages: discoverability, modularity, and extensibility. In
    particular, we will see how to deal with variables in Scala; methods, classes,
    and objects in Scala; packages and package objects; traits and trait linearization;
    and Java interoperability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 3](part0093.html#2OM4A1-21aec46d8593429cacea59dbdcd64e1c), *Functional
    Programming Concepts*, showcases the functional programming concepts in Scala.
    More specifically, we will learn several topics, such as why Scala is an arsenal
    for the data scientist, why it is important to learn the Spark paradigm, pure
    functions, and higher-order functions (HOFs). A real-life use case using HOFs
    will be shown too. Then, we will see how to handle exceptions in higher-order
    functions outside of collections using the standard library of Scala. Finally,
    we will look at how functional Scala affects an object''s mutability.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter4](part0117.html#3FIHQ1-21aec46d8593429cacea59dbdcd64e1c), *Collection
    APIs*, introduces one of the features that attract most Scala users--the Collections
    API. It''s very powerful and flexible, and has lots of operations coupled. We
    will also demonstrate the capabilities of the Scala Collection API and how it
    can be used in order to accommodate different types of data and solve a wide range
    of different problems. In this chapter, we will cover Scala collection APIs, types
    and hierarchy, some performance characteristics, Java interoperability, and Scala
    implicits.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 5](part0148.html#4D4J81-21aec46d8593429cacea59dbdcd64e1c), *Tackle
    Big Data - Spark Comes to the Party,* outlines data analysis and big data; we
    see the challenges that big data poses, how they are dealt with by distributed
    computing, and the approaches suggested by functional programming. We introduce
    Google''s MapReduce, Apache Hadoop, and finally, Apache Spark, and see how they
    embraced this approach and these techniques. We will look into the evolution of
    Apache Spark: why Apache Spark was created in the first place and the value it
    can bring to the challenges of big data analytics and processing.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c), *Start
    Working with Spark - REPL and RDDs,* covers how Spark works; then, we introduce
    RDDs, the basic abstractions behind Apache Spark, and see that they are simply
    distributed collections exposing Scala-like APIs. We will look at the deployment
    options for Apache Spark and run it locally as a Spark shell. We will learn the
    internals of Apache Spark, what RDDs are, DAGs and lineages of RDDs, Transformations,
    and Actions.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c), *Special
    RDD Operations,* focuses on how RDDs can be tailored to meet different needs,
    and how these RDDs provide new functionalities (and dangers!) Moreover, we investigate
    other useful objects that Spark provides, such as broadcast variables and Accumulators.
    We will learn aggregation techniques, shuffling.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c), *Introduce
    a Little Structure - SparkSQL,* teaches how to use Spark for the analysis of structured
    data as a higher-level abstraction of RDDs and how Spark SQL''s APIs make querying
    structured data simple yet robust. Moreover, we introduce datasets and look at
    the differences between datasets, DataFrames, and RDDs. We will also learn to
    join operations and window functions to do complex data analysis using DataFrame
    APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c), *Stream
    Me Up, Scotty - Spark Streaming,* takes you through Spark Streaming and how we
    can take advantage of it to process streams of data using the Spark API. Moreover,
    in this chapter, the reader will learn various ways of processing real-time streams
    of data using a practical example to consume and process tweets from Twitter.
    We will look at integration with Apache Kafka to do real-time processing. We will
    also look at structured streaming, which can provide real-time queries to your
    applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c), *Everything
    is Connected - GraphX,* in this chapter, we learn how many real-world problems
    can be modeled (and resolved) using graphs. We will look at graph theory using
    Facebook as an example, Apache Spark''s graph processing library GraphX, VertexRDD
    and EdgeRDDs, graph operators, aggregateMessages, TriangleCounting, the Pregel
    API, and use cases such as the PageRank algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 11](part0343.html#A73GU1-21aec46d8593429cacea59dbdcd64e1c), *Learning
    Machine Learning - Spark MLlib and ML*, the purpose of this chapter is to provide
    a conceptual introduction to statistical machine learning. We will focus on Spark''s
    machine learning APIs, called Spark MLlib and ML. We will then discuss how to
    solve classification tasks using decision trees and random forest algorithms and
    regression problem using linear regression algorithm. We will also show how we
    could benefit from using one-hot encoding and dimensionality reductions algorithms
    in feature extraction before training a classification model. In later sections,
    we will show a step-by-step example of developing a collaborative filtering-based
    movie recommendation system.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 12](part0383.html#BD87E1-21aec46d8593429cacea59dbdcd64e1c), *Advanced
    Machine Learning Best Practices*, provides theoretical and practical aspects of
    some advanced topics of machine learning with Spark. We will see how to tune machine
    learning models for optimized performance using grid search, cross-validation,
    and hyperparameter tuning. In a later section, we will cover how to develop a
    scalable recommendation system using ALS, which is an example of a model-based
    recommendation algorithm. Finally, a topic modelling application will be demonstrated
    as a text clustering technique'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 13](part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c), *My Name
    is Bayes, Naive Bayes,* states that machine learning in big data is a radical
    combination that has created great impact in the field of research, in both academia
    and industry. Big data imposes great challenges on ML, data analytics tools, and
    algorithms to find the real value. However, making a future prediction based on
    these huge datasets has never been easy. Considering this challenge, in this chapter,
    we will dive deeper into ML and find out how to use a simple yet powerful method
    to build a scalable classification model and concepts such as multinomial classification,
    Bayesian inference, Naive Bayes, decision trees, and a comparative analysis of
    Naive Bayes versus decision trees.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 14](part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Put Some Order - Cluster Your Data with Spark MLlib,* gets you started on how
    Spark works in cluster mode with its underlying architecture. In previous chapters,
    we saw how to develop practical applications using different Spark APIs. Finally,
    we will see how to deploy a full Spark application on a cluster, be it with a
    pre-existing Hadoop installation or without.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 15](part0458.html#DKP1K1-21aec46d8593429cacea59dbdcd64e1c), *Text
    Analytics Using Spark ML,* outlines the wonderful field of text analytics using
    Spark ML. Text analytics is a wide area in machine learning and is useful in many
    use cases, such as sentiment analysis, chat bots, email spam detection, natural
    language processing, and many many more. We will learn how to use Spark for text
    analysis with a focus on use cases of text classification using a 10,000 sample
    set of Twitter data. We will also look at LDA, a popular technique to generate
    topics from documents without knowing much about the actual text, and will implement
    text classification on Twitter data to see how it all comes together.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 16](part0480.html#E9OE01-21aec46d8593429cacea59dbdcd64e1c), *Spark
    Tuning,* digs deeper into Apache Spark internals and says that while Spark is
    great in making us feel as if we are using just another Scala collection, we shouldn''t
    forget that Spark actually runs in a distributed system. Therefore, throughout
    this chapter, we will cover how to monitor Spark jobs, Spark configuration, common
    mistakes in Spark app development, and some optimization techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 17](part0511.html#F7AFE1-21aec46d8593429cacea59dbdcd64e1c), *Time
    to Go to ClusterLand - Deploying Spark on a Cluster,* explores how Spark works
    in cluster mode with its underlying architecture. We will see Spark architecture
    in a cluster, the Spark ecosystem and cluster management, and how to deploy Spark
    on standalone, Mesos, Yarn, and AWS clusters. We will also see how to deploy your
    app on a cloud-based AWS cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 18](part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c), *Testing
    and Debugging Spark,* explains how difficult it can be to test an application
    if it is distributed; then, we see some ways to tackle this. We will cover how
    to do testing in a distributed environment, and testing and debugging Spark applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 19](part0571.html#H0HH61-21aec46d8593429cacea59dbdcd64e1c), *PySpark
    & SparkR,* covers the other two popular APIs for writing Spark code using R and
    Python, that is, PySpark and SparkR. In particular, we will cover how to get started
    with PySpark and interacting with DataFrame APIs and UDFs with PySpark, and then
    we will do some data analytics using PySpark. The second part of this chapter
    covers how to get started with SparkR. We will also see how to do data processing
    and manipulation, and how to work with RDD and DataFrames using SparkR, and finally,
    some data visualization using SparkR.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Appendix A](part0593.html#HLGTI1-21aec46d8593429cacea59dbdcd64e1c), *Accelerating
    Spark with Alluxio*, shows how to use Alluxio with Spark to increase the speed
    of processing. Alluxio is an open source distributed memory storage system useful
    for increasing the speed of many applications across platforms, including Apache
    Spark. We will explore the possibilities of using Alluxio and how Alluxio integration
    will provide greater performance without the need to cache the data in memory
    every time we run a Spark job.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Appendix B](part0612.html#I7KO81-21aec46d8593429cacea59dbdcd64e1c), *Interactive
    Data Analytics with Apache Zepp**e**lin*, says that from a data science perspective,
    interactive visualization of your data analysis is also important. Apache Zeppelin
    is a web-based notebook for interactive and large-scale data analytics with multiple
    backends and interpreters. In this chapter, we will discuss how to use Apache
    Zeppelin for large-scale data analytics using Spark as the interpreter in the
    backend.'
  prefs: []
  type: TYPE_NORMAL
- en: What you need for this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All the examples have been implemented using Python version 2.7 and 3.5 on
    an Ubuntu Linux 64 bit, including the TensorFlow library version 1.0.1\. However,
    in the book, we showed the source code with only Python 2.7 compatible. Source
    codes that are Python 3.5+ compatible can be downloaded from the Packt repository.
    You will also need the following Python modules (preferably the latest versions):'
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0.0 (or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hadoop 2.7 (or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java (JDK and JRE) 1.7+/1.8+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala 2.11.x (or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 2.7+/3.4+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R 3.1+ and RStudio 1.0.143 (or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eclipse Mars, Oxygen, or Luna (latest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven Eclipse plugin (2.9 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven compiler plugin for Eclipse (2.3.2 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maven assembly plugin for Eclipse (2.4.1 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operating system:** Linux distributions are preferable (including Debian,
    Ubuntu, Fedora, RHEL, and CentOS) and to be more specific, for Ubuntu it is recommended
    to have a complete 14.04 (LTS) 64-bit (or later) installation, VMWare player 12,
    or Virtual box. You can run Spark jobs on Windows (XP/7/8/10) or Mac OS X (10.4.7+).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware configuration:** Processor Core i3, Core i5 (recommended), or Core
    i7 (to get the best results). However, multicore processing will provide faster
    data processing and scalability. You will need least 8-16 GB RAM (recommended)
    for a standalone mode and at least 32 GB RAM for a single VM--and higher for cluster.
    You will also need enough storage for running heavy jobs (depending on the dataset
    size you will be handling), and preferably at least 50 GB of free disk storage
    (for standalone word missing and for an SQL warehouse).'
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anyone who wishes to learn how to perform data analysis by harnessing the power
    of Spark will find this book extremely useful. No knowledge of Spark or Scala
    is assumed, although prior programming experience (especially with other JVM languages)
    will be useful in order to pick up the concepts quicker. Scala has been observing
    a steady rise in adoption over the past few years, especially in the fields of
    data science and analytics. Going hand in hand with Scala is Apache Spark, which
    is programmed in Scala and is widely used in the field of analytics. This book
    will help you leverage the power of both these tools to make sense of big data.
  prefs: []
  type: TYPE_NORMAL
- en: Conventions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, you will find a number of text styles that distinguish between
    different kinds of information. Here are some examples of these styles and an
    explanation of their meaning. Code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles are shown as follows: "The next lines of code read the link and assign
    it to the to the `BeautifulSoup` function."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**New terms**and **important words** are shown in bold. Words that you see
    on the screen, for example, in menus or dialog boxes, appear in the text like
    this: "Clicking the Next button moves you to the next screen."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Reader feedback
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome. Let us know what you think about
    this book-what you liked or disliked. Reader feedback is important for us as it
    helps us develop titles that you will really get the most out of. To send us general
    feedback, simply e-mail `feedback@packtpub.com`, and mention the book's title
    in the subject of your message. If there is a topic that you have expertise in
    and you are interested in either writing or contributing to a book, see our author
    guide at [www.packtpub.com/authors](http://www.packtpub.com/authors).
  prefs: []
  type: TYPE_NORMAL
- en: Customer support
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you are the proud owner of a Packt book, we have a number of things
    to help you to get the most from your purchase.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the example code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can download the example code files for this book from your account at
    [http://www.packtpub.com](http://www.packtpub.com). If you purchased this book
    elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files e-mailed directly to you. You can download the
    code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register to our website using your e-mail address and password.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Hover the mouse pointer on the SUPPORT tab at the top.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Code Downloads & Errata.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the Search box.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the book for which you're looking to download the code files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose from the drop-down menu where you purchased this book from.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Code Download.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR / 7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg / iZip / UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip / PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics](https://github.com/PacktPublishing/Scala-and-Spark-for-Big-Data-Analytics).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the color images of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We also provide you with a PDF file that has color images of the screenshots/diagrams
    used in this book. The color images will help you better understand the changes
    in the output. You can download this file from [https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf](https://www.packtpub.com/sites/default/files/downloads/ScalaandSparkforBigDataAnalytics_ColorImages.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: Errata
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we have taken every care to ensure the accuracy of our content, mistakes
    do happen. If you find a mistake in one of our books-maybe a mistake in the text
    or the code-we would be grateful if you could report this to us. By doing so,
    you can save other readers from frustration and help us improve subsequent versions
    of this book. If you find any errata, please report them by visiting [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details of your errata. Once your errata are verified, your submission will
    be accepted and the errata will be uploaded to our website or added to any list
    of existing errata under the Errata section of that title. To view the previously
    submitted errata, go to [https://www.packtpub.com/books/content/support](https://www.packtpub.com/books/content/support)
    and enter the name of the book in the search field. The required information will
    appear under the Errata section.
  prefs: []
  type: TYPE_NORMAL
- en: Piracy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Piracy of copyrighted material on the Internet is an ongoing problem across
    all media. At Packt, we take the protection of our copyright and licenses very
    seriously. If you come across any illegal copies of our works in any form on the
    Internet, please provide us with the location address or website name immediately
    so that we can pursue a remedy. Please contact us at `copyright@packtpub.com`
    with a link to the suspected pirated material. We appreciate your help in protecting
    our authors and our ability to bring you valuable content.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have a problem with any aspect of this book, you can contact us at `questions@packtpub.com`,
    and we will do our best to address the problem.
  prefs: []
  type: TYPE_NORMAL
