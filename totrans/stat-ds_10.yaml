- en: Boosting your Database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explain what statistical boosting is, how it works,
    and introduce the notion of using statistical boosting to better understand data
    in a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have again broken the subjects in this chapter down into the following important
    areas for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition and purpose of statistical boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you can learn from boosting (to help) your database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using R to illustrate boosting methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definition and purpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we can consider a common definition you may find online:'
  prefs: []
  type: TYPE_NORMAL
- en: Boosting is a machine learning ensemble meta-algorithm for primarily reducing
    bias, and also variance in supervised learning, and a family of machine learning
    algorithms which convert weak learners to strong ones.
  prefs: []
  type: TYPE_NORMAL
- en: -Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Boosting_(machine_learning)](https://en.wikipedia.org/wiki/Boosting_(machine_learning))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reminder: In statistics, ensemble methods use multiple learning algorithms
    to obtain better predictive performance than could be obtained from any of the
    fundamental or basic learning algorithms (although results vary by data and data
    model).'
  prefs: []
  type: TYPE_NORMAL
- en: Before we head into the details behind statistical boosting, it is imperative
    that we take some time here to first understand bias, variance, noise, and what
    is meant by a weak learner, and a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: The following sections will cover these terms and related concepts.
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start out with a discussion on the statistical bias.
  prefs: []
  type: TYPE_NORMAL
- en: A statistic is biased if it is calculated in such a way that it is analytically
    dissimilar to the population parameter being estimated.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best explanations for bias that I've come across is the concept of
    a scale that is off zero by a small amount. In this scenario, the scale will give
    slightly over-estimated results. In other words, when someone steps on the scale,
    the total weight may be over or understated (which might make that person conclude
    that the diet they are on is working better than it really is).
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, data scientists need to recognize that there are actually several
    categories that are routinely used to define statistical bias. The next section
    lists these categories of bias along with examples.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing bias is somewhat subjective since some of the categories will seem
    to overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many categories of bias, including the following specific examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Selection bias**: This is when individual observations are more likely to
    be selected for study than others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectrum bias**: This occurs when data scientists evaluate results on biased
    samples, leading to an overestimate of the sensitivity and specificity of the
    test(s).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator bias**: This is the difference between an estimator''s expected
    value and the true value of the parameter being estimated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Omitted-variable bias**: This bias will occur in estimating the parameters
    in a regression analysis when the assumed specification neglects an independent
    variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection bias**: This occurs when a character or event is more likely to
    be observed for a particular set of study subjects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling bias**: This bias occurs when a statistical error is imposed due
    to an error in the sampling data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement bias**: This occurs then there is a systematic problem with test
    content, testing administration, and/or scoring procedures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Funding bias**: This type of bias can lead to a selection of specific outcomes,
    observations, test samples, or test procedures that favor a study''s financial
    sponsor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reporting bias**: Bias of this type involves askew in the availability of
    data, which causes observations of a certain type or collection to be more likely
    to be reported as a result or to affect performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Analytical bias**: This occurs based on the method or process used to evaluate
    the results of certain observations or the performance of a statistical model
    as a whole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exclusion bias**: This category of bias may arise based upon a process or
    procedure that has the potential to systematically exclude certain samples or
    observations from a statistical study.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attrition bias**: When participants in a study or statistical project leave
    the program or process. In other words, a group or category of a project may leave
    or be removed and will no longer be considered by data scientists.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall bias**: When the accuracy or completeness of a study''s participants
    does not align due to misrecollections of past events or characteristics of what
    is being studied. This results in an over-estimation or under-estimation of the
    results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observer bias**: This type of bias occurs when the researcher subconsciously
    influences the data due to cognitive bias, where judgment may alter how an observation
    or study is carried out/how results are recorded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confounding bias**: This type of bias occurs when factors affecting the same
    information in a study are misleading or otherwise confusing to the researcher
    or data scientist.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negativity bias**: This occurs when a data scientist is inclined to give
    more weight or value to negative characteristics, events, or outcomes, just because
    they are negative.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Representative bias**: This occurs when data scientists take something for
    granted based upon certain observed characteristics identified within a group
    or certain observations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recency bias**: This category of bias occurs when the recent experiences
    and observations of a data scientist are used (or are given more value) to predict
    future outcomes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And one of my favorite types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data-snooping bias**: This happens when the data scientist forms an incorrect
    opinion or makes a hypothesis, then proceeds to mine data that is especially in
    defense of that notion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Causes of bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Bias* is a term that you will find is commonly thrown around in the field
    of statistics and, almost always, bias is equivalent to (or with) a negative or
    bad incident. In fact, even beyond the realm of statistics, bias almost always
    results in trouble or some form of distress.'
  prefs: []
  type: TYPE_NORMAL
- en: Consider bias as favoritism. Favoritism that is present in the data collection
    process, for example, will typically result in misleading results or incorrect
    assumptions.
  prefs: []
  type: TYPE_NORMAL
- en: Bias can arise in various ways and, as a data scientist, one must be familiar
    with these occasions. Actually, bias can be introduced to a statistical project
    at any time or phase.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common times that bias is introduced is at the very start or
    beginning of a project when data is collected or selected. This is the worst,
    as almost every effort and all work completed afterwards will be suspect or, most
    likely, incorrect.
  prefs: []
  type: TYPE_NORMAL
- en: Bias data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A major source of bias is the way data is collected. Frankly, researchers who
    are inexperienced, or hoping for a certain result, may use inferior data collection
    methods or practices or actually collect data in ways that expose a particular
    emphasis or lead to an anticipated or expected result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some things to look for in data collection methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Surveys that are constructed with a particular slant or emphasis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing a known group with a particular background to respond to a survey
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reporting of data in misleading categorical groupings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A non-randomness sample selection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Systematic measurement errors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias sample selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The process of sample selection or sampling is also subject to the introduction
    of bias. Sample bias occurs when the sample does not accurately represent a population.
    The bias that results from an unrepresentative sample is called **selection bias**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Issues that can result in introducing bias to a statistical sample include:'
  prefs: []
  type: TYPE_NORMAL
- en: The timing of taking the sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The length or size of the sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The level of difficulty of the question
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Undercoverage (of the population)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nonresponses incorrectly used in a sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voluntary responses incorrectly used within a sample
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The manner (in which the subjects in the sample were contacted (phone, mail,
    door-to-door, and so on), or how the observation data was split)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enough about bias. Let's move on to the next section, where we will cover statistical
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In statistical theory ([https://en.wikipedia.org/wiki/Statistics](https://en.wikipedia.org/wiki/Statistics)),
    the concept of variance is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The expectation ([https://en.wikipedia.org/wiki/Expected_value](https://en.wikipedia.org/wiki/Expected_value))
    of the squared deviation of a random variable from its mean or, in other words,
    it is a measurement of just how far a set of random numbers are spread out from
    their average value.
  prefs: []
  type: TYPE_NORMAL
- en: The practice of the analysis of variance (or simply variance analysis) involves
    a data scientist evaluating the difference between two figures. Typically, this
    process applies financial or operational data in an attempt to identify and determine
    the cause of the variance. In applied statistics, there are different forms of
    variance analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'Variance and the analysis of variance is a big topic within the field and study
    of statistics, where it plays a key role in the following statistical practices:'
  prefs: []
  type: TYPE_NORMAL
- en: Descriptive statistics ([https://en.wikipedia.org/wiki/Descriptive_statistics](https://en.wikipedia.org/wiki/Descriptive_statistics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodness of fit ([https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hypothesis testing ([https://en.wikipedia.org/wiki/Statistical_hypothesis_testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo sampling ([https://en.wikipedia.org/wiki/Monte_Carlo_method](https://en.wikipedia.org/wiki/Monte_Carlo_method))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical inference ([https://en.wikipedia.org/wiki/Statistical_inference](https://en.wikipedia.org/wiki/Statistical_inference))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You''ll find the following to be true with variance:'
  prefs: []
  type: TYPE_NORMAL
- en: Whenever there is a need for the statistical analysis of data, a data scientist
    will more than likely start with the process of variance analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical variance provides data scientists with a measuring stick to gauge
    how the data distributes itself (about the mean or an expected value)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike range (which only looks at extreme values), variance looks at all the
    data points and concludes their distribution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANOVA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a data scientist, when you are speaking about the process or practice of
    **analysis of variance**, you are speaking of **ANOVA. **ANOVA can be understood
    as an assortment of methods that are used in the investigation of found or potential
    differences (variances) amongst group means and their accompanying procedures.
  prefs: []
  type: TYPE_NORMAL
- en: 'ANOVA is studied and used in the field of statistics in three distinct styles.
    These styles are determined and defined by the number of independent variables
    a data scientist is working with or interested in:'
  prefs: []
  type: TYPE_NORMAL
- en: One-way ANOVA (deals with just one independent variable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two-way ANOVA (uses or focuses on two independent variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: N-way ANOVA (when the data scientist is interested in more than two independent
    variables)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a data scientist or researcher conducts an ANOVA, they are endeavoring
    to conclude whether there is a statistically significant difference between groups
    within their population. If they find that there is a difference, they will then
    go on to determine where the group differences are.
  prefs: []
  type: TYPE_NORMAL
- en: Noise
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Noise or, to the data scientist, statistical noise, is the popular expression
    for acknowledged amounts of unexplained variation or variability in a sample,
    population, or data source.
  prefs: []
  type: TYPE_NORMAL
- en: The actual use of the term *noise* can be traced to early signal processing,
    where it was coined as a way of referring to undesirable (electromagnetic) energy
    that was found to be degrading the quality of signals and data.
  prefs: []
  type: TYPE_NORMAL
- en: To the data or database developer, consider the example of running a simple
    database query to determine the performance of a particular sales region of an
    organization. If your SQL query returns sales for every sales region, one might
    consider the additional sales regions—in the context of this exercise—noise that
    perhaps renders the sales information useless (again, in the context of trying
    to focus on a particular sales region). To resolve this condition, of course,
    one could restructure the query so that it filters out unwanted regions or manipulate
    the results of the query to remove the noise of unwanted regions. Keep in mind,
    in statistics, it may be unrealistic to recreate the data source.
  prefs: []
  type: TYPE_NORMAL
- en: Noisy data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outside of statistics, people often use the term statistical noise to dismiss
    any data that they aren't interested in. An example of this is a professional
    football team's stadium, where fans cheering interferes with the ability of the
    visiting team to communicate. The noise is an inconvenience.
  prefs: []
  type: TYPE_NORMAL
- en: Within statistics though, when a data scientist acknowledges the presence of
    noise within a sample, it means that any results from statistical sampling might
    not be duplicated if the process were repeated. In this case, the sample may become
    noisy data and rendered meaningless because of the existence of too much variation.
  prefs: []
  type: TYPE_NORMAL
- en: The effort of unraveling the noise from the true signal has pretty much always
    been a major emphasis in statistics (so that the meaningful data could be used
    by researchers), however, the percentage of noisy data that is meaningful is often
    too insignificant to be of much use.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, the term *noisy data* has grown to also refer to *any data that is
    not machine-readable*, such as unstructured text and even any data altered in
    some way that is no longer compatible with the program used to create it. Happily,
    advances in analytical tools are steadily overcoming these types of statistical
    obstacles (think IBM Watson analytics, but there are many others).
  prefs: []
  type: TYPE_NORMAL
- en: Some of the most commonly accepted examples of statistical noise include Gaussian
    noise, shot noise, and white noise. Enough of this noise about (statistical) noise!
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to learners.
  prefs: []
  type: TYPE_NORMAL
- en: Weak and strong learners
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A nice segue (back) into the topic of boosting is a statistical algorithm or
    model's ability to improve its ability to predict overtime, that is, its performance.
  prefs: []
  type: TYPE_NORMAL
- en: If you are reading this book and have reached this section of this chapter,
    the assumption is that you understand the concept of machine learning, as it is
    related to statistical prediction-making. Learning is a computer or model's ability
    to learn (how to make predictions based upon data) without being explicitly programmed.
  prefs: []
  type: TYPE_NORMAL
- en: We use the term *explicitly* to mean *hardcoded* selections based upon data
    values.
  prefs: []
  type: TYPE_NORMAL
- en: If you build upon this concept, you can come to understand that a computer or
    model whose intention or objective is to make good predictions (to guess an outcome
    correctly) based upon data will perform or produce results that are somewhere
    between incorrect (bad) and correct (or good).
  prefs: []
  type: TYPE_NORMAL
- en: One can also then say that the computer or model could perhaps improve its performance
    with more experience (more data) and could improve learning at some rate.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, a data scientist will label a computer or model (the learner) as a weak
    or strong learner, based on its performance (or its ability to predict or guess
    outcomes).
  prefs: []
  type: TYPE_NORMAL
- en: In the field of statistics and data science, one can also refer to a *learner*
    as a classifier or predictor.
  prefs: []
  type: TYPE_NORMAL
- en: So, what qualifies a learner as weak or strong?
  prefs: []
  type: TYPE_NORMAL
- en: A weak learner is one that, no matter what the data looks like (meaning the
    distribution of values within the data the model is being trained on), will always
    perform better than chance when it tries to label the data.
  prefs: []
  type: TYPE_NORMAL
- en: We qualify doing better than chance as always having an error rate which is
    less than half.
  prefs: []
  type: TYPE_NORMAL
- en: Weak to strong
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Better than random* *guessing* is fundamentally the one and only prerequisite
    for a weak learner. So, as long as an algorithm or model can consistently beat
    random guessing, applying a boosting algorithm will be able to increase the accuracy
    of the model''s predictions (its performance) and consequently convert the model
    from being a weak learner to a strong learner.'
  prefs: []
  type: TYPE_NORMAL
- en: Take note, data scientists agree that increasing a model's predictive ability
    or performance even to ever so slightly better than random guessing results means
    success.
  prefs: []
  type: TYPE_NORMAL
- en: When a data scientist considers the options for improving the performance of
    a model (or converting a weak learner to a strong learner), numerous factors need
    to be considered.
  prefs: []
  type: TYPE_NORMAL
- en: These factors include model bias, processing time, and complexity. Let's explain
    each a little.
  prefs: []
  type: TYPE_NORMAL
- en: Model bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We talked about *statistical bias* in an earlier section of this chapter. The
    level of bias identified within a statistical model needs to be considered. Typically,
    the lower the amount of bias, the better, since some methods for improving on
    a weak learner—such as boosting—can overfit, resulting in misleading results.
  prefs: []
  type: TYPE_NORMAL
- en: Training and prediction time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whether or not, the approach for improving a weak learner's performance adds
    significantly to the amount of time a model takes to learn, train, or predict
    on a data subset. Usually, the more you train a model, the better the results,
    so if you are anticipating requiring hundreds of training iterations, you need
    to consider how much longer that effort or process will take if your improvements
    increase the training iteration by 100%.
  prefs: []
  type: TYPE_NORMAL
- en: Complexity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, there is an assumption that a weak learner is computationally simple
    in design (it's a weak learner, right?), but that is not always the case. Understanding
    the level of complexity of an algorithm or model before choosing an approach for
    improving performance is critical in the decision-making process.
  prefs: []
  type: TYPE_NORMAL
- en: Which way?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which way (which approach) a data scientist will go or take to improve a model's
    performance and convert it from a weak learner into a strong learner will ultimately
    depend on many factors, but in the end, the approach taken depends on the individual
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost (also known as **Adaptive Boosting**) is an iterative algorithm using
    a designated number of iterations or rounds to improve on a weak learner. This
    algorithm starts by training/testing a weak learner on data, weighting each example
    equally. The examples which are misclassified get their weights increased for
    the next round(s), while those that are correctly classified get their weights
    decreased.
  prefs: []
  type: TYPE_NORMAL
- en: We will know about AdaBoost later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Back to boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At this point, we have covered all of the topics most pertinent to boosting,
    so let's now get back to the main event, statistical boosting.
  prefs: []
  type: TYPE_NORMAL
- en: We have already offered a description of what statistical boosting is and what
    it is used for (a learning algorithm intended to reduce bias and variance and
    convert weak learners into strong ones).
  prefs: []
  type: TYPE_NORMAL
- en: Key to this concept is the idea of how learners inherently behave, with a weak
    learner defined as one which is only slightly correlated with the true classification
    (it can label examples better than random guessing). In contrast, a strong learner
    is one that is well-correlated with the true classification.
  prefs: []
  type: TYPE_NORMAL
- en: How it started
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting an algorithm in an attempt to improve performance is, in reality, hypothetical.
    That is, it is a question every data scientist should ask for their statistical
    algorithm or model.
  prefs: []
  type: TYPE_NORMAL
- en: This is known in statistics as the hypothesis-boosting question and is all about
    the data scientist finding a way to even slightly improve a learning process (turning
    a weak learner into a strong(er) learner).
  prefs: []
  type: TYPE_NORMAL
- en: The idea of a strong learner only implies a slightly improved learner--actually,
    only slightly better than random guessing.
  prefs: []
  type: TYPE_NORMAL
- en: In the data science or statistics world, the hypothesis-boosting question also
    implies the actual existence of an efficient algorithm that outputs a hypothesis
    of arbitrary accuracy for the problem being solved. These algorithms (that improve
    learners) have quickly become known simply as **boosting**.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, data scientists interchangeably use terms to identify the same thing,
    and boosting is no different, as some data scientists will refer to boosting as
    *resampling* or *combining*.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back to our previous mention of a package named **AdaBoost**, which is short
    for **adaptive boosting**. AdaBoost is a boosting approach referred to as an *ensemble
    learning algorithm*. Ensemble learning is when multiple learners are used in conjunction
    with each other to build a stronger learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost works by selecting a base algorithm and then iteratively improving
    it by accounting for the incorrectly classified examples in the training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'A wonderful explanation of boosting and AdaBoost can be found online: *Better
    living through AdaBoost* [http://bbacon.org/Better-Living-Through-AdaBoost](http://bbacon.org/Better-Living-Through-AdaBoost).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aforementioned article describes how AdaBoost works:'
  prefs: []
  type: TYPE_NORMAL
- en: AdaBoost trains a model on a data subset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weak learners (based upon performance) are weighted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process is repeated
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In narrative form, the AdaBoost boosting logic can be explained in the following
    way:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The process works by building a model on training data and then measuring the
    results'' accuracy on that training data, then:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The individual results that were erroneous in the model are assigned a larger
    weight (or weighted more) than those that were correct, and then the model is
    *retrained* again using these new weights. This logic is then repeated multiple
    times, adjusting the weights of individual observations each time based on whether
    they were correctly classified or not in the last iteration!
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The AdaBoost algorithm was originally offered by *Freund* and *Schapire* in
    the Journal of *Computer and System Sciences* in a 1997 paper titled *A Decision-Theoretic
    Generalization of On-Line Learning and an Application to Boosting*.
  prefs: []
  type: TYPE_NORMAL
- en: What you can learn from boosting (to help) your database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Thinking from the perspective or point of view of a database developer, you
    may be trying to conceptualize the process of boosting. As we''ve done throughout
    this book, here, we''ll try to use a database-oriented example to help our understanding
    of boosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/afc226d4-457b-4aeb-a77b-10357eac5485.png)'
  prefs: []
  type: TYPE_IMG
- en: Our example starts with a relational database. There, effective indexes are
    one of the best ways to improve the performance of a database application. Without
    an effective (strong?) index, the database engine is like a reader trying to find
    a phrase within a reference book by having to take the time to examine each and
    every page. But if the reader uses the reference book's index, the reader can
    then complete the task in a much shorter time (better performance = better results).
  prefs: []
  type: TYPE_NORMAL
- en: In database terms, a table scan occurs when there is no available index identified
    to boost the performance of a database query. In a table scan, the database engine
    examines each and every row in the table to satisfy the query results.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important jobs for a database developer is finding the best
    index to use when generating an execution plan (for a query). Most major databases
    offer tools to show you execution plans for a query and help in optimizing and
    tuning query indexes.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding situation might be compared to repetitively testing queries in
    a database, scoring individual performances (in returning the query results) until
    an efficient (or strong) index is determined.
  prefs: []
  type: TYPE_NORMAL
- en: This then has the effect of improving the performance of the database query
    so that it becomes a strong responder (or, if you will, a strong learner).
  prefs: []
  type: TYPE_NORMAL
- en: Using R to illustrate boosting methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to further illustrate the use of boosting, we should have an example.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll take a high-level look at a thought-provoking prediction
    problem drawn from *Mastering Predictive Analytics with R, Second Edition,* James
    D. Miller and Rui Miguel Forte, August 2017 ([https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition)).
  prefs: []
  type: TYPE_NORMAL
- en: In this original example, patterns made by radiation on a telescope camera are
    analyzed in an attempt to predict whether a certain pattern came from gamma rays
    leaking into the atmosphere or from regular background radiation.
  prefs: []
  type: TYPE_NORMAL
- en: Gamma rays leave distinctive elliptical patterns and so we can create a set
    of features to describe these. The dataset used is the *MAGIC Gamma Telescope
    Data Set*, hosted by the *UCI Machine Learning* *Repository* at [http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope](http://archive.ics.uci.edu/ml/datasets/MAGIC+Gamma+Telescope).
  prefs: []
  type: TYPE_NORMAL
- en: 'This data consists of 19,020 observations, holding the following list of attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Type** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| `FLENGTH` | Numerical | The major axis of the ellipse (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FWIDTH` | Numerical | The minor axis of the ellipse (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FSIZE` | Numerical | Logarithm to the base ten of the sum of the content
    of all pixels in the camera photo |'
  prefs: []
  type: TYPE_TB
- en: '| `FCONC` | Numerical | Ratio of the sum of the two highest pixels over `FSIZE`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `FCONC1` | Numerical | Ratio of the highest pixel over `FSIZE` |'
  prefs: []
  type: TYPE_TB
- en: '| `FASYM` | Numerical | Distance from the highest pixel to the centre, projected
    onto the major axis (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FM3LONG` | Numerical | Third root of the third moment along the major axis
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FM3TRANS` | Numerical | Third root of the third moment along the minor axis
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `FALPHA` | Numerical | Angle of the major axis with the vector to the origin
    (degrees) |'
  prefs: []
  type: TYPE_TB
- en: '| `FDIST` | Numerical | Distance from the origin to the centre of the ellipse
    (mm) |'
  prefs: []
  type: TYPE_TB
- en: '| `CLASS` | Binary | Gamma rays (g) or Background Hadron Radiation (b) |'
  prefs: []
  type: TYPE_TB
- en: Prepping the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, various steps need to be performed on our example data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data is first loaded into an R data frame object named `magic`, recoding
    the `CLASS` output variable to use classes `1` and `-1` for gamma rays and background
    radiation respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the data is split into two files: a training data and a test data frame
    using an 80-20 split:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The model used for boosting is a simple multilayer perceptron with a single
    hidden layer leveraging R's `nnet` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Neural networks, (covered in [Chapter 9](224b964c-c313-4435-b36a-96f77fbabd9b.xhtml),
    *Databases and Neural Networks*) often produce higher accuracy when inputs are
    normalized, so, in this example, before training any models, this preprocessing
    is performed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Boosting is designed to work best with weak learners, so a very small number
    of hidden neurons in the model's hidden layer are used.
  prefs: []
  type: TYPE_NORMAL
- en: Concretely, we will begin with the simplest possible multilayer perceptron that
    uses a single hidden neuron. To understand the effect of using boosting, a baseline
    performance is established by training a single neural network (and measuring
    its performance).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is to accomplish the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This establishes that we have a baseline accuracy of around 79.5 percent. Not
    too bad, but can boost to improve upon this score?
  prefs: []
  type: TYPE_NORMAL
- en: To that end, the function `AdaBoostNN()`, which is shown as follows, is used.
    This function will take input from a data frame, the name of the output variable,
    the number of single hidden layer neural network models to be built, and finally,
    the number of hidden units these neural networks will have.
  prefs: []
  type: TYPE_NORMAL
- en: The function will then implement the AdaBoost algorithm and return a list of
    models with their corresponding weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function uses the following logic:'
  prefs: []
  type: TYPE_NORMAL
- en: First, initialize empty lists of models and model weights (`alphas`). Compute
    the number of observations in the training data, storing this in the variable
    `n`. The name of the output column provided is then used to create a formula that
    describes the neural network that will be built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the dataset used, this formula will be `CLASS ~ .`, meaning that the neural
    network will compute `CLASS` as a function of all the other columns as input features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, initialize the weights vector and define a loop that will run for `M`
    iterations in order to build `M` models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In every iteration, the first step is to use the current setting of the weights
    vector to train a neural network using as many hidden units as specified in the
    input, `hidden_units`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, compute a vector of predictions that the model generates on the training
    data using the `predict()` function. By comparing these predictions to the output
    column of the training data, calculate the errors that the current model makes
    on the training data. This then allows the computation of the error rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This error rate is set as the weight of the current model and, finally, the
    observation weights to be used in the next iteration of the loop are updated according
    to whether each observation was correctly classified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The weight vector is then normalized and we are ready to begin the next iteration!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: After completing `M` iterations, output a list of models and their corresponding
    model weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ready for boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is now a function able to train our ensemble classifier using AdaBoost,
    but we also need a function to make the actual predictions. This function will
    take in the output list produced by our training function, `AdaBoostNN()`, along
    with a test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function is `AdaBoostNN.predict()` and it is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This function first extracts the models and the model weights (from the list
    produced by the previous function). A matrix of predictions is created, where
    each column corresponds to the vector of predictions made by a particular model.
    Thus, there will be as many columns in this matrix as the models that we used
    for boosting.
  prefs: []
  type: TYPE_NORMAL
- en: We then multiply the predictions produced by each model with their corresponding
    model weight. For example, every prediction from the first model is in the first
    column of the prediction matrix and will have its value multiplied by the first
    model weight *α[1]*.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, the matrix of weighted observations is reduced into a single vector
    of observations by summing the weighted predictions for each observation and taking
    the sign of the result. This vector of predictions is then returned by the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an experiment, we will train ten neural network models with a single hidden
    unit and see if boosting improves accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, boosting ten models shows a marginal improvement in accuracy,
    but perhaps training more models might make more of a difference.
  prefs: []
  type: TYPE_NORMAL
- en: As we have stated several times in this chapter, even a marginal improvement
    in performance qualifies as converting a weak learner into a strong one!
  prefs: []
  type: TYPE_NORMAL
- en: Example results
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the preceding example, you may conclude that, for the neural networks with
    one hidden unit, as the number of boosting models increases, we see an improvement
    in accuracy, but after 100 models, this tapers off and is actually slightly less
    for 200 models. The improvement over the baseline of a single model is substantial
    for these networks. When we increase the complexity of our learner by having a
    hidden layer with three hidden neurons, we get a much smaller improvement in performance.
    At 200 models, both ensembles perform at a similar level, indicating that, at
    this point, our accuracy is being limited by the type of model trained.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discovered *s*tatistical boosting, first providing an explanation
    of the key concepts used in statistics relevant to the topic of boosting, thus
    helping to define boosting itself.
  prefs: []
  type: TYPE_NORMAL
- en: We also contemplated the notion of using statistical boosting to better understand
    data in just about every database.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, will again strive to use developer terminologies, this
    time in an effort to define a support vector machine, identify various applications
    for its use, and walk through an example of using a simple SVM to classify the
    data in a database.
  prefs: []
  type: TYPE_NORMAL
