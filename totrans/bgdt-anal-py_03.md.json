["```py\nhdfs://hadoopnamenode.domainname/path/to/file\n```", "```py\n    hdfs dfs -ls /\n    ```", "```py\n    hdfs dfs -put C:/Users/admin/Desktop/Lesson03/new_data.csv /\n    ```", "```py\n    hdfs dfs -mkdir /data\n    ```", "```py\n    hdfs dfs -mv /data_file.csv /data\n    ```", "```py\n    hdfs dfs -mv /data/new_data.csv /data/other_data.csv\n    ```", "```py\n    hadoop fs -ls /data\n    ```", "```py\n    other_data.csv\n    ```", "```py\n    from pyspark.sql import SparkSession\n    >>> spark = SparkSession \\\n        .builder \\\n        .appName(“Python Spark Session”) \\\n        .getOrCreate()\n    ```", "```py\n    df = spark.read.csv(‘/data/mydata.csv’, header=True)\n    ```", "```py\n    df.show()\n    +------+----+-------+\n    |  name| age| height|\n    +------+----+-------+\n    |  Jonh|  22|   1.80|\n    |Hughes|  34|   1.96|\n    |  Mary|  27|   1.56|\n    +------+----+-------+\n    ```", "```py\n    df[‘age’].Column[‘age’]\n    ```", "```py\n    df.select(df[‘name’])DataFrame[age: string]\n    ```", "```py\n    df.select(df[‘age’]).show()\n    +---+\n    |age|\n    +---+\n    | 22|\n    | 34|\n    | 27|\n    +---+\n    ```", "```py\n    df.select(df[‘age’], df[‘height’]).show()\n    +---+------+\n    |age|height|\n    +---+------+\n    | 22|  1.80|\n    | 34|  1.96|\n    | 27|  1.56|\n    +---+------+\n    ```", "```py\n    from pyspark.sql import SparkSession\n    spark = SparkSession \\\n        .builder \\\n        .appName(“Python Spark Session”) \\\n        .getOrCreate()\n    ```", "```py\n    df = spark.read.json(‘hdfs://hadoopnamenode/data/myjsonfile.json’)\n    ```", "```py\n    df = spark.read.json(‘hdfs://data/myjsonfile.json’)\n    ```", "```py\n    url = “jdbc:postgresql://posgreserver:5432/mydatabase”\n    properties = {“user”: “my_postgre_user”,  password: “mypassword”, “driver”: “org.postgresql.Driver”}\n    df = spark.read.jdbc(url, table = “mytable”, properties = properties)\n    ```", "```py\n    df.write.csv(‘results.csv’, header=True)\n    ```", "```py\n    df = spark.write.jdbc(url, table = “mytable”, properties = properties)\n    ```", "```py\n    df = spark.read.csv(‘hdfs:/data/very_large_file.csv’, header=True)\n    ```", "```py\n    df.write.parquet(‘hdfs:/data/data_file’, compression=”snappy”)\n    ```", "```py\n    df_pq = spark.read.parquet(“hdfs:/data/data_file”)\n    ```", "```py\nhdfs -fs ls /data/data_file\nyear=2015\nyear=2016\nyear=2017\nhdfs -fs ls /data/data_file/year=2017\nmonth=01\nmonth=02\nmonth=03\nmonth=04\nmonth=05\n```", "```py\ndf.write.parquet(“hdfs:/data/data_file_partitioned”, partitionBy=[“year”, “month”])\n```", "```py\ndf.write.partittionBy([“year”, “month”]).format(“parquet”).save(“hdfs:/data/data_file_partitioned”)\n```", "```py\n    url = “jdbc:postgresql://posgreserver:5432/timestamped_db”\n    properties = {“user”: “my_postgre_user”,  password: “mypassword”, “driver”: “org.postgresql.Driver”}\n    ```", "```py\n    df = spark.read.jdbc(url, table = “sales”, properties = properties)\n    ```", "```py\n    df.write.parquet(“hdfs:/data/data_file_partitioned”, partitionBy=[“year”, “month”, “day”], compression=”snappy”)\n    ```", "```py\n    from operator import add\n    rdd_df = spark.read.text(“/shake.txt”).rdd\n    ```", "```py\n    lines = rdd_df.map(lambda line: line[0])\n    ```", "```py\n    lines.collect()\n    ```", "```py\n    lines.count()\n    ```", "```py\n    splits = lines.flatMap(lambda x: x.split(‘ ‘))\n    lower_splits = splits.map(lambda x: x.lower().strip())\n    ```", "```py\n    prep = [‘the’, ‘a’, ‘,’, ‘.’]\n    ```", "```py\n    tokens = lower_splits.filter(lambda x: x and x not in prep)\n    ```", "```py\n    token_list = tokens.map(lambda x: [x, 1])\n    ```", "```py\n    count = token_list.reduceByKey(add).sortBy(lambda x: x[1], ascending=False)\n    count.collect()\n    ```"]