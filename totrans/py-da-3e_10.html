<html><head></head><body>
        

                            
                    Signal Processing and Time Series
                
            
            
                
<p>Signal processing is a subdomain of electrical engineering and applied mathematics. It covers the analysis and processing of time-related variables or variables that change over time, such as analog and digital signals. Analog signals are non-digitized signals, such as radio or telephone signals. Digital signals are digitized, discrete, time-sampled signals, such as computer and digital device signals. Time-series analysis is the category of signal processing that deals with ordered or sequential lists of observations. This data can be ordered hourly, daily, weekly, monthly, or annually. The time component in the time series plays a very important role. We need to extract all the relations in the data with respect to time. There are lots of examples that are related to time-series analysis, such as the production and sales of a product, predicting stock prices on an hourly or daily basis, economic forecasts, and census analysis.</p>
<p>In this chapter, our main focus is on signal processing and time-series operations using the NumPy, SciPy, <kbd>pandas</kbd>, and <kbd>statsmodels</kbd> libraries. This chapter will be helpful for data analysts to understand trends and patterns and forecast sales, stock prices, production, population, rainfall, and weather temperature.</p>
<p>We will cover the following topics in this chapter:</p>
<ul>
<li>The <kbd>statsmodels</kbd> modules</li>
<li>Moving averages</li>
<li>Window functions</li>
<li>Defining cointegration</li>
<li>STL decomposition</li>
<li>Autocorrelation</li>
<li>Autoregressive models</li>
</ul>
<ul>
<li>ARMA models</li>
<li>Generating periodic signals</li>
<li>Fourier analysis</li>
<li>Spectral analysis filtering</li>
</ul>
<h1 id="uuid-053b3b36-e65e-491e-af65-4976f3adcaf7">Technical requirements</h1>
<p>This chapter has the following technical requirements:</p>
<ul>
<li>You can find the code and the dataset at the following GitHub link: <a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter08">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/tree/master/Chapter08</a>.</li>
<li>All the code blocks are in the <kbd>Ch8.ipynb</kbd> file.</li>
<li>This chapter uses two CSV files (<kbd>beer_production.csv</kbd> and <kbd>sales.csv</kbd>) for practice purposes.</li>
<li>In this chapter, we will use the <kbd>pandas</kbd> and Scikit-learn Python libraries.</li>
</ul>
<h1 id="uuid-d2dc96f5-4247-40bd-8d5e-62e0b1640aaf" class="">The statsmodels modules</h1>
<p><kbd>statsmodels</kbd> is an open source Python module that offers functionality for various statistical operations, such as central values (mean, mode, and median), dispersion measures (standard deviation and variance), correlations, and hypothesis tests.</p>
<p>Let's install <kbd>statsmodels</kbd> using <kbd>pip</kbd> and run the following command:</p>
<pre><strong>pip3 install statsmodels</strong></pre>
<p><kbd>statsmodels</kbd> provides the <kbd>statsmodels.tsa</kbd> submodule for time-series operations. <kbd>statsmodels.tsa</kbd> provides useful time-series methods and techniques, such as autoregression, autocorrelation, partial autocorrelation, moving averages, SimpleExpSmoothing, Holt's linear, Holt-Winters, ARMA, ARIMA, <strong>vector autoregressive</strong> (<strong>VAR</strong>) models, and lots of helper functions, which we will explore in the upcoming sections.</p>
<h1 id="uuid-fe4fb34a-afba-4374-8a81-1352c2197f2c">Moving averages</h1>
<p>Moving averages, or rolling means, are time-series filters that filter impulsive responses by averaging the set or window of observations. It uses window size concepts and finds the average of the continuous window slides for each period. The simple moving average can be represented as follows:</p>
<p class="CDPAlignLeft CDPAlign">                                            <img src="img/348b3a98-fd85-4c49-99da-fba169304086.png"/></p>
<p>There are various types of moving averages available, such as centered, double, and weighted moving averages. Let's find the moving average using the <kbd>rolling()</kbd> function, but before that, we'll first load the data and visualize it:</p>
<pre># import needful libraries<br/>import pandas as pd<br/>import statsmodels.api as sm<br/>import matplotlib.pyplot as plt<br/><br/># Read dataset<br/>sales_data = pd.read_csv('sales.csv')<br/><br/># Setting figure size<br/>plt.figure(figsize=(10,6))<br/><br/># Plot original sales data<br/>plt.plot(sales_data['Time'], sales_data['Sales'], label="Sales-Original")<br/><br/># Rotate xlabels<br/>plt.xticks(rotation=60)<br/><br/># Add legends<br/>plt.legend()<br/><br/>#display the plot<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div><img src="img/f27f7d5f-9df1-4603-a77b-ce85d0b519ef.png"/></div>
<p>In the preceding code, we have read the sales dataset of 36 months from January 2017 to December 2019 and plotted it using Matplotlib. Now, we will compute the moving average using the rolling function:</p>
<pre># Moving average with window 3<br/>sales_data['3MA']=sales_data['Sales'].rolling(window=3).mean()<br/><br/># Moving average with window 5<br/>sales_data['5MA']=sales_data['Sales'].rolling(window=5).mean()<br/><br/># Setting figure size<br/>plt.figure(figsize=(10,6))<br/><br/># Plot original sales data<br/>plt.plot(sales_data['Time'], sales_data['Sales'], label="Sales-Original", color="blue")<br/><br/># Plot 3-Moving Average of sales data<br/>plt.plot(sales_data['Time'], sales_data['3MA'], label="3-Moving Average(3MA)", color="green")<br/><br/># Plot 5-Moving Average of sales data<br/>plt.plot(sales_data['Time'], sales_data['5MA'], label="5-Moving Average(5MA)", color="red")<br/><br/># Rotate xlabels<br/>plt.xticks(rotation=60)<br/><br/># Add legends<br/>plt.legend()<br/><br/># Display the plot<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div><img src="img/aeeef19b-b968-4207-983b-0adb4b10e638.png"/></div>
<p>In the preceding code, we computed the 3 and 5 moving averages using the rolling mean and displayed the line plot using Matplotlib. Now, let's see different types of window functions for moving averages in the next section.</p>
<h1 id="uuid-9b51306c-e2ac-4b5c-b3b2-5b7fdf983a26">Window functions</h1>
<p>NumPy offers several window options that can compute weights in a rolling window as we did in the previous section.</p>
<p class="mce-root">The window function uses an interval for spectral analysis and filter design (for more background information, refer to <a href="http://en.wikipedia.org/wiki/Window_function">http://en.wikipedia.org/wiki/Window_function</a>). The boxcar window is a rectangular window with the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><em>w(n) = 1</em></p>
<p>The triangular window is shaped like a triangle and has the following formula:</p>
<div><img src="img/ec4ec393-f982-43b3-a864-1d7a1e8817a7.png" style=""/></div>
<p>Here, <em>L</em> can be equal to <em>N</em>, <em>N</em>+1, or <em>N</em>–1.</p>
<p>If the value of <em>L</em> is <em>N</em>–1, it is known as the Bartlett window and has the following formula:</p>
<div><img src="img/715dfe5d-e6dc-4d40-899a-ab61d45b9d98.png" style=""/></div>
<div><img src="img/aaf1be2b-80d5-491c-89aa-42eaf91bb297.png" style=""/></div>
<p>In the <kbd>pandas</kbd> module, the <kbd>DataFrame.rolling()</kbd> function provides the same functionality using the <kbd>win_type</kbd> parameter for different window functions. Another parameter is the window for defining the size of the window, which is easy to set as shown in the previous section. Let's use the <kbd>win_type</kbd> parameter and try different window functions:</p>
<pre># import needful libraries<br/>import pandas as pd<br/>import statsmodels.api as sm<br/>import matplotlib.pyplot as plt<br/><br/># Read dataset<br/>sales_data = pd.read_csv('sales.csv', index_col ="Time")<br/><br/># Apply all the windows on given DataFrame<br/>sales_data['boxcar']=sales_data.Sales.rolling(3, win_type ='boxcar').mean()<br/>sales_data['triang']=sales_data.Sales.rolling(3, win_type ='triang').mean()<br/>sales_data['hamming']=sales_data.Sales.rolling(3, win_type ='hamming').mean()<br/>sales_data['blackman']=sales_data.Sales.rolling(3, win_type ='blackman').mean()<br/><br/>#Plot the rolling mean of all the windows<br/>sales_data.plot(kind='line',figsize=(10,6))</pre>
<p>This results in the following output:</p>
<div><img src="img/09dea12c-959b-4080-a1dc-3bc52f73ffe7.png"/></div>
<p>In the preceding code block, we have plotted the rolling mean for different window functions, such as boxcar, triangular, hamming, and Blackman window, using the <kbd>win_type</kbd> parameter in the <kbd>rolling()</kbd> function. Now, let's learn how to find a correlation between two time series using cointegration. </p>
<h1 id="uuid-00f47391-a0cd-4192-bb02-abbb256e3f89">Defining cointegration</h1>
<p>Cointegration is just like a correlation that can be viewed as a superior metric to define the relatedness of two time series. Cointegration is the stationary behavior of the linear combination of two time series. In this way, the trend of the following equation must be stationary:</p>
<p class="CDPAlignCenter CDPAlign"><em>y(t) - a x(t)</em></p>
<p>Consider a drunk man and his dog out on a walk. Correlation tells us whether they are going in the same direction. Cointegration tells us something about the distance over time between the man and his dog. We will show cointegration using randomly generated time-series and real data. The <strong>Augmented Dickey-Fuller</strong> (<strong>ADF</strong>) test tests for a unit root in a time series and can be used to determine the stationarity of time series.</p>
<p>Let's see an example to understand the cointegration of two time series.</p>
<p>You can check out the full code for this example at the following GitHub link:<a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter08/Ch8.ipynb"> </a><a href="https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter08/Ch8.ipynb">https://github.com/PacktPublishing/Python-Data-Analysis-Third-Edition/blob/master/Chapter08/Ch8.ipynb</a>. </p>
<p>Let's get started with the cointegration demo:</p>
<ol>
<li>Import the required libraries and define the following function to calculate the ADF statistic:</li>
</ol>
<pre style="padding-left: 60px"># Import required library<br/>import statsmodels.api as sm<br/>import pandas as pd<br/>import statsmodels.tsa.stattools as ts<br/>import numpy as np<br/><br/># Calculate ADF function <br/>def calc_adf(x, y):<br/>    result = sm.OLS(x, y).fit()<br/>    return ts.adfuller(result.resid)</pre>
<ol start="2">
<li>Load the Sunspot data into a NumPy array:</li>
</ol>
<pre style="padding-left: 60px"># Read the Dataset<br/>data = sm.datasets.sunspots.load_pandas().data.values<br/>N = len(data)</pre>
<ol start="3">
<li>Generate a sine wave and calculate the cointegration of the sine with itself:</li>
</ol>
<pre style="padding-left: 60px"># Create Sine wave and apply ADF test  <br/>t = np.linspace(-2 * np.pi, 2 * np.pi, N)<br/>sine = np.sin(np.sin(t))<br/>print("Self ADF", calc_adf(sine, sine))</pre>
<p style="padding-left: 60px">The code should print the following:</p>
<pre style="padding-left: 60px">Self ADF (-5.0383000037165746e-16, 0.95853208606005591, 0, 308,<br/>{'5%': -2.8709700936076912, '1%': -3.4517611601803702, '10%':<br/>-2.5717944160060719}, -21533.113655477719)</pre>
<p style="padding-left: 60px">In the printed results, the first value represents the ADF metric and the second value represents the p-value. As you can see, the p-value is very high. The following values are the lag and sample size. The dictionary at the end gives the t-distribution values for this exact sample size.</p>
<ol start="4">
<li>Now, add noise to the sine to demonstrate how noise will influence the signal:</li>
</ol>
<pre style="padding-left: 60px"># Apply ADF test on Sine and Sine with noise <br/>noise = np.random.normal(0, .01, N)<br/>print("ADF sine with noise", calc_adf(sine, sine + noise))</pre>
<p style="padding-left: 60px">With the noise, we get the following results:</p>
<pre style="padding-left: 60px">ADF sine with noise (-7.4535502402193075, 5.5885761455106898e- 11, 3, 305, {'5%': -2.8710633193086648, '1%': -3.4519735736206991, '10%': -2.5718441306100512}, -1855.0243977703672)</pre>
<p style="padding-left: 60px">The p-value has gone down considerably. The ADF metric here, <kbd>-7.45</kbd>, is lower than all the critical values in the dictionary. All these are strong arguments to reject cointegration.</p>
<ol start="5">
<li>Let's generate a cosine of a larger magnitude and offset. Again, let's add noise to it:</li>
</ol>
<pre style="padding-left: 60px"># Apply ADF test on Sine and Cosine with noise <br/>cosine = 100 * np.cos(t) + 10<br/><br/>print("ADF sine vs cosine with noise", calc_adf(sine, cosine + noise))</pre>
<p style="padding-left: 60px">The following values get printed:</p>
<pre style="padding-left: 60px"><strong>ADF sine vs cosine with noise (-17.927224617871534, 2.8918612252729532e-30, 16, 292, {'5%': -2.8714895534256861, '1%': -3.4529449243622383, '10%': -2.5720714378870331}, -11017.837238220782)<br/></strong></pre>
<p style="padding-left: 60px">Similarly, we have strong arguments to reject cointegration. Checking for cointegration between the sine and sunspots gives the following output:</p>
<pre style="padding-left: 60px">print("Sine vs sunspots", calc_adf(sine, data))</pre>
<p style="padding-left: 60px">The following values get printed:</p>
<pre style="padding-left: 60px"><strong>Sine vs sunspots (-6.7242691810701016, 3.4210811915549028e-09, 16, 292,</strong><br/><strong>{'5%': -2.8714895534256861, '1%': -3.4529449243622383, </strong><br/><strong>'10%': -2.5720714378870331}, -1102.5867415291168)</strong></pre>
<p style="padding-left: 60px">The confidence levels are roughly the same for the pairs used here because they are dependent on the number of data points, which doesn't vary much. The outcome is summarized in the following table:</p>
<table style="width: 100%;border-collapse: collapse" class="a" border="1">
<tbody>
<tr>
<td>
<p><strong>Pair</strong></p>
</td>
<td>
<p><strong>Statistic</strong></p>
</td>
<td>
<p><strong>p-value</strong></p>
</td>
<td>
<p><strong>5%</strong></p>
</td>
<td>
<p><strong>1%</strong></p>
</td>
<td>
<p><strong>10%</strong></p>
</td>
<td>
<p><strong>Reject</strong></p>
</td>
</tr>
<tr>
<td>
<p>Sine with self</p>
</td>
<td>
<p>-5.03E-16</p>
</td>
<td>
<p>0.95</p>
</td>
<td>
<p>-2.87</p>
</td>
<td>
<p>-3.45</p>
</td>
<td>
<p>-2.57</p>
</td>
<td>
<p>No</p>
</td>
</tr>
<tr>
<td>
<p>Sine versus sine with noise</p>
</td>
<td>
<p>-7.45</p>
</td>
<td>
<p>5.58E-11</p>
</td>
<td>
<p>-2.87</p>
</td>
<td>
<p>-3.45</p>
</td>
<td>
<p>-2.57</p>
</td>
<td>
<p>Yes</p>
</td>
</tr>
<tr>
<td>
<p>Sine versus cosine with noise</p>
</td>
<td>
<p>-17.92</p>
</td>
<td>
<p>2.89E-30</p>
</td>
<td>
<p>-2.87</p>
</td>
<td>
<p>-3.45</p>
</td>
<td>
<p>-2.57</p>
</td>
<td>
<p>Yes</p>
</td>
</tr>
<tr>
<td>
<p>Sine versus sunspots</p>
</td>
<td>
<p>-6.72</p>
</td>
<td>
<p>3.42E-09</p>
</td>
<td>
<p>-2.87</p>
</td>
<td>
<p>-3.45</p>
</td>
<td>
<p>-2.57</p>
</td>
<td>
<p>Yes</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the preceding table, the results are summarized for all four sine waves and their significance level with rejection/acceptance is discussed. Let's now move on to another important topic of the chapter, which is STL decomposition of any time series.</p>
<h1 id="uuid-ba2f5984-4ae9-4bca-a36b-66f181b1d14f">STL decomposition</h1>
<p><strong>STL</strong> stands for <strong>seasonal</strong> <strong>and trend decomposition</strong> <strong>using LOESS</strong>. STL is a time-series decomposition method that can decompose an observed signal into a trend, seasonality, and residual. It can estimate non-linear relationships and handle any type of seasonality. The <kbd>statsmodels.tsa.seasonal</kbd> subpackage offers the <kbd>seasonal_decompose</kbd> method for splitting a given input signal into trend, seasonality, and residual.</p>
<p>Let's see the following example to understand STL decomposition:</p>
<pre># import needful libraries<br/>import pandas as pd<br/>import matplotlib.pyplot as plt<br/>from statsmodels.tsa.seasonal import seasonal_decompose<br/><br/># Read the dataset<br/>data = pd.read_csv('beer_production.csv')<br/>data.columns= ['date','data']<br/><br/># Change datatype to pandas datetime<br/>data['date'] = pd.to_datetime(data['date'])<br/>data=data.set_index('date')<br/><br/># Decompose the data<br/>decomposed_data = seasonal_decompose(data, model='multiplicative')<br/><br/># Plot decomposed data<br/>decomposed_data.plot()<br/><br/># Display the plot<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div><img src="img/046872eb-5130-47a0-ad50-32e5ff6ac47d.png" style=""/></div>
<p>In the preceding code block, the given time-series signal is decomposed into trend, seasonal, and residual components using the <kbd>seasonal_decompose()</kbd> function of the <kbd>statsmodels</kbd> module. Let's now jump to autocorrelation to understand the relationship between a time series and its lagged series.</p>
<h1 id="uuid-31ae3488-dce0-48ab-9705-62ccfe9efe48">Autocorrelation</h1>
<p>Autocorrelation, or lagged correlation, is the correlation between a time series and its lagged series. It indicates the trend in the dataset. The autocorrelation formula can be defined as follows:</p>
<div><img src="img/ec5f82f8-a046-410c-abbf-d0204caee611.png" style=""/></div>
<p>We can calculate the autocorrelation using the NumPy <kbd>correlate()</kbd> function to calculate the actual autocorrelation of sunspot cycles. We can also directly visualize the autocorrelation plot using the <kbd>autocorrelation_plot()</kbd> function. Let's compute the autocorrelation and visualize it:</p>
<pre># import needful libraries<br/>import pandas as pd<br/>import numpy as np<br/>import statsmodels.api as sm<br/>import matplotlib.pyplot as plt<br/><br/># Read the dataset<br/>data = sm.datasets.sunspots.load_pandas().data<br/><br/># Calculate autocorrelation using numpy<br/>dy = data.SUNACTIVITY - np.mean(data.SUNACTIVITY)<br/>dy_square = np.sum(dy ** 2)<br/><br/># Cross-correlation<br/>sun_correlated = np.correlate(dy, dy, mode='full')/dy_square<br/>result = sun_correlated[int(len(sun_correlated)/2):]<br/><br/># Diplay the Chart<br/>plt.plot(result)<br/><br/># Display grid<br/>plt.grid(True)<br/><br/># Add labels<br/>plt.xlabel("Lag")<br/><br/>plt.ylabel("Autocorrelation")<br/># Display the chart<br/>plt.show()</pre>
<p>This results in the following output:</p>
<div><img src="img/d1632364-68b7-4dd0-96d3-2cb448ae27c0.png" style=""/></div>
<p>In the preceding code block, we have seen an autocorrelation example using the NumPy module. Let's compute the autocorrelation plot produced by <kbd>pandas</kbd>:</p>
<pre>from pandas.plotting import autocorrelation_plot<br/><br/># Plot using pandas function<br/>autocorrelation_plot(data.SUNACTIVITY)</pre>
<p>This results in the following output:</p>
<div><img src="img/022ddef4-7af7-4af0-84d0-38d6d7e85a29.png" style=""/></div>
<p>In the preceding code block, we have produced an autocorrelation plot using the <kbd>autocorrelation_plot()</kbd> function of the <kbd>pandas</kbd> library. It is easier to draw the autocorrelation plot using the <kbd>pandas</kbd> library compared to the NumPy library. Let's now jump to autoregressive models for time-series prediction.</p>
<h1 id="uuid-c30ba957-5db8-497a-9b50-8e7756c12dd1">Autoregressive models</h1>
<p>Autoregressive models are time-series models used to predict future incidents. The following formula shows this:</p>
<div><img src="img/b8354b08-b851-4de9-bc55-6ed57d7a43ac.png" style=""/></div>
<p>In the preceding formula, <em>c</em> is a constant and the last term is a random component, also known as white noise.</p>
<p class="mce-root"/>
<p>Let's build the autoregression model using the <kbd>statsmodels.tsa</kbd> subpackage:</p>
<ol>
<li>Import the libraries and read the dataset:</li>
</ol>
<pre style="padding-left: 60px"># import needful libraries<br/>from statsmodels.tsa.ar_model import AR<br/>from sklearn.metrics import mean_absolute_error<br/>from sklearn.metrics import mean_squared_error<br/>import matplotlib.pyplot as plt<br/>import statsmodels.api as sm<br/>from math import sqrt<br/><br/># Read the dataset<br/>data = sm.datasets.sunspots.load_pandas().data</pre>
<ol start="2">
<li>Split the Sunspot data into train and test sets:</li>
</ol>
<pre style="padding-left: 60px"># Split data into train and test set<br/>train_ratio=0.8<br/><br/>train=data[:int(train_ratio*len(data))]<br/>test=data[int(train_ratio*len(data)):]</pre>
<ol start="3">
<li>Train and fit the autoregressive model:</li>
</ol>
<pre style="padding-left: 60px"># AutoRegression Model training<br/>ar_model = AR(train.SUNACTIVITY)<br/>ar_model = ar_model.fit()<br/><br/># print lags and<br/>print("Number of Lags:", ar_model.k_ar)<br/>print("Model Coefficients:\n", ar_model.params)</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px">Number of Lags: 15<br/>Model Coefficients:<br/>const            9.382322<br/>L1.SUNACTIVITY   1.225684<br/>L2.SUNACTIVITY  -0.512193<br/>L3.SUNACTIVITY  -0.130695<br/>L4.SUNACTIVITY   0.193492<br/>L5.SUNACTIVITY  -0.168907<br/>L6.SUNACTIVITY   0.054594<br/>L7.SUNACTIVITY  -0.056725<br/>L8.SUNACTIVITY   0.109404<br/>L9.SUNACTIVITY   0.108993<br/>L10.SUNACTIVITY -0.117063<br/>L11.SUNACTIVITY  0.200454<br/>L12.SUNACTIVITY -0.075111<br/>L13.SUNACTIVITY -0.114437<br/>L14.SUNACTIVITY  0.177516<br/>L15.SUNACTIVITY -0.091978<br/>dtype: float64</pre>
<p style="padding-left: 60px">In the preceding code, we have read the Sunspot dataset and split it into two parts: train and test sets. Then, we built the autoregressive model by creating an instance and fitting a model. Let's make predictions and assess the model's performance.</p>
<ol start="4">
<li>Perform predictions and assess the model:</li>
</ol>
<pre style="padding-left: 60px"># make predictions<br/>start_point = len(train)<br/>end_point = start_point + len(test)-1<br/>pred = ar_model.predict(start=start_point, end=end_point, dynamic=False)<br/><br/># Calculate errors<br/>mae = mean_absolute_error(test.SUNACTIVITY, pred)<br/>mse = mean_squared_error(test.SUNACTIVITY, pred)<br/>rmse = sqrt(mse)<br/>print("MAE:",mae)<br/>print("MSE:",mse)<br/>print("RMSE:",rmse)</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px">MAE: 31.17846098350052<br/>MSE: 1776.9463826165913<br/>RMSE: 42.15384184883498</pre>
<p style="padding-left: 60px">In the preceding code block, we have made the predictions on the test dataset and assessed the model's performance using <strong>Mean Absolute Error</strong> (<strong>MAE</strong>), <strong>Mean Squared Error</strong> (<strong>MSE</strong>), and <strong>Root Mean Squared Error</strong> (<strong>RMSE</strong>). Let's plot the line plot for the original series and prediction series.</p>
<ol start="5">
<li>Let's plot the predicted and original series to understand the forecasting results in a better way:</li>
</ol>
<pre style="padding-left: 60px"># Setting figure size<br/>plt.figure(figsize=(10,6))<br/><br/># Plot test data<br/>plt.plot(test.SUNACTIVITY, label='Original-Series')<br/><br/># Plot predictions<br/>plt.plot(pred, color='red', label='Predicted Series')<br/><br/># Add legends<br/>plt.legend()<br/><br/># Display the plot<br/>plt.show()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/7ee59e6a-6336-42ce-be84-475d25d70eb6.png"/></div>
<p>In the preceding plot, we can see the original series and predicted series using the autoregressive model. After generating the autoregressive model, we need to jump to one more advanced approach for time-series prediction, which is <strong>Autoregressive Moving Average</strong> (<strong>ARMA</strong>).</p>
<h1 id="uuid-906458c8-fc61-43bc-ba67-f02efff48c5a">ARMA models</h1>
<p>The ARMA model blends autoregression and moving averages. The ARMA model is commonly referred to as ARMA(<em>p</em>,<em>q</em>), where <em>p</em> is the order of the autoregressive part, and <em>q</em> is the order of the moving average:</p>
<div><img src="img/46808db6-b1c1-4d1a-90fb-2818aad116fd.png" style=""/></div>
<p>In the preceding formula, just like in the autoregressive model formula, we have a constant and a white noise component; however, we try to fit the lagged noise components as well:</p>
<ol>
<li>Import the libraries and read the dataset:</li>
</ol>
<pre style="padding-left: 60px"># import needful libraries<br/>import statsmodels.api as sm<br/>from statsmodels.tsa.arima_model import ARMA<br/>from sklearn.metrics import mean_absolute_error<br/>from sklearn.metrics import mean_squared_error<br/>import matplotlib.pyplot as plt<br/>from math import sqrt<br/><br/># Read the dataset<br/>data = sm.datasets.sunspots.load_pandas().data<br/>data.drop('YEAR',axis=1,inplace=True)</pre>
<ol start="2">
<li>Split the Sunspot data into train and test sets:</li>
</ol>
<pre style="padding-left: 60px"># Split data into train and test set<br/>train_ratio=0.8<br/>train=data[:int(train_ratio*len(data))]<br/>test=data[int(train_ratio*len(data)):]</pre>
<ol start="3">
<li>Train and fit the autoregressive model:</li>
</ol>
<pre style="padding-left: 60px"># AutoRegression Model training<br/>arma_model = ARMA(train, order=(10,1))<br/>arma_model = arma_model.fit()</pre>
<p class="mce-root"/>
<ol start="4">
<li>Perform predictions and assess the model:</li>
</ol>
<pre style="padding-left: 60px"># make predictions<br/>start_point = len(train)<br/>end_point = start_point + len(test)-1<br/>pred = arma_model.predict(start_point,end_point)<br/><br/># Calculate errors<br/>mae = mean_absolute_error(test.SUNACTIVITY, pred)<br/>mse = mean_squared_error(test.SUNACTIVITY, pred)<br/>rmse = sqrt(mse)<br/>print("MAE:",mae)<br/>print("MSE:",mse)<br/>print("EMSE:",rmse)</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px">MAE: 33.95457845540467<br/>MSE: 2041.3857010355755<br/>EMSE: 45.18169652675268</pre>
<ol start="5">
<li>Let's plot the predicted and original series to understand the forecasting results in a better way:</li>
</ol>
<pre style="padding-left: 60px"># Setting figure size<br/>plt.figure(figsize=(10,6))<br/><br/># Plot test data<br/>plt.plot(test, label='Original-Series')<br/><br/># Plot predictions<br/>plt.plot(pred, color='red', label='Predicted Series')<br/><br/># Add legends<br/>plt.legend()<br/><br/># Display the plot<br/>plt.show()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/cd0b9027-d622-4cff-920c-8a7a04fbfab1.png"/></div>
<p>In the preceding code, we have read the Sunspot dataset and split it into two parts: train and test sets. Then, we built the ARMA model by creating an instance and fitting a model. We made the predictions on the test dataset and assessed the model performance using MAE, MSE, and RMSE. Finally, we saw the line plot for the original series and prediction series. Let's jump to one more important topic, which is generating periodic signals.</p>
<h1 id="uuid-d22565b9-4744-4232-9c6a-6ae6edffd365">Generating periodic signals</h1>
<p>Many natural phenomena are regular and trustworthy, such as an accurate clock. Some phenomena exhibit patterns that seem regular. A group of scientists found three cycles in the sunspot activity with the Hilbert-Huang transform (see <a href="https://en.wikipedia.org/wiki/Hilbert%E2%80%93Huang_transform">https://en.wikipedia.org/wiki/Hilbert%E2%80%93Huang_transform</a>). The cycles have a duration of 11, 22, and 100 years, approximately. Normally, we would simulate a periodic signal using trigonometric functions such as a sine function. You probably remember a bit of trigonometry from high school. That's all we need for this example. Since we have three cycles, it seems reasonable to create a model that is a linear combination of three sine functions. This just requires a tiny adjustment of the code for the autoregressive model:</p>
<ol>
<li>Create model, error, and fit functions:</li>
</ol>
<pre style="padding-left: 60px"># Import required libraries<br/>import numpy as np<br/>import statsmodels.api as sm<br/>from scipy.optimize import leastsq<br/>import matplotlib.pyplot as plt<br/><br/># Create model function<br/>def model(p, t):<br/>    C, p1, f1, phi1 , p2, f2, phi2, p3, f3, phi3 = p<br/>    return C + p1 * np.sin(f1 * t + phi1) + p2 * np.sin(f2 * t + phi2) +p3 * np.sin(f3 * t + phi3)<br/><br/># Create error function<br/>def error(p, y, t):<br/>    return y - model(p, t)<br/><br/># Create fit function<br/>def fit(y, t):<br/>    p0 = [y.mean(), 0, 2 * np.pi/11, 0, 0, 2 * np.pi/22, 0, 0, 2 * np.pi/100, 0]<br/>    params = leastsq(error, p0, args=(y, t))[0]<br/>    return params</pre>
<ol start="2">
<li>Let's load the dataset:</li>
</ol>
<pre style="padding-left: 60px"># Load the dataset<br/>data_loader = sm.datasets.sunspots.load_pandas()<br/>sunspots = data_loader.data["SUNACTIVITY"].values<br/>years = data_loader.data["YEAR"].values</pre>
<ol start="3">
<li>Apply and fit the model:</li>
</ol>
<pre style="padding-left: 60px"># Apply and fit the model<br/>cutoff = int(.9 * len(sunspots))<br/>params = fit(sunspots[:cutoff], years[:cutoff])<br/>print("Params", params)<br/><br/>pred = model(params, years[cutoff:])<br/>actual = sunspots[cutoff:]</pre>
<ol start="4">
<li>Print the results:</li>
</ol>
<pre style="padding-left: 60px">print("Root mean square error", np.sqrt(np.mean((actual - pred) ** 2)))<br/>print("Mean absolute error", np.mean(np.abs(actual - pred)))<br/>print("Mean absolute percentage error", 100 *<br/>np.mean(np.abs(actual - pred)/actual))<br/>mid = (actual + pred)/2<br/>print("Symmetric Mean absolute percentage error", 100 *<br/>       np.mean(np.abs(actual - pred)/mid))<br/>print("Coefficient of determination", 1 - ((actual - pred) <br/>        **2).sum()/ ((actual - actual.mean()) ** 2).sum())</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<pre style="padding-left: 60px"><strong>Params [47.1880006  28.89947462   0.56827279 6.51178464  4.55214564</strong><br/><strong>        0.29372076 -14.30924768 -18.16524123 0.06574835 -4.37789476]</strong><br/><strong>Root mean square error 59.56205597915569</strong><br/><strong>Mean absolute error 44.58158470150657</strong><br/><strong>Mean absolute percentage error 65.16458348768887</strong><br/><strong>Symmetric Mean absolute percentage error 78.4480696873044</strong><br/><strong>Coefficient of determination -0.3635315489903188</strong></pre>
<p style="padding-left: 60px">The first line displays the coefficients of the model we attempted. We have an MAE of 44, which means that we are off by that amount in either direction on average. We also want the coefficient of determination to be as close to 1 as possible to have a good fit. Instead, we get a negative value, which is undesirable. Let's create a graph to understand the results in detail.</p>
<ol start="5">
<li>Plot the original and predicted series:</li>
</ol>
<pre style="padding-left: 60px">year_range = data_loader.data["YEAR"].values[cutoff:]<br/><br/># Plot the actual and predicted data points<br/>plt.plot(year_range, actual, 'o', label="Sunspots")<br/>plt.plot(year_range, pred, 'x', label="Prediction")<br/>plt.grid(True)<br/><br/># Add labels<br/>plt.xlabel("YEAR")<br/>plt.ylabel("SUNACTIVITY")<br/><br/># Add legend<br/>plt.legend()<br/><br/># Display the chart<br/>plt.show()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/d5513638-c82d-4233-9cc0-6d3a00b744a0.png" style=""/></div>
<p>From the preceding graph, we can conclude that the model is not able to capture the actual pattern of the series. This is why we get a negative coefficient of determination or R-squared. Now, we will look at another important technique for time-series analysis, Fourier analysis.</p>
<h1 id="uuid-5251d36f-3c6e-4c39-8928-ad4599c5c0d4">Fourier analysis</h1>
<p>Fourier analysis uses the Fourier series concept thought up by the mathematician Joseph Fourier. The Fourier series is a mathematical method used to represent functions as an infinite series of sine and cosine terms. The functions in question can be real- or complex-valued:</p>
<div><img src="img/09cd1f56-052d-45c9-8953-8386f00c90fc.png" style=""/></div>
<p>For Fourier analysis, the most competent algorithm is <strong>Fast</strong> <strong>Fourier Transform</strong> (<strong>FFT</strong>). FFT decomposes a signal into different frequency signals. This means it produces a frequency spectrum of a given signal. The SciPy and NumPy libraries provide functions for FFT.</p>
<p>The <kbd>rfft()</kbd> function performs FFT on real-valued data. We could also have used the <kbd>fft()</kbd> function, but it gives a warning on this Sunspot dataset. The <kbd>fftshift()</kbd> function moves the zero-frequency component to the middle of the spectrum.</p>
<p>Let's see the following example to understand FFT:</p>
<ol>
<li>Import the libraries and read the dataset:</li>
</ol>
<pre style="padding-left: 60px"># Import required library<br/>import numpy as np<br/>import statsmodels.api as sm<br/>import matplotlib.pyplot as plt<br/>from scipy.fftpack import rfft<br/>from scipy.fftpack import fftshift<br/><br/># Read the dataset<br/>data = sm.datasets.sunspots.load_pandas().data<br/><br/># Create Sine wave<br/>t = np.linspace(-2 * np.pi, 2 * np.pi, len(data.SUNACTIVITY.values))<br/>mid = np.ptp(data.SUNACTIVITY.values)/2<br/>sine = mid + mid * np.sin(np.sin(t))</pre>
<p>                2. Compute the FFT for sine waves and sunspots:</p>
<pre style="padding-left: 60px"># Compute FFT for Sine wave<br/>sine_fft = np.abs(fftshift(rfft(sine)))<br/>print("Index of max sine FFT", np.argsort(sine_fft)[-5:])<br/><br/># Compute FFT for sunspots dataset<br/>transformed = np.abs(fftshift(rfft(data.SUNACTIVITY.values)))<br/>print("Indices of max sunspots FFT", np.argsort(transformed)[-5:])</pre>
<ol start="3">
<li>Create the subplots:</li>
</ol>
<pre style="padding-left: 60px"># Create subplots<br/>fig, axs = plt.subplots(3,figsize=(12,6),sharex=True)<br/>fig.suptitle('Power Specturm')<br/>axs[0].plot(data.SUNACTIVITY.values, label="Sunspots")<br/>axs[0].plot(sine, lw=2, label="Sine")<br/>axs[0].legend() # Set legends<br/>axs[1].plot(transformed, label="Transformed Sunspots")<br/>axs[1].legend() # Set legends<br/>axs[2].plot(sine_fft, lw=2, label="Transformed Sine")<br/>axs[2].legend() # Set legends<br/><br/># Display the chart<br/>plt.show()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/94e62b77-09a9-4614-9f81-fe9a80b38752.png" style=""/></div>
<p>In the preceding code, first, we read the Sunspot dataset and created the sine wave. After that, we computed the FFT for the sine wave and the <kbd>SUNACTIVITY</kbd> column. Finally, we plotted the three graphs for the original series and sine wave and transformed sunspots and sine wave.</p>
<h1 id="uuid-7ab3717d-1721-4937-9286-e0332b1a8e4c">Spectral analysis filtering</h1>
<p>In the previous section, we discussed the amplitude spectrum of the dataset. Now is the time to explore the power spectrum. The power spectrum of any physical signal can display the energy distribution of the signal. We can easily change the code and display the power spectrum by squaring the transformed signal using the following syntax:</p>
<pre>plt.plot(transformed ** 2, label="Power Spectrum")</pre>
<p class="mceNonEditable">We can also plot the phase spectrum using the following Python syntax:</p>
<pre>plt.plot(np.angle(transformed), label="Phase Spectrum")</pre>
<p>Let's see the complete code for the power and phase spectrum for the Sunspot dataset:</p>
<ol>
<li>Import the libraries and read the dataset:</li>
</ol>
<pre style="padding-left: 60px"># Import required library<br/>import numpy as np<br/>import statsmodels.api as sm<br/>from scipy.fftpack import rfft<br/>from scipy.fftpack import fftshift<br/>import matplotlib.pyplot as plt<br/><br/># Read the dataset<br/>data = sm.datasets.sunspots.load_pandas().data</pre>
<ol start="2">
<li>Compute <kbd>FFT</kbd>, <kbd>Spectrum</kbd>, and <kbd>Phase</kbd>:</li>
</ol>
<pre style="padding-left: 60px"># Compute FFT<br/>transformed = fftshift(rfft(data.SUNACTIVITY.values))<br/><br/># Compute Power Spectrum<br/>power=transformed ** 2<br/><br/># Compute Phase<br/>phase=np.angle(transformed)</pre>
<ol start="3">
<li>Create the subplot:</li>
</ol>
<pre style="padding-left: 60px"># Create subplots<br/>fig, axs = plt.subplots(3,figsize=(12,6),sharex=True)<br/>fig.suptitle('Power Specturm')<br/>axs[0].plot(data.SUNACTIVITY.values, label="Sunspots")<br/>axs[0].legend() # Set legends<br/>axs[1].plot(power, label="Power Spectrum")<br/>axs[1].legend() # Set legends<br/>axs[2].plot(phase, label="Phase Spectrum")<br/>axs[2].legend() # Set legends<br/><br/># Display the chart<br/>plt.show()</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<div><img src="img/89e49697-731b-4b91-ad92-092f2664662f.png" style=""/></div>
<p class="mce-root"/>
<p>In the preceding code, first, we read the Sunspot dataset and computed the FFT for the <kbd>SUNACTIVITY</kbd> column. After this, we computed the power and phase spectrum for the transformed FFT. Finally, we plotted the three graphs for the original series and the power and phase spectrums using subplots.</p>
<h1 id="uuid-24b075c4-71c4-4147-a87b-31d74d34d241">Summary</h1>
<p>In this chapter, the time-series examples we used were annual sunspot cycles data, sales data, and beer production. We learned that it's common to try to derive a relationship between a value and another data point or a combination of data points with a fixed number of periods in the past in the same time series. We learned how moving averages convert the random variation trend into a smooth trend using a window size. We learned how the <kbd>DataFrame.rolling()</kbd> function provides <kbd>win_type</kbd> string parameters for different window functions. Cointegration is similar to correlation and is a metric to define the relatedness of two time series. We also focused on STL decomposition, autocorrelation, autoregression, the ARMA model, Fourier analysis, and spectral analysis filtering.</p>
<p>The next chapter, <a href="f3b0dd01-2d5b-41dc-9878-3b7f7eff5e66.xhtml">Chapter 9</a>, <em>Supervised Learning – Regression Analysis</em>, will focus on the important topics of regression analysis and logistic regression in Python. The chapter starts with multiple linear regression, multicollinearity, dummy variables, and model evaluation measures. In the later sections of the chapter, the focus will be on logistic regression.</p>


            

            
        
    </body></html>