["```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3,4,5,6))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at <console>:25\n\nscala> rdd_one.take(100)\nres45: Array[Int] = Array(1, 2, 3, 4, 5, 6)\n\n```", "```py\nscala> val rdd_two = rdd_one.map(i => i * 3)\nrdd_two: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[29] at map at <console>:27\n\nscala> rdd_two.take(10)\nres46: Array[Int] = Array(3, 6, 9, 12, 15, 18)\n\n```", "```py\nscala> val rdd_three = rdd_two.map(i => i+2)\nrdd_three: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[30] at map at <console>:29\n\nscala> rdd_three.take(10)\nres47: Array[Int] = Array(5, 8, 11, 14, 17, 20)\n\n```", "```py\nscala> rdd_one.toDebugString\nres48: String = (8) ParallelCollectionRDD[28] at parallelize at <console>:25 []\n\nscala> rdd_two.toDebugString\nres49: String = (8) MapPartitionsRDD[29] at map at <console>:27 []\n | ParallelCollectionRDD[28] at parallelize at <console>:25 []\n\nscala> rdd_three.toDebugString\nres50: String = (8) MapPartitionsRDD[30] at map at <console>:29 []\n | MapPartitionsRDD[29] at map at <console>:27 []\n | ParallelCollectionRDD[28] at parallelize at <console>:25 []\n\n```", "```py\nscala> val rdd_four = rdd_three.map(i => (\"str\"+(i+2).toString, i-2))\nrdd_four: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[33] at map at <console>:31\n\nscala> rdd_four.take(10)\nres53: Array[(String, Int)] = Array((str7,3), (str10,6), (str13,9), (str16,12), (str19,15), (str22,18))\n\n```", "```py\nscala> val upperCaseRDD = statesPopulationRDD.map(_.toUpperCase)\nupperCaseRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[69] at map at <console>:27\n\nscala> upperCaseRDD.take(10)\nres86: Array[String] = Array(STATE,YEAR,POPULATION, ALABAMA,2010,4785492, ALASKA,2010,714031, ARIZONA,2010,6408312, ARKANSAS,2010,2921995, CALIFORNIA,2010,37332685, COLORADO,2010,5048644, DELAWARE,2010,899816, DISTRICT OF COLUMBIA,2010,605183, FLORIDA,2010,18849098)\n\n```", "```py\nscala> val statesPopulationRDD = sc.textFile(\"statesPopulation.csv\") statesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[47] at textFile at <console>:25\n scala> statesPopulationRDD.first\nres4: String = State,Year,Population\n\nscala> statesPopulationRDD.take(5)\nres5: Array[String] = Array(State,Year,Population, Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995)\n\nscala> val pairRDD = statesPopulationRDD.map(record => (record.split(\",\")(0), record.split(\",\")(2)))\npairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[48] at map at <console>:27\n\nscala> pairRDD.take(10)\nres59: Array[(String, String)] = Array((Alabama,4785492), (Alaska,714031), (Arizona,6408312), (Arkansas,2921995), (California,37332685), (Colorado,5048644), (Delaware,899816), (District of Columbia,605183), (Florida,18849098))\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1.0,2.0,3.0))\nrdd_one: org.apache.spark.rdd.RDD[Double] = ParallelCollectionRDD[52] at parallelize at <console>:25\n\nscala> rdd_one.mean\nres62: Double = 2.0\n\nscala> rdd_one.min\nres63: Double = 1.0\n\nscala> rdd_one.max\nres64: Double = 3.0\n\nscala> rdd_one.stdev\nres65: Double = 0.816496580927726\n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => (record.split(\",\")(0), record.split(\",\")(2)))\npairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[60] at map at <console>:27\n\nscala> pairRDD.saveAsSequenceFile(\"seqfile\")\n\nscala> val seqRDD = sc.sequenceFile[String, String](\"seqfile\")\nseqRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[62] at sequenceFile at <console>:25\n\nscala> seqRDD.take(10)\nres76: Array[(String, String)] = Array((State,Population), (Alabama,4785492), (Alaska,714031), (Arizona,6408312), (Arkansas,2921995), (California,37332685), (Colorado,5048644), (Delaware,899816), (District of Columbia,605183), (Florida,18849098))\n\n```", "```py\nclass CoGroupedRDD[K] extends RDD[(K, Array[Iterable[_]])] \n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => (record.split(\",\")(0), record.split(\",\")(2)))\npairRDD: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[60] at map at <console>:27\n\nscala> val pairRDD2 = statesPopulationRDD.map(record => (record.split(\",\")(0), record.split(\",\")(1)))\npairRDD2: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[66] at map at <console>:27\n\nscala> val cogroupRDD = pairRDD.cogroup(pairRDD2)\ncogroupRDD: org.apache.spark.rdd.RDD[(String, (Iterable[String], Iterable[String]))] = MapPartitionsRDD[68] at cogroup at <console>:31\n\nscala> cogroupRDD.take(10)\nres82: Array[(String, (Iterable[String], Iterable[String]))] = Array((Montana,(CompactBuffer(990641, 997821, 1005196, 1014314, 1022867, 1032073, 1042520),CompactBuffer(2010, 2011, 2012, 2013, 2014, 2015, 2016))), (California,(CompactBuffer(37332685, 37676861, 38011074, 38335203, 38680810, 38993940, 39250017),CompactBuffer(2010, 2011, 2012, 2013, 2014, 2015, 2016))),\n\n```", "```py\nclass ShuffledRDD[K, V, C] extends RDD[(K, C)] \n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => (record.split(\",\")(0), 1))\npairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[82] at map at <console>:27\n\nscala> pairRDD.take(5)\nres101: Array[(String, Int)] = Array((State,1), (Alabama,1), (Alaska,1), (Arizona,1), (Arkansas,1))\n\nscala> val shuffledRDD = pairRDD.reduceByKey(_+_)\nshuffledRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[83] at reduceByKey at <console>:29\n\nscala> shuffledRDD.take(5)\nres102: Array[(String, Int)] = Array((Montana,7), (California,7), (Washington,7), (Massachusetts,7), (Kentucky,7))\n\n```", "```py\nclass UnionRDD[T: ClassTag]( sc: SparkContext, var rdds: Seq[RDD[T]]) extends RDD[T](sc, Nil)\n\nUnionRDD by combining the elements of the two RDDs:\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[85] at parallelize at <console>:25\n\nscala> val rdd_two = sc.parallelize(Seq(4,5,6))\nrdd_two: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[86] at parallelize at <console>:25\n\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[87] at parallelize at <console>:25\n\nscala> rdd_one.take(10)\nres103: Array[Int] = Array(1, 2, 3)\n\nscala> val rdd_two = sc.parallelize(Seq(4,5,6))\nrdd_two: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[88] at parallelize at <console>:25\n\nscala> rdd_two.take(10)\nres104: Array[Int] = Array(4, 5, 6)\n\nscala> val unionRDD = rdd_one.union(rdd_two)\nunionRDD: org.apache.spark.rdd.RDD[Int] = UnionRDD[89] at union at <console>:29\n\nscala> unionRDD.take(10)\nres105: Array[Int] = Array(1, 2, 3, 4, 5, 6)\n\n```", "```py\nclass HadoopRDD[K, V] extends RDD[(K, V)]\n\n```", "```py\nscala> val statesPopulationRDD = sc.textFile(\"statesPopulation.csv\")\nstatesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[93] at textFile at <console>:25\n\nscala> statesPopulationRDD.toDebugString\nres110: String =\n(2) statesPopulation.csv MapPartitionsRDD[93] at textFile at <console>:25 []\n | statesPopulation.csv HadoopRDD[92] at textFile at <console>:25 []\n\n```", "```py\nclass NewHadoopRDD[K, V](\n sc : SparkContext,\n inputFormatClass: Class[_ <: InputFormat[K, V]],\n keyClass: Class[K],\n valueClass: Class[V],\n @transient private val _conf: Configuration)\nextends RDD[(K, V)]\n\nNewHadoopRDD takes an input format class, a key class, and a value class. Let's look at examples of NewHadoopRDD.\n```", "```py\nscala> val rdd_whole = sc.wholeTextFiles(\"wiki1.txt\")\nrdd_whole: org.apache.spark.rdd.RDD[(String, String)] = wiki1.txt MapPartitionsRDD[3] at wholeTextFiles at <console>:31\n\nscala> rdd_whole.toDebugString\nres9: String =\n(1) wiki1.txt MapPartitionsRDD[3] at wholeTextFiles at <console>:31 []\n | WholeTextFileRDD[2] at wholeTextFiles at <console>:31 []\n\n```", "```py\nimport org.apache.hadoop.mapreduce.lib.input.KeyValueTextInputFormat\n\nimport org.apache.hadoop.io.Text\n\nval newHadoopRDD = sc.newAPIHadoopFile(\"statesPopulation.csv\", classOf[KeyValueTextInputFormat], classOf[Text],classOf[Text])\n\n```", "```py\nscala> val statesPopulationRDD = sc.textFile(\"statesPopulation.csv\")\nstatesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[157] at textFile at <console>:26\n\nscala> statesPopulationRDD.take(5)\nres226: Array[String] = Array(State,Year,Population, Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995)\n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => record.split(\",\")).map(t => (t(0), (t(1), t(2))))\npairRDD: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[160] at map at <console>:28\n\nscala> pairRDD.take(5)\nres228: Array[(String, (String, String))] = Array((State,(Year,Population)), (Alabama,(2010,4785492)), (Alaska,(2010,714031)), (Arizona,(2010,6408312)), (Arkansas,(2010,2921995)))\n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => record.split(\",\")).map(t => (t(1), (t(0), t(2))))\npairRDD: org.apache.spark.rdd.RDD[(String, (String, String))] = MapPartitionsRDD[162] at map at <console>:28\n\nscala> pairRDD.take(5)\nres229: Array[(String, (String, String))] = Array((Year,(State,Population)), (2010,(Alabama,4785492)), (2010,(Alaska,714031)), (2010,(Arizona,6408312)), (2010,(Arkansas,2921995)))\n\n```", "```py\ndef groupByKey(partitioner: Partitioner): RDD[(K, Iterable[V])] \n\ndef groupByKey(numPartitions: Int): RDD[(K, Iterable[V])] \n\n```", "```py\ndef reduceByKey(partitioner: Partitioner, func: (V, V) => V): RDD[(K, V)]\n\ndef reduceByKey(func: (V, V) => V, numPartitions: Int): RDD[(K, V)] \n\ndef reduceByKey(func: (V, V) => V): RDD[(K, V)] \n\n```", "```py\ndef aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) => U,\n combOp: (U, U) => U): RDD[(K, U)] \n\ndef aggregateByKey[U: ClassTag](zeroValue: U, numPartitions: Int)(seqOp: (U, V) => U,\n combOp: (U, U) => U): RDD[(K, U)] \n\ndef aggregateByKey[U: ClassTag](zeroValue: U)(seqOp: (U, V) => U,\n combOp: (U, U) => U): RDD[(K, U)] \n\n```", "```py\ndef combineByKey[C](createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, numPartitions: Int): RDD[(K, C)]\n\ndef combineByKey[C](createCombiner: V => C, mergeValue: (C, V) => C, mergeCombiners: (C, C) => C, partitioner: Partitioner, mapSideCombine: Boolean = true, serializer: Serializer = null): RDD[(K, C)]\n\n```", "```py\nscala> val statesPopulationRDD = sc.textFile(\"statesPopulation.csv\").filter(_.split(\",\")(0) != \"State\") \nstatesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[1] at textFile at <console>:24\n\nscala> statesPopulationRDD.take(10)\nres27: Array[String] = Array(Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995, California,2010,37332685, Colorado,2010,5048644, Delaware,2010,899816, District of Columbia,2010,605183, Florida,2010,18849098, Georgia,2010,9713521)\n\n```", "```py\nscala> val pairRDD = statesPopulationRDD.map(record => record.split(\",\")).map(t => (t(0), (t(1).toInt, t(2).toInt)))\npairRDD: org.apache.spark.rdd.RDD[(String, (Int, Int))] = MapPartitionsRDD[26] at map at <console>:26\n\nscala> pairRDD.take(10)\nres15: Array[(String, (Int, Int))] = Array((Alabama,(2010,4785492)), (Alaska,(2010,714031)), (Arizona,(2010,6408312)), (Arkansas,(2010,2921995)), (California,(2010,37332685)), (Colorado,(2010,5048644)), (Delaware,(2010,899816)), (District of Columbia,(2010,605183)), (Florida,(2010,18849098)), (Georgia,(2010,9713521)))\n\n```", "```py\nscala> val groupedRDD = pairRDD.groupByKey.map(x => {var sum=0; x._2.foreach(sum += _._2); (x._1, sum)})\ngroupedRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[38] at map at <console>:28\n\nscala> groupedRDD.take(10)\nres19: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))\n\n```", "```py\n\nscala> val reduceRDD = pairRDD.reduceByKey((x, y) => (x._1, x._2+y._2)).map(x => (x._1, x._2._2))\nreduceRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[46] at map at <console>:28\n\nscala> reduceRDD.take(10)\nres26: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))\n\n```", "```py\nInitialize the array\nscala> val initialSet = 0\ninitialSet: Int = 0\n\nprovide function to add the populations within a partition\nscala> val addToSet = (s: Int, v: (Int, Int)) => s+ v._2\naddToSet: (Int, (Int, Int)) => Int = <function2>\n\nprovide funtion to add populations between partitions\nscala> val mergePartitionSets = (p1: Int, p2: Int) => p1 + p2\nmergePartitionSets: (Int, Int) => Int = <function2>\n\nscala> val aggregatedRDD = pairRDD.aggregateByKey(initialSet)(addToSet, mergePartitionSets)\naggregatedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[41] at aggregateByKey at <console>:34\n\nscala> aggregatedRDD.take(10)\nres24: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))\n\n```", "```py\ncreatecombiner function\nscala> val createCombiner = (x:(Int,Int)) => x._2\ncreateCombiner: ((Int, Int)) => Int = <function1>\n\nfunction to add within partition\nscala> val mergeValues = (c:Int, x:(Int, Int)) => c +x._2\nmergeValues: (Int, (Int, Int)) => Int = <function2>\n\nfunction to merge combiners\nscala> val mergeCombiners = (c1:Int, c2:Int) => c1 + c2\nmergeCombiners: (Int, Int) => Int = <function2>\n\nscala> val combinedRDD = pairRDD.combineByKey(createCombiner, mergeValues, mergeCombiners)\ncombinedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[42] at combineByKey at <console>:34\n\nscala> combinedRDD.take(10)\nres25: Array[(String, Int)] = Array((Montana,7105432), (California,268280590), (Washington,48931464), (Massachusetts,46888171), (Kentucky,30777934), (Pennsylvania,89376524), (Georgia,70021737), (Tennessee,45494345), (North Carolina,68914016), (Utah,20333580))\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[120] at parallelize at <console>:25\n\nscala> rdd_one.getNumPartitions\nres202: Int = 8\n\n```", "```py\npartitionIndex = hashcode(key) % numPartitions\n\n```", "```py\nscala> val str = \"hello\"\nstr: String = hello\n\nscala> str.hashCode\nres206: Int = 99162322\n\nscala> val numPartitions = 8\nnumPartitions: Int = 8\n\nscala> val partitionIndex = str.hashCode % numPartitions\npartitionIndex: Int = 2\n\n```", "```py\nimport org.apache.spark.RangePartitioner\nscala> val statesPopulationRDD = sc.textFile(\"statesPopulation.csv\")\nstatesPopulationRDD: org.apache.spark.rdd.RDD[String] = statesPopulation.csv MapPartitionsRDD[135] at textFile at <console>:26\n\nscala> val pairRDD = statesPopulationRDD.map(record => (record.split(\",\")(0), 1))\npairRDD: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[136] at map at <console>:28\n\nscala> val rangePartitioner = new RangePartitioner(5, pairRDD)\nrangePartitioner: org.apache.spark.RangePartitioner[String,Int] = org.apache.spark.RangePartitioner@c0839f25\n\nscala> val rangePartitionedRDD = pairRDD.partitionBy(rangePartitioner)\nrangePartitionedRDD: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[130] at partitionBy at <console>:32\n\nscala> pairRDD.mapPartitionsWithIndex((i,x) => Iterator(\"\"+i + \":\"+x.length)).take(10)\nres215: Array[String] = Array(0:177, 1:174)\n\nscala> rangePartitionedRDD.mapPartitionsWithIndex((i,x) => Iterator(\"\"+i + \":\"+x.length)).take(10)\nres216: Array[String] = Array(0:70, 1:77, 2:70, 3:63, 4:71)\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at <console>:25\n\nscala> val i = 5\ni: Int = 5\n\nscala> val bi = sc.broadcast(i)\nbi: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(147)\n\nscala> bi.value\nres166: Int = 5\n\nscala> rdd_one.take(5)\nres164: Array[Int] = Array(1, 2, 3)\n\nscala> rdd_one.map(j => j + bi.value).take(5)\nres165: Array[Int] = Array(6, 7, 8)\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[109] at parallelize at <console>:25\n\nscala> val m = scala.collection.mutable.HashMap(1 -> 2, 2 -> 3, 3 -> 4)\nm: scala.collection.mutable.HashMap[Int,Int] = Map(2 -> 3, 1 -> 2, 3 -> 4)\n\nscala> val bm = sc.broadcast(m)\nbm: org.apache.spark.broadcast.Broadcast[scala.collection.mutable.HashMap[Int,Int]] = Broadcast(178)\n\nscala> rdd_one.map(j => j * bm.value(j)).take(5)\nres191: Array[Int] = Array(2, 6, 12)\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at <console>:25\n\nscala> val k = 5\nk: Int = 5\n\nscala> val bk = sc.broadcast(k)\nbk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)\n\nscala> rdd_one.map(j => j + bk.value).take(5)\nres184: Array[Int] = Array(6, 7, 8)\n\nscala> bk.unpersist\n\nscala> rdd_one.map(j => j + bk.value).take(5)\nres186: Array[Int] = Array(6, 7, 8)\n\n```", "```py\nscala> val rdd_one = sc.parallelize(Seq(1,2,3))\nrdd_one: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[101] at parallelize at <console>:25\n\nscala> val k = 5\nk: Int = 5\n\nscala> val bk = sc.broadcast(k)\nbk: org.apache.spark.broadcast.Broadcast[Int] = Broadcast(163)\n\nscala> rdd_one.map(j => j + bk.value).take(5)\nres184: Array[Int] = Array(6, 7, 8)\n\nscala> bk.destroy\n\n```", "```py\nscala> rdd_one.map(j => j + bk.value).take(5)\n17/05/27 14:07:28 ERROR Utils: Exception encountered\norg.apache.spark.SparkException: Attempted to use Broadcast(163) after it was destroyed (destroy at <console>:30)\n at org.apache.spark.broadcast.Broadcast.assertValid(Broadcast.scala:144)\n at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$writeObject$1.apply$mcV$sp(TorrentBroadcast.scala:202)\n at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$wri\n\n```", "```py\nscala> val acc1 = sc.longAccumulator(\"acc1\")\nacc1: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355, name: Some(acc1), value: 0)\n\nscala> val someRDD = statesPopulationRDD.map(x => {acc1.add(1); x})\nsomeRDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[99] at map at <console>:29\n\nscala> acc1.value\nres156: Long = 0  /*there has been no action on the RDD so accumulator did not get incremented*/\n\nscala> someRDD.count\nres157: Long = 351\n\nscala> acc1.value\nres158: Long = 351\n\nscala> acc1\nres145: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 10355, name: Some(acc1), value: 351)\n\n```", "```py\nclass MyAccumulator extends AccumulatorV2[Int, Int] {\n  //simple boolean check\n override def isZero: Boolean = ??? //function to copy one Accumulator and create another one override def copy(): AccumulatorV2[Int, Int] = ??? //to reset the value override def reset(): Unit = ??? //function to add a value to the accumulator override def add(v: Int): Unit = ??? //logic to merge two accumulators override def merge(other: AccumulatorV2[Int, Int]): Unit = ??? //the function which returns the value of the accumulator override def value: Int = ???\n}\n\n```", "```py\nimport org.apache.spark.util.AccumulatorV2\n\n```", "```py\ncase class YearPopulation(year: Int, population: Long)\n\n```", "```py\n\nclass StateAccumulator extends AccumulatorV2[YearPopulation, YearPopulation] { \n      //declare the two variables one Int for year and Long for population\n      private var year = 0 \n private var population:Long = 0L\n\n      //return iszero if year and population are zero\n      override def isZero: Boolean = year == 0 && population == 0L\n\n      //copy accumulator and return a new accumulator\n     override def copy(): StateAccumulator = { \n val newAcc = new StateAccumulator \n newAcc.year =     this.year \n newAcc.population = this.population \n newAcc \n }\n\n       //reset the year and population to zero \n       override def reset(): Unit = { year = 0 ; population = 0L }\n\n       //add a value to the accumulator\n       override def add(v: YearPopulation): Unit = { \n year += v.year \n population += v.population \n }\n\n       //merge two accumulators\n      override def merge(other: AccumulatorV2[YearPopulation, YearPopulation]): Unit = { \n other match { \n case o: StateAccumulator => { \n year += o.year \n population += o.population \n } \n case _ => \n } \n }\n\n       //function called by Spark to access the value of accumulator\n       override def value: YearPopulation = YearPopulation(year, population)\n}\n\n```", "```py\nval statePopAcc = new StateAccumulator\n\nsc.register(statePopAcc, \"statePopAcc\")\n\n```", "```py\n\nval statesPopulationRDD = sc.textFile(\"statesPopulation.csv\").filter(_.split(\",\")(0) != \"State\")\n\nscala> statesPopulationRDD.take(10)\nres1: Array[String] = Array(Alabama,2010,4785492, Alaska,2010,714031, Arizona,2010,6408312, Arkansas,2010,2921995, California,2010,37332685, Colorado,2010,5048644, Delaware,2010,899816, District of Columbia,2010,605183, Florida,2010,18849098, Georgia,2010,9713521)\n\n```", "```py\nstatesPopulationRDD.map(x => { \n val toks = x.split(\",\") \n val year = toks(1).toInt \n val pop = toks(2).toLong \n statePopAcc.add(YearPopulation(year, pop)) \n x\n}).count\n\n```", "```py\nscala> statePopAcc\nres2: StateAccumulator = StateAccumulator(id: 0, name: Some(statePopAcc), value: YearPopulation(704550,2188669780))\n\n```"]