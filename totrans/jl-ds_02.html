<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch02" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 2. Data Munging</h1></div></div></div><p class="calibre11">It is said that around 50% of the data scientist's time goes into transforming raw data into a usable format. Raw data can be in any format or size. It can be structured like RDBMS, semi-structured like CSV, or unstructured like regular text files. These contain some valuable information. And to extract that information, it has to be converted into a data structure or a usable format from which an algorithm can find valuable insights. Therefore, usable format refers to the data in a model that can be consumed in the data science process. This usable format differs from use case to use case.</p><p class="calibre11">This chapter will guide you through data munging, or the process of preparing the data. It covers the following topics:</p><div><ul class="itemizedlist"><li class="listitem">What is data munging?</li><li class="listitem">DataFrames.jl</li><li class="listitem">Uploading data from a file</li><li class="listitem">Finding the required data</li><li class="listitem">Joins and indexing</li><li class="listitem">Split-Apply-Combine strategy</li><li class="listitem">Reshaping the data</li><li class="listitem">Formula (ModelFrame and ModelMatrix)</li><li class="listitem">PooledDataArray</li><li class="listitem">Web scraping</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec17" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is data munging?</h1></div></div></div><p class="calibre11">Munging comes from the term "munge," which was coined by some students of Massachusetts Institute of Technology, USA. It is considered one of the most essential parts of the data science process; it involves collecting, aggregating, cleaning, and organizing the data to be consumed by the algorithms designed to make discoveries or to create models. This involves numerous steps, including extracting data from the data source and then parsing or transforming the data into a predefined data structure. Data munging is also referred to as data wrangling.</p><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec19" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The data munging process</h2></div></div></div><p class="calibre11">So what's the data munging process? As mentioned, data can be in any format and the data science process may require data from multiple sources. This data aggregation phase includes scraping it from websites, downloading thousands of <code class="literal">.txt</code> or <code class="literal">.log</code> files, or gathering the data from RDBMS or NoSQL data stores.</p><p class="calibre11">It is very rare to find data in a format that can be used directly by the data science process. The data received is generally in a format unsuitable for modeling and analysis. Generally, algorithms require data to be stored in a tabular format or in matrices. This phase of converting the gathered raw data into the required format can get very complex and time consuming. But this phase creates the foundation of the sophisticated data analysis that can now be done.</p><p class="calibre11">It is good to define the structure of the data that you will be feeding the algorithms in advance. This data structure is defined according to the nature of the problem. The algorithms that you have designed or will be designing should not just be able to accept this format of data, but they should also be able to easily identify the patterns, find the outliers, make discoveries, or meet whatever the desired outcomes are.</p><p class="calibre11">After defining how the data will be structured, you define the process to achieve that. This is like a pipeline that will accept some forms of data and will give out meaningful data in a predefined format. This phase consists of various steps. These steps include converting data from one form to another, which may or may not require string operations or regular expressions, and finding the missing values and outliers.</p><p class="calibre11">Generally, data science problems revolve around two kinds of data. These two kinds of data will be either categorical or numerical. Categorical data comes with labels. These labels are formed by some group of values. For example, we can treat weather with categorical features. Weather can be sunny, cloudy, rainy, foggy, or snowy. These labels are formed when the underlying values are associated with one of the groups of the data (which comes under a label). These labels have some unique characteristics and we may not be able to apply arithmetic operations on them.</p><p class="calibre11">Numerical data is much more common, for example, temperature. Temperature will be in floating-point numbers and we can certainly apply mathematical operations on it. Every value is comparable with other values in the dataset, so we can say that they have a direct relation with each other.</p></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec18" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is a DataFrame?</h1></div></div></div><p class="calibre11">A DataFrame is a data structure that has labeled columns, which individually may have different data types. Like a SQL table or a spreadsheet, it has two dimensions. It can also be thought of as a list of dictionaries, but fundamentally, it is different.</p><p class="calibre11">DataFrames are the recommended data structure for statistical analysis. Julia provides a package called <code class="literal">DataFrames.jl</code> , which have all necessary functions to work with DataFrames.</p><p class="calibre11">Julia's package, DataFrames, provides three data types:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">NA</code>: A missing value in Julia is represented by a specific data type, <code class="literal">NA.</code></li><li class="listitem"><code class="literal">DataArray</code>: The array type defined in the standard Julia library, though it has many features, doesn't provide any specific functionalities for data analysis. DataArray provided in <code class="literal">DataFrames.jl</code> provides such features (for example, if we required to store in an array some missing values).</li><li class="listitem"><code class="literal">DataFrame</code>: DataFrame is 2-D data structure, like spreadsheets. It is much like R or pandas's DataFrames, and provides many functionalities to represent and analyze data.</li></ul></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec20" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The NA data type and its importance</h2></div></div></div><p class="calibre11">In the real world, we come across data with missing values. It is very common but it's not provided in Julia by default. This functionality is added using the <code class="literal">DataFrames.jl</code> package. The DataFrames package brings with it DataArray packages, which provide NA data type. Multiple dispatch is one of the most powerful features of Julia and NA is one such example. Julia has NA type, which provides the singleton object NA that we are using to represent missing values.</p><p class="calibre11">Why is the NA data type needed?</p><p class="calibre11">Suppose, for example, we have a dataset having floating-point numbers:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; x = [1.1, 2.2, 3.3, 4.4, 5.5, 6.6]</strong>
</pre><p class="calibre11">This will create a six-element <code class="literal">Array{Float64,1}</code>.</p><p class="calibre11">Now, suppose this dataset has a missing value at position [1]. That means instead of 1.1, there is no value. This cannot be represented by the array type in Julia. When we try to assign an NA value, we get this error:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; x[1] = NA 
LoadError: UndefVarError: NA not defined 
while loading In[2], in expression starting on line 1 
</strong>
</pre><p class="calibre11">Therefore, right now we cannot add <code class="literal">NA</code> values to the array that we have created.</p><p class="calibre11">So, to load the data into an array that does have <code class="literal">NA</code> values, we use <code class="literal">DataArray.</code> This enables us to have NA values in our dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using DataArrays 
julia&gt; x = DataArray([1.1, 2.2, 3.3, 4.4, 5.5, 6.6]) 
</strong>
</pre><p class="calibre11">This will create a six-element <code class="literal">DataArrays.DataArray{Float64,1}</code>.</p><p class="calibre11">So, when we try to have an <code class="literal">NA</code> value, it gives us:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; X[1] = NA 
NA 
julia&gt; x 
6-element DataArrays.DataArray{Float64,1}: 
 1.1 
 2.2 
 3.3 
 4.4 
 5.5 
 6.6 
</strong>
</pre><p class="calibre11">Therefore, by using DataArrays, we can handle missing data. One more feature provided is that NA doesn't always affect functions applied on the particular dataset. So, the method that doesn't involve an NA value or is not affected by it can be applied on the dataset. If it does involve the NA value, then it will give NA as the result.</p><p class="calibre11">In the following example, we are applying the mean function and <code class="literal">true || x</code>. The mean function doesn't work as it involves an NA value, but <code class="literal">true || x</code> works as expected:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; true || x 
True 
 
julia&gt; true &amp;&amp; x[1] 
NA 
 
julia&gt; mean(x) 
NA 
 
julia&gt; mean(x[2:6]) 
4.4 
</strong>
</pre></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec21" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>DataArray – a series-like data structure</h2></div></div></div><p class="calibre11">In the previous section, we discussed how DataArrays are used to store datasets containing missing (NA) values, as Julia's standard Array type cannot do so.</p><p class="calibre11">There are other features similar to Julia's Array type. Type aliases of Vector (one-dimensional Array type) and Matrix (two-dimensional Array type) are DataVector and DataMatrix provided by DataArray.</p><p class="calibre11">Creating a 1-D DataArray is similar to creating an Array:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using DataArrays</strong>
<strong class="calibre19">julia&gt; dvector = data([10,20,30,40,50])</strong>
<strong class="calibre19">5-element DataArrays.DataArray{Int64,1}:</strong>
<strong class="calibre19">10</strong>
<strong class="calibre19">20</strong>
<strong class="calibre19">30</strong>
<strong class="calibre19">40</strong>
<strong class="calibre19">50</strong>
</pre><p class="calibre11">Here, we have NA values, unlike in Arrays. Similarly, we can create a 2-D DataArray, which will be a DataMatrix:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; dmatrix = data([10 20 30; 40 50 60])</strong>
<strong class="calibre19">2x3 DataArrays.DataArray{Int64,2}:</strong>
<strong class="calibre19">10 20 30</strong>
<strong class="calibre19">40 50 60</strong>
<strong class="calibre19">julia&gt; dmatrix[2,3]</strong>
<strong class="calibre19">60</strong>
</pre><p class="calibre11">In the previous example, to the calculate mean, we used slicing. This is not a convenient method to remove or not to consider the NA values while applying a function. A much better way is to use <code class="literal">dropna</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; dropna(x)</strong>
<strong class="calibre19">5-element Array{Float64,1}:</strong>
<strong class="calibre19">2.2</strong>
<strong class="calibre19">3.3</strong>
<strong class="calibre19">4.4</strong>
<strong class="calibre19">5.5</strong>
<strong class="calibre19">6.6</strong>
</pre></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec22" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>DataFrames – tabular data structures</h2></div></div></div><p class="calibre11">Arguably, this is the most important and commonly used data type in statistical computing, whether it is in R (data.frame) or Python (Pandas). This is due to the fact that all the real-world data is mostly in tabular or spreadsheet-like format. This cannot be represented by a simple DataArray:</p><pre class="programlisting">julia&gt; df = DataFrame(Name = ["Ajava Rhodiumhi", "Las Hushjoin"],
            Count = [14.04, 17.3],
            OS = ["Ubuntu", "Mint"])</pre><p class="calibre11">
</p><div><img src="img/B05321_02_01.jpg" alt="DataFrames – tabular data structures" class="calibre41"/></div><p class="calibre11">
</p><p class="calibre11">This dataset, for example, can't be represented using DataArray. The given dataset has the following features because it cannot be represented by DataArray:</p><div><ul class="itemizedlist"><li class="listitem">This dataset has different types of data in different columns. These different data types in different columns cannot be represented using a matrix. Matrix can only contain values of one type.</li><li class="listitem">It is a tabular data structure and records have relations with other records in the same row of different columns. Therefore, it is a must that all the columns are of the same length. Vectors cannot be used because same-length columns cannot be enforced using them. Therefore, a column in DataFrame is represented by DataArray.</li><li class="listitem">In the preceding example, we can see that the columns are labeled. This labeling helps us to easily become familiar with the data and access it without the need to remember its exact positions. So, the columns are accessible using numerical indices and also by their label.</li></ul></div><p class="calibre11">Therefore, due to these reasons, the DataFrames package is used. So, DataFrames are used to represent tabular data having DataArrays as columns.</p><p class="calibre11">In the given example, we constructed a DataFrame by:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; df = DataFrame(Name = ["Ajava Rhodiumhi", "Las Hushjoin"], 
Count = [14.04, 17.3], 
OS = ["Ubuntu", "Mint"]) 
</strong>
</pre><p class="calibre11">Using the keyword arguments, column names can be defined.</p><p class="calibre11">Let's take another example by constructing a new DataFrame:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; df2 = DataFrame() 
 
julia&gt; df2[:X] = 1:10 
 
julia&gt; df2[:Y] = ["Head", "Tail", 
"Head", "Head", 
"Tail", "Head", 
"Tail", "Tail", 
"Head", "Tail"] 
julia&gt; df2 
</strong>
</pre><p class="calibre11">To find out the size of the DataFrame created, we use the size function:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; size(df2) 
(10, 2) 
</strong>
</pre><p class="calibre11">Here, <code class="literal">10</code> refers to the number of rows and <code class="literal">2</code> refers to the number of columns.</p><p class="calibre11">To view the first few lines of the dataset, we use <code class="literal">head()</code>, and for the last few lines, we use the <code class="literal">tail()</code> function:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; head(df2) 
</strong>
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_02.jpg" alt="DataFrames – tabular data structures" class="calibre42"/></div><p class="calibre11">
</p><p class="calibre11">As we have given names to the columns of the DataFrame, these can be accessed using these names.</p><p class="calibre11">For example:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; df2[:X] 
10-element DataArrays.DataArray{Int64,1}: 
 1 
 2 
 3 
 4 
 5 
 6 
... 
</strong>
</pre><p class="calibre11">This simplifies access to the columns as we can give meaningful names to real-world datasets that have numerous columns without the need to remember their numeric indices.</p><p class="calibre11">If needed, we can also rename using these columns by using the rename function:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; rename!(df2, :X,  :newX) 
</strong>
</pre><p class="calibre11">If there is a need to rename multiple columns, then it is done by using this:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; rename!(df2, {:X =&gt; :newX, :Y =&gt; :newY}) 
</strong>
</pre><p class="calibre11">But right now, we are sticking to old column names for ease of use.</p><p class="calibre11">Julia also provides a function called <code class="literal">describe()</code>, which summarizes the entire dataset. For a dataset with many columns, it can turn out to be very useful:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; describe(df2) X
Min 1.0
1st Qu. 3.25
Median 5.5
Mean 5.5
3rd Qu. 7.75
Max 10.0
NAs 0
NA% 0.0%

Y
Length 10
Type ASCIIString
NAs 0
NA% 0.0%
Unique 2</strong>
</pre></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec23" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Installation and using DataFrames.jl</h2></div></div></div><p class="calibre11">Installation is quite straightforward as it is a registered Julia package:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; Pkg.update() 
julia&gt; Pkg.add("DataFrames") 
</strong>
</pre><p class="calibre11">This adds all the required packages to the current namespace. To use the <code class="literal">DataFrames</code> package:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using DataFrames 
</strong>
</pre><p class="calibre11">It is also good to have classical datasets that are common for learning purposes. These datasets can be found in the <code class="literal">RDatasets</code> package:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; Pkg.add("RDatasets") 
</strong>
</pre><p class="calibre11">The list of the R packages available can be found using:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Rdatasets.packages() 
</strong>
</pre><p class="calibre11">Here, you can see this:</p><pre class="programlisting">
<strong class="calibre19">datasets - The R Datasets Package 
</strong>
</pre><p class="calibre11">It contains datasets available to R. To use this <code class="literal">dataset</code>, simply use the following:</p><pre class="programlisting">
<strong class="calibre19">using RDatasets 
iris_dataset = dataset("datasets", "iris") 
</strong>
</pre><p class="calibre11">Here, dataset is the function that takes two arguments.</p><p class="calibre11">The first argument is the name of the package and the second is the name of the dataset that we want to load.</p><p class="calibre11">In the following example, we loaded the famous iris dataset into the memory. You can see that the <code class="literal">dataset()</code> function has returned a DataFrame. The dataset contains five columns: <code class="literal">SepalLength</code>, <code class="literal">SepalWidth</code>, <code class="literal">PetalLength</code>, <code class="literal">PetalWidth</code>, and <code class="literal">Species</code>. It is quite easy to understand the data. A large number of samples have been taken for every species, and the length and width of sepal and petal have been measured, which can be used later to distinguish between them:</p><p class="calibre11">
</p><div><img src="img/B05321_02_03.jpg" alt="Installation and using DataFrames.jl" class="calibre43"/></div><p class="calibre11">
</p><p class="calibre11">Actual data science problems generally do not deal with the artificial randomly generated data or data read through the command line. But they work on data that is loaded from files or any other external source. These files can have data in any format and we may have to process it before loading it to the dataframe.</p><p class="calibre11">Julia provides a <code class="literal">readtable()</code> function that can be used to read a tabular file in a dataframe. Generally, we come across datasets in comma-separated or tab-separated formats (CSV or TSV). The <code class="literal">readtable()</code> works perfectly with them.</p><p class="calibre11">We can give the location of the file as UTF8String and the separator type to the readtable() function as arguments. The default separator type is comma (',') for CSV, tab ('\t') for TSV, and whitespace (' ') for WSV.</p><p class="calibre11">In the following example, we load the sample iris dataset into a dataframe using the <code class="literal">readtable()</code> function.</p><p class="calibre11">Although the iris dataset is available in the RDatasets package, we will download the CSV to work with the external datasets. The iris CSV can be downloaded from <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/iris.csv">https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/datasets/data/iris.csv</a>.</p><p class="calibre11">Remember to put the downloaded CSV into the current working directory (from where the REPL was started—generally it is the <code class="literal">~/home/&lt;username&gt;</code> directory):</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using DataFramesjulia&gt; df_iris_sample =
  readtable("iris_sample.csv",
  separator = ',')
julia&gt; df_iris_sample</strong>
</pre><p class="calibre11">It is the same dataset that we used in the previous example, but now we are loading the data from a CSV file.</p><p class="calibre11">The <code class="literal">readtable()</code> is used in a similar way for other text-based datasets such as TSV, WSV, or TXT. Suppose the same iris dataset is in TSV, WSV, or TXT format. It will be used in a similar way:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; df_iris_sample = readtable("iris_dataset.tsv", 
separator='\t') 
</strong>
</pre><p class="calibre11">And for example, if we have a dataset without a header and separated by <code class="literal">;</code>, we would use <code class="literal">readtable()</code> as follows:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; df_random_dataset = readtable("random_dataset.txt",                                                                    header=false, separator=';') 
</strong>
</pre><p class="calibre11">The <code class="literal">readtable()</code> exploits Julia's functionality of multiple dispatch and has been implemented with different method behaviors:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; methods(readtable)</strong>
<strong class="calibre19">3 methods for generic function readtable:</strong>
<strong class="calibre19">readtable(io::IO) at /home/anshul/.julia/v0.4/DataFrames/src/dataframe/io.jl:820</strong>
<strong class="calibre19">readtable(io::IO, nbytes::Integer) at /home/anshul/.julia/v0.4/DataFrames/src/dataframe/io.jl:820</strong>
<strong class="calibre19">readtable(pathname::AbstractString) at /home/anshul/.julia/v0.4/DataFrames/src/dataframe/io.jl:930</strong>
</pre><p class="calibre11">We can see that there are three methods for the <code class="literal">readtable()</code> function.</p><p class="calibre11">These methods implement some of the advanced options to ease the loading and to support various kinds of data formats:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">header::Bool</code>: In the iris example we used, we had headers such as Sepal Length, Sepal Width, and so on, which makes it easier to describe the data. But headers are not always available in the dataset. The default value of <code class="literal">header</code> is <code class="literal">true</code>; therefore, whenever headers are not available, we pass the argument as false.</li><li class="listitem"><code class="literal">separator::Char</code>: Data in a file must have been organized in the file in a way to form a tabular structure. This is generally by using <code class="literal">,</code>, <code class="literal">\t</code>, <code class="literal">;</code>, or combinations of these sometimes. The <code class="literal">readtable()</code> guesses the separator type by the extension of the file, but it is a good practice to provide it manually.</li><li class="listitem"><code class="literal">nastrings::Vector{ASCIIString}</code>: Suppose there are missing values or some other values and we want NA to replace them. This is done using nastrings. By default, it takes empty records and replaces them with NA.</li><li class="listitem"><code class="literal">truestrings::Vector{ASCIIString}</code>: This transforms the strings to Boolean, true. It is used when we want a set of strings to be treated as true in the dataset. By default, <code class="literal">True</code>, <code class="literal">true</code>, <code class="literal">T</code>, and <code class="literal">t</code> are transformed if no argument is given.<div><ul class="itemizedlist1"><li class="listitem"><code class="literal">falsestrings::Vector{ASCIIString}</code>: This works just like truestrings but transforms the strings to Boolean, false. By default, <code class="literal">False</code>, <code class="literal">false</code>, <code class="literal">F</code>, and <code class="literal">f</code> are transformed if no argument is given.</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><code class="literal">nrows::Int</code>: If we want only a specific number of rows to be read by <code class="literal">readtable()</code>, we use nrows as the argument. By default, it is <code class="literal">-1</code>, which means that <code class="literal">readtable()</code> will read the whole file.</li><li class="listitem"><code class="literal">names::Vector{Symbol}</code>: If we want some specific names for our columns, different from what is mentioned in the header, then we use names. Here, we pass a vector having the names of the columns that we want to use. By default, it is <code class="literal">[]</code>, which means the names in the headers should be used if they are there; otherwise, the numeric indices must be used.</li><li class="listitem"><code class="literal">eltypes::Vector{DataType}</code>: We can specify the column types by passing a vector, by using eltypes. It is an empty vector (<code class="literal">[]</code>) by default if nothing is passed.</li><li class="listitem"><code class="literal">allowcomments::Bool</code>: In the dataset, we may have records having comments with them. These comments can be ignored. By default, it is <code class="literal">false</code>.</li><li class="listitem"><code class="literal">commentmark::Char</code>: If we are using allowcomments, we will also have to mention the character (symbol) where the comment starts. By default, it is <code class="literal">#</code>.</li><li class="listitem"><code class="literal">ignorepadding::Bool</code>: Our dataset might not be as perfect as we want. The records may contain whitespace characters on either side. This can be ignored using ignorepadding. By default, it is true.</li><li class="listitem"><code class="literal">skipstart::Int</code>: Our dataset can have some rows describing the data with the header that we might not want, or we just want to skip the first few rows. This is done by skipstart, by specifying the number of rows to skip. By default, it is 0 and will read the entire file.</li><li class="listitem"><code class="literal">skiprows::Vector{Int}</code>: If want to skip some specific rows in the data then skiprows is used. We only need to specify the indices of the rows in a vector that we want to skip. By default, it is <code class="literal">[]</code> and will read the entire file.</li><li class="listitem"><code class="literal">skipblanks::Bool</code>: As mentioned earlier, our dataset may not be perfect. There can be some blank lines if we have scraped the data from the Web or extracted the data from other sources. We can skip these blank lines by using skipblanks. By default it is true, but we can choose otherwise if we do not want it.</li><li class="listitem"><code class="literal">encoding::Symbol</code>: We can specify the encoding of the file if it is other than UTF8.</li></ul></div><div><div><div><div><h3 class="title5"><a id="ch02lvl3sec1" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Writing the data to a file</h3></div></div></div><p class="calibre11">We may also want to output our results or transform a dataset and store it in a file. In Julia we do this by using the <code class="literal">writetable()</code> function. It is very similar to the <code class="literal">readtable()</code> function that we discussed in the last section.</p><p class="calibre11">For example, we want to write the <code class="literal">df_iris_sample</code> dataframe into a CSV file:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; writetable("output_df_iris.csv", df_iris_sample)</strong>
</pre><p class="calibre11">This is the way of writing to a file with the default set of arguments. One visible difference is that we are passing the dataframe that we want to write with the name of the file that we want to write to.</p><p class="calibre11">
<code class="literal">writetable()</code> also accepts various arguments such as <code class="literal">readtable()</code>.</p><p class="calibre11">We could have also written the previous statement like this with the separator defined:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; writetable("output_df_iris.csv", df_iris_sample, separator = ',')</strong>
</pre><p class="calibre11">Similarly, we can have a header and quote marks in the arguments.</p></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec24" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Working with DataFrames</h2></div></div></div><p class="calibre11">We will follow or inherit some of the traditional strategies to manipulate the data. We will go through these strategies and methods in this section and discuss how and why they are important to data science.</p><div><div><div><div><h3 class="title5"><a id="ch02lvl3sec2" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding DataFrames joins</h3></div></div></div><p class="calibre11">While working with multiple datasets, we often need to merge the datasets in a particular fashion to make the analysis easier or to use it with a particular function.</p><p class="calibre11">We will be using the <em class="calibre23">Road Safety Data</em> published by the Department for Transport, UK, and it is open under the OGL-Open Government Licence.</p><p class="calibre11">The datasets can be found here: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://data.gov.uk/dataset/road-accidents-safety-data">https://data.gov.uk/dataset/road-accidents-safety-data</a>.</p><p class="calibre11">We will be using two datasets:</p><div><ul class="itemizedlist"><li class="listitem">Road Safety: Accidents 2015</li><li class="listitem">Road Safety: Vehicles 2015</li></ul></div><div><div><h3 class="title6"><a id="note5" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre26">
</p><p class="calibre26">
<code class="literal">DfTRoadSafety_Accidents_2015</code> contains columns such as <code class="literal">Accident_Index</code>, <code class="literal">Location_Easting_OSGR</code>, <code class="literal">Location_Northing_OSGR</code>, <code class="literal">Longitude</code>, <code class="literal">Latitude</code>, <code class="literal">Police_Force</code>, <code class="literal">Accident_Severity</code>, <code class="literal">Number_of_Vehicles</code>, <code class="literal">Number_of_Casualties</code>, <code class="literal">Date</code>, <code class="literal">Day_of_Week</code>, <code class="literal">Time</code>, and so on. <code class="literal">DfTRoadSafety_Vehicles_2015</code> contains columns such as <code class="literal">Accident_Index</code>, <code class="literal">Vehicle_Reference</code>, <code class="literal">Vehicle_Type</code>, <code class="literal">Towing_and_Articulation</code>, <code class="literal">Vehicle_Manoeuvre</code>, <code class="literal">Vehicle_Location-Restricted_Lane</code>, <code class="literal">Junction_Location</code>, <code class="literal">Skidding_and_Overturning</code>, <code class="literal">Hit_Object_in_Carriageway</code>, and so on.</p><p class="calibre26">
</p></div></div><p class="calibre11">We can see that <code class="literal">Accident_Index</code> is a common field and is unique. It is used as the index in the dataset.</p><p class="calibre11">First we will be making the DataFrames package available and then we will load the data. We load the data into two different dataframes using the readtable function that we discussed earlier:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using DataFrames 
 
julia&gt; DfTRoadSafety_Accidents_2015 = readtable("DfTRoadSafety_Accidents_2015.csv") 
 
julia&gt; head(DfTRoadSafety_Accidents_2015)  
</strong>
</pre><p class="calibre11">
</p><div><img src="img/image_02_004.jpg" alt="Understanding DataFrames joins" class="calibre45"/></div><p class="calibre11">
</p><p class="calibre11">The first dataset is loaded into the DataFrame and we try getting information about the dataset using <code class="literal">head</code>. It gives a few starting columns.</p><p class="calibre11">If we are more interested in knowing the names of the columns, we can use the <code class="literal">names</code> function:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; names(DfTRoadSafety_Accidents_2015) 
32-element Array{Symbol,1}: 
 :_Accident_Index                             
 :Location_Easting_OSGR                       
 :Location_Northing_OSGR                      
 :Longitude                                   
 :Latitude                                    
 :Police_Force                                
 :Accident_Severity                           
 :Number_of_Vehicles                          
 :Number_of_Casualties                        
 :Date                                        
 :Day_of_Week                                 
 :Time                                        
 :Local_Authority_District_                   
                                             
 :x2nd_Road_Class                             
 :x2nd_Road_Number                            
 :Pedestrian_Crossing_Human_Control           
 :Pedestrian_Crossing_Physical_Facilities     
 :Light_Conditions                            
 :Weather_Conditions                          
 :Road_Surface_Conditions                     
 :Special_Conditions_at_Site                  
 :Carriageway_Hazards                         
 :Urban_or_Rural_Area                         
 :Did_Police_Officer_Attend_Scene_of_Accident 
 :LSOA_of_Accident_Location 
</strong>
</pre><p class="calibre11">Similarly, we will be loading the second dataset in a dataframe:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; DfTRoadSafety_Vehicles_2015 = readtable("DfTRoadSafety_Vehicles_2015.csv")  
</strong>
</pre><p class="calibre11">The second dataset is loaded into the memory.</p><p class="calibre11">Later we will delve deeper, but for now let's do a full join between the two datasets. A join between these two datasets will tell us which accident involved which vehicles:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; DfTRoadSafety_Vehicles_2015 = readtable("DfTRoadSafety_Vehicles_2015.csv") 
 
julia&gt; full_DfTRoadSafety_2015 = 
join(DfTRoadSafety_Accidents_2015, 
DfTRoadSafety_Vehicles_2015, 
on = :_Accident_Index)</strong>
</pre><p class="calibre11">
</p><div><img src="img/image_02_005.jpg" alt="Understanding DataFrames joins" class="calibre46"/></div><p class="calibre11">
</p><p class="calibre11">We can see that the full join has worked. Now we have the data, which can tell us the time of the accident, the location of the vehicle, and many more details.</p><p class="calibre11">The benefit is that the join is really easy to do and is really quick, even over large datasets.</p><p class="calibre11">We have read about other joins available in relation databases. Julia's DataFrames package provides these joins too:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Inner join</strong>: The output, which is the DataFrame, contains only those rows that have keys in both the dataframes.</li><li class="listitem"><strong class="calibre19">Left join</strong>: The output DataFrame has the rows for keys that are present in the first (left) DataFrame, irrespective of them being present in the second (right) DataFrame.</li><li class="listitem"><strong class="calibre19">Right join</strong>: The output DataFrame has the rows for keys that are present in the second (right) DataFrame, irrespective of them being present in the first (left) DataFrame.</li><li class="listitem"><strong class="calibre19">Outer join</strong>: The output DataFrame has the rows for the keys that are present in the first or second DataFrame, which we are joining.</li><li class="listitem"><strong class="calibre19">Semi join</strong>: The output DataFrame has only the rows from the first (left) DataFrame for the keys that are present in both the first (left) and second (right) DataFrames. The output contains only rows from the first DataFrame.</li><li class="listitem"><strong class="calibre19">Anti join</strong>: The output DataFrame has the rows for keys that are present in the first (left) DataFrame but rows for the same keys are not present in the second (right) DataFrame. The output contains only rows from the first DataFrame.</li><li class="listitem"><strong class="calibre19">Cross join</strong>: The output DataFrame has the rows that are the Cartesian product of the rows from the first DataFrame (left) and the second DataFrame (right).</li></ul></div><p class="calibre11">Cross join doesn't involve a key; therefore it is used like this:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; cross_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, kind = :cross)  
</strong>
</pre><p class="calibre11">Here we have used the <code class="literal">kind</code> argument to pass the type of join that we want. Other joins are also done using this argument.</p><p class="calibre11">The kind of join that we want to use is done using the <code class="literal">kind</code> argument.</p><p class="calibre11">Let's understand this using a simpler dataset. We will create a dataframe and will apply different joins on it:</p><pre class="programlisting">j<strong class="calibre19">ulia&gt; left_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :left)  
</strong>
</pre><p class="calibre11">For left join, we can use:</p><pre class="programlisting">julia&gt; Cities = ["Delhi","Amsterdam","Hamburg"][rand(1:3, 10)] 
 
julia&gt; df1 = DataFrame(Any[[1:10], Cities, 
        rand(10)], [:ID, :City, :RandomValue1]) 
 
julia&gt; df2 = DataFrame(ID = 1:10, City = Cities, 
        RandomValue2 = rand(100:110, 10))  
</pre><p class="calibre11">This created two dataframes having 10 rows. The first dataframe, df1, has three columns: <code class="literal">ID</code>, <code class="literal">City</code>, and <code class="literal">RandomValue1</code>. The second dataframe has df2 with three columns: <code class="literal">ID</code>, <code class="literal">City</code>, and <code class="literal">RandomValue2</code>.</p><p class="calibre11">Applying full join, we can use:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; full_df1_df2 = join(df1,df2, 
                on = [:ID, :City]) 
</strong>
</pre><p class="calibre11">We have used two columns to apply the join.</p><p class="calibre11">This will generate:</p><p class="calibre11">
</p><div><img src="img/image_02_006.jpg" alt="Understanding DataFrames joins" class="calibre47"/></div><p class="calibre11">
</p><p class="calibre11">Other joins can also be applied using the <code class="literal">kind</code> argument. Let's go through our old dataset of accidents and vehicles.</p><p class="calibre11">The different joins using <code class="literal">kind</code> are:</p><pre class="programlisting">julia&gt; right_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :right) 
 
julia&gt; inner_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :inner) 
 
julia&gt; outer_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :outer) 
 
julia&gt; semi_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :semi) 
 
julia&gt; anti_DfTRoadSafety_2014 = join(DfTRoadSafety_Accidents_2014, DfTRoadSafety_Vehicles_2014, on = :_Accident_Index, kind = :anti) 
</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec25" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Split-Apply-Combine strategy</h2></div></div></div><p class="calibre11">A paper was published by Hadley Wickham (Wickham, Hadley. "The split-apply-combine strategy for data analysis." <em class="calibre23">Journal of Statistical Software</em> 40.1 (2011): 1-29), defining the Split-Apply-Combine strategy for data analysis. In this paper, he explained why it is good to break up a big problem into manageable pieces, independently operate on each piece, obtain the necessary results, and then put all the pieces back together.</p><p class="calibre11">This is needed when a dataset contains a large number of columns and for some operations all the columns are not necessary. It is better to split the dataset and then apply the necessary functions; and we can always put the dataset back together.</p><p class="calibre11">This is done using the by function by takes three arguments:</p><div><ul class="itemizedlist"><li class="listitem">DataFrame (this is the dataframe that we would be splitting)</li><li class="listitem">The column name (or numerical index) on which the DataFrame would be split</li><li class="listitem">A function that can be applied on every subset of the DataFrame</li></ul></div><p class="calibre11">Let's try to apply by to our same dataset:</p><p class="calibre11">
</p><div><img src="img/B05321_02_07.jpg" alt="The Split-Apply-Combine strategy" class="calibre48"/></div><p class="calibre11">
</p><p class="calibre11">The <code class="literal">aggregate()</code> function provides an alternative to apply the Split-Apply-Combine strategy. The <code class="literal">aggregate()</code> function uses the same three arguments:</p><div><ul class="itemizedlist"><li class="listitem">DataFrame (this is the DataFrame that we would be splitting)</li><li class="listitem">The column name (or numerical index) on which the DataFrame would be split</li><li class="listitem">A function that can be applied on the every subset of the DataFrame</li></ul></div><p class="calibre11">The function provided in the third argument is applied to every column, which wasn't used in splitting up the DataFrame.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec26" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Reshaping the data</h2></div></div></div><p class="calibre11">The use case may require data to be in a different shape than we currently have. To facilitate this, Julia provides reshaping of the data.</p><p class="calibre11">Let's use the same dataset that we were using, but before that let's check the size of the dataset:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; size(DfTRoadSafety_Accidents_2014) 
(146322,32) 
</strong>
</pre><p class="calibre11">We can see that there are greater than 100,000 rows. Although we can work on this data, for simplicity of understanding, let's take a smaller dataset.</p><p class="calibre11">Datasets provided in RDataset are always good to start with. We will use the tried and tested iris dataset for this.</p><p class="calibre11">We will import <code class="literal">RDatasets</code> and <code class="literal">DataFrames</code> (if we have started a new terminal session):</p><pre class="programlisting">
<strong class="calibre19">julia&gt; using RDatasets, DataFrames 
</strong>
</pre><p class="calibre11">Then, we will load the iris dataset into a <code class="literal">DataFrame</code>. We can see that the dataset has 150 rows and 5 columns:</p><p class="calibre11">
</p><div><img src="img/B05321_02_08.jpg" alt="Reshaping the data" class="calibre49"/></div><p class="calibre11">
</p><p class="calibre11">Now we use the <code class="literal">stack()</code> function to reshape the dataset. Let's use it without any arguments except the DataFrame.</p><p class="calibre11">Stack works by creating a dataframe for categorical variables with all of the information one by one:</p><p class="calibre11">
</p><div><img src="img/B05321_02_09.jpg" alt="Reshaping the data" class="calibre50"/></div><p class="calibre11">
</p><p class="calibre11">We can see that our dataset has been stacked. Here we have stacked all the columns. We can also provide specific columns to stack:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; iris_dataframe [:id] = 1:size(iris_dataframe, 1)  
# create a new column to track the id of the row 
 
Julia&gt; iris_stack = (iris_dataframe,  [1:4]) 
</strong>
</pre><p class="calibre11">The second argument depicts the columns that we want to stack. We can see in the result that column 1 to 4 have been stacked, which means we have reshaped the dataset into a new dataframe:</p><pre class="programlisting">
<strong class="calibre19">Julia&gt; iris_stack = stack(iris_dataframe,  [1:4]) 
 
Julia&gt; size(iris_stack) 
(600,4) 
Julia&gt; head(iris_stack) 
</strong>
</pre><p class="calibre11">
</p><div><img src="img/image_02_010.jpg" alt="Reshaping the data" class="calibre51"/></div><p class="calibre11">
</p><p class="calibre11">We can see that there is a new column <code class="literal">:id</code>. That's the identifier of the stacked dataframe. Its value is repeated the number of times the rows are repeated.</p><p class="calibre11">As all the columns are included in the resultant DataFrame, there is repetition for some columns. These columns are actually the identifiers for this DataFrame and are denoted by the column (<code class="literal">id</code>). Other than the identifiers column (<code class="literal">:id</code>), there are two more columns, <code class="literal">:variable</code> and <code class="literal">:values</code>. These are the columns that actually contain the stacked values.</p><p class="calibre11">
</p><div><img src="img/image_02_011.jpg" alt="Reshaping the data" class="calibre52"/></div><p class="calibre11">
</p><p class="calibre11">We can also provide a third argument (optional). This is the column whose values are repeated. Using this, we can specify which column to include and which not to include.</p><p class="calibre11">
</p><div><img src="img/B05321_02_12.jpg" alt="Reshaping the data" class="calibre53"/></div><p class="calibre11">
</p><p class="calibre11">The <code class="literal">melt()</code> function is similar to the stack function but has some special features. Here we need to specify the identifier columns and the rest are stacked:</p><pre class="programlisting">Julia&gt; iris_melt = stack(iris_dataframe, [1:4]) 
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_13.jpg" alt="Reshaping the data" class="calibre54"/></div><p class="calibre11">
</p><p class="calibre11">The remaining columns are stacked with the assumption that they contain measured variables.</p><p class="calibre11">Opposite to stack and melt is unstack, which is used to convert from a long format to wide format. We need to specify the identifier columns and variable/value columns to the unstack function:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; unstack(iris_melt, :id, :variable, :value) 
</strong>
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_14.jpg" alt="Reshaping the data" class="calibre55"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">:id</code> (identifier) in the arguments of the unstack can be skipped if the remaining columns are unique:</p><pre class="programlisting">julia&gt; unstack(iris_melt, :variable, :value) 
</pre><p class="calibre11">
<code class="literal">meltdf</code> and <code class="literal">stackdf</code> are two additional functions that work like melt and stack but also provide a view into the original wide DataFrame:</p><pre class="programlisting">Julia&gt; iris_stackdf = stackdf(iris_dataframe) 
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_15.jpg" alt="Reshaping the data" class="calibre56"/></div><p class="calibre11">
</p><p class="calibre11">This seems exactly similar to the stack function, but we can see the difference by looking at their storage representation.</p><p class="calibre11">To look at the storage representation, dump is used. Let's apply it to the stack function:</p><p class="calibre11">
</p><div><img src="img/B05321_02_16.jpg" alt="Reshaping the data" class="calibre57"/></div><p class="calibre11">
</p><div><ul class="itemizedlist"><li class="listitem">Here, we can see that <code class="literal">:variable</code> is of type <code class="literal">Array(Symbol,(600,))</code></li><li class="listitem"><code class="literal">:value</code> is of type <code class="literal">DataArrays.DataArray{Float64,1}(600)</code></li><li class="listitem">Identifier (<code class="literal">:Species</code>) is of type <code class="literal">DataArrays.PooledDataArray{ASCIIString,UInt8,1}(600)</code></li></ul></div><p class="calibre11">Now, we will look at the storage representation of <code class="literal">stackdf</code>:</p><p class="calibre11">
</p><div><img src="img/B05321_02_17.jpg" alt="Reshaping the data" class="calibre58"/></div><p class="calibre11">
</p><p class="calibre11">Here, we can see that:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">:variable</code> is of type <code class="literal">DataFrames.RepeatedVector{Symbol}</code>. Variable is repeated n times, where n refers to the number of rows in the original <code class="literal">AbstractDataFrame</code>.</li><li class="listitem"><code class="literal">:value</code> is of type <code class="literal">DataFrames.StackedVector</code>. This facilitates the view of the columns stacked together as in the original DataFrame.</li><li class="listitem">Identifier (<code class="literal">:Species</code>) is of type <code class="literal">Species: DataFrames.RepeatedVector{ASCIIString}</code>. The original column is repeated n times where n is the number of the columns stacked.</li></ul></div><p class="calibre11">Using these AbstractVectors, we are now able to create views, thus saving memory by using this implementation.</p><p class="calibre11">Reshaping functions don't provide the capabilities to perform aggregation. So to perform aggregation, a combination of the Split-Apply-Combine strategy with reshaping is used.</p><p class="calibre11">We will use <code class="literal">iris_stack</code>:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; iris_stack = stack(iris_dataframe) 
</strong>
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_18.jpg" alt="Reshaping the data" class="calibre59"/></div><p class="calibre11">
</p><p class="calibre11">Here, we created a new column having the mean values of the columns according to the species. We can now unstack this.</p><p class="calibre11">
</p><div><img src="img/B05321_02_19.jpg" alt="Reshaping the data" class="calibre60"/></div><p class="calibre11">
</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec27" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sorting a dataset</h2></div></div></div><p class="calibre11">Sorting is one of the most used techniques in data analysis. Sorting is facilitated in Julia by calling the <code class="literal">sort</code> or <code class="literal">sort!</code> function.</p><p class="calibre11">The difference between the <code class="literal">sort</code> and <code class="literal">sort!</code> is that <code class="literal">sort!</code> works in-place, which sorts the actual array rather than creating a copy.</p><p class="calibre11">Let's use the <code class="literal">sort!</code> function on the iris dataset:</p><p class="calibre11">
</p><div><img src="img/B05321_02_20.jpg" alt="Sorting a dataset" class="calibre61"/></div><p class="calibre11">
</p><p class="calibre11">We can see that the columns are not sorted according to <code class="literal">[:SepalLength, :SepalWidth, :PetalLength, :PetalWidth]</code>. But these are actually sorted according to the :Species column.</p><p class="calibre11">The sorting function takes some arguments and provides a few features. For example, to sort in reverse, we have:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; sort!(iris_dataframe, rev = true) 
</strong>
</pre><p class="calibre11">To sort some specific columns, we have:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; sort!(iris_dataframe, cols = [:SepalLength, :PetalLength]) 
</strong>
</pre><p class="calibre11">We can also use the by function with <code class="literal">sort!</code> to apply another function on the DataFrame or the single column.</p><p class="calibre11">
</p><div><img src="img/B05321_02_21.jpg" alt="Sorting a dataset" class="calibre62"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">order</code> is used to specify ordering a specific column amongst a set of columns.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec28" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Formula - a special data type for mathematical expressions</h2></div></div></div><p class="calibre11">Data science involves various statistical formulas to get insights from data. The creation and application of these formulas is one of the core processes of data science. It maps input variables with some function and mathematical expression to an output.</p><p class="calibre11">Julia facilitates this by providing a formula type in the <code class="literal">DataFrame</code> package, which is used with the symbol <code class="literal">~</code>. <code class="literal">~</code> is a binary operator. For example:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; formulaX = A ~ B + C</strong>
</pre><p class="calibre11">For statistical modeling, it is recommended to use ModelMatrix, which constructs a Matrix{Float64}, making it more suited to fit in a statistical model. Formula can also be used to transform to a ModelFrame object from a DataFrame, which is a wrapper over it, to meet the needs of statistical modeling.</p><p class="calibre11">Create a dataframe with random values:</p><p class="calibre11">
</p><div><img src="img/B05321_02_22.jpg" alt="Formula - a special data type for mathematical expressions" class="calibre63"/></div><p class="calibre11">
</p><p class="calibre11">Use formula to transform it into a <code class="literal">ModelFrame</code> object:</p><p class="calibre11">
</p><div><img src="img/B05321_02_23.jpg" alt="Formula - a special data type for mathematical expressions" class="calibre64"/></div><p class="calibre11">
</p><p class="calibre11">Creating a <code class="literal">ModelMatrix</code> from a <code class="literal">ModelFrame</code> is quite easy:</p><p class="calibre11">
</p><div><img src="img/B05321_02_24.jpg" alt="Formula - a special data type for mathematical expressions" class="calibre65"/></div><p class="calibre11">
</p><p class="calibre11">There is an extra column containing only <code class="literal">value = 1.0</code>. It is used in a regression model to fit an intercept term.</p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec29" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Pooling data</h2></div></div></div><p class="calibre11">To analyze huge datasets efficiently, PooledDataArray is used. DataArray uses an encoding that represents a full string for every entry of a vector. This is not very efficient, especially for large datasets and memory-intensive algorithms.</p><p class="calibre11">Our use case more often deals with factors involving a small number of levels:</p><p class="calibre11">
</p><div><img src="img/B05321_02_25.jpg" alt="Pooling data" class="calibre66"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">PooledDataArray</code> uses indices in a small pool of levels instead of strings to represent data efficiently.</p><p class="calibre11">
</p><div><img src="img/B05321_02_26.jpg" alt="Pooling data" class="calibre67"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">PooledDataArray</code> also provides us with the functionality to find out the levels of the factor using the levels function:</p><p class="calibre11">
</p><div><img src="img/B05321_02_27.jpg" alt="Pooling data" class="calibre68"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">PooledDataArray</code> even provides a compact function to efficiently use memory:</p><pre class="programlisting">Julia&gt; pooleddatavector = compact (pooleddatavector) 
</pre><p class="calibre11">Then, it provides a pool function for converting a single column when factors are encoded not in <code class="literal">PooledDataArray</code> columns but in DataArray or DataFrame:</p><pre class="programlisting">Julia&gt;  pooleddatavector = pool(datavector) 
</pre><p class="calibre11">
</p><div><img src="img/B05321_02_28.jpg" alt="Pooling data" class="calibre69"/></div><p class="calibre11">
</p><p class="calibre11">
<code class="literal">PooledDataArray</code> facilitates the analysis of categorical data, as columns in ModelMatrix are treated as 0-1 indicator columns. Each of the levels of PooledDataArray is associated with one column. </p></div><div><div><div><div><h2 class="title3"><a id="ch02lvl2sec30" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Web scraping</h2></div></div></div><p class="calibre11">Real-world use cases also include scraping data from the Web for analysis. Let's build a small web scraper to fetch Reddit posts.</p><p class="calibre11">For this, we will need the JSON and Requests packages:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; Pkg.add("JSON") 
julia&gt; Pkg.add("Requests") 
 
# import the required libraries 
julia&gt; using JSON, Requests 
 
# Use the reddit URL to fetch the data from 
julia&gt; reddit_url = https://www.reddit.com/r/Julia/ 
 
# fetch the data and store it in a variable 
julia&gt; response = get("$(reddit_url)/.json") 
Response(200 OK, 21 headers, 55426 bytes in body) 
 
# Parse the data received using JSON.parse 
julia&gt; dataReceived = JSON.parse(Requests.text(response)) 
# Create the required objects 
julia&gt; nextRecord = dataReceived["data"]["after"] 
julia&gt; counter = length(dataReceived["data"]["children"]) 
 
</strong>
</pre><p class="calibre11">Here, we defined a URL from where we will be scraping the data. We are scraping from Julia's section on Reddit.</p><p class="calibre11">Then, we are getting the content from the defined URL using the get function from the Requests package. We can see that we've got response 200 OK with the data:</p><pre class="programlisting">
<strong class="calibre19">julia&gt; statuscode(response) 
200 
 
julia&gt; HttpCommon.STATUS_CODES[200] 
"OK"  
</strong>
</pre><p class="calibre11">We then parse the JSON data received using the JSON parser provided by the JSON package of Julia. We can now start reading the record.</p><p class="calibre11">
</p><div><img src="img/B05321_02_29.jpg" alt="Web scraping" class="calibre70"/></div><p class="calibre11">
</p><p class="calibre11">We can store the data received in an Array or DataFrame (depending on the use case and ease of use). Here, we are using an Array to store the parsed data. We can check the data stored in an Array.</p><p class="calibre11">
</p><div><img src="img/B05321_02_30.jpg" alt="Web scraping" class="calibre71"/></div><p class="calibre11">
</p><p class="calibre11">Suppose we only need to see the title of these posts and know what we have scraped; we just need to know in which column they are.</p><p class="calibre11">
</p><div><img src="img/B05321_02_31.jpg" alt="Web scraping" class="calibre72"/></div><p class="calibre11">
</p><p class="calibre11">We can now see the title of the Reddit posts. But what if we had too many columns or we had some missing values? DataFrames would definitely be a better option.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec19" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we learned what data munging is and why it is necessary for data science. Julia provides functionalities to facilitate data munging with the DataFrames.jl package, with features such as these:</p><div><ul class="itemizedlist"><li class="listitem"><code class="literal">NA</code>: A missing value in Julia is represented by a specific data type, NA.</li><li class="listitem"><code class="literal">DataArray</code>: DataArray provided in the <code class="literal">DataFrames.jl</code> provides features such as allowing us to store some missing values in an array.</li><li class="listitem"><code class="literal">DataFrame</code>: DataFrame is 2-D data structure like spreadsheets. It is very similar to R or pandas's dataframes, and provides many functionalities to represent and analyze data. DataFrames has many features well suited for data analysis and statistical modeling.</li><li class="listitem">A dataset can have different types of data in different columns.</li><li class="listitem">Records have a relation with other records in the same row of different columns of the same length.</li><li class="listitem">Columns can be labeled. Labeling helps us to easily become familiar with the data and access it without the need to remember their numerical indices.</li></ul></div><p class="calibre11">We learned about importing data from a file using the <code class="literal">readtable()</code> function and exporting data to a file. The <code class="literal">readtable()</code> function provides flexibility when using many arguments.</p><p class="calibre11">We also explored joining of datasets, such as RDBMS tables. Julia provides various joins that we can exploit according to our use case.</p><p class="calibre11">We discussed the Split-Apply-Combine Strategy, one of the most widely used techniques deployed by data scientists, and why it is needed. We went through reshaping or pivoting data using stack and melt (stackdf, meltdf) functions and explored the various possibilities involved. We were also introduced to <code class="literal">PooledDataArray</code> and learned why it is required for efficient memory management.</p><p class="calibre11">We were introduced to web scraping, which is sometimes a must for a data scientist to gather data. We also used the Requests package to fetch an HTTP response.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch02lvl1sec20" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://julia.readthedocs.org/en/latest/manual/">http://julia.readthedocs.org/en/latest/manual/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dataframesjl.readthedocs.io/en/latest/">http://dataframesjl.readthedocs.io/en/latest/</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://data.gov.uk/dataset/road-accidents-safety-data">https://data.gov.uk/dataset/road-accidents-safety-data</a></li><li class="listitem">Wickham, Hadley. "The split-apply-combine strategy for data analysis." <em class="calibre23">Journal of Statistical Software</em> 40.1 (2011): 1-29</li></ul></div></div></div>



  </body></html>