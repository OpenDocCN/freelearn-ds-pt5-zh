- en: Big Data Mining with NoSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **NoSQL** was first used by Carlo Strozzi, who, in 1998, released the
    Strozzi NoSQL opensource relational database. In the late 2000s, new paradigms
    in database architecture emerged, many of which did not adhere to the strict constraints
    required of relational database systems. These databases, due to their non-conformity
    with standard database conventions such as ACID compliance, were soon grouped
    under a broad category known as NoSQL.
  prefs: []
  type: TYPE_NORMAL
- en: Each NoSQL database claims to be optimal for certain use cases. Although few
    of them would fit the requirements to be a general-purpose database management
    system, they all leverage a few common themes across the spectrum of NoSQL systems.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will visit some of the broad categories of NoSQL database
    management systems. We will discuss the primary drivers that initiated the migration
    to NoSQL database systems and how such databases solved specific business needs
    that led to their widespread adoption, and conclude with a few hands-on NoSQL
    exercises.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics covered in this chapter include:'
  prefs: []
  type: TYPE_NORMAL
- en: Why NoSQL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NoSQL databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In-memory databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columnar databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document-oriented databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key-value databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other NoSQL types and summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hands-on exercise on NoSQL systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why NoSQL?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term NoSQL generally means *Not Only SQL*: that is, the underlying database
    has properties that are different to those of common and traditional database
    systems. As such, there is no clear distinction that qualifies a database as NoSQL,
    other than the fact that they do not provide the characteristics of ACID compliance.
    As such, it would be helpful to understand the nature of ACID properties that
    have been the mainstay of database systems for many decades, as well as discuss,
    in brief, the significance of BASE and CAP, two other terminologies central to
    databases today.'
  prefs: []
  type: TYPE_NORMAL
- en: The ACID, BASE, and CAP properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first proceed with ACID and SQL.
  prefs: []
  type: TYPE_NORMAL
- en: ACID and SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ACID stands for atomicity, consistency, isolation, and durability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Atomicity**: This indicates that database transactions either execute in
    full or do not execute at all. In other words, either all transactions should
    be committed, that is, persisted in their entirety, or not committed at all. There
    is no scope for a partial execution of a transaction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**: The constraints on the data, that is, the rules that determine
    data management within a database, will be consistent throughout the database.
    Different instances will not abide by rules that are any different to those in
    other instances of the database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation**: This property defines the rules of how concurrent operations
    (transactions) will read and write data. For example, if a certain record is being
    updated while another process reads the same record, the isolation level of the
    database system will determine which version of the data would be returned back
    to the user.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Durability**: The durability of a database system generally indicates that
    committed transactions will remain persistent even in the event of a system failure.
    This is generally managed by the use of transaction logs that databases can refer
    to during recovery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The reader may observe that all the properties defined here relate primarily
    to database transactions. A **transaction** is a unit of operation that abides
    by the aforementioned rules and makes a change to the database. For example, a
    typical cash withdrawal from an ATM may have the following logical pathway:'
  prefs: []
  type: TYPE_NORMAL
- en: User withdraws cash from an ATM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The bank checks the current balance of the user
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The database system deducts the corresponding amount from the user's account
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The database system updates the amount in the user's account to reflect the
    change
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As such, most databases in popular use prior to the mid-1990s, such as Oracle,
    Sybase, DB2, and others, were optimized for recording and managing transactional
    data. Until this time, most databases were responsible for managing transactional
    data. The rapid growth of the internet in the mid-90s led to new types of data
    that did not necessarily require the strict ACID compliance requirements. Videos
    on YouTube, music on Pandora, and corporate email records are all examples of
    use cases where a a transactional database does not add value beyond simply functioning
    as a technology layer for storing data.
  prefs: []
  type: TYPE_NORMAL
- en: The BASE property of NoSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By the late 2000s, data volume had surged and it was apparent that a new alternative
    model was required in order to manage the data. This new model, called BASE, became
    a foundational topic that replaced ACID as the preferred model of database management
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: '**BASE** stands for **B**asically **A**vailable **S**oft-state **E**ventually
    consistency. This implies that the database is *basically*Â available for use most
    of the time; that is, there can be periods during which the services are unavailable
    (and hence additional redundancy measures should be implemented). *Soft-state*
    means that the state of the system cannot be guaranteed - different instances
    of the same data might have different content as it may not have yet captured
    recent updates in another part of the cluster. Finally, *eventually* consistent
    implies that although the database might not be in the same state at all times,
    it will eventually get to the same state; that is, become *consistent*.'
  prefs: []
  type: TYPE_NORMAL
- en: The CAP theorem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First introduced in the late 1990s by Eric Allen Brewer, the CAP theorem categorizes
    the constraints, or more generally the characteristics, of distributed database
    systems. In brief, the CAP theorem postulates that strictly speaking, database
    systems can guarantee only two of the three properties defined by CAP, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Consistency**: The data should be consistent across all instances of the
    database and hence, when queried, should provide a coherent result across all
    nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability**: Irrespective of the state of any individual node, the system
    will always respond with a result upon a query being executed (whether or not
    it is the most recent commit)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Partition tolerance**: This implies that when nodes are separated across
    a network, the system should continue to function normally even if any node loses
    interconnectivity to another node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It might be evident from this that, since in a cluster nodes will be connected
    over a *network* which, by nature can be disrupted, partition tolerance has to
    be guaranteed in order for the system to continue performing normally. In this
    case, the contention lies with choosing between consistency and availability.
    For example, if the system has to be consistent; that is, show the most recent
    commit across all nodes, all the nodes cannot be *available* all at the same time
    as some nodes might not have the most recent commit. In this case, a query on
    a new update will not execute until all nodes have been updated with the new data.
    In case of availability, in similar terms, we cannot guarantee consistency, since
    to be available at all times means that some nodes will not have the same data
    as another node if a new update has not been written onto the respective node.
  prefs: []
  type: TYPE_NORMAL
- en: There is a great deal of confusion as well as contention between deciding on
    whether to ensure consistency or to ensure availability, and as such databases
    have been categorized as being either **CP** or **AP**. For the purpose of this
    exercise, we need not get caught up in the terminologies as that would lead to
    a rather abstract and philosophical discussion. The information on the aforementioned
    terminologies has been primarily provided to reflect upon some of the foundational
    theories driving the development of databases.
  prefs: []
  type: TYPE_NORMAL
- en: The need for NoSQL technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While most database systems were initially designed to manage transactions,
    the growth of internet-related technologies and new types of data that did not
    require the strict puritan nature of transactional systems necessitated the development
    of alternative frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, storing the following types of data does not necessarily require
    a complex *transactional database*:'
  prefs: []
  type: TYPE_NORMAL
- en: Emails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Media such as audio/video files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social network messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Website HTML pages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, the increase in users, and as a consequence, data volume, signaled
    the need for developing more robust architectures with the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalable to manage ever increasing data volume
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leverage commodity hardware to decrease dependency on expensive hardware
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide distributed processing capability across multiple nodes to process large-scale
    datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be fault-tolerant/provide high availability to handle node and site failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable implies that the system can accommodate the increase in data volume
    by increasing the number of nodes, namely, by scaling horizontally. Further, increasing
    the number of nodes should have minimal impact on the performance of the system.
  prefs: []
  type: TYPE_NORMAL
- en: Fault-tolerant implies that the system should be able to handle node failures,
    which won't be uncommon in a large distributed system with hundreds if not thousands
    of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This led to the development of various groundbreaking and influentialÂ systems,
    of which perhaps the most notable were Google Bigtable and Amazon Dynamo.
  prefs: []
  type: TYPE_NORMAL
- en: Google Bigtable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bigtable was a project that was initiated in 2004Â to manage both scalability
    and performance of the data used for various projects at Google. The seminal paper
    that describes the characteristics of the system was released in 2006 ([https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf))Â titled
    *Bigtable: A Distributed Storage System for Structured Data*. In essence, Bigtable
    was a *column-store* (more on this later) where each value could be uniquely identified
    using a row key, a column key, and a timestamp. It was one of the first mainstream
    databases that epitomized the benefits of storing data in a columnar format rather
    than using the more common row-based layout. Although columnar databases such
    as kdb+ and Sybase IQ existed prior to Bigtable, the use of the method by an industry
    leader to manage petabyte-scale information brought the concept into the limelight.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official site of Bigtable summarizes the key-value proposition:'
  prefs: []
  type: TYPE_NORMAL
- en: Bigtable is designed to handle massive workloads at consistent low latency and
    high throughput, so it's a great choice for both operational and analytical applications,
    including IoT, user analytics, and financial data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Since the introduction of Bigtable, several other NoSQL databases adopted the
    convention of columnar data layout; most notably HBase and Accumulo, which are
    both Apache projects.
  prefs: []
  type: TYPE_NORMAL
- en: The Bigtable solution is today available for use at [https://cloud.google.com/bigtable/](https://cloud.google.com/bigtable/)
    where it can be purchased on a subscription basis. The fee for smaller amounts
    of data is quite nominal and reasonable, whereas larger installations would require
    more extensive implementations.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon Dynamo
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shortly after Google announced Bigtable, Amazon followed with the announcement
    of its internal Dynamo database at the 21st ACM Symposium on Operating Systems
    Principles held in October, 2007 ([http://www.sosp2007.org](http://www.sosp2007.org)).
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, now available on Werner Vogels' site at [http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf),
    Amazon described a key-value store called Dynamo that was used to power some of
    Amazon's most critical internal services such as S3 on AWS. The paper brought
    to bear some key concepts such as key-value storage, consistent hashing, and vector
    clocks, among others, that were implemented in Dynamo.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, Dynamo offered an alternative to Bigtable's columnar storage for large-scale
    datasets by introducing a fundamentally different method that leveraged key-value
    associations.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will discuss the various types of NoSQL technologies
    and how each of them has characteristics that make them optimal for certain use
    cases. NoSQL has ushered in a paradigm shift in how we treat databases, and has
    provided a much-needed alternative view to data management at a scale that was
    not feasible previously.
  prefs: []
  type: TYPE_NORMAL
- en: NoSQL databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In our discussion of NoSQL types and databases, we will primarily focus on
    the following characteristics of NoSQL databases:'
  prefs: []
  type: TYPE_NORMAL
- en: In-memory databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Columnar databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Document-oriented databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key-value databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph databases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other NoSQL types and summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most types of NoSQL used in the industry today fall into one or more of these
    categories. The next few sections will discuss the high-level properties of each
    of these NoSQL offerings, their main advantages, and products in the market that
    fall into the respective categories.
  prefs: []
  type: TYPE_NORMAL
- en: In-memory databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**In-memory databases**, as the name implies, leverage the computer memory;
    that is, the RAM, to store datasets. Before we look into how in-memory databases
    work, it would be worthwhile to recollect how data transfer happens in a typical
    computer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a7357ec-76ba-4ba4-a9b8-957b76539cb5.png)'
  prefs: []
  type: TYPE_IMG
- en: Simple Data Flow Computer Hierarchy
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding image, data traverses from disk to memory to the CPU.
    This is a very high-level generalization of the exact process as there are conditions
    under which the CPU does not need to send an instruction to read data from memory
    (such as when the data is already present in the CPU L2 Cache - a part of the
    CPU that contains memory reserved for caching data), but fundamentally the process
    is linear between the CPU, RAM, and disk.
  prefs: []
  type: TYPE_NORMAL
- en: Data that is stored on disk can be transferred to the memory at a certain rate
    that is dependent on the I/O (Input/Output) throughput of the disk. It takes approximately
    10-20 milliseconds (ms) to access data from disk. While the exact number varies
    depending on the size of the data, the minimum seek time (time for the disk to
    find the location of the data) in itself is approximately 10-15 ms. Compare this
    with the time it takes to fetch data from memory, which is approximately 100 nanoseconds.
    Finally, it takes approximately 7 ns to read data from the CPU L2 Cache.
  prefs: []
  type: TYPE_NORMAL
- en: To put this into perspective, the disk access time of 15 milliseconds, namely,
    15,000,000 nanoseconds is 150,000 times *slower* than the time it takes to access
    data from memory. In other words, data that is already present in memory can be
    read at an astounding 150 thousand times faster relative to disk. This is essentially
    true of reading random data. The time to read sequential data is arguably less
    sensational, but still nearly an order of magnitude faster.
  prefs: []
  type: TYPE_NORMAL
- en: If the disk and RAM were represented as cars, the RAM *car* would have gone
    all the way to the moon and be on its way back in the time it would take the disk
    car to go barely two miles. That is how large the difference is.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, it is natural to conclude from this that if the data were stored in RAM,
    especially in the case of larger datasets, the access time would be dramatically
    lower, and consequently the time to process the data (at least on the I/O level)
    would be significantly reduced.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, all data in terms of databases was stored on disk. With the advent
    of the internet, the industry started leveraging *memcached,* which provided a
    means to store data in key-value pairs in memory via an API. For example, it was,
    and still is, common for MySQL databases to leverage the memcached API to cache
    objects in memory to optimize read speeds as well as reduce the load on the primary
    (MySQL) database.
  prefs: []
  type: TYPE_NORMAL
- en: However, as data volumes started to increase, the complexity of using the database
    and memcached method started to take it's toll, and databases that were exclusively
    designed to store data in memory (and sometimes both on disk and in memory) were
    being developed at a rapid pace.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, in-memory databases such as Redis started replacing memcached as
    the fast cache store for driving websites. In the case of Redis, although the
    data would be held in memory as key-value pairs, there was an option to persist
    the data on disk. This differentiated it from solutions such as memcached that
    were strictly memory caches.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary drivers of the move towards in-memory databases can be summarized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Complexity of managing increasing volumes of data such as web traffic by the
    traditional, for example, MySQL + memcached combination
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduced RAM costs, making it more affordable to purchase larger sizes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall industry drive towards NoSQL technologies that led to increased focus
    and community participation towards the development of newer, innovative database
    platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster data manipulation in memory provided a means to reduce I/O overhead in
    situations that demanded ultra-fast, low-latency processing of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Today, some of the leading options for databases that provide in-memory capabilities
    in the industry include:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Open source** | **Commercial** |'
  prefs: []
  type: TYPE_TB
- en: '| Redis | Kdb+ |'
  prefs: []
  type: TYPE_TB
- en: '| memcacheDB | Oracle TimesTen |'
  prefs: []
  type: TYPE_TB
- en: '| Aerospike | SAP HANA |'
  prefs: []
  type: TYPE_TB
- en: '| VoltDB | HP Vertica |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Ignite | Altibase |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Geode | Oracle Exalytics |'
  prefs: []
  type: TYPE_TB
- en: '| MonetDB | MemSQL |'
  prefs: []
  type: TYPE_TB
- en: Note that some of these support hybrid architectures whereby data can reside
    in memory as well as on disk. In general, data would be transferred from memory
    to disk for persistence. Also, note that some commercial in-memory databases offer
    community editions that can be downloaded and used at no charge within the terms
    of the licenses applicable to the respective solution. In these cases, they are
    both open source as well as commercial.
  prefs: []
  type: TYPE_NORMAL
- en: Columnar databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Columnar databases have existed since the 90s, but came to prominence after
    the release of Google Bigtable as mentioned earlier. They are, in essence, a method
    of storing data that is optimized for querying very large volumes of data in a
    fast and efficient manner relative to row-based/tuple-based storage.
  prefs: []
  type: TYPE_NORMAL
- en: The benefits of columnar databases, or more concretely storing each column of
    data independently, can be illustrated with a simple example.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a table consisting of 100 million household addresses and phone numbers.
    Consider also a simple query that requires the user to find the number of households
    in the state of New York, in the city of Albany, built after 1990\. We'll create
    a hypothetical table to illustrate the difference in querying the data row by
    row versus column by column.
  prefs: []
  type: TYPE_NORMAL
- en: '**Hardware characteristics**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Average disk read speed: 200 MB per second'
  prefs: []
  type: TYPE_NORMAL
- en: '**Database characteristics**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table name: `housedb`'
  prefs: []
  type: TYPE_NORMAL
- en: Total rows = 100 million
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total rows with State NY = Two million
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total rows with State NY and City Albany = 10,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total rows with State NY and City Albany and YearBuilt > 1990 = 500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data size**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us assume that the size of each of the data of each row is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: PlotNumber, YearBuilt each = 8 bytes = total 16 bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Owner, Address, State and City each = 12 bytes = Total 48 bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Net size in bytes of each row = 16 + 48 = 64 bytes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that the actual size will be higher, as there are several other considerations
    such as indexing and other table optimizations and related overheads that we won't
    consider here for the sake of simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: We will also assume that the columnar database maintains an implicit row index
    that permits querying the data at certain indices in each column *vector*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table shows the first 4 records:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PlotNumber** | **Owner** | **Address** | **State** | **City** | **YearBuilt**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | John | 1 Main St. | WA | Seattle | 1995 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Mary | 20 J. Ave. | NY | Albany | 1980 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Jane | 5 45^(th) St. | NY | Rye Brook | 2001 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | John | 10 A. Blvd. | CT | Stamford | 2010 |'
  prefs: []
  type: TYPE_TB
- en: 'In total, the table has 100 million records. The last few are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PlotNumber** | **Owner** | **Address** | **State** | **City** | **YearBuilt**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 99999997 | Jim | 23 B. Lane | NC | Cary | 1995 |'
  prefs: []
  type: TYPE_TB
- en: '| 99999998 | Mike | 5 L. Street | NY | Syracuse | 1993 |'
  prefs: []
  type: TYPE_TB
- en: '| 99999999 | Tim | 10 A. Blvd. | NY | Albany | 2001 |'
  prefs: []
  type: TYPE_TB
- en: '| 100000000 | Jack | 10 A. Blvd. | CT | Stamford | 2010 |'
  prefs: []
  type: TYPE_TB
- en: 'The query we will run against this dataset is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Scenario A: Searching row by row**'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first scenario, if we did a naÃ¯ve row-by-row search, since the data
    for each column is not stored separately, but the data for each row is scanned,
    we would have to query across:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 million * 64 bytes (size of each row in bytes) = 6,400 million bytes = approximately
    6000 MB of data
  prefs: []
  type: TYPE_NORMAL
- en: At a disk read speed of say, 200 MBps, this means it would take approximately
    6000 / 200 = 30 seconds to read all the records to find the matching entries.
  prefs: []
  type: TYPE_NORMAL
- en: '**Scenario B: Searching column by column**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming each column of data resides in individual files representing the respective
    columns, we will look each where clause individually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Where clause part 1**: `where State like ''NY''`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The State column, as described earlier, has 100 million entries each of size
    12 bytes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we only need to search across:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 million * 12 bytes = 1,200 million bytes = 1,000 MB of data.
  prefs: []
  type: TYPE_NORMAL
- en: At a data read rate of 200 MBps, this would take 200 MB, and it would take 1000
    / 200 = 5 seconds to read the column of data.
  prefs: []
  type: TYPE_NORMAL
- en: This returns two million records (as noted earlier database characteristics)
  prefs: []
  type: TYPE_NORMAL
- en: '**Where clause part 2**: `City like ''Albany''`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding step, we had narrowed our window of search to two million records
    that satisfied the criteria of State NY. In the second where clause step, now,
    we need not query across all 100 million records. Instead, we can simply look
    at the two million records that satisfied the criteria to determine which ones
    belong to City Albany.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we only need to search across:'
  prefs: []
  type: TYPE_NORMAL
- en: '*2 million * 12 bytes = 24 million bytes = approximately 20 MB of data*.'
  prefs: []
  type: TYPE_NORMAL
- en: At a data read rate of 200 MBps, this would take 0.1 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: This returns 10,000 records (as noted earlier in Database Characteristics).
  prefs: []
  type: TYPE_NORMAL
- en: '**Where clause part 3**: `YearBuilt > 1990`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the preceding step, we further narrowed our window of search to 10,000 records
    fulfilling both the criteria of State NY and City Albany. In this step, we will
    query 10,000 records in the YearBuilt column to find which ones fulfil the criteria
    of YearBuilt > 1990.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we only need to search across:'
  prefs: []
  type: TYPE_NORMAL
- en: '*10,000 * 16 bytes = 160,000 bytes = approximately 150 KB of data*.'
  prefs: []
  type: TYPE_NORMAL
- en: At a data read rate of 200 MBps, this would take 0.00075 seconds, which we can
    round to zero seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the net time spent in querying across the data was:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Where clause part 1: `where State like ''NY''` - five seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where clause part 2: `City like ''Albany''` - 0.1 seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Where clause part 3: `YearBuilt > 1990` - zero seconds'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Net time taken to read the data = 5.1 seconds.
  prefs: []
  type: TYPE_NORMAL
- en: 'Important: Note that the actual read or more specifically, scan performance,
    depends on various other factors. The **size of the tuple** (row), the time to
    reconstruct the tuple (**tuple reconstruction**), **bandwidth of memory** (how
    fast data can be read into the CPU from Main Memory, and so on), **cache line
    size** and other factors. In practice, there would be various levels of abstractions
    due to which the actual performance may be slower. Further there are other considerations
    such as hardware architecture and parallel operations that can affect positively
    or otherwise the overall performance. These topics are more advanced and require
    dedicated reading.Â The analysis here focuses exclusively on the disk I/O, which
    is one of the critical aspects of overall performance at a high-level.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example demonstrates the benefits of querying data that has been
    stored in columns from a query performance or efficiency perspective based on
    the size of the data. There is also another benefit offered by columnar data,
    which is that it allows storage of tables that may have arbitrary schema in columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the first four rows of the prior table. If, for example, we had missing
    information in some of the rows, that would lead to sparse columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PlotNumber** | **Owner** | **Address** | **State** | **City** | **YearBuilt**
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | John | 1 Main St. | *NULL* | Seattle | 1995 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Mary | 20 J. Ave. | NY | *NULL* | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Jane | *NULL* | NY | Rye Brook | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | John | 10 A. Blvd. | CT | *NULL* | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: 'Instead of populating NULL values, we can instead create a `Column Family`
    called `Complete_Address` that can contain an arbitrary number of key-value pairs
    corresponding to only those fields that have corresponding data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PlotNumber** | **Owner** | **Complete_Address** |  | **YearBuilt** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | John | Address: 1 Main St. | City: Seattle | 1995 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Mary | Address: 20 J. Ave. | State: NY | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Jane | State: NY | City: Rye Brook | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | John | Address: 10 A. Blvd. | State: CT | *NULL* |'
  prefs: []
  type: TYPE_TB
- en: 'A third and very important benefit offered by columnar databases is the ability
    to retrieve data based on three keys: a row key, a column key, and a timestamp
    that uniquely identifies each record, permitting very fast access to the data
    in question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, since the Owner field can change when the property (PlotNumber)
    is sold, we can add another field that denotes the date of the record; that is,
    the date that the record corresponds to. This would allow us to distinguish among
    properties that had a change of ownership whilst all the other data remained the
    same:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **PlotNumber** | **Owner** | **Address** | **State** | **City** | **YearBuilt**
    | **RecordDate** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | John | 1 Main St. | WA | Seattle | 1995 | 2001.04.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Mary | 20 J. Ave. | NY | Albany | 1980 | 2007.05.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Jane | 5 45^(th) St. | NY | Rye Brook | 2001 | 2001.10.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | John | 10 A. Blvd. | CT | Stamford | 2010 | 2003.07.20 |'
  prefs: []
  type: TYPE_TB
- en: 'Since there can be multiple records for each PlotNumber to accommodate change
    of ownership, we can now define three keys that could uniquely identify each cell
    of data in each record, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Row key: `PlotNumber`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Column key: The column name'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Timestamp key: `RecordDate`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each cell in each record in the table will thus have a unique three-value pair
    that distinguishes it from the other cells.
  prefs: []
  type: TYPE_NORMAL
- en: Databases such as Bigtable, Cassandra, and others employ this method to perform
    data analysis at scale both expeditiously and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the popular columnar databases are listed as follows. Note that there
    may be repetitions as databases can have multiple NoSQL properties (such as both
    in-memory and columnar):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Open source** | **Commercial** |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Parquet | Kdb+ |'
  prefs: []
  type: TYPE_TB
- en: '| MonetDB | Teradata |'
  prefs: []
  type: TYPE_TB
- en: '| MariaDB | SAP HANA |'
  prefs: []
  type: TYPE_TB
- en: '| Druid | HP Vertica |'
  prefs: []
  type: TYPE_TB
- en: '| HBase | Oracle Exadata |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Kudu | ParAccel |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Arrow | Actian Vector |'
  prefs: []
  type: TYPE_TB
- en: Document-oriented databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Document-based or document-oriented** databases became prominent as a means
    of storing data that had variable structures; that is, there was no fixed schema
    that each record would fit into all the time. Additionally, the document may have
    both a structured as well as an *unstructured* part.'
  prefs: []
  type: TYPE_NORMAL
- en: Structured data is, in essence, data that can be stored in a tabular format
    such as in a spreadsheet. Data stored in Excel spreadsheets or MySQL tables all
    belong to the class of structured datasets. Data that cannot be represented in
    a strict tabular format such as books, audio files, video files, or social network
    messages are considered unstructured data. As such, in document-oriented databases,
    we will primarily work with structured and unstructured text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'An intuitive explanation of data that can contain both structured and unstructured
    text can be found in the example of a **phone diary**. Although these have become
    increasingly rare with the growth of digital data storage, many of us would remember
    a time when phone numbers were written in pocketbooks. The following image shows
    how we store data in a phone diary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c2e56c2-7734-46a1-9bf9-4e35cd49d90a.png)'
  prefs: []
  type: TYPE_IMG
- en: Address Book (Semi-Structured Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding example, the following fields can be considered as structured:'
  prefs: []
  type: TYPE_NORMAL
- en: Name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Address
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tel and Fax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a line underneath the Address field where the user can enter arbitrary
    information, for example,Â met at a conference in 2015, works at company abc. This
    is essentially a note that the diary keeper wrote when entering the specific information.
    Since there is no defining characteristic of a free-form field such as this, it
    could also contain information such as a second phone number, or an alternative
    address and other information. This would qualify as an unstructured text.
  prefs: []
  type: TYPE_NORMAL
- en: Further, since the other fields are not interdependent, a user may write the
    address but not the phone number, or the name and phone number but not the address.
  prefs: []
  type: TYPE_NORMAL
- en: A document-oriented database, by virtue of its ability to store schema-free
    data; that is, data that does not conform to any fixed schema such as fixed columns
    with fixed datatypes, would hence be an appropriate platform to store this information.
  prefs: []
  type: TYPE_NORMAL
- en: As such, since a phone diary contains a much smaller volume of data, in practice,
    we could store it in other formats, but the necessity for document-oriented datasets
    becomes apparent when we are working with large-scale data containing both structured
    and unstructured information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the example of a phone diary, the data could be stored in a document-oriented
    dataset in JSON format, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**JSON**, which stands for **J**ava**S**cript **O**bject **N**otation, provides
    a means of representing data in a portable text-based key-value pair format. Today,
    data in JSON is ubiquitous across the industry and has become the standard in
    storing data that does not have a fixed schema. It is also a great medium to exchange
    structured data, and as such is used for such datasets frequently.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding illustration provides a basic example to convey how document-oriented
    databases work. As such, it is a very simple and hopefully intuitive example.
    In practice, document-oriented databases such as MongoDB and CouchDB are used
    to store gigabytes and terabytes of information.
  prefs: []
  type: TYPE_NORMAL
- en: For example, consider a website that stores data on users and their movie preferences.
    Each user may have multiple movies they have watched, rated, recommended, movies
    that they have added to their wishlist, and other such artifacts. In such a case,
    where there are various arbitrary elements in the dataset, many of which are optional
    and many of which might contain multiple values (for example, multiple movies
    recommended by a user), a JSON format to capture information becomes optimal.
    This is where document-oriented databases provide a superior and optimal platform
    to store and exchange data.
  prefs: []
  type: TYPE_NORMAL
- en: More concretely, databases such as MongoDB store information in BSON format
    - a binary version of JSON documents that have additional optimizations to accommodate
    datatypes, Unicode characters, and other features to improve upon the performance
    of basic JSON documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more comprehensive example of a JSON document stored in MongoDB could be
    data stored about airline passengers that contains information on numerous attributes
    specific to individual passengers, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Each entry is uniquely identified by the `_id` field, which allows us to directly
    query information relevant to the specific user and retrieve data without having
    to query across millions of records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, document-oriented databases are used to store a diverse range of datasets.
    Examples include the use of such Â the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Log files and log file-related information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Articles and other text-based published materials
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Geolocation data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User/user account-related information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many more use cases that are optimal for document/JSON based storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well-known document-oriented databases include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Open source** | **Commercial** |'
  prefs: []
  type: TYPE_TB
- en: '| MongoDB | Azure Cosmos DB |'
  prefs: []
  type: TYPE_TB
- en: '| CouchDB | OrientDB |'
  prefs: []
  type: TYPE_TB
- en: '| Couchbase Server | Marklogic |'
  prefs: []
  type: TYPE_TB
- en: Key-value databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Key-value databases** operate on the principle of structuring data as pairs
    of values corresponding to keys. To highlight the benefits of key-value databases,
    it would help to revisit the significance of hash maps, a common term prevalent
    in computer science to specify a unique data-structure that provides a constant-time
    lookup for key pairs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An intuitive example for a hash table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Consider a collection of 500 books and five bookcases. Each bookcase has five
    shelves. The books can be placed in an arbitrary order, but that would make it
    incredibly difficult to find a specific book and you may need to go through hundreds
    of books before locating the one you need. One method of categorizing the books
    would be to assign ranges of letters to each of the bookshelves, for example,
    A-E, F-J, K-O, P-T, U-Z, and use the first letter of the name of the book to assign
    it to a specific shelf. However, suppose you have a disproportionate number of
    books that start with the letters A-E. This means that the case assigned for A-E
    would have a much higher number of books relative to the other ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'A more elegant alternative could be to assign a value to each of the books
    and use the respective value to determine which bookcase or bookshelf the book
    belongs to. To assign a number, a specific value to each book, we could sum up
    the numbers corresponding to each letter of the title of the book using a range
    of 1-26 for the letters A-Z respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57f7faff-1435-44a3-978a-116c059ef15e.png)'
  prefs: []
  type: TYPE_IMG
- en: Our Simple Hash Map
  prefs: []
  type: TYPE_NORMAL
- en: Since we have five bookcases, each with five shelves, we have a total of 25
    shelves. One method of allocating a book to a specific shelf would be to take
    the numeric value of the book obtained by summing the letters in the title and
    dividing the value by 26\. Any number, when divided by 25, will yield a remainder
    between 0-25; that is, 26 unique values. We can use this value then to assign
    the book to a particular shelf. This then becomes our self-created hash function.
  prefs: []
  type: TYPE_NORMAL
- en: Of the 25 shelves, each of them is now assigned a numeric value corresponding
    to the values 0-25 respectively, with the last shelf being assigned the values
    24 and 25\. For example, shelf zero is assigned to store books whose numeric value
    divided by 26 yields zero, shelf one is assigned to store books whose numeric
    value divided by 26 yields one, and shelf 25 is assigned to store books whose
    numeric value divided by 26 yields 24 or 25.
  prefs: []
  type: TYPE_NORMAL
- en: An example will help to illustrate this concept more concretely.
  prefs: []
  type: TYPE_NORMAL
- en: 'Book name: **HAMLET**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Numeric value of title:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb17fd9d-b49f-409b-989b-963f5b7f7594.png)'
  prefs: []
  type: TYPE_IMG
- en: Hash values
  prefs: []
  type: TYPE_NORMAL
- en: Sum total of the numeric value = 8 + 1 + 13 + 12 + 5 + 20 = 59
  prefs: []
  type: TYPE_NORMAL
- en: Divide number by 26 = 2, remainder seven
  prefs: []
  type: TYPE_NORMAL
- en: Hence, the book is assigned to shelf number seven.
  prefs: []
  type: TYPE_NORMAL
- en: We have essentially found a way to methodically assign a shelf to each individual
    book, and because we have a fixed rule, when a new request for a book arrives,
    we can find it almost instantaneously since we will know the shelf corresponding
    to the book.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding method illustrates the concept of hashing, and in practice, we
    would use a hash function that would find a unique value for each book, and assuming
    we could get an arbitrary number of bookshelves and slots in which we can place
    the books, we could simply use the plain numeric value of the book to identify
    which shelf it would belong to.
  prefs: []
  type: TYPE_NORMAL
- en: There would be cases where two books would have the same numeric value, and
    in those cases we could stack the books in the slot corresponding to the number.
    In computer science, this effect of multiple values corresponding to a key is
    known as a collision, and in those cases we would assign multiple items by means
    of a list or similar datatype.
  prefs: []
  type: TYPE_NORMAL
- en: In real-life use cases, we have much more complex items to work with than the
    simple example of books. Generally, we'd use more complex hash functions that
    lower the chance of collision and accordingly assign the key-value pair. The data
    would be stored in a contiguous array in memory and hence, when a request for
    a certain key arrived, we could instantaneously find the value by using the hash
    function to identify the location in memory where the data resides.
  prefs: []
  type: TYPE_NORMAL
- en: Hence, using key-value pairs to store data can be immensely powerful because
    the time to retrieve information corresponding to a key can be very fast as there
    is no need to search through a long list to identify a matching key.
  prefs: []
  type: TYPE_NORMAL
- en: Key-value databases employ the same principle of assigning unique keys to each
    record, and the data corresponding to each key is stored in the corresponding
    location. In our discussion of MongoDB, we saw that records were assigned a certain
    key identified by the `_id` value in each record. In practice, we could use this
    value to retrieve the corresponding data in constant time.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before, memcached used to be the preferred method to store data
    in key-value pairs for web services that required very fast access to frequently
    used data. In essence, it served as a memory cache to store temporary information.
    With the advent of NoSQL databases, new platforms that extended the limited use
    case of memcached became prominent. Solutions such as Redis offered not only the
    ability to store data in key-value pairs in memory, but also the ability to persist
    the data on disk. In addition, these key-value stores supported horizontal scaling,
    which permitted the distribution of key-value pairs across hundreds of nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantage of key-value storage was that the data could not be queried
    with the same flexibility as standard databases, which supported multiple levels
    of indexing and a more richer set of SQL commands. Nevertheless, the benefits
    of constant time lookup implied that for use cases that required a key-value structure,
    there were few other solutions that were comparable in both performance and efficiency.
    For instance, a shopping website with thousands of users could store user profile
    information in a key-value database and be able to look up individual information
    by simply applying a hash function corresponding to, for example, the user ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today, key-value databases use a variety of methods to store data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SSTables**: A file of sorted key-value pairs represented as strings (and
    directly mapped to the **Google File System** (**GFS**)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**B-trees**: Balanced trees where values are identified by traversing along
    leaves/nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bloom filters**: A more optimal key-value method used when the number of
    keys is high. It uses multiple hash functions to set the bit-value to one in an
    array corresponding to keys.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shards**: A process involving partitioning data across multiple nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Well known key-value databases include:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Open source** | **Commercial** |'
  prefs: []
  type: TYPE_TB
- en: '| Redis | Amazon DynamoDB |'
  prefs: []
  type: TYPE_TB
- en: '| Cassandra | Riak |'
  prefs: []
  type: TYPE_TB
- en: '| Aerospike | Oracle NoSQL |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Ignite | Azure Cosmos DB |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Accumulo | Oracle Berkeley DB |'
  prefs: []
  type: TYPE_TB
- en: Graph databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Graph databases** provide an efficient representation of data with records
    that have inter-relationships. Typical examples are your social network friend
    list, LinkedIn contacts, Netflix movie subscribers. By leveraging optimized algorithms
    for searching on tree-based/graph data structures, graph databases can locate
    information in a novel manner relative to other NoSQL solutions. In such a structure,
    discrete information and properties are represented as leaves, edges, and nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows an atypical representation of a network that can
    be queried to discover or find complex inter-relationships using a graph database.
    In practice, production graph databases contain millions of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9586b4cb-0fb0-4d1d-9a3b-71c89696d7f5.png)'
  prefs: []
  type: TYPE_IMG
- en: Graph Database
  prefs: []
  type: TYPE_NORMAL
- en: Although they are not as prevalent as other types of NoSQL database, graph-based
    platforms are used for business-critical areas. For instance, credit card companies
    use graph databases to find new products that an individual cardholder may be
    interested in by querying across millions of datapoints to assess purchasing behavior
    of other cardholders with similar purchasing patterns. Social network websites
    use graph databases to compute similarity scores, provide friend suggestions,
    and other related metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Well-known graph databases includeÂ the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Open source** | **Commercial** |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Giraph | Datastax Enterprise Graph |'
  prefs: []
  type: TYPE_TB
- en: '| Neo4j | Teradata Aster |'
  prefs: []
  type: TYPE_TB
- en: '| JanusGraph | Oracle Spatial and Graph |'
  prefs: []
  type: TYPE_TB
- en: '| Apache Ignite |  |'
  prefs: []
  type: TYPE_TB
- en: Other NoSQL types and summary of other types of databases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section described some of the commonly known NoSQL paradigms in use today.
    There are several other emerging platforms that have their own strengths and unique
    characteristics. A brief overview of some of them is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Type** | **Feature** |'
  prefs: []
  type: TYPE_TB
- en: '| Object-oriented databases | Databases that leverage concepts in object-oriented
    programming to store data represented as objects. |'
  prefs: []
  type: TYPE_TB
- en: '| Cloud databases | Databases offered by cloud vendors such as Amazon, Microsoft,
    and Google that are only available on their respective cloud platforms such as
    Amazon Redshift, Azure SQL Database, and Google BigQuery. |'
  prefs: []
  type: TYPE_TB
- en: '| GPU databases | A more recent entrant in the world of databases that leverage
    GPU (graphic processing unit) cards to process data. Examples include MapD, Kinetica,
    and others. |'
  prefs: []
  type: TYPE_TB
- en: '| FPGA-accelerated databases | With Intel soon announcing the release of new
    chips that would have embedded FPGAs, companies such as Baidu have started developing
    FPGA-accelerated systems that leverage FPGA processing power to improve SQL query
    performance. |'
  prefs: []
  type: TYPE_TB
- en: '| Stream processing/IoT databases | Databases, or more generally platforms,
    that are optimized for processing streaming data such as from medical devices
    and sensors. One of the most popular examples of such a system is Apache Storm.
    |'
  prefs: []
  type: TYPE_TB
- en: A question often asked is whether there is one NoSQL database that is optimal
    for all use cases. While the databases can have multiple features that support
    numerous elements of NoSQL systems (generally known as multi-modal databases),
    in practice, a single solution that performs universally well across a broad set
    of use cases is rare. In real-world use cases, companies generally implement more
    than one solution to meet data mining needs. In the next section, we will complete
    a few hands-on exercises with real-world datasets using NoSQL solutions discussed
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Nobel Laureates data with MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first exercise, we will use **MongoDB**, one of the leading document-oriented
    databases, to analyze Nobel Laureates from 1902-present. MongoDB provides a simple
    and intuitive interface to work with JSON files. As discussed earlier, JSON is
    a flexible format that allows representing data using a structured approach.
  prefs: []
  type: TYPE_NORMAL
- en: JSON format
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Consider the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Firstname** | **Lastname** | **Information** |'
  prefs: []
  type: TYPE_TB
- en: '| John | 15 | Subject: History, Grade B |'
  prefs: []
  type: TYPE_TB
- en: '| Jack | 18 | Subject: Physics, Grade A |'
  prefs: []
  type: TYPE_TB
- en: '| Jill | 17 | Subject: Physics, Grade A+ |'
  prefs: []
  type: TYPE_TB
- en: The Information field contains a column containing multiple values categorized
    under Subject and Grade. Such columns that contain multiple data are also known
    as columns with nested data.
  prefs: []
  type: TYPE_NORMAL
- en: Portability has been an important aspect of transferring data from one system
    to another. In general, ODBC connectors are used to transfer data between database
    systems. Another common format is CSV files with the data represented as comma-separated
    values. CSV files are optimal for structured data that doesn't contain more complex
    data structures such as nested values. In such cases, JSON provides an optimal
    and structured way to capture and preserve information using a key-value pair
    syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JSON representation, the table can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice that theÂ `Information`Â key contains two keys, `Subject` and `Grade`,
    with each having a corresponding value.
  prefs: []
  type: TYPE_NORMAL
- en: Today, most product developers and vendors accommodate the ingestion of JSON-formatted
    data. Also, due to the simple manner in which complex relationships can be expressed
    as well as exchanged in text format, JSON has become immensely popular across
    the world in the developer community.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB captures data in JSON format. It internally stores them in BSONâan optimized
    binary representation of the JSON data.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and using MongoDB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MongoDB is supported on all major platforms such as Windows, Linux, and OS X
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: The details for installing MongoDB can be found on their official website at
    [https://docs.mongodb.com/manual/installation/](https://docs.mongodb.com/manual/installation/).
    Note that we will be using the MongoDB Community Edition.
  prefs: []
  type: TYPE_NORMAL
- en: For our exercise, we will re-use the Linux CentOS environment from our Cloudera
    Hadoop Distribution VM.
  prefs: []
  type: TYPE_NORMAL
- en: The exercise is however not dependent on the platform on which you install MongoDB.
    Once the installation has been completed, you can execute the commands indicated
    in this chapter on any other supported platform. If you have access to a separate
    Linux machine, you can use that as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will visit some of the common semantics of MongoDB and also download two
    datasets to compute the highest number of Nobel Prizes grouped by continent. The
    complete dump of the Nobel Prize data on Nobel Laureates is available from [nobelprize.org](https://www.nobelprize.org).
    The data contains all the primary attributes of Laureates. We wish to integrate
    this data with demographic information on the respective countries to extract
    more interesting analytical information:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Download MongoDB**: MongoDB can be downloaded from [https://www.mongodb.com/download-center#community](https://www.mongodb.com/download-center#community).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To determine which version is applicable for us, we checked the version of
    Linux installed on the CDH VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the information, we have to use the CentOS version of MongoDB, and
    accordingly, following the instructions atÂ [https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/](https://docs.mongodb.com/manual/tutorial/install-mongodb-on-red-hat/),
    we installed the software, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the contents of the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa4ee91d-8d97-4306-bea2-00886989948f.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting up MongoDB repository
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in the following screenshot, type `Y` for Yes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9dcc8115-8f1b-44fa-99d6-a48a5e7e1455.png)'
  prefs: []
  type: TYPE_IMG
- en: Saving the .repo file
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the file as shown in the image as follows. This will now allow us to install
    `mongo-db`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73965396-a01a-47ff-9c5b-dcfda1f44c1b.png)'
  prefs: []
  type: TYPE_IMG
- en: Writing and Saving the .repo file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Open a new terminal and download the JSON data files as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79111799-378b-41db-b833-2ed975307233.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting Open Terminal from Terminal App on Mac OS X
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the file needs to be slightly modified. The code is shown in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89ea74b9-5871-4841-91b6-66b3003aaee8.png)'
  prefs: []
  type: TYPE_IMG
- en: Modifying the .json file for our application
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In order to combine the data in `laureate.json` with country-specific information,
    we need to download the `countryInfo.txt` from [geonames.org](http://geonames.org)Â 
    We will now download the second file that we need for the exercise, `country.json`.
    We will use both `laureates.json` and `country.json` for the exercise.
  prefs: []
  type: TYPE_NORMAL
- en: '`### country.json`: Download it from [http://www.geonames.org](http://www.geonames.org)
    (license: [https://creativecommons.org/licenses/by/3.0/](https://creativecommons.org/licenses/by/3.0/)).
    Modify the start and end of the JSON string to import into MongoDB as shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Query a field - find all Nobel Laureates who were born in the US and received
    a Nobel Prize in Physics. Note that here we have a nested field (category is under
    prizes as shown). Hence, we will use the dot notation as shown in the coming image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Image illustrating `category`, one of the nested fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aad94718-e2e9-47dc-9789-3c79f0eb94ba.png)'
  prefs: []
  type: TYPE_IMG
- en: Nested JSON Fields
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: There are many other operations that can be performed, but the intention of
    the prior section was to introduce MongoDB at a high level with a simple use case.
    The URLs given in this chapter contain more in-depth information on using MongoDB.
  prefs: []
  type: TYPE_NORMAL
- en: There are also several visualization tools in the industry that are used to
    interact with and visualize data stored in MongoDB collections using a point-and-click
    interface. A simple yet powerful tool called MongoDB Compass is available at [https://www.mongodb.com/download-center?filter=enterprise?jmp=nav#compass.](https://www.mongodb.com/download-center?filter=enterprise?jmp=nav#compass)
  prefs: []
  type: TYPE_NORMAL
- en: 'Navigate to the previously mentioned URL and download the version of Compass
    that is appropriate for your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a0c6779-586c-43e0-b01a-1218f24f78ec.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading MongoDB Compass
  prefs: []
  type: TYPE_NORMAL
- en: 'After installation, you''ll see a welcome screen. Click on Next until you see
    the main dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e5fdace-8e95-40e7-a0d8-5e2af7313798.png)'
  prefs: []
  type: TYPE_IMG
- en: MongoDB Compass Screenshot
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Performance to view the current status of MongoDB:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c0f2af9b-958b-4ee4-ab32-1168f5ccd99e.png)'
  prefs: []
  type: TYPE_IMG
- en: MongoDB Performance Screen
  prefs: []
  type: TYPE_NORMAL
- en: 'Expand the nobel database by clicking on the arrow next to the word on the
    left sidebar. You can click and drag on different parts of the bar charts and
    run ad hoc queries. This is very useful if you want to get an overall understanding
    of the dataset without necessarily having to run all queries by hand, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef06d694-d31f-458a-9c50-d9da40944e0a.png)'
  prefs: []
  type: TYPE_IMG
- en: Viewing our file in MongoDB Compass
  prefs: []
  type: TYPE_NORMAL
- en: Tracking physician payments with real-world data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Physicians and hospitals alike receive payments from various external organizations,
    such as pharmaceutical companies who engage sales representatives to not only
    educate practitioners on their products, but also provide gifts or payments in
    kind or otherwise. In theory, gifts or payments made to physicians are not intended
    to influence their prescribing behavior, and pharmaceutical companies adopt careful
    measures to maintain checks and balances on payments being made to healthcare
    providers.
  prefs: []
  type: TYPE_NORMAL
- en: In 2010, President Obama's signature **Affordable Care Act**Â (**ACA**), also
    known in popular parlance as Obamacare, went into effect. Alongside the ACA, a
    separate legislation known as the Sunshine Act made reporting items of monetary
    value (directly or indirectly) mandatory for pharmaceutical companies and other
    organizations. While such rules existed in the past, rarely were such rules available
    in the public domain. By making detailed payment records made to all physicians
    available publicly, the Sunshine Act introduced an unprecedented level of transparency
    in monetary dealings involving healthcare providers.
  prefs: []
  type: TYPE_NORMAL
- en: The data is freely available on the website of CMS Open Payments at [https://openpaymentsdata.cms.gov](https://openpaymentsdata.cms.gov).
  prefs: []
  type: TYPE_NORMAL
- en: The site provides an interface to query the data, but does not have any means
    to perform large-scale data aggregation. For example, if a user wanted to find
    the total payments made in the state of CT, there is no simple and easy way to
    run the query through the default web-based tool. An API that provides the functionality
    is available, but requires a degree of familiarity and technical knowledge to
    use effectively. There are third-party products that provide such facilities,
    but in most cases they are expensive, and end users cannot modify the software
    to their particular needs.
  prefs: []
  type: TYPE_NORMAL
- en: In this tutorial, we will develop a fast, highly efficient web-based application
    to analyze tens of millions of records that capture payments made to physicians
    in 2016\. We will be using a combination of a NoSQL database, R, and RStudio to
    create the final product - the web-based portal through which end users can query
    the database in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The technologies we will use to develop the application are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kdb+ NoSQL database: [http://www.kx.com](http://www.kx.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RStudio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the tutorial, I will be using the VM image we downloaded for our Hadoop
    exercise. The tools can also be installed on Windows, Mac, and other Linux machines.
    The choice of the VM is mainly to provide a consistent and local OS independent
    platform.
  prefs: []
  type: TYPE_NORMAL
- en: Installing kdb+, R, and RStudio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Packt Data Science VM download has been provided,Â which contains all the necessary
    software required for this chapter. However, if you prefer to install the software
    on your local machine instead, instructions, have been provided in the following
    sections. You can skip the installation sections and proceed directly to the section
    on *Developing the Open Payment Application.*
  prefs: []
  type: TYPE_NORMAL
- en: Installing kdb+
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**kdb+** is a time-series, in-memory, columnar database that has been used
    in the financial industry for almost 20 years. It is one of the fastest database
    platforms available for performing large-scale data mining, but one that is not
    as well-known as other NoSQL tools due to the fact that it has been used almost
    exclusively by hedge funds and investment banks for most of its existence. In
    particular, due to its speed and low overhead in processing vast amounts of data,
    it is used by algorithmic trading desks that engage in high-frequency trading.'
  prefs: []
  type: TYPE_NORMAL
- en: With kdb+, it is fairly simple to analyze tens of millions and even hundreds
    of millions of records on a laptop. The main constraints would be at a hardware
    level - such as the amount of memory, disk space, and CPU that is available to
    process the data. In this tutorial, we will install the free 32-bit edition of
    kdb+ available for non-commercial use.
  prefs: []
  type: TYPE_NORMAL
- en: kdb+ is not open source, but academic institutes can use the 64-bit license
    at no charge by writing to `academic@kx.com`.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are certain key characteristics of kdb+ that make it very well suited
    to large-scale data analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-level implementation**: The database is written in C, thus reducing common
    causes of performance issues with most contemporary NoSQL databases that rely
    heavily on Java, which implements multiple layers of abstraction to provide processing
    capabilities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Architectural simplicity**: The entire binary for the kdb+ database is about
    500-600 KB. This is a fraction of the size of an MP3 song and can be easily downloaded
    even on a dial-up connection'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MapReduce**: The database implements an internal MapReduce process that allows
    queries to execute across multiple cores simultaneously'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**No installation**: The database requires no system-level privileges and users
    can start using kdb+ with their user account on most systems'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enterprise-ready**: The database has been used for nearly 20 years and is
    a very mature product used in global enterprise environments for analysis of high-frequency
    trading data among otherÂ applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wide availability of interfaces**: The database has a wide range of interfaces
    for languages such as C, C++,C#, Java, R, Python, MATLAB, and others to allow
    easy integration with existing software'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The steps to install kdb+ are given as follows. Please note that if you are
    using the Packt Data Science VM, no additional installation is necessary. The
    instructions have been provided primarily for users who would like to install
    the software afresh.
  prefs: []
  type: TYPE_NORMAL
- en: Although the instructions are for Linux, the installation process is also quite
    simple for both Windows and Macs. The instructions herein are geared towards the
    Packt Data Science VM. The instructions for downloading the Packt Data Science
    VM was provided in [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml),Â *The
    Analytics Toolkit*
  prefs: []
  type: TYPE_NORMAL
- en: 'Visit [www.kx.com](http://www.kx.com) and click on the **Download** drop-down
    option from the Connect with us menu item. You may also directly go to the download
    page located at [https://kx.com/download/](https://kx.com/download/):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e9289227-5a28-4ab9-904b-c657bd510ec5.png)'
  prefs: []
  type: TYPE_IMG
- en: Kx Systems Homepage
  prefs: []
  type: TYPE_NORMAL
- en: 'The download page is as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b9d30ed-0252-4ce2-a401-54f17703e792.png)'
  prefs: []
  type: TYPE_IMG
- en: Downloading KDB+
  prefs: []
  type: TYPE_NORMAL
- en: Click on Download on the next page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You'll be taken to [https://kx.com/download/](https://kx.com/download/) where
    you can select the respective download of your choice after agreeing to the terms.
    If you are using the VM, download the *Linux-86 version*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Select Save File to save the downloaded ZIP file in your Downloads folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d2c2ea7a-99f1-4c01-8108-f1c3022ad91a.png)'
  prefs: []
  type: TYPE_IMG
- en: KDB+ 32-bit license terms
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the folder where the file was downloaded and copy the ZIP file under
    your home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/579d8cf0-22ad-4b55-8b29-c28309916eaa.png)'
  prefs: []
  type: TYPE_IMG
- en: KDB+ Zip file download
  prefs: []
  type: TYPE_NORMAL
- en: 'For Mac or Linux systems, this will be the `~/` folder. In Windows, copy the
    ZIP file under `C:\` and unzip to extract the `q` folder. The following instructions
    are mainly for Linux-based systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Installing R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The frontend of the application will be developed using R. There are three
    options for installing R to complete the tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have installed Microsoft R from [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml),Â *The
    Analytics Toolkit*,Â and will be using your local machine for the tutorial, no
    further installation is necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alternatively, if you will be using the Packt Data Science Virtualbox VM, no
    further installation will be needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you plan to install R from the official R website, the binary can be downloaded
    from any of the download sites (mirrors) listed at [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7f9ebd70-ea4f-476f-b714-919b06cc8d90.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing Open Source R
  prefs: []
  type: TYPE_NORMAL
- en: Installing RStudio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will use RStudio in order to build our web-based application. You can either
    download the binary for RStudio from the website or install it from the terminal.
    RStudio is available in two versions - RStudio Desktop and RStudio Server. Both
    versions can be used to build the application. The Server version provides an
    interface that can be used by multiple users, whereas the Desktop version is generally
    used locally on the user's machine.
  prefs: []
  type: TYPE_NORMAL
- en: The instructions also appear in [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml),Â *The
    Analytics Toolkit*. They have been provided here for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two methods to complete the installation for the R tutorial:'
  prefs: []
  type: TYPE_NORMAL
- en: If you will be using the Packt Data Science VM, no further installation is necessary.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you will be using your local machine for the tutorial, you can download RStudio
    Desktop from [https://www.rstudio.com/products/rstudio/download/#download](https://www.rstudio.com/products/rstudio/download/#download)
    or RStudio Server (only for Linux users) from [https://www.rstudio.com/products/rstudio/download-server/](https://www.rstudio.com/products/rstudio/download-server/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following instructions have been provided for users wishing to download
    RStudio from the vendor''s website and perform a fresh installation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to the website of [https://www.rstudio.com](https://www.rstudio.com)Â and
    click on **Products** | **RStudio**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e837fc9d-1779-490c-b54c-b67d629a5459.png)'
  prefs: []
  type: TYPE_IMG
- en: Open Source R Studio Desktop Versions
  prefs: []
  type: TYPE_NORMAL
- en: 'On the RStudio page, click on **Download RStudio Desktop**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5f21797-de0a-45d2-9ccc-60f722008cee.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting RStudio Desktop
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the free version of RStudio Desktop:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84689f5f-ffdb-45d6-a79c-c82d51286f1d.png)'
  prefs: []
  type: TYPE_IMG
- en: Selecting Open Source R Studio Desktop
  prefs: []
  type: TYPE_NORMAL
- en: RStudio is available for Windows, Mac, and Linux.
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the appropriate executable for your system and proceed to perform
    the installation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28deddcd-3a2f-41fd-9a98-f972e2431336.png)'
  prefs: []
  type: TYPE_IMG
- en: RStudio Binaries (Versions)
  prefs: []
  type: TYPE_NORMAL
- en: The CMS Open Payments Portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will begin developing our application for CMS Open Payments.
  prefs: []
  type: TYPE_NORMAL
- en: The Packt Data Science VM contains all the necessary software for this tutorial.
    To download the VM, please refer to [Chapter 3](5ca02405-8ab4-4274-8611-af003aab7c9f.xhtml),Â *The
    Analytics Toolkit*.
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the CMS Open Payments data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CMS Open Payments data is available directly as a web-based download from
    the CMS website. We''ll download the data using the Unix wget utility, but first
    we have to register with the CMS website to get our own API key:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Go to [https://openpaymentsdata.cms.gov](https://openpaymentsdata.cms.gov)
    and click on the Sign In link at the top-right of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bbfb3158-5088-41d9-b493-9e5a32e32215.png)'
  prefs: []
  type: TYPE_IMG
- en: Homepage of CMS OpenPayments
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Sign Up**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe7d94fb-fb98-4fa3-8c90-8985828bfdd3.png)'
  prefs: []
  type: TYPE_IMG
- en: Sign-Up Page on CMS OpenPayments
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter your information and click on the **Create My Account** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57d32c92-42e6-4e9c-a8b7-dcdde3218f79.png)'
  prefs: []
  type: TYPE_IMG
- en: Sign-Up Form for CMS OpenPayments
  prefs: []
  type: TYPE_NORMAL
- en: '**Sign In** to your account:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b96353c-5ee6-41a9-86d9-73976eb3c805.png)'
  prefs: []
  type: TYPE_IMG
- en: Signing into CMS OpenPayments
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Manage** under **Packt Developer''s Applications**. Note that Applications
    here refers to apps that you may create that will query data available on the
    CMS website:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f7312ecd-95c0-4b9e-8a99-d3fc66d3d842.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating 'Applications'
  prefs: []
  type: TYPE_NORMAL
- en: 'Assign a name for the application (examples are shown in the following image):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/936b7e4a-ffa6-4c41-96ca-a5e6d695f7e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Defining an application
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll get a notification that the **Application Token** has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/457be983-e31e-4bd4-9435-8509fdb84e4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating the Application Token
  prefs: []
  type: TYPE_NORMAL
- en: 'The system will generate an **App Token**. Copy the **App Token**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0db9fc01-86f3-4a94-bb24-b8aed05f5c32.png)'
  prefs: []
  type: TYPE_IMG
- en: The Application Token
  prefs: []
  type: TYPE_NORMAL
- en: Now, log in to the Packt Data Science VM as user packt and execute the following
    shell command after replacing the term `YOURAPPTOKEN` with the one that you were
    assigned (it will be a long string of characters/numbers). Note that for the tutorial,
    we will only download a few of the columns and restrict the data to only physicians
    (the other option is hospitals).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can reduce the volume of the data downloaded by reducing the value of the
    limit specified at the end of the command to a lower number. In the command, we
    have used `12000000` (12 million), which would let us download the entire 2016
    dataset representing physician payments. The application will still work if, for
    example, you were to download only one million entries instead of the approximately
    11-12 million records.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: Two approaches are shown below. One using the Token and the other without
    using the Token. Application Tokens allow users to have a higher throttling limit.
    More information can be found atÂ [https://dev.socrata.com/docs/app-tokens.html](https://dev.socrata.com/docs/app-tokens.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Important**: It is possible to also download the file without using an app
    token. However, the method should be used sparingly. The URL to download the file
    without using an application token is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Creating the Q application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the process of creating the kdb+/Q application, beginning
    with the process of loading data from the database and creating the scripts that
    will serve as the backend for the application.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Log in to the VM using the ID `packt` (password: `packt`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a4e6913-3fe7-45d7-8d12-6b2bdb2f5699.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging into the Packt VM
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The backend code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the script completes, exit from the Q prompt by typing in `\\` and pressing
    *Enter*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy the following text into a file called `cms.q`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Creating the frontend web portal
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**R Shiny**, a package intended to make development of web-based applications
    simple, started gaining traction since it was introduced in around 2012-2013\.
    In general, R developers tend not to be very frontend development savvy as their
    main areas of work would be related to statistics or similar disciplines.'
  prefs: []
  type: TYPE_NORMAL
- en: As data science, as a profession and a mainstream activity became popular, the
    need to create sophisticated web-based applications became necessary as a means
    of delivering results to end users in a dynamic environment.
  prefs: []
  type: TYPE_NORMAL
- en: JavaScript, which had all but lost its original appeal, made a surprise comeback
    and soon enough the web world was abuzz with the release of various leading JavaScript
    packages for web development and visualization, such as D3, Angular, Ember, and
    others ever since 2010-2011.
  prefs: []
  type: TYPE_NORMAL
- en: But these were mostly used by seasoned JavaScript developers, few of whom were
    also proficient in R. Developing a solution that would help bridge the gap between
    JavaScript web-based application development and R programming became a necessity
    for R developers to showcase and share their work with a broader audience.
  prefs: []
  type: TYPE_NORMAL
- en: R ShinyÂ platform for developers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: R Shiny introduced a platform for R developers to create JavaScript-based web
    applications without having to get involved, or, for that,Â matter even be proficient
    in JavaScript.
  prefs: []
  type: TYPE_NORMAL
- en: In order to build our application, we will leverage R Shiny and create an interface
    to connect to the CMS Open Payments data we set up in the prior section.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using your own R installation (locally), you''ll need to install
    a few R packages. Note that if you are using a Linux workstation, you may need
    to install some additional Linux packages. For example, in Ubuntu Linux, you''ll
    need to install the following. You may already have some of the packages, in which
    case you''ll receive a message indicating that no further changes were needed
    for the respective package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you are using the Packt Data Science VM, you can proceed directly to developing
    the application as these Linux packages have already been installed for you.
  prefs: []
  type: TYPE_NORMAL
- en: The Shiny application requires a few additional R packages to provide all its
    functionalities. Note that R packages are different from the Linux packages described
    previously. R packages, which number in the thousands, provide specialized functions
    for specific subject areas. For the web application, we will install a few R packages
    that will let us leverage some of the features in the web-based application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps outline the process of creating the web portal:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in to RStudio. If you are using the Packt Data Science VM, go toÂ `http://localhost:8787/auth-sign-in`.
    Log in with the user IDÂ **packt**Â and passwordÂ **packt**Â (same as user ID).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that if you had installed RStudio locally, you''ll not have a separate
    login screen. The instruction is purely for the Packt Data Science VM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12c68685-ff73-4579-a21c-98849e8b3b39.png)'
  prefs: []
  type: TYPE_IMG
- en: Logging into RStudio Server (Only for Packt VM)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you receive an error message stating that the site cannot be loaded, it
    may be due to the fact that the port forwarding has not been set up. To fix the
    issue, make the following changes:'
  prefs: []
  type: TYPE_NORMAL
- en: In VirtualBox, right-click on theÂ VMÂ and selectÂ Settings.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Click onÂ Network under SettingsÂ and expand the arrow next to **Advanced**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/06f21a60-4570-4f94-899f-56ac4a3252c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Setting up the VM parameters
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Port Forwarding and add a rule to forward port 8787 from the VM to
    the host. The rule marked as Packt Rule has to be added, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/6b314a56-7f40-446f-91be-e897722d4fd5.png)'
  prefs: []
  type: TYPE_IMG
- en: Configuring Port Forwarding
  prefs: []
  type: TYPE_NORMAL
- en: 'After logging in, you''ll see the following screen. This is the interface for
    RStudio, which you''ll be using to complete the exercise. We''ll discuss R and
    RStudio in much more detail in later chapters, and this section illustrates the
    process to create the basic web application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c1d7dd91-08c0-4587-beff-d4778a0b216b.png)'
  prefs: []
  type: TYPE_IMG
- en: The RStudio Console
  prefs: []
  type: TYPE_NORMAL
- en: Install the necessary R packages. Click onÂ FileÂ |Â R ScriptÂ and copy and paste
    the code below.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, click onÂ SourceÂ to execute the following lines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c6b69841-8844-46fc-8402-ce3d0ce6b64c.png)'
  prefs: []
  type: TYPE_IMG
- en: Installing required packages in R via RStudio
  prefs: []
  type: TYPE_NORMAL
- en: 'Click onÂ File|New File|Shiny Web App:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '>![](img/c8c2361f-3059-4d7b-a1cf-b050cac17ceb.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new RShiny Application
  prefs: []
  type: TYPE_NORMAL
- en: 'Type in `cmspackt` underÂ application nameÂ and click onÂ Create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1ae74872-4594-43a0-8ced-0261aee782cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Assigning a name to the RShiny Application
  prefs: []
  type: TYPE_NORMAL
- en: 'This will create a `cmspackt` folder in the home directory, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d1c3d4f-f324-4da0-b296-a15bd122852f.png)'
  prefs: []
  type: TYPE_IMG
- en: The app.R file for the R Shiny Application
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy and paste the following code into theÂ `app.R`Â section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Click onÂ New FolderÂ in the lower-right box:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a039acf3-9819-4b74-928f-9e1cd6368a76.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating a folder for CSS files
  prefs: []
  type: TYPE_NORMAL
- en: 'Rename the new folder toÂ `cmspackt/www`,Â shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/607404fd-4ad4-4764-a04c-e27d0ebff3a3.png)'
  prefs: []
  type: TYPE_IMG
- en: Assigning a name to the folder
  prefs: []
  type: TYPE_NORMAL
- en: 'Click onÂ FileÂ |Â New FileÂ |Text File:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/27544c53-5a66-4238-ae1a-8ae9de529128.png)'
  prefs: []
  type: TYPE_IMG
- en: Creating the CSS File
  prefs: []
  type: TYPE_NORMAL
- en: 'Copy and paste the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Click on File |Â Save As to save the file, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/3cb929de-b700-4f88-b88e-124f2689ec3b.png)'
  prefs: []
  type: TYPE_IMG
- en: Select Save As for the CSS File
  prefs: []
  type: TYPE_NORMAL
- en: 'Save as `/home/packt/cmspackt/www/packt.css`, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1ed14459-0a0a-4fc0-aaea-c89a574cf876.png)'
  prefs: []
  type: TYPE_IMG
- en: Saving the CSS File
  prefs: []
  type: TYPE_NORMAL
- en: Your application is now ready for use!
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together - The CMS Open Payments application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the prior sections, we have learned how to:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the backend database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the code for the backend database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set up RStudio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create the R Shiny application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To start the application, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the Q application, make sure you are in the home directory. Type pwd and
    hit Enter. This will show the present working directory of `/home/packt` as shown
    in the coming image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, type `q` and hit Enter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the `q` prompt, type in `\l cms.q`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note that `cms.q` is the file we created in our earlier section when developing
    the Q application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script will load the database and return back to the `q)` prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21059576-46c9-4666-bdb7-d8b5c1c04210.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Putting it all together: Loading the CMS KDB+ Q Script in KDB+ Session'
  prefs: []
  type: TYPE_NORMAL
- en: Launch the CMS Open Payment application
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In RStudio, open theÂ `app.R`Â file (which contains the R Code) and click on
    Run App at the top-right, shown as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/58e42772-85d4-407f-af98-f324ec235ade.png)'
  prefs: []
  type: TYPE_IMG
- en: Running the RShiny Application
  prefs: []
  type: TYPE_NORMAL
- en: 'This will launch the web application, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63e5338e-1ad6-4996-94ff-b1eff05d1ba9.png)'
  prefs: []
  type: TYPE_IMG
- en: The RShiny Application
  prefs: []
  type: TYPE_NORMAL
- en: 'We have now finished developing a complete CMS Open Payments application that
    allows the end user to filter, aggregate, and analyze the data. Now, you can run
    queries by selecting various options on the screen. There are two functionalities
    in the app:'
  prefs: []
  type: TYPE_NORMAL
- en: Filtering data (default view)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating data (you can switch to this option by selecting Aggregate Data
    from the Display Type menu)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**A filtering example**: To see payments made by a company for a certain drug
    in the state of NY:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3be31cd1-a953-4cc6-a6ad-2e29e2071aeb.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the RShiny Application
  prefs: []
  type: TYPE_NORMAL
- en: Note that the system processed 11 million records in 21 milliseconds, as shown
    in the header message. The name of the company and the product has been blanked
    out in the screenshot for privacy, but you are free to try out different options
    for both fields.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in the default VM, we are using only one core with very limited memory,
    and the speed with which the data is processed using kdb+ even on a laptop with
    very limited resources easily exceeds the performance of many well-to-do commercial
    solutions.
  prefs: []
  type: TYPE_NORMAL
- en: '**An aggregation example**: To see total payments grouped by state, payment
    category, and payment nature for a specific company and product, select the options
    for the fields *Aggregate Data* and *Calculate Metrics*. Please note that the
    name of the company and the product have been hidden in the screenshot for privacy
    reasons only.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note the message at the top that states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5abd0a19-3bf8-45ff-a9aa-9d41ab2e3125.png)'
  prefs: []
  type: TYPE_IMG
- en: Log message indicating query and application performance
  prefs: []
  type: TYPE_NORMAL
- en: This indicates the speed with which the underlying kdb+ database processed the
    data. In this case, it filtered and *aggregated 11 million records in 22 milliseconds*
    for the given options.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d5b097a-918a-4613-a01d-8572efd32706.png)'
  prefs: []
  type: TYPE_IMG
- en: CMS OpenPayments Application Screenshot
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduced the concept of NoSQL. The term has gained popularity
    in recent years, especially due to its relevance and direct application to **big
    data** analytics. We discussed the core terminologies in NoSQL, their various
    types, and popular software used in the industry for such capabilities. We concluded
    with a couple of tutorials using MongoDB and kdb+.
  prefs: []
  type: TYPE_NORMAL
- en: We also built an application using R and R Shiny to create a dynamic web interface
    to interact with the data loaded in kdb+.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will introduce another common technology in data science today,
    known as Spark. It is yet another toolkit that empowers data scientists across
    the world today.
  prefs: []
  type: TYPE_NORMAL
