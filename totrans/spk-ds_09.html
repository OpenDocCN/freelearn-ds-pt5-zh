<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch09" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 9. Visualizing Big Data</h1></div></div></div><p class="calibre11">Proper data visualization has solved many business problems in the past without much statistics or machine learning being involved. Even today, with so many technological advancements, applied statistics, and machine learning, proper visuals are the end deliverables for business users to consume information or the output of some analyses. Conveying the right information in the right format is something that data scientists yearn for, and an effective visual is worth a million words. Also, representing the models and the insights generated in a way that is easily consumable by the business is extremely important. Nonetheless, exploring big data visually is very cumbersome and challenging. Since Spark is designed for big data processing, it also supports big data visualization along with it. There are many tools and techniques that have been built on Spark for this purpose.</p><p class="calibre11">The previous chapters outlined how to model structured and unstructured data and generate insights from it. In this chapter, we will look at data visualization from two broad perspectives-one is from a data scientist's perspective—where visualization is the basic need to explore and understand the data effectively, and the other is from a business user's perspective, where the visuals are end deliverables to the business and must be easily comprehendible. We will explore various data visualization tools such as <em class="calibre22">IPythonNotebook</em> and <em class="calibre22">Zeppelin</em> that can be used on Apache Spark.</p><p class="calibre11">As a prerequisite for this chapter, a basic understanding of SQL and programming in Python, Scala, or other such frameworks, is nice to have. The topics covered in this chapter are listed as follows:</p><div><ul class="itemizedlist"><li class="listitem">Why visualize data?<div><ul class="itemizedlist1"><li class="listitem">A data engineer's perspective</li><li class="listitem">A data scientist's perspective</li><li class="listitem">A business user's perspective</li></ul></div><p class="calibre31">
</p></li></ul></div><div><ul class="itemizedlist"><li class="listitem">Data visualization tools<div><ul class="itemizedlist1"><li class="listitem">IPython notebook</li><li class="listitem">Apache Zeppelin</li><li class="listitem">Third-party tools</li></ul></div><p class="calibre31">
</p></li><li class="listitem">Data visualization techniques<div><ul class="itemizedlist1"><li class="listitem">Summarizing and visualizing</li><li class="listitem">Subsetting and visualizing</li><li class="listitem">Sampling and visualizing</li><li class="listitem">Modeling and visualizing</li></ul></div><p class="calibre31">
</p></li></ul></div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec69" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Why visualize data?</h1></div></div></div><p class="calibre11">Data visualization deals with representing data in a visual form so as to enable people to understand the underlying patterns and trends. Geographical maps, the bar and line charts of the seventeenth century, are some examples of early data visualizations. Excel is perhaps a familiar data visualization tool that most of us have already used. All data analytics tools have been equipped with sophisticated, interactive data visualization dashboards. However, the recent surge in big data, streaming, and real-time analytics has been pushing the boundaries of these tools and they seem to be bursting at the seams. The idea is to make the visualizations look simple, accurate, and relevant while hiding away all the complexity. As per the business needs, any visualization solution should ideally have the following characteristics:</p><div><ul class="itemizedlist"><li class="listitem">Interactivity</li><li class="listitem">Reproducibility</li><li class="listitem">Control over the details</li></ul></div><p class="calibre11">Apart from these, if the solution allows users to collaborate over the visuals or reports and share with each other, then that would make up an end-to-end visualization solution.</p><p class="calibre11">Big data visualization in particular poses its own challenges because we may end up with more data than pixels on the screen. Manipulating large data usually requires memory- and CPU-intensive processing and may have long latency. Add real-time or streaming data to the mix and the problem becomes even more challenging. Apache Spark is designed from the ground up just to tackle this latency by parallelizing CPU and memory usage. Before exploring the tools and techniques to visualize and work with big data, let's first understand the visualization needs of data engineers, data scientists, and business users.</p><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec103" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>A data engineer's perspective</h2></div></div></div><p class="calibre11">Data engineers play a crucial role in almost every data-driven requirement: sourcing data from different data sources, consolidating them, cleaning and preprocessing them, analyzing them, and then the final reporting with visuals and dashboards. Their activities can be broadly stated as follows:</p><div><ul class="itemizedlist"><li class="listitem">Visualize the data from different sources to be able to integrate and consolidate it to form a single data matrix</li><li class="listitem">Visualize and find various anomalies in the data, such as missing values, outliers and so on (this could be while scraping, sourcing, ETLing, and so on) and get those fixed</li><li class="listitem">Advise the data scientists on the properties and characteristics of the dataset</li><li class="listitem">Explore various possible ways of visualizing the data and finalize the ones that are more informative and intuitive as per the business requirement</li></ul></div><p class="calibre11">Observe here that the data engineers not only play a key role in sourcing and preparing the data, but also take a call on the most suitable visualization outputs for the business users. They usually work very closely to the business as well to have a very clear understanding on the business requirement and the specific problem at hand.</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec104" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>A data scientist's perspective</h2></div></div></div><p class="calibre11">A data scientist's need for visualizing data is different from that of data engineers. Please note that in some businesses, there are professionals who play a dual role of data engineers and data scientists.</p><p class="calibre11">Data scientists need to visualize data to be able to take the right decisions in performing statistical analysis and ensure proper execution of the analytics projects. They would like to slice and dice data in various possible ways to find hidden insights. Let's take a look at some example requirements that a data scientist might have to visualize the data:</p><div><ul class="itemizedlist"><li class="listitem">See the data distribution of the individual variables</li><li class="listitem">Visualize outliers in the data</li><li class="listitem">Visualize the percentage of missing data in a dataset for all variables</li><li class="listitem">Plot the correlation matrix to find the correlated variables</li><li class="listitem">Plot the behavior of residuals after a regression</li><li class="listitem">After a data cleaning or transformation activity, plot the variable again and see how it behaves</li></ul></div><p class="calibre11">Please note that some of the things just mentioned are quite similar to the case of data engineers. However, data scientists could have a more scientific/statistical intention behind such analyses. For example, data scientists may see an outlier from a different perspective and treat it statistically, but a data engineer might think of the various options that could have triggered this.</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec105" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>A business user's perspective</h2></div></div></div><p class="calibre11">A business user's perspective is completely different from that of data engineers or data scientists. Business users are usually the consumers of information! They would like to extract more and more information from the data, and for that, the correct visuals play a key role. Also, most business questions are more complex and causal these days. The old-school reports are no longer enough. Let's look at some example queries that business users would like to extract from reports, visuals, and dashboards:</p><div><ul class="itemizedlist"><li class="listitem">Who are the high-value customers in so-and-so region?</li><li class="listitem">What are the common characteristics of these customers?</li><li class="listitem">Predict whether a new customer would be high-value</li><li class="listitem">Advertising in which media would give maximum ROI?</li><li class="listitem">What if I do not advertise in a newspaper?</li><li class="listitem">What are the factors influencing a customer's buying behavior?</li></ul></div></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec70" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data visualization tools</h1></div></div></div><p class="calibre11">Out of the many different visualization options, choosing the right visual depends on specific requirements. Similarly, selecting a visualization tool depends on both the target audience and the business requirement.</p><p class="calibre11">Data scientists or data engineers would prefer a more interactive console for quick and dirty analysis. The visuals they use are usually not intended for business users. They would like to dissect the data in every possible way to get more meaningful insights. So, they usually prefer a notebook-type interface that supports these activities. A notebook is an interactive computational environment where they can combine code chunks and plot data for explorations. There are notebooks such as <strong class="calibre19">IPython</strong>/<strong class="calibre19">Jupyter</strong> or <strong class="calibre19">DataBricks</strong>, to name a few available options.</p><p class="calibre11">Business users would prefer a more intuitive and informative visual that they can share with each other or use to generate reports. They expect to receive the end result through visuals. There are hundreds and thousands of tools, including some popular ones such as <strong class="calibre19">Tableau</strong>, that businesses use; but quite often, developers have to custom-build specific types for some unique requirements and expose them through web applications. Microsoft's <strong class="calibre19">PowerBI</strong> and open source solutions such as <strong class="calibre19">Zeppelin</strong> are a few examples.</p><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec106" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>IPython notebook</h2></div></div></div><p class="calibre11">The IPython/Jupyter notebook on top of Spark's <strong class="calibre19">PySpark</strong> API is an excellent combination for data scientists to explore and visualize the data. The notebook internally spins up a new instance of the PySpark kernel. There are other kernels available; for example, the Apache <strong class="calibre19">Toree</strong> kernel can be used to support Scala as well.</p><p class="calibre11">For many data scientists, it is the default choice because of its capability of integrating text, code, formula, and graphics in one JSON document file. The IPython notebook supports <code class="literal">matplotlib</code>, which is a 2D visualization library that can produce production-quality visuals. Generating plots, histograms, scatterplots, charts, and so on becomes easy and simple. It also supports the <code class="literal">seaborn</code> library, which is actually built upon matplotlib but is easy to use as it provides higher level abstraction and hides the underlying complexities.</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec107" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Apache Zeppelin</h2></div></div></div><p class="calibre11">Apache Zeppelin is built upon JVM and integrates well with Apache Spark. It is a browser-based or frontend-based open source tool that has its own notebook. It supports Scala, Python, R, SQL, and other graphical modules to serve as a visualization solution not only to business users but also to data scientists. In the following section on visualization techniques, we will take a look at how Zeppelin supports Apache Spark code to generate interesting visuals. You need to download Zeppelin (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a>) in order to try out the examples.</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec108" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Third-party tools</h2></div></div></div><p class="calibre11">There are many products that support Apache Spark as the underlying data processing engine and are built to fit in the organizational big data ecosystem. While leveraging the processing power of Spark, they provide the visualization interface that supports a variety of interactive visuals, and they also support collaboration. Tableau is one such example of a tool that leverages Spark.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec71" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data visualization techniques</h1></div></div></div><p class="calibre11">Data visualization is at the center of every stage in the data analytics life cycle. It is especially important for exploratory analysis and for communicating results. In either case, the goal is to transform data into a format that's efficient for human consumption. The approach of delegating the transformation to client-side libraries does not scale to large datasets. The transformation has to happen on the server side, sending only the relevant data to the client for rendering. Most of the common transformations are available in Apache Spark out of the box. Let's have a closer look at these transformations.</p><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec109" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summarizing and visualizing</h2></div></div></div><p class="calibre11">
<strong class="calibre19">Summarizing and visualizing</strong> is a technique used by many <strong class="calibre19">Business Intelligence </strong>(<strong class="calibre19">BI</strong>) tools. Since summarization will be a concise dataset regardless of the size of the underlying dataset, the graphs look simple enough and easy to render. There are various ways to summarize the data such as aggregating, pivoting, and so on. If the rendering tool supports interactivity and has drill-down capabilities, the user gets to explore subsets of interest from the complete data. We will show how to do the summarization rapidly and interactively with Spark through the Zeppelin notebook.</p><p class="calibre11">The following image shows the Zeppelin notebook with source code and a grouped bar chart. The dataset contains 24 observations with sales information of two products, <strong class="calibre19">P1</strong> and <strong class="calibre19">P2</strong>, for 12 months. The first cell contains code to read a text file and register data as a temporary table. This cell uses the default Spark interpreter using Scala. The second cell uses the SQL interpreter which is supported by out-of-the-box visualization options. You can switch the chart types by clicking on the right icon. Note that the visualization is similar for either Scala or Python or R interpreters.</p><p class="calibre11">Summarization examples are as follows:</p><div><ol class="orderedlist"><li class="listitem1">The source code to read data and register as a SQL View:<p class="calibre31">
<strong class="calibre19">Scala (default)</strong>:</p><p class="calibre31">
</p><div><img src="img/image_09_001.jpg" alt="Summarizing and visualizing" class="calibre140"/></div><p class="calibre31">
</p><p class="calibre31">
<strong class="calibre19">PySpark</strong>:</p><p class="calibre31">
</p><div><img src="img/image_09_002.jpg" alt="Summarizing and visualizing" class="calibre141"/></div><p class="calibre31">
</p><p class="calibre31">
<strong class="calibre19">R</strong>:</p><p class="calibre31">
</p><div><img src="img/image_09_003.jpg" alt="Summarizing and visualizing" class="calibre142"/></div><p class="calibre31">
</p><p class="calibre31">All three are reading the data file and registering as a temporary SQL view. Note that minor differences exist in the preceding three scripts. For example, we need to remove the header row for R and set the column names. The next step is to produce the visualization, which works from the <code class="literal">%sql</code> interpreter. The following first picture shows the script to produce the quarterly sales for each product. It also shows the chart types available out of the box, followed by the settings and their selection. You can collapse the settings after making selections. You can even make use of Zeppelin's in-built dynamic forms, say to accept a product during runtime. The second picture shows the actual output.</p></li><li class="listitem1">The script to produce quarterly sales for two products:<p class="calibre31">
</p><div><img src="img/image_09_004.jpg" alt="Summarizing and visualizing" class="calibre143"/></div><p class="calibre31">
</p></li><li class="listitem1">The output produced:<p class="calibre31">
</p><div><img src="img/image_09_005.jpg" alt="Summarizing and visualizing" class="calibre144"/></div><p class="calibre31">
</p></li></ol></div><p class="calibre11">We have seen Zeppelin's inbuilt visualization in the preceding example. But we can use other plotting libraries as well. Our next example utilizes the PySpark interpreter with matplotlib in Zeppelin to draw a histogram. This example code computes bin intervals and bin counts using RDD's histogram function and brings in just this summarized data to the driver node. Frequency is provided as weights while plotting the bins to give the same visual understanding as a normal histogram but with very low data transfer.</p><p class="calibre11">The histogram examples are as follows:</p><p class="calibre11">
</p><div><img src="img/image_09_006.jpg" alt="Summarizing and visualizing" class="calibre145"/></div><p class="calibre11">
</p><p class="calibre11">This is the generated output (it may come as a separate window):</p><p class="calibre11">
</p><div><img src="img/image_09_007.jpg" alt="Summarizing and visualizing" class="calibre146"/></div><p class="calibre11">
</p><p class="calibre11">In the preceding example of preparing histograms, note that the bucket counts could be parameterized using the inbuilt dynamic forms support.</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec110" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Subsetting and visualizing</h2></div></div></div><p class="calibre11">Sometimes, we may have a large dataset but we may be interested only in a subset of it. Divide and conquer is one approach where we explore a small portion of data at a time. Spark allows data subsetting using SQL-like filters and aggregates on row-column datasets as well as graph data. Let us perform SQL subsetting first, followed by a GraphX example.</p><p class="calibre11">The following example takes bank data available with Zeppelin and extracts a few relevant columns of data related to managers only. It uses the <code class="literal">google visualization library</code> to plot a bubble chart. The data was read using PySpark. Data subsetting and visualization are carried out using R. Note that we can choose any of the interpreters to these tasks and the choice here was just arbitrary.</p><p class="calibre11">The data subsetting example using SQL is as follows:</p><div><ol class="orderedlist"><li class="listitem1">Read data and register the SQL view:<p class="calibre31">
</p><div><img src="img/image_09_008.jpg" alt="Subsetting and visualizing" class="calibre147"/></div><p class="calibre31">
</p></li><li class="listitem1">Subset managers' data and show a bubble plot:<p class="calibre31">
</p><div><img src="img/image_09_009.jpg" alt="Subsetting and visualizing" class="calibre148"/></div><p class="calibre31">
</p></li></ol></div><p class="calibre11">The next example demonstrates some GraphX processing that uses data provided by the <strong class="calibre19">Stanford Network Analysis Project</strong> (<strong class="calibre19">SNAP</strong>). The script extracts a subgraph covering a given set of nodes. Here, each node represents a Facebook ID and an edge represents a connection between the two nodes (or people). Further, the script identifies direct connections for a given node (id: 144). These are the level 1 nodes. Then it identifies the direct connections to these <em class="calibre22">level 1 nodes</em>, which form <em class="calibre22">level 2 nodes</em> to the given node. Even though a second-level contact may be connected to more than one first-level contact, it is shown only once thereby forming a connection tree without crisscrossing edges. Since the connection tree may have too many nodes, the script limits up to three connections at level 1 as well as level 2, thereby showing only 12 nodes under the given root node (one root + three level 1 nodes + three of each level 2 nodes).</p><p class="calibre11">
<strong class="calibre19">Scala</strong>
</p><pre class="programlisting">//Subset and visualize 
//GraphX subset example 
//Datasource: http://snap.stanford.edu/data/egonets-Facebook.html  
import org.apache.spark.graphx._ 
import org.apache.spark.graphx.util.GraphGenerators 
//Load edge file and create base graph 
val base_dir = "../data/facebook" 
val graph = GraphLoader.edgeListFile(sc,base_dir + "/0.edges") 
 
//Explore subgraph of a given set of nodes 
val circle = "155  99  327  140  116  147  144  150  270".split("\t").map( 
       x=&gt; x.toInt) 
val subgraph = graph.subgraph(vpred = (id,name) 
     =&gt; circle.contains(id)) 
println("Edges: " + subgraph.edges.count +  
       " Vertices: " + subgraph.vertices.count) 
        
//Create a two level contact tree for a given node  
//Step1: Get all edges for a given source id 
val subgraph_level1 = graph.subgraph(epred= (ed) =&gt;  
    ed.srcId == 144) 
     
//Step2: Extract Level 1 contacts 
import scala.collection.mutable.ArrayBuffer 
val lvl1_nodes : ArrayBuffer[Long] = ArrayBuffer() 
subgraph_level1.edges.collect().foreach(x=&gt; lvl1_nodes+= x.dstId) 
 
//Step3: Extract Level 2 contacts, 3 each for 3 lvl1_nodes 
import scala.collection.mutable.Map 
val linkMap:Map[Long, ArrayBuffer[Long]] = Map() //parent,[Child] 
val lvl2_nodes : ArrayBuffer[Long] = ArrayBuffer() //1D Array 
var n : ArrayBuffer[Long] = ArrayBuffer() 
for (i &lt;- lvl1_nodes.take(3)) {    //Limit to 3 
    n = ArrayBuffer() 
    graph.subgraph(epred = (ed) =&gt; ed.srcId == i &amp;&amp; 
        !(lvl2_nodes contains ed.dstId)).edges.collect(). 
             foreach(x=&gt; n+=x.dstId) 
    lvl2_nodes++=n.take(3)    //Append to 1D array. Limit to 3 
  linkMap(i) = n.take(3)  //Assign child nodes to its parent 
 } 
  
 //Print output and examine the nodes 
 println("Level1 nodes :" + lvl1_nodes) 
 println("Level2 nodes :" + lvl2_nodes) 
 println("Link map :" + linkMap) 
  
 //Copy headNode to access from another cell 
 z.put("headNode",144) 
 //Make a DataFrame out of lvl2_nodes and register as a view 
 val nodeDF = sc.parallelize(linkMap.toSeq).toDF("parentNode","childNodes") 
 nodeDF.createOrReplaceTempView("node_tbl") 
</pre><div><div><h3 class="title5"><a id="note16" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">Note the use of <code class="literal">z.put</code> and <code class="literal">z.get</code>. This is a mechanism to exchange data between cells/interpreters in Zeppelin.</p></div></div><p class="calibre11">Now that we have created a data frame with level 1 contacts and their direct contacts, we are all set to draw the tree. The following script uses the graph visualization library igraph and Spark R.</p><p class="calibre11">Extract nodes and edges. Plot the tree:</p><p class="calibre11">
</p><div><img src="img/image_09_010.jpg" alt="Subsetting and visualizing" class="calibre149"/></div><p class="calibre11">
</p><p class="calibre11">The preceding script gets parent nodes from the nodes table, which are the parents of level 2 nodes as well as direct connections to the given head node. Ordered pairs of head nodes and level 1 nodes are created and assigned to <code class="literal">edges1</code>. The next step explodes the array of level 2 nodes to form one row per each array element. The data frame thus obtained is transposed and pasted to form edge pairs. Since paste converts data into strings, they are reconverted to numeric. These are the level 2 edges. The level 1 and level 2 edges are concatenated to form a single list of edges. These are fed to form the graph as shown next. Note that the smudge in <code class="literal">headNode</code> is 144, though not visible in the following figure:</p><p class="calibre11">
</p><div><img src="img/image_09_011.jpg" alt="Subsetting and visualizing" class="calibre150"/><p>Connection tree for the given node</p></div><p class="calibre11">
</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec111" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Sampling and visualizing</h2></div></div></div><p class="calibre11">Sampling and visualizing has been used by statisticians for a long time. Through sampling techniques, we take a portion of the dataset and work on it. We will show how Spark supports different sampling techniques such as <strong class="calibre19">random sampling</strong>, <strong class="calibre19">stratified sampling</strong>, and <strong class="calibre19">sampleByKey</strong>, and so on. The following example is created using the Jupyter notebook, PySpark kernel, and <code class="literal">seaborn</code> library. The data file is the bank dataset provided by Zeppelin. The first plot shows the balance for each education category. The colors indicate marital status.</p><p class="calibre11">Read data and take a random sample of 5%:</p><p class="calibre11">
</p><div><img src="img/image_09_012.jpg" alt="Sampling and visualizing" class="calibre151"/></div><p class="calibre11">
</p><p class="calibre11">Render data using <code class="literal">stripplot</code>:</p><p class="calibre11">
</p><div><img src="img/image_09_013.jpg" alt="Sampling and visualizing" class="calibre152"/></div><p class="calibre11">
</p><p class="calibre11">The preceding example showed a random sample of available data, which is much better than completely plotting the population. But if the levels in the categorical variable of interest (in this case, <code class="literal">education</code>) are too many, then this plot becomes hard to read. For example, if we want to plot the balance for job instead of <code class="literal">education</code>, there will be too many strips, making the picture look cluttered. Instead, we can take desired sample of desired categorical levels only and then examine the data. Note that this is different from subsetting because we will not be able to specify the sample ratio in normal subsetting using SQL <code class="literal">WHERE</code> clauses. We need to use <code class="literal">sampleByKey</code> for that, as shown next. The following example takes only two jobs and with specific sampling ratios:</p><p class="calibre11">
</p><div><img src="img/image_09_014.jpg" alt="Sampling and visualizing" class="calibre153"/><p>Stratified sampling</p></div><p class="calibre11">
</p></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec112" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Modeling and visualizing</h2></div></div></div><p class="calibre11">Modeling and visualizing are possible with Spark's <strong class="calibre19">MLLib</strong> and <strong class="calibre19">ML</strong> modules. Spark's unified programming model and diverse programming interfaces enable combining these techniques into a single environment to get insights from the data. We have already covered most of the modeling techniques in the previous chapters. However, here are a few examples for your reference:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre19">Clustering</strong>: K-means, Gaussian Mixture Modeling</li><li class="listitem"><strong class="calibre19">Classification and regression</strong>: Linear model, Decision tree, Naïve Bayes, SVM</li><li class="listitem"><strong class="calibre19">Dimensionality reduction</strong>: Singular value decomposition, Principal component analysis</li><li class="listitem"><strong class="calibre19">Collaborative Filtering</strong></li><li class="listitem"><strong class="calibre19">Statistical testing</strong>: Correlations, Hypothesis testing</li></ul></div><p class="calibre11">The following example takes a model from the <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch07.xhtml" title="Chapter 7.  Extending Spark with SparkR">Chapter 7</a>, <em class="calibre22">Extending Spark with SparkR</em>, which tries to predict the students' pass or fail results using a Naïve Bayes model. The idea is to make use of the out-of-the-box functionality provided by Zeppelin and inspect the model behavior. So, we load the data, perform data preparation, build the model, and run the predictions. Then we register the predictions as an SQL view so as to harness inbuilt visualization:</p><pre class="programlisting">//Model visualization example using zeppelin visualization  
 Prepare Model and predictions 
</pre><p class="calibre11">
</p><div><img src="img/image_09_015.jpg" alt="Modeling and visualizing" class="calibre154"/></div><p class="calibre11">
</p><p class="calibre11">The next step is to write the desired SQL query and define the appropriate settings. Note the use of the UNION operator in SQL and the way the match column is defined.</p><p class="calibre11">Define SQL to view model performance:</p><p class="calibre11">
</p><div><img src="img/image_09_016.jpg" alt="Modeling and visualizing" class="calibre155"/></div><p class="calibre11">
</p><p class="calibre11">The following picture helps us understand where the model prediction deviates from the actual data. Such visualizations are helpful in taking business users' inputs since they do not require any prior knowledge of data science to comprehend:</p><p class="calibre11">
</p><div><img src="img/image_09_017.jpg" alt="Modeling and visualizing" class="calibre156"/><p>Visualize model performance</p></div><p class="calibre11">
</p><p class="calibre11">We usually evaluate statistical models with error metrics, but visualizing them graphically instead of seeing the numbers makes them more intuitive because it is usually easier to understand a diagram than numbers in a table. For example, the preceding visualization can be easily understood by people outside the data science community as well.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec72" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">In this chapter, we explored most of the widely used visualization tools and techniques supported on Spark in a big data setup. We explained some of the techniques with code snippets for better understanding of visualization needs at different stages of the data analytics life cycle. We also saw how business requirements are satisfied with proper visualization techniques by addressing the challenges of big data.</p><p class="calibre11">The next chapter is the culmination of all the concepts explained till now . We will walk through the Complete Data Analysis Life Cycle through an example dataset.</p></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch09lvl1sec73" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div><ul class="itemizedlist"><li class="listitem">21 Essential Data Visualization Tools: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html">http://www.kdnuggets.com/2015/05/21-essential-data-visualization-tools.html</a></li><li class="listitem">Apache Zeppelin notebook home page: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://zeppelin.apache.org/">https://zeppelin.apache.org/</a></li><li class="listitem">Jupyter notebook home page: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://jupyter.org/">https://jupyter.org/</a></li><li class="listitem">Using IPython Notebook with Apache Spark: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/">http://hortonworks.com/hadoop-tutorial/using-ipython-notebook-with-apache-spark/</a> </li><li class="listitem">Apache Toree, which enables interactive workloads between applications and Spark cluster. Can be used with jupyter to run Scala code: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://toree.incubator.apache.org/">https://toree.incubator.apache.org/</a></li><li class="listitem">GoogleVis package using R: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html">https://cran.rproject.org/web/packages/googleVis/vignettes/googleVis_examples.html</a></li><li class="listitem">GraphX Programming Guide: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/graphx-programming-guide.html">http://spark.apache.org/docs/latest/graphx-programming-guide.html</a></li><li class="listitem">Going viral with R's igraph package: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://www.r-bloggers.com/going-viral-with-rs-igraph-package/">https://www.r-bloggers.com/going-viral-with-rs-igraph-package/</a></li><li class="listitem">Plotting with categorical data: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial">https://stanford.edu/~mwaskom/software/seaborn/tutorial/categorical.html#categorical-tutorial</a></li></ul></div><div><div><div><div><h2 class="title3"><a id="ch09lvl2sec113" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Data source citations</h2></div></div></div><p class="calibre11">
<strong class="calibre19">Bank data source (citation)</strong>
</p><div><ul class="itemizedlist"><li class="listitem">[Moro et al., 2011] S. Moro, R. Laureano and P. Cortez. Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology</li><li class="listitem">In P. Novais et al. (Eds.), Proceedings of the European Simulation and Modelling Conference - ESM'2011, pp. 117-121, Guimarães, Portugal, October, 2011. EUROSIS</li><li class="listitem">Available at [pdf] <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://hdl.handle.net/1822/14838">http://hdl.handle.net/1822/14838</a></li><li class="listitem">[bib] http://www3.dsi.uminho.pt/pcortez/bib/2011-esm-1.txt</li></ul></div><p class="calibre11">
<strong class="calibre19">Facebook data Source (citation)</strong>
</p><div><ul class="itemizedlist"><li class="listitem">J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012.</li></ul></div></div></div></div>



  </body></html>