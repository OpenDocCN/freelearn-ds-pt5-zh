<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 8. Creating Ensemble Models"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch08" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 8. Creating Ensemble Models</h1></div></div></div><p class="calibre11">A group of people have the ability to take better decisions than a single individual, especially when each group member comes in with their own biases. The ideology is also true for machine learning.</p><p class="calibre11">When single algorithms are not capable to generate the true prediction function, then ensemble machine-learning methods are used. When there is more focus on the performance of the model rather than the training time and the complexity of the model, then ensemble methods are preferred.</p><p class="calibre11">In this chapter, we will discuss:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">What is ensemble learning?</li><li class="listitem">Constructing ensembles.</li><li class="listitem">Combination strategies.</li><li class="listitem">Boosting, bagging, and injecting randomness.</li><li class="listitem">Random forests.</li></ul></div><div class="calibre2" title="What is ensemble learning?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec65" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is ensemble learning?</h1></div></div></div><p class="calibre11">Ensemble learning is a machine learning method where various models are prepared to work on the same problem. It is a process where multiple models are generated and the results obtained from them are combined to produce the final result. Moreover, ensemble models are inherently parallel; therefore, they are much more efficient at training and testing if we have access to multiple processors:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Ordinary methods of machine learning</strong></span>: These use training data for learning a specific hypothesis.</li><li class="listitem"><span class="strong"><strong class="calibre19">Ensemble learning</strong></span>: This uses training data to build a set of hypothesis. These hypotheses are combined to build the final model.</li></ul></div><p class="calibre11">Therefore, it can be said that ensemble learning is the way toward preparing different individual learners for an objective function that utilizes different strategies, and in the long run, combines these learnings.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_08_01.jpg" alt="What is ensemble learning?" class="calibre207"/></div><p class="calibre11">
</p><div class="calibre2" title="Understanding ensemble learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec107" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Understanding ensemble learning</h2></div></div></div><p class="calibre11">Ensemble learning, as we discussed, combines the learning of the various individual learners. It is the aggregation of multiple learned models that aim to improve accuracy:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Base learners: Each individual learner is called a base learner. Base learners may be suited for a specific situation, but are not good in generalization.</li><li class="listitem">Due to the weak generalization capability of the base learner, they are not suited for every scenario.</li><li class="listitem">Ensemble learning uses these base (weak) learners to construct a strong learner which results in a comparatively much more accurate model.</li><li class="listitem">Generally, decision tree algorithms are used as the base learning algorithm. Using the same kind of learning algorithm results in homogeneous learners. However, different algorithms can also be used, which will result in heterogeneous learners.</li></ul></div></div><div class="calibre2" title="How to construct an ensemble"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec108" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How to construct an ensemble</h2></div></div></div><p class="calibre11">It is recommended that the base learners should be as diverse as possible. This enables the ensemble to handle and predict most of the situation with better accuracy. This diversity can be produced using different subsets of the dataset, manipulating or transforming the inputs, and using different techniques simultaneously for learning.</p><p class="calibre11">Also, when the individual base learners have a high accuracy then there is a good chance of having good accuracy with the ensemble.</p><p class="calibre11">Typically, construction of an ensemble is a two-step process:</p><div class="calibre2"><ol class="orderedlist"><li class="listitem1">The first step is to create the base learners. They are generally constructed in parallel, but if base learners are influenced by the previously formed base learners then they are constructed in a sequential manner.</li><li class="listitem1">The second step is to combine these base learners and create an ensemble which is best suited to the use case.</li></ol></div><p class="calibre11">By using different types of base learners and combination techniques, we can altogether produce different ensemble learning models.</p><p class="calibre11">There are different ways to implement the ensemble model:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Sub-sample the training dataset</li><li class="listitem">Manipulate the input features</li><li class="listitem">Manipulate the output features</li><li class="listitem">Inject randomness</li><li class="listitem">Learning parameters of the classifier can be modified</li></ul></div><div class="calibre2" title="Combination strategies"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Combination strategies</h3></div></div></div><p class="calibre11">Combination strategies can be classified into two categories:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Static combiners</li><li class="listitem">Adaptive combiners</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Static combiners: </strong></span>The combiner choice standard is autonomous of the component vector. Static methodologies can be comprehensively partitioned into trainable and non-trainable.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Trainable</strong></span>: The combiner goes through a different training stage to enhance the performance of the ensemble. Here are two approaches that are widely used:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Weighted averaging</strong></span>: The yield of every classifier is weighted by its very own performance measure:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Measuring the accuracy of prediction on a different validation set</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><span class="strong"><strong class="calibre19">Stacked generalization</strong></span>: The yield of the ensemble is treated as the feature vector to a meta-classifier</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Non-trainable</strong></span>: Performance of the individual classifier does not have an affect on the voting. Different combiners might be utilized. This depends upon the sort of yield delivered by the classifier:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Voting</strong></span>: This is used when a single class label is generated by every classifier. Every classifier votes for a specific class. The class that receives the larger part vote on the ensemble wins.</li><li class="listitem"><span class="strong"><strong class="calibre19">Averaging</strong></span>: When a confidence estimate is created by every classifier then averaging is utilized. The class that has the highest number of posterior in the ensemble wins.</li><li class="listitem"><span class="strong"><strong class="calibre19">Borda counts</strong></span>: This is used when a rank is produced by every classifier.</li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Adaptive combiners</strong></span>: This is a type of combiner function that is dependent on the feature vector given as input:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">A function that is local to every region</li><li class="listitem">Divide and conquer methodology creates modular ensembles and simple classifiers specializing in different regions of I/O space</li><li class="listitem">The individual specialists are required to perform well in their region of ability and not for all inputs</li></ul></div></div></div><div class="calibre2" title="Subsampling training dataset"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec109" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Subsampling training dataset</h2></div></div></div><p class="calibre11">The learner is considered to be unstable if the output classifier has to undergo radical changes when there are some small variations in the training data:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Unstable learners</strong></span>: Decision trees, neural networks, and so on</li><li class="listitem"><span class="strong"><strong class="calibre19">Stable learners</strong></span>: Nearest neighbor, linear regression, and so on</li></ul></div><p class="calibre11">This particular technique is more suited for the unstable learners.</p><p class="calibre11">Two very common techniques used in subsampling are:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Bagging</li><li class="listitem">Boosting</li></ul></div><div class="calibre2" title="Bagging"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Bagging</h3></div></div></div><p class="calibre11">Bagging is also known as Bootstrap Aggregation. It generates the additional data that is used for training by using sub-sampling on the same dataset with replacement. It creates multiple combinations with repetitions to generate the training dataset of the same size.</p><p class="calibre11">As sampling with replacement is done, on an average, each classifier is trained on 63.2% of the training example.</p><p class="calibre11">After training on these multiple datasets, bagging combines the result by majority voting. The class that received the most number of votes wins. By using these multiple datasets, bagging aims to reduce the variance. Accuracy is improved if the induced classifiers are uncorrelated.</p><p class="calibre11">Random forest, a type of ensemble learning, uses bagging and is one of the most powerful methods.</p><p class="calibre11">Let's go through the bagging algorithm.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">Training</strong></span>:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">For iteration, <span class="strong"><em class="calibre23">t=1 to T</em></span>:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Sample randomly from the training dataset with replacement <span class="strong"><em class="calibre23">N</em></span> samples</li><li class="listitem">Base learner is trained (for example, decision tree or neural network) on this sample</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">
<span class="strong"><strong class="calibre19">Test</strong></span>:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">For test sample, <span class="strong"><em class="calibre23">t=1 to T</em></span>:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Start all the models that were trained</li><li class="listitem">Prediction is done on the basis of the following:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><span class="strong"><strong class="calibre19">Regression</strong></span>: Averaging</li><li class="listitem"><span class="strong"><strong class="calibre19">Classification</strong></span>: Majority vote</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_08_02-1.jpg" alt="Bagging" class="calibre208"/></div><p class="calibre11">
</p><div class="calibre2" title="When does bagging work?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title8"><a id="ch08lvl4sec0" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>When does bagging work?</h4></div></div></div><p class="calibre11">Bagging works in scenarios where there would have been over-fitting if it wasn't used. Let's go through these scenarios:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Under-fitting</strong></span>:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><span class="strong"><strong class="calibre19">High bias</strong></span>: Models are not good enough and don't fit the training data well</li><li class="listitem"><span class="strong"><strong class="calibre19">Small variance</strong></span>: Whenever there is a change in the dataset, there is a very small change that needs to be done in the classifier</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><span class="strong"><strong class="calibre19">Over-fitting</strong></span>:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><span class="strong"><strong class="calibre19">Small bias</strong></span>: Models fit too well to the training data</li><li class="listitem"><span class="strong"><strong class="calibre19">Large variance</strong></span>: Whenever there is a small change in the dataset, there is a large or drastic change that needs to be done in the classifier</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">Bagging aims to reduce the variance without affecting the bias. Therefore, the dependency of the model on the training dataset is reduced.</p></div></div><div class="calibre2" title="Boosting"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec61" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Boosting</h3></div></div></div><p class="calibre11">Boosting is different to the bagging approach. It is based on the PAC framework, which is the Probably Approximately Correct framework.</p><p class="calibre11">
<span class="strong"><strong class="calibre19">PAC learning</strong></span>: The probability of having larger confidence and smaller accuracy than the misclassification error:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><strong class="calibre19">Accuracy</strong></span>: This is the percentage of correctly classifying the samples in the test dataset</li><li class="listitem"><span class="strong"><strong class="calibre19">Confidence</strong></span>: This is the probability of achieving the accuracy in the given experiment</li></ul></div><div class="calibre2" title="Boosting approach"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title8"><a id="ch08lvl4sec1" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Boosting approach</h4></div></div></div><p class="calibre11">The boosting approach is based on the concept of the "weak learner". When an algorithm performs somewhat better than 50% in binary classification tasks, then it is called a weak learner. The approach is to combine multiple weak learners together, and the aim is to:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Improve confidence</li><li class="listitem">Improve accuracy</li></ul></div><p class="calibre11">This is done by training these different weak learners on different datasets.</p></div><div class="calibre2" title="Boosting algorithm"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title8"><a id="ch08lvl4sec2" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Boosting algorithm</h4></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Training:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Samples are taken randomly from the dataset.</li><li class="listitem">First, learner <span class="strong"><em class="calibre23">h1</em></span> is trained on the sample.</li><li class="listitem">Accuracy of this learner, <span class="strong"><em class="calibre23">h1</em></span>, is evaluated on the dataset.</li><li class="listitem">A similar process is followed using different samples for multiple learners. They are divided such that they classify differently.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Test:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">On the test dataset, the learning is applied using the majority vote of all the learners</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">Confidence can also be boosted in a similar way, as boosting the accuracy with some trade-off.</p><p class="calibre11">Boosting is less of an algorithm and rather it is a "framework". The main aim of this framework is to take a weak learning algorithm W and turn it into a strong learning algorithm. We will now discuss AdaBoost, which is short for "adaptive boosting algorithm". AdaBoost became famous because it was one of the first successful and practical boosting algorithms.</p><p class="calibre11">It does not require a large number of hyper-parameters to be defined and executes in a polynomial time. The benefit of this algorithm is that it has the ability to automatically adapt to the data given to it.</p></div></div><div class="calibre2" title="AdaBoost – boosting by sampling"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec62" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>AdaBoost – boosting by sampling</h3></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">After <span class="strong"><em class="calibre23">n</em></span> iterations, a weak learner having distribution <span class="strong"><em class="calibre23">D</em></span> over the training set is provided by the boosting:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">All examples have the equal probability of being selected as the first component</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Sub-sampling of the training set is done in accordance with distribution <span class="strong"><em class="calibre23">Dn</em></span> and a model is trained by the weak learner</li><li class="listitem">The weights of misclassified instances are adjusted in a way that subsequent classifiers work on comparatively difficult cases</li><li class="listitem">A distribution <span class="strong"><em class="calibre23">D(n+1)</em></span> is generated with the probability of misclassified samples increasing and correctly classified samples decreasing</li><li class="listitem">After <span class="strong"><em class="calibre23">t</em></span> iterations, according to the performance of the models, the votes of individual hypotheses are weighted</li></ul></div><p class="calibre11">The strength of AdaBoost derives from the adaptive resampling of examples, not from the final weighted combination.</p><div class="calibre2" title="What is boosting doing?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title8"><a id="ch08lvl4sec3" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>What is boosting doing?</h4></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Every classifier has a specialization on the specific subset of dataset</li><li class="listitem">An algorithm concentrates on examples with increasing levels of difficulty</li><li class="listitem">Boosting is able to reduce variance (similar to bagging)</li><li class="listitem">It is also able to eliminate the consequences of high bias of the weak learner (not present in bagging)</li><li class="listitem">Train versus test errors performance:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">We can reduce the train errors to nearly 0</li><li class="listitem">Overfitting is not present and is evident by the test errors</li></ul></div><p class="calibre44">
</p></li></ul></div></div><div class="calibre2" title="The bias and variance decomposition"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h4 class="title8"><a id="ch08lvl4sec4" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The bias and variance decomposition</h4></div></div></div><p class="calibre11">Let's discuss how bagging and boosting affect the bias-variance decomposition of the classification error:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Features of errors that can be expected from a learning algorithm:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">A bias term is a measure of performance of the classifier with respect to the target function</li><li class="listitem">Variance terms measure the robustness of the classifier; if there is a change in the training dataset, then how is the model affected by it?</li></ul></div><p class="calibre44">
</p></li><li class="listitem">Bagging and boosting are capable to reduce the variance term, and therefore reducing the errors in the model</li><li class="listitem">It is also proved that boosting attempts to reduce the bias term since it focuses on misclassified samples</li></ul></div></div></div></div><div class="calibre2" title="Manipulating the input features"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec110" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Manipulating the input features</h2></div></div></div><p class="calibre11">One other technique with which we can generate multiple classifiers is by manipulating the set of input features which we feed to the learning algorithm.</p><p class="calibre11">We can select different subsets of features and networks of different sizes. The input features subsets may be manually selected rather than in an automated fashion. This technique is widely used in image processing: one of the very famous examples is Principle Component Analysis.</p><p class="calibre11">The ensemble classifier generated in many experiments was able to perform like an actual human.</p><p class="calibre11">It was also found that when we delete even a few of the features that we gave as the input, it affects the performance of the classifier. This affects the overall voting, and the ensemble thus generated is not able to perform up to the expectations.</p></div><div class="calibre2" title="Injecting randomness"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec111" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Injecting randomness</h2></div></div></div><p class="calibre11">This is another universally useful technique for producing an ensemble of classifiers. In this method, we inject randomness into the learner algorithm. Neural networks with back-propagation are also created using the same technique for hidden weights. On the off chance that the calculation is connected to the same preparing illustrations, but with various starting weights, the subsequent classifier can be very diverse.</p><p class="calibre11">One of the most computationally costly parts of outfits of decision trees involves preparing the decision tree. This is quick for decision stumps; however, for more profound trees, it can be restrictively costly.</p><p class="calibre11">The costly part is picking the tree structure. Once the tree structure is picked, it is extremely cheap to fill in the leaves (which is the predictions of the trees) utilizing the training data. A shockingly productive and successful option is to utilize trees with altered structures and random features. Accumulations of trees are called forests, thus classifiers fabricated like this are called random forests.</p><p class="calibre11">It takes three arguments:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The training data</li><li class="listitem">The depth of the decision trees</li><li class="listitem">The number form</li></ul></div><p class="calibre11">The calculation produces each of the K trees freely which makes it simple to parallelize. For every tree, it develops a full binary tree of a given depth. The elements utilized at the branches of this tree are chosen randomly, regularly with substitution, implying that the same element can seem numerous at times, even in one branch. Based on the training data, the leaves that will perform the actual predictions are filled. This last step is the main time when the training data is utilized. The subsequent classifier then only involves the voting of the K-numerous random trees.</p><p class="calibre11">The most astonishing thing about this methodology is that it works strikingly well. It tends to work best when the greater part of the components are not significant, since the quantity of features chosen for any given tree is minimal. A portion of the trees will query the useless features.</p><p class="calibre11">These trees will basically make random forecasts. Be that as it may, a portion of the trees will query good features and will make good forecasts (on the grounds that the leaves are assessed in light of the training data). In the event that you have enough trees, the arbitrary ones will wash out as noise, and just the good trees will affect the final classification.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Random forests"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec66" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Random forests</h1></div></div></div><p class="calibre11">Random forests were developed by Leo Breiman and Adele Cutler. Their strength in the field of machine learning has been shown nicely in a blog entry at Strata 2012: "Ensembles of decision trees (often known as random forests) have been the most successful general-purpose algorithm in modern times", as they "automatically identify the structure, interactions, and relationships in the data".</p><p class="calibre11">Moreover, it has been noticed that "most Kaggle solutions have no less than one top entry that vigorously utilizes this methodology". Random forests additionally have been the preferred algorithm for recognizing the body part in Microsoft's Kinect, which is a movement detecting information gadgets for Xbox consoles and Windows PCs.</p><p class="calibre11">Random forests comprises of a group of decision trees. We will consequently begin to analyze decision trees.</p><p class="calibre11">A decision tree, as discussed previously, is a tree-like chart where on each node there is a choice, in view of one single feature. Given an arrangement of features, the tree is navigated from node to node, as indicated by these decisions, until you come to a leaf. The name of this leaf is the expectation for the given list of features. A straightforward decision tree could be utilized to choose what you have to bring with you when going out.</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_08_03.jpg" alt="Random forests" class="calibre209"/></div><p class="calibre11">
</p><p class="calibre11">Each tree is constructed in the following way:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Randomly take an <span class="strong"><em class="calibre23">n</em></span> sample case where n is the number of cases in the training set.<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">Replacement is used to select these n cases. This particular set of data is used to construct the tree.</li></ul></div><p class="calibre44">
</p></li><li class="listitem">The node is split by <span class="strong"><em class="calibre23">x&lt;X</em></span>, where <span class="strong"><em class="calibre23">X</em></span> is the number of input variables. The <span class="strong"><em class="calibre23">X</em></span> doesn't change with the growth of the forest.</li><li class="listitem">Pruning is not done as trees are allowed to go the maximum depth.</li></ul></div><p class="calibre11">The error rate in a random forest is dependent on the following (as given in the original paper):</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">When correlation among the trees increases, the error rate in the random forest increases</li><li class="listitem">When individual trees are weak, the error rate increases and it decreases when the individual trees are strengthened</li></ul></div><p class="calibre11">It is found that <span class="strong"><em class="calibre23">x</em></span>, which was mentioned previously, has an effect on correlation and strength. Increasing <span class="strong"><em class="calibre23">x</em></span> increases both strength and correlation, and decreasing <span class="strong"><em class="calibre23">x</em></span> decreases both. We try to find the particular value or the range where the <span class="strong"><em class="calibre23">x</em></span> should lie to have the minimal error.</p><p class="calibre11">We use the <span class="strong"><strong class="calibre19">oob</strong></span> (<span class="strong"><strong class="calibre19">out of bag</strong></span>) error to find the best value or the range of the value of <span class="strong"><em class="calibre23">x</em></span>.</p><p class="calibre11">This tree does not classify every one of the points effectively. We could transform this by expanding the tree depth. Thus, the tree can foresee the specimen data 100% effectively, just by taking in the noise in the examples. In the most extreme case, the calculation is proportional to a word reference containing each specimen. This is known as over-fitting and prompts awful results when utilizing it for out of test forecasts. Keeping in mind the end goal to overcome over-fitting, we can prepare numerous decision trees by presenting weights for the examples and just considering an irregular subset of the features for every split. A definite conclusion of the random forest will be controlled by a lion's share vote on the trees' forecasts. This method is otherwise called bagging. It diminishes the variance (errors from noise in the training set) without expanding the bias (errors because of the insufficient adaptability of the model).</p><div class="calibre2" title="Features of random forests"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec112" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Features of random forests</h2></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem">A random forest is highly accurate as compared to the existing algorithms.</li><li class="listitem">It can be used effectively and efficiently on big data. They are fast and don't require expensive hardware to run.</li><li class="listitem">One of the key features of a random forest is its ability to deal several numbers of input variables.</li><li class="listitem">A random forest can show the generalization error estimate during the process of construction of the forest. It can also give the important variables for the classification.</li><li class="listitem">When the data is sparse, a random forest is an effective algorithm with good accuracy. It can also work on prediction of missing data.</li><li class="listitem">The models generated can be utilized later on data that we might receive in the future.</li><li class="listitem">In unbalanced datasets, it provides the feature to balance the error present in the class population.</li><li class="listitem">Some of these features can be used for unsupervised clustering too and also as an effective method for outlier detection.</li><li class="listitem">One other key feature of a random forest is that it doesn't overfit.</li></ul></div></div><div class="calibre2" title="How do random forests work?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec113" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>How do random forests work?</h2></div></div></div><p class="calibre11">To comprehend and utilize the different choices, additional data about how they are figured is valuable. The vast majority of the alternatives rely upon two information objects produced by random forests.</p><p class="calibre11">At the point when the training set for the present tree is drawn by sampling with replacement, around 33% of the cases are left well enough alone for the example.</p><p class="calibre11">As more and more trees are added to the random forest, the information from the oob helps to generate the estimate of the classification error. After the construction of every tree, the greater part of the information keeps running down the tree, and vicinities are computed for every pair of cases.</p><p class="calibre11">On the other hand, if the same terminal node is shared by two cases then we increase their proximity by 1. After the complete analysis, these proximities are normalized. Proximities are utilized as a part of supplanting missing information, finding outliers, and delivering in-depth perspectives of the information.</p></div><div class="calibre2" title="The out-of-bag (oob) error estimate"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec114" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The out-of-bag (oob) error estimate</h2></div></div></div><p class="calibre11">Random forests eliminate the requirement to cross-validate to achieve the unbiased estimate of the test set error. During the construction, it is estimated as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Every tree is constructed utilizing an alternate bootstrap sample from the original information. Around 33% of the cases are left alone for the bootstrap test and not utilized as a part of the development of the kth tree.</li><li class="listitem">There may be some cases that were not considered during the construction of the trees. We put these cases down the kth to achieve a classification. It produces a classification test set in around 33% of the trees. Towards the end of the run, take <span class="strong"><em class="calibre23">j</em></span> to be the class that received the vast majority of the votes each time the case <span class="strong"><em class="calibre23">n</em></span> was oob. The number of times that <span class="strong"><em class="calibre23">j</em></span> is not equivalent to the true class of <span class="strong"><em class="calibre23">n</em></span> at the midpoint of all the cases is the oob error estimate. This follows being unbiased in the various tests.</li></ul></div><div class="calibre2" title="Gini importance"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec63" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Gini importance</h3></div></div></div><p class="calibre11">The impurity measure is frequently used for decision trees. Misclassification is measured and is called gini impurity which applies in the context where there are multiple classifiers present.</p><p class="calibre11">There is also a gini coefficient. This is applicable to binary classification. It needs a classifier that is able to rank the sample according to the probability of being in the right class.</p></div><div class="calibre2" title="Proximities"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h3 class="title5"><a id="ch08lvl3sec64" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Proximities</h3></div></div></div><p class="calibre11">As mentioned before, we don't prune the trees while constructing the random forest. Therefore, the terminal nodes don't have many numbers of instances.</p><p class="calibre11">To find the proximity measure, we run all the cases from the training set down the tree. Let's say case x and case y arrive at the same terminal node, we then increase the proximity by 1.</p><p class="calibre11">After the run, we take twice the number of trees and divide the proximity of the case which was increased by 1 by this number.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Implementation in Julia"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec67" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Implementation in Julia</h1></div></div></div><p class="calibre11">Random forests are available in the Julia-registered packages from Kenta Sato:</p><pre class="programlisting">
<span class="strong"><strong class="calibre19">Pkg.update() Pkg.add("RandomForests") 
</strong></span>
</pre><p class="calibre11">This is a CART-based random forest implementation in Julia. This package supports:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Classification models</li><li class="listitem">Regression models</li><li class="listitem">Out-of-bag (OOB) errors</li><li class="listitem">Feature importances</li><li class="listitem">Various configurable parameters</li></ul></div><p class="calibre11">There are two separate models available in this package:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Classification</li><li class="listitem">Regression</li></ul></div><p class="calibre11">Each model has its own constructor that is trained by applying the fit method. We can configure these constructors with some keyword arguments listed as follows:</p><pre class="programlisting">RandomForestClassifier(;n_estimators::Int=10, 
                        max_features::Union(Integer, FloatingPoint, Symbol)=:sqrt, 
                        max_depth=nothing, 
                        min_samples_split::Int=2, 
                        criterion::Symbol=:gini) 
</pre><p class="calibre11">This one is for the classification:</p><pre class="programlisting">RandomForestRegressor(;n_estimators::Int=10, 
                       max_features::Union(Integer, FloatingPoint, Symbol)=:third, 
                       max_depth=nothing, 
                       min_samples_split::Int=2) 
</pre><p class="calibre11">This one is for the regression:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><code class="literal">n_estimators</code>: This is the number of weak estimators</li><li class="listitem"><code class="literal">max_features</code>: This is the number of candidate features at each split<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">If Integer is given, the fixed number of features are used</li><li class="listitem">If FloatingPoint is given, the proportions of the given value (0.0, 1.0] are used</li><li class="listitem">If Symbol is given, the number of candidate features is decided by a strategy<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><code class="literal">:sqrt: ifloor(sqrt(n_features))</code></li><li class="listitem"><code class="literal">:third: div(n_features, 3)</code></li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre44">
</p></li><li class="listitem"><code class="literal">max_depth</code>: The maximum depth of each tree<div class="calibre2"><ul class="itemizedlist1"><li class="listitem">The default argument nothing means there is no limitation of the maximum depth</li></ul></div><p class="calibre44">
</p></li><li class="listitem"><code class="literal">min_samples_split</code>: The minimum number of sub-samples to try to split a node</li><li class="listitem"><code class="literal">criterion</code>: The criterion of the impurity measure (classification only)<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><code class="literal">:</code>gini: Gini index</li><li class="listitem"><code class="literal">:entropy</code>: Cross entropy</li></ul></div><p class="calibre44">
</p></li></ul></div><p class="calibre11">
<code class="literal">RandomForestRegressor</code> always uses the mean squared error for its impurity measure. Currently, there are no configurable criteria for the regression model.</p><div class="calibre2" title="Learning and prediction"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec115" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Learning and prediction</h2></div></div></div><p class="calibre11">For our example, we will be using the amazing "DecisionTree" package provided by Ben Sadeghi.</p><p class="calibre11">The package supports the following available models:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><code class="literal">DecisionTreeClassifier</code></li><li class="listitem"><code class="literal">DecisionTreeRegressor</code></li><li class="listitem"><code class="literal">RandomForestClassifier</code></li><li class="listitem"><code class="literal">RandomForestRegressor</code></li><li class="listitem"><code class="literal">AdaBoostStumpClassifier</code></li></ul></div><p class="calibre11">Installation is straightforward:</p><pre class="programlisting">Pkg.add("DecisionTree") 
</pre><p class="calibre11">Let us start with the classification example:</p><pre class="programlisting">using RDatasets: dataset 
using DecisionTree 
</pre><p class="calibre11">We now take the famous iris dataset:</p><pre class="programlisting">iris = dataset("datasets", "iris") 
features = convert(Array, iris[:, 1:4]); 
labels = convert(Array, iris[:, 5]); 
</pre><p class="calibre11">This generates a pruned tree classifier:</p><pre class="programlisting"># train full-tree classifier 
model = build_tree(labels, features) 
# prune tree: merge leaves having &gt;= 90% combined purity (default: 100%) 
model = prune_tree(model, 0.9) 
# pretty print of the tree, to a depth of 5 nodes (optional) 
print_tree(model, 5) 
</pre><p class="calibre11">
</p><div class="mediaobject"><img src="Images/B05321_08_04.jpg" alt="Learning and prediction" class="calibre210"/></div><p class="calibre11">
</p><p class="calibre11">It generates such a tree given in the previous image. We now apply this learned model:</p><pre class="programlisting"># apply learned model 
apply_tree(model, [5.9,3.0,5.1,1.9]) 
# get the probability of each label 
apply_tree_proba(model, [5.9,3.0,5.1,1.9], ["setosa", "versicolor", "virginica"]) 
# run n-fold cross validation for pruned tree, 
# using 90% purity threshold pruning, and 3 CV folds 
accuracy = nfoldCV_tree(labels, features, 0.9, 3) 
</pre><p class="calibre11">It generates the following result:</p><pre class="programlisting">Fold 1 
Classes:   
3x3 Array{Int64,2}: 
 15   0   0 
  1  13   0 
  0   1  20 
Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy:  
3x3 Array{Int64,2}: 
 18   0  0 
  0  18  5 
  0   1  8 
3x3 Array{Int64,2}: 
 17   0   0 
  0  11   2 
  0   3  17 
0.96 
Kappa:    0.9391727493917275 
 
Fold 2 
Classes:  Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy: 0.88 
Kappa:    0.8150431565967939 
 
Fold 3 
Classes:  Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy: 0.9 
Kappa:    0.8483929654335963 
 
Mean Accuracy: 0.9133333333333332 
</pre><p class="calibre11">Now let's train the random forest classifier:</p><pre class="programlisting"># train random forest classifier 
# using 2 random features, 10 trees, 0.5 portion of samples per tree (optional), and a maximum tree depth of 6 (optional) 
model = build_forest(labels, features, 2, 10, 0.5, 6) 
</pre><p class="calibre11">It generates the random forest classifier:</p><pre class="programlisting">3x3 Array{Int64,2}: 
 14   0   0 
  2  15   0 
  0   5  14 
3x3 Array{Int64,2}: 
 19   0   0 
  0  15   3 
  0   0  13 
3x3 Array{Int64,2}: 
 17   0   0 
  0  14   1 
  0   0  18 
</pre><p class="calibre11">Now we will apply this learned model and check the accuracy:</p><pre class="programlisting"># apply learned model 
apply_forest(model, [5.9,3.0,5.1,1.9]) 
# get the probability of each label 
apply_forest_proba(model, [5.9,3.0,5.1,1.9], ["setosa", "versicolor", "virginica"]) 
# run n-fold cross validation for forests 
# using 2 random features, 10 trees, 3 folds and 0.5 of samples per tree (optional) 
accuracy = nfoldCV_forest(labels, features, 2, 10, 3, 0.5) 
</pre><p class="calibre11">The result is as follows:</p><pre class="programlisting">Fold 1 
Classes:  Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy: 0.86 
Kappa:    0.7904191616766468 
 
Fold 2 
Classes:  Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy: 0.94 
Kappa:    0.9096929560505719 
 
Fold 3 
Classes:  Any["setosa","versicolor","virginica"] 
Matrix:    
Accuracy: 0.98 
Kappa:    0.9698613622664255 
 
Mean Accuracy: 0.9266666666666666 
 
3-element Array{Float64,1}: 
 0.86 
 0.94 
 0.98 
</pre><p class="calibre11">Now let's train a regression tree:</p><pre class="programlisting">n, m = 10^3, 5 ; 
features = randn(n, m); 
weights = rand(-2:2, m); 
labels = features * weights; 
# train regression tree, using an averaging of 5 samples per leaf (optional) 
model = build_tree(labels, features, 5) 
apply_tree(model, [-0.9,3.0,5.1,1.9,0.0]) 
# run n-fold cross validation, using 3 folds, averaging of 5 samples per leaf (optional) 
# returns array of coefficients of determination (R^2) 
r2 = nfoldCV_tree(labels, features, 3, 5) 
</pre><p class="calibre11">It generates the following tree:</p><pre class="programlisting">Fold 1 
Mean Squared Error:     3.300846200596437 
Correlation Coeff:      0.8888432175516764 
Coeff of Determination: 0.7880527098784421 
 
Fold 2 
Mean Squared Error:     3.453954624611847 
Correlation Coeff:      0.8829598153801952 
Coeff of Determination: 0.7713110081750566 
 
Fold 3 
Mean Squared Error:     3.694792045651598 
Correlation Coeff:      0.8613929927227013 
Coeff of Determination: 0.726445409019041 
 
Mean Coeff of Determination: 0.7619363756908465 
 
3-element Array{Float64,1}: 
 0.788053 
 0.771311 
 0.726445 
</pre><p class="calibre11">Now training a regression forest is made straightforward by the package:</p><pre class="programlisting"># train regression forest, using 2 random features, 10 trees, 
# averaging of 5 samples per leaf (optional), 0.7 of samples per tree (optional) 
model = build_forest(labels,features, 2, 10, 5, 0.7) 
# apply learned model 
apply_forest(model, [-0.9,3.0,5.1,1.9,0.0]) 
# run n-fold cross validation on regression forest 
# using 2 random features, 10 trees, 3 folds, averaging of 5 samples/leaf (optional), 
# and 0.7 porition of samples per tree (optional) 
# returns array of coefficients of determination (R^2) 
r2 = nfoldCV_forest(labels, features, 2, 10, 3, 5, 0.7) 
</pre><p class="calibre11">It generates the following output:</p><pre class="programlisting">Fold 1 
Mean Squared Error:     1.9810655619597397 
Correlation Coeff:      0.9401674806129654 
Coeff of Determination: 0.8615574830022655 
 
Fold 2 
Mean Squared Error:     1.9359831066335886 
Correlation Coeff:      0.950439305213504 
Coeff of Determination: 0.8712750380735376 
 
Fold 3 
Mean Squared Error:     2.120355686915558 
Correlation Coeff:      0.9419270107183548 
Coeff of Determination: 0.8594402239360724 
 
Mean Coeff of Determination: 0.8640909150039585 
 
3-element Array{Float64,1}: 
 0.861557 
 0.871275 
 0.85944  
</pre></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Why is ensemble learning superior?"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec68" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Why is ensemble learning superior?</h1></div></div></div><p class="calibre11">To comprehend the generalization power of ensemble learning being superior to an individual learner, Dietterich provided three reasons.</p><p class="calibre11">These three reasons help us understand the reason for the superiority of ensemble learning leading to a better hypothesis:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The training information won't give adequate data to picking a single best learner. For instance, there might be numerous learners performing similarly well on the training information set. In this way, joining these learners might be a superior decision.</li><li class="listitem">The second reason is that, the search procedures of the learning algorithms may be defective. For instance, regardless of the possibility that there exists a best hypothesis, the learning algorithms may not be able to achieve that due to various reasons including generation of an above average hypothesis. Ensemble learning can improve on that part by increasing the possibility to achieve the best hypothesis.</li><li class="listitem">The third reason is that one target function may not be present in the hypothesis space that we are searching in. This target function may lie in a combination of various hypothesis spaces, which is similar to combining various decision trees to generate the random forest.</li></ul></div><p class="calibre11">There are numerous hypothetical studies on acclaimed ensemble techniques. For example, boosting and bagging are the methods to achieve these three points discussed.</p><p class="calibre11">It is also observed that boosting does not experience the ill effects of over-fitting even after countless, and now and then it is even ready to diminish the generalization error after the training error has achieved zero. Although numerous scientists have considered this marvel, hypothetical clarifications are still in belligerence.</p><p class="calibre11">The bias-variance decomposition is frequently utilized as a part of studying the execution of ensemble techniques. It is observed that bagging is able to nearly eliminate the variance, and by doing so becomes ideal to attach to learners that experience huge variance, such as unstable learners, decision trees, or neural networks.</p><p class="calibre11">Boosting is able to minimize the bias, notwithstanding diminishing the variance, and by doing so becomes more viable to weak learners such as decision trees.</p><div class="calibre2" title="Applications of ensemble learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch08lvl2sec116" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Applications of ensemble learning</h2></div></div></div><p class="calibre11">Ensemble learning is used widely in applications, such as:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Optical character recognition</li><li class="listitem">Text categorization</li><li class="listitem">Face recognition</li><li class="listitem">Computer-aided medical diagnosis</li></ul></div><p class="calibre11">Ensemble learning can be used in nearly all scenarios where machine learning techniques are used.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec69" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">Ensemble learning is a method for generating highly accurate classifiers by combining weak or less accurate ones. In this chapter, we discussed some of the methods for constructing ensembles and went through the three fundamental reasons why ensemble methods are able to outperform any single classifier within the ensemble.</p><p class="calibre11">We discussed bagging and boosting in detail. Bagging, also known as Bootstrap Aggregation, generates the additional data that is used for training by using sub-sampling on the same dataset with replacement. We also learned why AdaBoost performs so well and understood in detail about random forests. Random forests are highly accurate and efficient algorithms that don't overfit. We also studied how and why they are considered as one of the best ensemble models. We implemented a random forest model in Julia using the "DecisionTree" package.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch08lvl1sec70" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf">http://cs.nju.edu.cn/zhouzh/zhouzh.files/publication/springerEBR09.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf">http://web.engr.oregonstate.edu/~tgd/publications/mcs-ensembles.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf">http://www.machine-learning.martinsewell.com/ensembles/ensemble-learning.pdf</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf">http://web.cs.wpi.edu/~xkong/publications/papers/ecml11.pdf</a></li></ul></div></div></div>



  </body></html>