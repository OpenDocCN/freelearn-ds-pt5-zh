<html><head></head><body><div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Chapter 7.  Extending Spark with SparkR"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title"><a id="ch07" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Chapter 7.  Extending Spark with SparkR </h1></div></div></div><p class="calibre11">Statisticians and data scientists have been using R to solve challenging problems in almost every field, ranging from bioinformatics to election campaigns. They prefer R due to its powerful visualization capabilities, strong community, and rich package ecosystem for statistics and machine learning. Many academic institutions around the world teach data science and statistics using the R language.</p><p class="calibre11">R was originally created by and for statisticians in around the mid-1990s with a goal to deliver a better and more user-friendly way to perform data analysis. R was initially used in academics and research. As businesses became increasingly aware of the role of data science in their business growth, the number of data analysts using R in the corporate sector started growing as well. The R language user base is considered to be more than two million strong, after being in existence for two decades.</p><p class="calibre11">One of the driving factors behind all this success is the fact that R is designed to make the life of the analyst easier but not that of the computer. R is inherently single-threaded and it can only process datasets that completely fit in a single machine's memory. But nowadays, R users are working with increasingly larger datasets. Seamless integration of modern-day distributed processing power underneath the well-established R language allows data scientists to leverage the best of both worlds. They can keep up with their ever-increasing business demands and continue to benefit from the flexibility of their favorite R language.</p><p class="calibre11">This chapter introduces SparkR, an R API to Spark for R programmers so that they can harness the power of Spark, without learning a new language. Since prior knowledge of R, R Studio, and data analysis skills are already assumed, this chapter does not attempt to introduce R. A very brief overview of the Spark compute engine is provided as a quick recap. The reader should go through the first three chapters of this book to gain a deeper understanding of the Spark programming model and DataFrames. This knowledge is extremely important because the developer has to understand which part of his code is executing in the local R environment and which part is being handled by the Spark compute engine. The topics covered in this chapter are as follows:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">SparkR basics</li><li class="listitem">Advantages of R with Spark and its limitations</li><li class="listitem">Programming with SparkR</li><li class="listitem">SparkR DataFrames</li><li class="listitem">Machine learning</li></ul></div><div class="calibre2" title="SparkR basics"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec54" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>SparkR basics</h1></div></div></div><p class="calibre11">R is a language and environment for statistical computing and graphics. SparkR is an R package that provides a lightweight frontend to enable Apache Spark access from R. The goal of SparkR is to combine the flexibility and ease of use provided by the R environment and the scalability and fault tolerance provided by the Spark compute engine. Let us recap the Spark architecture before discussing how SparkR realizes its goal.</p><p class="calibre11">Apache Spark is a fast, general-purpose, fault-tolerant framework for interactive and iterative computations on large, distributed datasets. It supports a wide variety of data sources as well as storage layers. It provides unified data access to combine different data formats, streaming data and defining complex operations using high-level, composable operators. You can develop your applications interactively using Scala, Python, or R shell (or Java without a shell). You can deploy it on your home desktop or you can run it on large clusters of thousands of nodes crunching petabytes of data.</p><div class="note" title="Note"><div class="inner"><h3 class="title2"><a id="note14" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Note</h3><p class="calibre25">SparkR originated in the AMPLab (<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://amplab.cs.berkeley.edu/">https://amplab.cs.berkeley.edu/</a>) to explore different techniques to integrate the usability of R with the scalability of Spark. It was released as an alpha component in Apache Spark 1.4, which was released in June 2015. The Spark 1.5 release had improved R usability and introduced the MLlib machine learning package with <span class="strong"><strong class="calibre19">Generalized Linear Models</strong></span> (<span class="strong"><strong class="calibre19">GLMs</strong></span>). The Spark 1.6 release that happened in January 2016 added some more features, such as model summary and feature interactions. The Spark 2.0 release that happened in July 2016 brought several important features, such as UDF, improved model coverage, DataFrames Window functions API, and so on.</p></div></div><div class="calibre2" title="Accessing SparkR from the R environment"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec82" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Accessing SparkR from the R environment</h2></div></div></div><p class="calibre11">You can start SparkR from R shell or R Studio. The entry point to SparkR is the SparkSession object, which represents the connection to the Spark cluster. The node on which R is running becomes the driver. Any objects created by the R program reside on this driver. Any objects created via SparkSession are created on the worker nodes in the cluster. The following diagram depicts the runtime view of R interaction with Spark running on a cluster. Note that R interpreter exists on every worker node in the cluster. The following figure does not show the cluster manager and it does not show the storage layer either. You could use any cluster manager (for example, Yarn or Mesos) and any storage option, such as HDFS, Cassandra, or Amazon S3:</p><p class="calibre11">
</p><div class="mediaobject"><img src="Images/image_07_001.jpg" alt="Accessing SparkR from the R environment" class="calibre138"/><div class="caption">Source: http://www.slideshare.net/Hadoop_Summit/w-145p210-avenkataraman.</div></div><p class="calibre11">
</p><p class="calibre11">A SparkSession object is created by passing information such as application name, memory, number of cores, and the cluster manager to connect to. Any interaction with the Spark engine is initiated via this SparkSession object. A SparkSession object is already created for you if you use SparkR shell. You have to explicitly create it otherwise. This object replaces SparkContext and SQLContext objects that existed in Spark 1.x releases. These objects still exist for backward compatibility. Even the preceding figure depicts SparkContext, which you should treat as SparkSession post Spark 2.0.</p><p class="calibre11">Now that we have understood how to access Spark from the R environment, let us examine the core data abstractions provided by the Spark engine.</p></div><div class="calibre2" title="RDDs and DataFrames"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec83" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>RDDs and DataFrames</h2></div></div></div><p class="calibre11">At the core of the Spark engine is its main data abstraction, called a <span class="strong"><strong class="calibre19">Resilient Distributed Dataset</strong></span> (<span class="strong"><strong class="calibre19">RDD</strong></span>). An RDD is composed of one or more data sources and is defined by the user as a series of transformations (aka lineage) on one or more stable (concrete) data sources. Every RDD or RDD partition knows how to recreate itself on failure using the lineage graph, thereby providing fault tolerance. RDD is an immutable data structure, implying that it is sharable between threads without synchronization overheads and hence amenable for parallelization. Operations on RDDs are either transformations or actions. Transformations are individual steps in the lineage. In other words, they are operations that create RDDs because every transformation is getting data from a stable data source or transforming an immutable RDD and creating another RDD. Transformations are simply declarations; they are not evaluated until an <span class="strong"><em class="calibre22">action</em></span> operation is applied on that RDD. Actions are the operations that utilize the RDDs.</p><p class="calibre11">Spark optimizes RDD computation based on the action on hand. For example, if the action is to read the first line, only one partition is computed, skipping the rest. It automatically performs in-memory computation with graceful degradation (spills it to disk when memory is insufficient) and distributes processing across all the cores. You may cache an RDD if it is frequently accessed in your program logic, thereby avoiding recomputing overhead.</p><p class="calibre11">The R language provides a two-dimensional data structure called a <span class="strong"><em class="calibre22">DataFrame</em></span> which makes data manipulation convenient. Apache Spark comes with its own DataFrames that are inspired by the DataFrame in R and Python (through Pandas). A Spark DataFrame is a specialized data structure that is built on top of the RDD data structure abstraction. It provides distributed DataFrame implementation that looks very similar to R DataFrame from the developer perspective and at the same time can support very large datasets. The Spark dataset API adds structure to DataFrames and this structure provides information for more optimization under the hood.</p></div><div class="calibre2" title="Getting started"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec84" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Getting started</h2></div></div></div><p class="calibre11">Now that we have understood the underlying data structures and the runtime view, it is time to run a few commands. In this section, we assume that you already have R and Spark successfully installed and added to the path. We also assume that the <code class="literal">SPARK_HOME</code> environment variable is set. Let us see how to access SparkR from R shell or R Studio:</p><pre class="programlisting">&gt; R  // Start R shell  
&gt; Sys.getenv("SPARK_HOME") //Confirm SPARK_HOME is set 
  &lt;Your SPARK_HOME path&gt; 
&gt; library(SparkR, lib.loc = 
    c(file.path(Sys.getenv("SPARK_HOME"), "R", "lib"))) 
 
Attaching package: 'SparkR' 
The following objects are masked from 'package:stats': 
 
    cov, filter, lag, na.omit, predict, sd, var, window 
 
The following objects are masked from 'package:base': 
 
    as.data.frame, colnames, colnames&lt;-, drop, endsWith, intersect, 
    rank, rbind, sample, startsWith, subset, summary, transform, union 
&gt; 
 
&gt; //Try help(package=SparkR) if you want to more information 
//initialize SparkSession object 
&gt;  sparkR.session()  
Java ref type org.apache.spark.sql.SparkSession id 1  
&gt; 
Alternatively, you may launch sparkR shell which comes with predefined SparkSession. 
 
&gt; bin/sparkR  // Start SparkR shell  
&gt;      // For simplicity sake, no Log messages are shown here 
&gt; //Try help(package=SparkR) if you want to more information 
&gt; 
</pre><p class="calibre11">This is all you need to do to access the power of Spark DataFrames from within the R environment.</p></div></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Advantages and limitations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec55" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Advantages and limitations</h1></div></div></div><p class="calibre11">The R language has long been the lingua franca of data scientists. Its simple-to-understand DataFrame abstraction, expressive APIs, and vibrant package ecosystem are exactly what the analysts needed. The main challenge was with the scalability. SparkR bridges that gap by providing distributed in-memory DataFrames without leaving the R eco-system. Such a symbiotic relationship allows users to gain the following benefits:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">There is no need for the analyst to learn a new language</li><li class="listitem">The SparkR APIs are similar to R APIs</li><li class="listitem">You can access SparkR from R studio, along with the autocomplete feature</li><li class="listitem">Performing interactive, exploratory analysis of a very large dataset is no longer hindered by memory limitations or long turnaround times</li><li class="listitem">Accessing data from different types of data sources becomes a lot easier. Most of the tasks which were imperative before have become declarative. Check <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://Chapter%204">Chapter 4</a>, <span class="strong"><em class="calibre22">Unified Data Access</em></span>, to learn more</li><li class="listitem">You can freely mix dplyr such as Spark functions, SQL, and R libraries that are still not available in Spark</li></ul></div><p class="calibre11">In spite of all the exciting advantages of combining the best of both worlds, there are still some limitations with this combination. These limitations may not impact every use case, but we need to be aware of them anyway:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">The inherent dynamic nature of R limits the information available for the catalyst optimizer. We may not get the full advantage of optimizations such as predicate pushback when compared to statically typed languages such as Scala.</li><li class="listitem">SparkR does not have support for all the machine learning algorithms that are already available in other APIs such as the Scala API.</li></ul></div><p class="calibre11">In summary, using Spark for data preprocessing and using R for analysis and visualization seems to be the best approach in the near future.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Programming with SparkR"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec56" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Programming with SparkR</h1></div></div></div><p class="calibre11">So far, we have understood the runtime model of SparkR and the basic data abstractions that provide the fault tolerance and scalability. We have understood how to access the Spark API from R shell or R studio. It's time to try out some basic and familiar operations:</p><pre class="programlisting">&gt; 
&gt; //Open the shell 
&gt; 
&gt; //Try help(package=SparkR) if you want to more information 
&gt; 
&gt; df &lt;- createDataFrame(iris) //Create a Spark DataFrame 
&gt; df    //Check the type. Notice the column renaming using underscore 
SparkDataFrame[Sepal_Length:double, Sepal_Width:double, Petal_Length:double, Petal_Width:double, Species:string] 
&gt; 
&gt; showDF(df,4) //Print the contents of the Spark DataFrame 
+------------+-----------+------------+-----------+-------+ 
|Sepal_Length|Sepal_Width|Petal_Length|Petal_Width|Species| 
+------------+-----------+------------+-----------+-------+ 
|         5.1|        3.5|         1.4|        0.2| setosa| 
|         4.9|        3.0|         1.4|        0.2| setosa| 
|         4.7|        3.2|         1.3|        0.2| setosa| 
|         4.6|        3.1|         1.5|        0.2| setosa| 
+------------+-----------+------------+-----------+-------+ 
&gt;  
&gt; head(df,2)  //Returns an R data.frame. Default 6 rows 
  Sepal_Length Sepal_Width Petal_Length Petal_Width Species 
1          5.1         3.5          1.4         0.2  setosa 
2          4.9         3.0          1.4         0.2  setosa 
&gt; //You can use take(df,2) to get the same results 
//Check the dimensions 
&gt; nrow(df) [1] 150 &gt; ncol(df) [1] 5 
</pre><p class="calibre11">The operations look very similar to R DataFrame functions because spark DataFrames are modeled based on R DataFrames and Python (Pandas) DataFrames. But the similarity may create confusion if you are not careful. You may accidentally end up choking your local machine by running a compute-intensive function on an R <code class="literal">data.frame</code>, thinking that the load will be distributed. For example, the intersect function has the same signature in both packages. You need to pay attention to whether the object is of class <code class="literal">SparkDataFrame</code> (Spark DataFrame) or <code class="literal">data.frame</code> (R DataFrame). You also need to minimize back and forth conversions between local R <code class="literal">data.frame</code> objects and Spark DataFrame objects. Let us get a feel for this distinction by trying out some examples:</p><pre class="programlisting">&gt; 
&gt; //Open the SparkR shell 
&gt; df &lt;- createDataFrame(iris) //Create a Spark DataFrame 
&gt; class(df) [1] "SparkDataFrame" attr(,"package") [1] "SparkR" 
&gt; df2 &lt;- head(df,2) //Create an R data frame 
&gt; class(df2) 
 [1] "data.frame" 
&gt; //Now try running some R command on both data frames 
&gt; unique(df2$Species)   //Works fine as expected [1] "setosa" &gt; unique(df$Species)    //Should fail Error in unique.default(df$Species) : unique() applies only to vectors &gt; class(df$Species)   //Each column is a Spark's Column class [1] "Column" attr(,"package") [1] "SparkR" &gt; class(df2$Species) [1] "character" 
</pre><div class="calibre2" title="Function name masking"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec85" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Function name masking</h2></div></div></div><p class="calibre11">Now that we have tried some basic operations, let us digress a little bit. We have to understand what happens when a loaded library has overlapping function names with the base package or some other package that was already loaded. This is sometimes referred to as function name overlapping, function masking, or name conflict. You might have noticed the messages mentioning the objects masked when the SparkR package is loaded. This is common for any package loaded into the R environment, and is not specific to SparkR alone. If the R environment already contains any function that has the same name as a function in the package being loaded, then any subsequent calls to that function exhibit the behavior of the function in the latest package loaded. If you want to access the previous function instead of the <code class="literal">SparkR</code> function, you need to explicitly prefix that function with its package name, as shown:</p><pre class="programlisting">//First try in R environment, without loading sparkR 
//Try sampling from a column in an R data.frame 
&gt;sample(iris$Sepal.Length,6,FALSE) //Returns any n elements [1] 5.1 4.9 4.7 4.6 5.0 5.4 &gt;sample(head(iris),3,FALSE) //Returns any 3 columns 
//Try sampling from an R data.frame 
//The Boolean argument is for with_replacement 
&gt; sample(head 
&gt; head(sample(iris,3,TRUE)) //Returns any 3 columns
  Species Species.1 Petal.Width
1  setosa    setosa         0.2 
2  setosa    setosa         0.2 
3  setosa    setosa         0.2 
4  setosa    setosa         0.2 
5  setosa    setosa         0.2 
6  setosa    setosa         0.4 
 
//Load sparkR, initialize sparkSession and then execute this  
&gt; df &lt;- createDataFrame(iris) //Create a Spark DataFrame 
&gt; sample_df &lt;- sample(df,TRUE,0.3) //Different signature 
&gt; dim(sample_df)  //Different behavior [1] 44  5 
&gt; //Returned 30% of the original data frame and all columns 
&gt; //Try with base prefix 
&gt; head(base::sample(iris),3,FALSE)  //Call base package's sample
  Species Petal.Width Petal.Length 
1  setosa         0.2          1.4
2  setosa         0.2          1.4 
3  setosa         0.2          1.3 
4  setosa         0.2          1.5 
5  setosa         0.2          1.4 
6  setosa         0.4          1.7 
</pre></div><div class="calibre2" title="Subsetting data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec86" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Subsetting data</h2></div></div></div><p class="calibre11">Subsetting operations on R DataFrames are quite flexible and SparkR tries to retain these operations with the same or similar equivalents. We have already seen some operations in the preceding examples but this section presents them in an ordered fashion:</p><pre class="programlisting">//Subsetting data examples 
&gt; b1 &lt;- createDataFrame(beaver1) 
//Get one column 
&gt; b1$temp 
Column temp    //Column class and not a vector 
&gt; //Select some columns. You may use positions too 
&gt; select(b1, c("day","temp")) 
SparkDataFrame[day:double, temp:double] 
&gt;//Row subset based on conditions 
&gt; head(subset(b1,b1$temp&gt;37,select= c(2,3))) 
  time  temp 
1 1730 37.07 
2 1740 37.05 
3 1940 37.01 
4 1950 37.10 
5 2000 37.09 
6 2010 37.02 
&gt; //Multiple conditions with AND and OR 
&gt; head(subset(b1, between(b1$temp,c(36.0,37.0)) |  
        b1$time %in% 900 &amp; b1$activ == 1,c(2:4)),2) 
 time  temp activ 
1  840 36.33     0 
2  850 36.34     0 
</pre><div class="note" title="Note"><div class="inner"><h3 class="title5"><a id="tip15" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Tip</h3><p class="calibre25">At the time of writing this book (Apache Spark 2.o release), row index based slicing is not available. You will not be able to get a specific row or range of rows using the <code class="literal">df[n,]</code> or <code class="literal">df[m:n,]</code> syntax.</p></div></div><pre class="programlisting">//For example, try on a normal R data.frame 
&gt; beaver1[2:4,] 
  day time  temp activ 
2 346  850 36.34     0 
3 346  900 36.35     0 
4 346  910 36.42     0 
//Now, try on Spark Data frame 
&gt; b1[2:4,] //Throws error 
Expressions other than filtering predicates are not supported in the first parameter of extract operator [ or subset() method. 
&gt; 
</pre></div><div class="calibre2" title="Column functions"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec87" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Column functions</h2></div></div></div><p class="calibre11">You will have already noticed the column functions <code class="literal">between</code> in the subsetting data section. These functions operate on the <code class="literal">Column</code> class. As the name suggests, these functions operate on a single column at a time and are usually used in subsetting DataFrames. There are several other handy column functions for common operations such as sorting, casting, and formatting. In addition to working on the values within a column, you can append columns to a DataFrame or drop one or more columns from a DataFrame. Negative column subscripts may be used to omit columns, similar to R. The following examples show the use of <code class="literal">Column</code> class functions in subset operations followed by adding and dropping columns:</p><pre class="programlisting">&gt; //subset using Column operation using airquality dataset as df 
&gt; head(subset(df,isNull(df$Ozone)),2) 
  Ozone Solar_R Wind Temp Month Day 
1    NA      NA 14.3   56     5   5 
2    NA     194  8.6   69     5  10 
&gt; 
&gt; //Add column and drop column examples 
&gt; b1 &lt;- createDataFrame(beaver1) 
 
//Add new column 
&gt; b1$inRetreat &lt;- otherwise(when(b1$activ == 0,"No"),"Yes") 
 head(b1,2) 
  day time  temp activ inRetreat 
1 346  840 36.33     0        No 
2 346  850 36.34     0        No 
&gt; 
//Drop a column.  
&gt; b1$day &lt;- NULL 
&gt; b1  // Example assumes b1$inRetreat does not exist 
SparkDataFrame[time:double, temp:double, activ:double] 
&gt; //Drop columns using negative subscripts 
&gt; b2 &lt;- b1[,-c(1,4)]  &gt; head(b2) 
   time  temp 
1  840 36.33 
2  850 36.34 
3  900 36.35 
4  910 36.42 
5  920 36.55 
6  930 36.69 
&gt;  
</pre></div><div class="calibre2" title="Grouped data"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec88" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Grouped data</h2></div></div></div><p class="calibre11">DataFrame data can be subgrouped using the <code class="literal">group_by</code> function similar to SQL. There are multiple ways of performing such operations. We introduce a slightly complex example in this section. Moreover, we use <code class="literal">%&gt;%</code>, aka the forward pipe operator, provided by the <code class="literal">magrittr</code> library, which provides a mechanism for chaining commands:</p><pre class="programlisting">&gt; //GroupedData example using iris data as df 
&gt; //Open SparkR shell and create df using iris dataset  
&gt; groupBy(df,"Species") 
GroupedData    //Returns GroupedData object 
&gt; library(magrittr)  //Load the required library 
//Get group wise average sepal length 
//Report results sorted by species name 
&gt;df2 &lt;- df %&gt;% groupBy("Species") %&gt;%  
          avg("Sepal_Length") %&gt;%  
          withColumnRenamed("avg(Sepal_Length)","avg_sepal_len") %&gt;% 
          orderBy ("Species") 
//Format the computed double column 
df2$avg_sepal_len &lt;- format_number(df2$avg_sepal_len,2) 
showDF(df2) 
+----------+-------------+ 
|   Species|avg_sepal_len| 
+----------+-------------+ 
|    setosa|         5.01| 
|versicolor|         5.94| 
| virginica|         6.59| 
+----------+-------------+ 
</pre><p class="calibre11">You can keep chaining the operations using the forward pipe operator. Look at the column renamed part of the code carefully. The column name argument is the output of previous operations, which would have completed before commencement of this operation and thus you can safely assume that the <code class="literal">avg(sepal_len)</code> column already exists. The <code class="literal">format_number</code> works as expected, and this is yet another handy <code class="literal">Column</code> operation.</p><p class="calibre11">The next section has another similar example with <code class="literal">GroupedData</code> and its equivalent implementation using <code class="literal">dplyr</code>.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="SparkR DataFrames"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec57" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>SparkR DataFrames</h1></div></div></div><p class="calibre11">In this section, we try out some useful, commonly used operations. First, we try out the traditional R/<code class="literal">dplyr</code> operations and then show equivalent operations using the SparkR API:</p><pre class="programlisting">&gt; //Open the R shell and NOT SparkR shell  
&gt; library(dplyr,warn.conflicts=FALSE)  //Load dplyr first 
//Perform a common, useful operation  
&gt; iris %&gt;%               
+   group_by(Species) %&gt;% +   summarise(avg_length = mean(Sepal.Length),  
+             avg_width = mean(Sepal.Width)) %&gt;% +   arrange(desc(avg_length)) 
Source: local data frame [3 x 3] 
     Species avg_length avg_width 
      (fctr)      (dbl)     (dbl) 
1  virginica      6.588     2.974 
2 versicolor      5.936     2.770 
3     setosa      5.006     3.428 
 
//Remove from R environment 
&gt; detach("package:dplyr",unload=TRUE) 
</pre><p class="calibre11">This operation is very similar to the SQL group and is followed by order. Its equivalent implementation in SparkR is also very similar to the <code class="literal">dplyr</code> example. Look at the following example. Pay attention to the method names and compare their positioning with respect to the preceding <code class="literal">dplyr</code> example:</p><pre class="programlisting">&gt; //Open SparkR shell and create df using iris dataset  
&gt; collect(arrange(summarize(groupBy(df,df$Species),  +     avg_sepal_length = avg(df$Sepal_Length), +     avg_sepal_width = avg(df$Sepal_Width)), +     "avg_sepal_length", decreasing = TRUE))  
     Species avg_sepal_length avg_sepal_width 
1     setosa            5.006           3.428 
2 versicolor            5.936           2.770 
3  virginica            6.588           2.974 
</pre><p class="calibre11">SparkR is intended to be as close to the existing R API as possible. So, the method names look very similar to <code class="literal">dplyr</code> methods. For example, look at the example which has <code class="literal">groupBy</code> whereas <code class="literal">dplyr</code> has <code class="literal">group_by</code>. SparkR supports redundant function names. For example, it has <code class="literal">group_by</code> as well as <code class="literal">groupBy</code> to cater to developers coming from different programming environments. The method names in <code class="literal">dplyr</code> and SparkR are again very close to the SQL keyword <code class="literal">GROUP BY</code>. But the sequence of these method calls is not the same. The example also showed an additional step of converting a Spark DataFrame to an R <code class="literal">data.frame</code> using <code class="literal">collect</code>. The methods are arranged inside out, in the sense that first the data is grouped, then summarized, and then arranged. This is understandable because in SparkR, the DataFrame created in the innermost method becomes the argument for its immediate predecessor and so on.</p><div class="calibre2" title="SQL operations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec89" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>SQL operations</h2></div></div></div><p class="calibre11">If you are not very happy with the syntax in the preceding example, you may want to try writing an SQL string as shown, which does exactly the same as the preceding but uses the good old SQL syntax:</p><pre class="programlisting">&gt; //Register the Spark DataFrame as a table/View 
&gt; createOrReplaceTempView(df,"iris_vw")  
//Look at the table structure and some rows
&gt; collect(sql(sqlContext, "SELECT * FROM iris_tbl LIMIT 5"))
    Sepal_Length Sepal_Width Petal_Length Petal_Width Species 
1          5.1         3.5          1.4         0.2  setosa 
2          4.9         3.0          1.4         0.2  setosa 
3          4.7         3.2          1.3         0.2  setosa 
4          4.6         3.1          1.5         0.2  setosa 
5          5.0         3.6          1.4         0.2  setosa 
&gt; //Try out the above example using SQL syntax 
&gt; collect(sql(sqlContext, "SELECT Species,       avg(Sepal_Length) avg_sepal_length,      avg(Sepal_Width) avg_sepal_width       FROM iris_tbl        GROUP BY Species       ORDER BY avg_sepal_length desc")) 
 
  Species avg_sepal_length avg_sepal_width 
 
1  virginica            6.588           2.974 
2 versicolor            5.936           2.770 
3     setosa            5.006           3.428 
</pre><p class="calibre11">The preceding example looks like the most natural way of implementing the operation on hand, if you are used to fetching data from RDBMS tables. But how are we doing this? The first statement tells Spark to register a temporary table (or, as the name suggests, a view, a logical abstraction of a table). This is not exactly the same as a database table. It is temporary in the sense that it is destroyed when the SparkSession object is destroyed. You are not explicitly writing data into any RDBMS datastore (you have to use <code class="literal">SaveAsTable</code> for that). But when once you register a Spark DataFrame as a temporary table, you are free to use SQL syntax to operate on that DataFrame. The next statement is a basic <code class="literal">SELECT</code> statement that displays column names followed by five rows, as dictated by the <code class="literal">LIMIT</code> keyword. The next SQL statement created a Spark DataFrame containing a Species column followed by two average columns sorted on the average sepal length. This DataFrame is in turn collected as an R <code class="literal">data.frame</code> by using collect. The final result is exactly the same as the preceding example. You are free to use either syntax. For more information and examples, check out the SQL section in<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://chapter%204">Chapter 4</a>, <span class="strong"><em class="calibre22">Unified Data Access</em></span>.</p></div><div class="calibre2" title="Set operations"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec90" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Set operations</h2></div></div></div><p class="calibre11">The usual set operations, such as <code class="literal">union</code>, <code class="literal">intersection</code>, and <code class="literal">minus</code>, are available out of the box in SparkR. In fact, when SparkR is loaded, the warning message shows <code class="literal">intersect</code> as one of the masked functions. The following examples are based on <code class="literal">beaver</code> datasets:</p><pre class="programlisting">&gt; //Create b1 and b2 DataFrames using beaver1 and beaver2 datasets 
&gt; b1 &lt;- createDataFrame(beaver1) 
&gt; b2 &lt;- createDataFrame(beaver2) 
//Get individual and total counts 
&gt; &gt; c(nrow(b1), nrow(b2), nrow(b1) + nrow(b2)) 
[1] 114 100 214 
//Try adding both data frames using union operation 
&gt; nrow(unionAll(b1,b2)) 
[1] 214     //Sum of two datsets 
&gt; //intersect example 
//Remove the first column (day) and find intersection 
showDF(intersect(b1[,-c(1)],b2[,-c(1)])) 
 
+------+-----+-----+ 
|  time| temp|activ| 
+------+-----+-----+ 
|1100.0|36.89|  0.0| 
+------+-----+-----+ 
&gt; //except (minus or A-B) is covered in machine learning examples   
</pre></div><div class="calibre2" title="Merging DataFrames"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec91" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Merging DataFrames</h2></div></div></div><p class="calibre11">The next example illustrates the joining of two DataFrames using the <code class="literal">merge</code> command. The first part of the example shows the R implementation and the next part shows the SparkR implementation:</p><pre class="programlisting">&gt; //Example illustrating data frames merging using R (Not SparkR) 
&gt; //Create two data frames with a matching column 
//Products df with two rows and two columns 
&gt; products_df &lt;- data.frame(rbind(c(101,"Product 1"), 
                    c(102,"Product 2"))) 
&gt; names(products_df) &lt;- c("Prod_Id","Product") 
&gt; products_df 
 Prod_Id   Product 
1     101 Product 1 
2     102 Product 2 
 
//Sales df with sales for each product and month 24x3 
&gt; sales_df &lt;- data.frame(cbind(rep(101:102,each=12), month.abb, 
                    sample(1:10,24,replace=T)*10)) 
&gt; names(sales_df) &lt;- c("Prod_Id","Month","Sales") 
 
//Look at first 2 and last 2 rows in the sales_df 
&gt; sales_df[c(1,2,23,24),] 
   Prod_Id Month Sales 
1      101   Jan    60 
2      101   Feb    40 
23     102   Nov    20 
24     102   Dec   100 
 
&gt; //merge the data frames and examine the data 
&gt; total_df &lt;- merge(products_df,sales_df) 
//Look at the column names 
&gt; colnames(total_df) 
&gt; [1] "Prod_Id" "Product" "Month"   "Sales" 
 
//Look at first 2 and last 2 rows in the total_df 
&gt; total_df[c(1,2,23,24),]     
   Prod_Id   Product Month Sales 
1      101 Product 1   Jan    10 
2      101 Product 1   Feb    20 
23     102 Product 2   Nov    60 
24     102 Product 2   Dec    10 
</pre><p class="calibre11">The preceding piece of code completely relies on R's base package. We have used the same names for join columns in both DataFrames for simplicity. The next piece of code demonstrates the same example using SparkR. It looks similar to the preceding code so look carefully for the differences:</p><pre class="programlisting">&gt; //Example illustrating data frames merging using SparkR 
&gt; //Create an R data frame first and then pass it on to Spark 
&gt; //Watch out the base prefix for masked rbind function 
&gt; products_df &lt;- createDataFrame(data.frame( 
    base::rbind(c(101,"Product 1"), 
    c(102,"Product 2")))) 
&gt; names(products_df) &lt;- c("Prod_Id","Product") 
&gt;showDF(products_df) 
+-------+---------+ 
|Prod_Id|  Product| 
+-------+---------+ 
|    101|Product 1| 
|    102|Product 2| 
+-------+---------+ 
&gt; //Create Sales data frame 
&gt; //Notice the as.data.frame similar to other R functions 
&gt; //No cbind in SparkR so no need for base:: prefix 
&gt; sales_df &lt;- as.DataFrame(data.frame(cbind( 
             "Prod_Id" = rep(101:102,each=12), 
"Month" = month.abb, 
"Sales" = base::sample(1:10,24,replace=T)*10))) 
&gt; //Check sales dataframe dimensions and some random rows  
&gt; dim(sales_df) 
[1] 24  3 
&gt; collect(sample(sales_df,FALSE,0.20)) 
  Prod_Id Month Sales 
1     101   Sep    50 
2     101   Nov    80 
3     102   Jan    90 
4     102   Jul   100 
5     102   Nov    20 
6     102   Dec    50 
&gt; //Merge the data frames. The following merge is from SparkR library 
&gt; total_df &lt;- merge(products_df,sales_df) 
// You may try join function for the same purpose 
//Look at the columns in total_df 
&gt; total_df 
SparkDataFrame[Prod_Id_x:string, Product:string, Prod_Id_y:string, Month:string, Sales:string] 
//Drop duplicate column 
&gt; total_df$Prod_Id_y &lt;- NULL    
&gt; head(total_df) 
  Prod_Id_x   Product Month Sales 
1       101 Product 1   Jan    40 
2       101 Product 1   Feb    10 
3       101 Product 1   Mar    90 
4       101 Product 1   Apr    10 
5       101 Product 1   May    50 
6       101 Product 1   Jun    70 
&gt; //Note: As of Spark 2.0 version, SparkR does not support 
    row sub-setting  
</pre><p class="calibre11">You may want to play with different types of joins, such as left outer join and right outer join, or different column names to get a better understanding of this function.</p></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Machine learning"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec58" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Machine learning</h1></div></div></div><p class="calibre11">SparkR provides wrappers on existing MLLib functions. R formulas are implemented as MLLib feature transformers. A transformer is an ML pipeline (<code class="literal">spark.ml</code>) stage that takes a DataFrame as input and produces another DataFrame as output, which generally contains some appended columns. Feature transformers are a type of transformers that convert input columns to feature vectors and these feature vectors are appended to the source DataFrame. For example, in linear regression, string input columns are one-hot encoded and numeric values are converted to doubles. A label column will be appended (if not there in the data frame already) as a replica of the response variable.</p><p class="calibre11">In this section, we cover example code for the Naive Bayes and Gaussian GLM models. We do not explain the models as such or the summaries they produce. Instead, we go straight away to how it can be done using SparkR.</p><div class="calibre2" title="The Naive Bayes model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec92" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Naive Bayes model</h2></div></div></div><p class="calibre11">The NaÃ¯ve Bayes model is an intuitively simple model that works with categorical data. We'll be training a sample dataset using the NaÃ¯ve Bayes model. We will not explain how the model works but move straight away to training the model using SparkR. If you want more information, please refer to <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch06.xhtml" title="Chapter 6.  Machine Learning">Chapter 6</a>, <span class="strong"><em class="calibre22">Machine Learning</em></span>.</p><p class="calibre11">This example takes a dataset with the average marks and attendance of twenty students. In fact, this dataset has already been introduced in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://Chapter%206">Chapter 6</a>, <span class="strong"><em class="calibre22">Machine Learning</em></span>, for training ensembles. However, let us revisit its contents.</p><p class="calibre11">The students are awarded <code class="literal">Pass</code> or <code class="literal">Fail</code> based on a set of well-defined rules. Two students with IDs <code class="literal">1009</code> and <code class="literal">1020</code> are granted <code class="literal">Pass</code>, even though they would have failed otherwise. Even though we do not provide the actual rules to the model, we expect the model to predict these two students' result as <code class="literal">Fail</code>. Here are the <code class="literal">Pass</code> / <code class="literal">Fail</code> criteria:</p><div class="calibre2"><ul class="itemizedlist"><li class="listitem">Marks &lt; 40 =&gt; Fail</li><li class="listitem">Poor attendance =&gt; Fail</li><li class="listitem">Marks above 40 and attendance Full =&gt; Pass</li><li class="listitem">Marks &gt; 60 and attendance at least Enough =&gt; PassThe following is an example to train Naive Bayes model:</li></ul></div><pre class="programlisting">//Example to train NaÃ¯ve Bayes model 
 
//Read file 
&gt; myFile &lt;- read.csv("../work/StudentsPassFail.csv") //R data.frame 
&gt; df &lt;- createDataFrame(myFile) //sparkDataFrame 
//Look at the data 
&gt; showDF(df,4) 
+---------+---------+----------+------+ 
|StudentId|Avg_Marks|Attendance|Result| 
+---------+---------+----------+------+ 
|     1001|     48.0|      Full|  Pass| 
|     1002|     21.0|    Enough|  Fail| 
|     1003|     24.0|    Enough|  Fail| 
|     1004|      4.0|      Poor|  Fail| 
+---------+---------+----------+------+ 
 
//Make three buckets out of Avg_marks 
// A &gt;60; 40 &lt; B &lt; 60; C &gt; 60 
&gt; df$marks_bkt &lt;- otherwise(when(df$Avg_marks &lt; 40, "C"), 
                           when(df$Avg_marks &gt; 60, "A")) 
&gt; df$marks_bkt &lt;- otherwise(when(df$Avg_marks &lt; 40, "C"), 
                           when(df$Avg_marks &gt; 60, "A")) 
&gt; df &lt;- fillna(df,"B",cols="marks_bkt") 
//Split train and test 
&gt; trainDF &lt;- sample(df,TRUE,0.7) 
&gt; testDF &lt;- except(df, trainDF) 
 
//Build model by supplying RFormula, training data 
&gt; model &lt;- spark.naiveBayes(Result ~ Attendance + marks_bkt, data = trainDF) 
&gt; summary(model) 
$apriori 
          Fail      Pass 
[1,] 0.6956522 0.3043478 
 
$tables 
     Attendance_Poor Attendance_Full marks_bkt_C marks_bkt_B 
Fail 0.5882353       0.1764706       0.5882353   0.2941176   
Pass 0.125           0.875           0.125       0.625       
 
//Run predictions on test data 
&gt; predictions &lt;- predict(model, newData= testDF) 
//Examine results 
&gt; showDF(predictions[predictions$Result != predictions$prediction, 
     c("StudentId","Attendance","Avg_Marks","marks_bkt", "Result","prediction")]) 
+---------+----------+---------+---------+------+----------+                     
|StudentId|Attendance|Avg_Marks|marks_bkt|Result|prediction| 
+---------+----------+---------+---------+------+----------+ 
|     1010|      Full|     19.0|        C|  Fail|      Pass| 
|     1019|    Enough|     45.0|        B|  Fail|      Pass| 
|     1014|      Full|     12.0|        C|  Fail|      Pass| 
+---------+----------+---------+---------+------+----------+ 
//Note that the predictions are not exactly what we anticipate but models are usually not 100% accurate 
</pre></div><div class="calibre2" title="The Gaussian GLM model"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h2 class="title3"><a id="ch07lvl2sec93" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>The Gaussian GLM model</h2></div></div></div><p class="calibre11">In this example, we try to predict temperature based on the values of ozone, solar radiation, and wind:</p><pre class="programlisting">&gt; //Example illustrating Gaussian GLM model using SparkR 
&gt; a &lt;- createDataFrame(airquality) 
//Remove rows with missing values 
&gt; b &lt;- na.omit(a) 
&gt; //Inspect the dropped rows with missing values 
&gt; head(except(a,b),2)    //MINUS set operation 
  Ozone Solar_R Wind Temp Month Day 
1    NA     186  9.2   84     6   4 
2    NA     291 14.9   91     7  14 
 
&gt; //Prepare train data and test data 
traindata &lt;- sample(b,FALSE,0.8) //Not base::sample 
testdata &lt;- except(b,traindata) 
 
&gt; //Build model 
&gt; model &lt;- glm(Temp ~ Ozone + Solar_R + Wind,  
          data = traindata, family = "gaussian") 
&gt; // Get predictions 
&gt; predictions &lt;- predict(model, newData = testdata) 
&gt; head(predictions[,c(predictions$Temp, predictions$prediction)], 
                 5) 
  Temp prediction 
1   90   81.84338 
2   79   80.99255 
3   88   85.25601 
4   87   76.99957 
5   76   71.75683 
</pre></div></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="Summary"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec59" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>Summary</h1></div></div></div><p class="calibre11">To date, SparkR does not support all algorithms available in Spark, but active development is happening to bridge the gap. The Spark 2.0 release has improved algorithm coverage, including NaÃ¯ve Bayes, k-means clustering, and survival regression. Check out the latest documentation for the supported algorithms. More work is underway in bringing out a CRAN release of SparkR, with better integration with R packages and Spark packages, and better RFormula support.</p></div></div>



  
<div id="sbo-rt-content" class="calibre1"><div class="calibre2" title="References"><div class="titlepage"><div class="calibre2"><div class="calibre2"><h1 class="title1"><a id="ch07lvl1sec60" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre9"/>References</h1></div></div></div><div class="calibre2"><ul class="itemizedlist"><li class="listitem"><span class="strong"><em class="calibre22">SparkR: The Past, Present and Future</em></span> by <span class="strong"><em class="calibre22">Shivaram Venkataraman: </em></span><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://shivaram.org/talks/sparkr-summit-2015.pdf">http://shivaram.org/talks/sparkr-summit-2015.pdf</a></li><li class="listitem"><span class="strong"><em class="calibre22">Enabling Exploratory Data Science with Spark and R</em></span> by <span class="strong"><em class="calibre22">Shivaram Venkataraman</em></span> and <span class="strong"><em class="calibre22">Hossein Falaki:</em></span><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r">http://www.slideshare.net/databricks/enabling-exploratory-data-science-with-spark-and-r</a></li><li class="listitem"><span class="strong"><em class="calibre22">SparkR: Scaling R Programs with Spark</em></span> by <span class="strong"><em class="calibre22">Shivaram Venkataraman</em></span> and others: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://shivaram.org/publications/sparkr-sigmod.pdf">http://shivaram.org/publications/sparkr-sigmod.pdf</a></li><li class="listitem"><span class="strong"><em class="calibre22">Recent Developments in SparkR for Advanced Analytics</em></span> by <span class="strong"><em class="calibre22">Xiangrui Meng</em></span>: <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf">http://files.meetup.com/4439192/Recent%20Development%20in%20SparkR%20for%20Advanced%20Analytics.pdf</a></li><li class="listitem">To understand RFormula, try out the following links:<div class="calibre2"><ul class="itemizedlist1"><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html">https://stat.ethz.ch/R-manual/R-devel/library/stats/html/formula.html</a></li><li class="listitem"><a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://spark.apache.org/docs/latest/ml-features.html#rformula">http://spark.apache.org/docs/latest/ml-features.html#rformula</a></li></ul></div><p class="calibre31">
</p></li></ul></div></div></div>



  </body></html>