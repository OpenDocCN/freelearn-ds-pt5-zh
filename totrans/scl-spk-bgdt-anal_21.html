<html><head></head><body>
        <section id="I7KO81-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Interactive Data Analytics with Apache Zeppelin</h1>
                
            
            <article>
                
<p class="mce-root">From a data science perspective, interactive visualization of your data analysis is also important. Apache Zeppelin, is a web-based notebook for interactive and large-scale data analytics with multiple backends and interpreters, such as Spark, Scala, Python, JDBC, Flink, Hive, Angular, Livy, Alluxio, PostgreSQL, Ignite, Lens, Cassandra, Kylin, Elasticsearch, <span>JDBC</span>, HBase, BigQuery, Pig, Markdown, Shell, and even more.</p>
<p class="mce-root">There is no doubt about Spark's ability to handle large-scale datasets in a scalable and fast way. However, one thing in Spark is missing--there is no real-time or interactive visualization support with it. Considering the aforementioned exciting features of Zeppelin, in this chapter, we will discuss how to use Apache Zeppelin for large-scale data analytics using Spark as the interpreter in the backend. In summary, the following topics will be covered:</p>
<ul class="calibre9">
<li class="mce-root1">Introduction to Apache Zeppelin</li>
<li class="mce-root1">Installation and getting started</li>
<li class="mce-root1">Data ingestion</li>
<li class="mce-root1">Data analytics</li>
<li class="mce-root1">Data visualization</li>
<li class="mce-root1">Data collaboration</li>
</ul>


            </article>

            
        </section>
    

        <section id="I8J8Q1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Introduction to Apache Zeppelin</h1>
                
            
            <article>
                
<p class="mce-root">Apache Zeppelin is a web-based notebook that enables you to do data analytics in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive, and collaborative documents with SQL, Scala, and more. The Apache Zeppelin interpreter concept allows any language/data-processing backend to be plugged into Zeppelin. Currently, Apache Zeppelin supports many interpreters, such as Apache Spark, Python, JDBC, Markdown, and Shell. Apache Zeppelin is a relatively new technology from the Apache Software Foundation, which enables data scientists, engineers, and practitioners to take the advantage of the data exploration, visualization, sharing, and collaboration features.</p>


            </article>

            
        </section>
    

        <section id="I9HPC1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Installation and getting started</h1>
                
            
            <article>
                
<p class="mce-root">Since using the other interpreters is not the goal of this book, but using Spark on Zeppelin is, all the code will be written using Scala. In this section, therefore, we will show how to configure Zeppelin using the binary package that contains only the Spark interpreter. Apache Zeppelin officially supports and is tested on the following environments:</p>
<table class="calibre24">
<tbody class="calibre5">
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Requirements</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Value/Version</strong></p>
</td>
<td class="calibre7">
<p class="mce-root"><strong class="calibre1">Other Requirements</strong></p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">Oracle JDK</p>
</td>
<td class="calibre7">
<p class="mce-root">1.7 or higher</p>
</td>
<td class="calibre7">
<p class="mce-root">Set <kbd class="calibre11">JAVA_HOME</kbd></p>
</td>
</tr>
<tr class="calibre6">
<td class="calibre7">
<p class="mce-root">OS</p>
</td>
<td class="calibre7">
<p class="mce-root">macOS 10.X+<br class="title-page-name"/>
Ubuntu 14.X+<br class="title-page-name"/>
CentOS 6.X+<br class="title-page-name"/>
Windows 7 Pro SP1+</p>
</td>
<td class="calibre7">-</td>
</tr>
</tbody>
</table>


            </article>

            
        </section>
    

        <section id="IAG9U1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Installation and configuration</h1>
                
            
            <article>
                
<p class="mce-root">As shown in the preceding table, Java is required to execute Spark codes on Zeppelin. Therefore, if not set up, install and set up Java on any of the aforementioned platforms. Alternatively, you can refer to <a href="part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 1</a>, <em class="calibre8">Introduction to Scala</em>, to learn how to set up Java on your machine.</p>
<p class="mce-root">The latest release of Apache Zeppelin can be downloaded from <a href="https://zeppelin.apache.org/download.html" class="calibre10">https://zeppelin.apache.org/download.html</a>. Each release comes with three options:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><strong class="calibre1">Binary package with all the interpreters</strong>: It contains support for many interpreters. For example, Spark, JDBC, Pig, Beam, Scio, BigQuery, Python, Livy, HDFS, Alluxio, Hbase, Scalding, Elasticsearch, Angular, Markdown, Shell, Flink, Hive, Tajo, Cassandra, Geode, Ignite, Kylin, Lens, Phoenix, and PostgreSQL are currently supported in Zeppelin.</li>
<li value="2" class="mce-root1"><strong class="calibre1">Binary package with the Spark interpreter</strong>: It usually contains only the Spark interpreter. It also contains the interpreter net-install script.</li>
<li value="3" class="mce-root1"><strong class="calibre1">Source</strong>: You can also build Zeppelin with all the latest changes from the GitHub repo (more to follow).</li>
</ol>
<p class="calibre48">To show how to install and configure Zeppelin, we have downloaded the binary package from the following site mirror:</p>
<p class="calibre48"><a href="http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz" class="calibre10">http://www.apache.org/dyn/closer.cgi/zeppelin/zeppelin-0.7.1/zeppelin-0.7.1-bin-netinst.tgz</a></p>
<p class="calibre48">Once you have downloaded it, unzip it somewhere in your machine. Suppose that the path where you have unzipped the file is <kbd class="calibre11">/home/Zeppelin/</kbd>.</p>


            </article>

            
        </section>
    

        <section id="IBEQG1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Building from source</h1>
                
            
            <article>
                
<p class="mce-root">You can also build Zeppelin with all the latest changes from the GitHub repo. If you want to build from source, you must first install the following tools:</p>
<ul class="calibre9">
<li class="mce-root1">Git: any version</li>
<li class="mce-root1">Maven: 3.1.x or higher</li>
<li class="mce-root1">JDK: 1.7 or higher</li>
<li class="mce-root1">npm: the latest version</li>
<li class="mce-root1">libfontconfig: the latest version</li>
</ul>
<p class="mce-root">If you haven't installed Git and Maven yet, check the build requirement instructions from <a href="http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements" class="calibre10">http://zeppelin.apache.org/docs/0.8.0-SNAPSHOT/install/build.html#build-requirements</a>. However, due to page limitation, we have not discussed all the steps in detail. If you're interested, you should refer to this URL for more details: <a href="http://zeppelin.apache.org/docs/snapshot/install/build.html" class="calibre10">http://zeppelin.apache.org/docs/snapshot/install/build.html</a>.</p>


            </article>

            
        </section>
    

        <section id="ICDB21-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Starting and stopping Apache Zeppelin</h1>
                
            
            <article>
                
<p class="mce-root">On all Unix-like platforms (for example, Ubuntu, macOS, and so on), use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ bin/zeppelin-daemon.sh start</strong>
</pre>
<p class="mce-root">If the preceding command is successfully executed, you should observe the following logs on the terminal:</p>
<div class="cdpaligncenter"><img class="image-border298" src="../images/00061.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 1</strong>: Starting Zeppelin from the Ubuntu terminal</div>
<p class="mce-root">If you are on Windows, use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ bin\zeppelin.cmd</strong>
</pre>
<p class="mce-root">After Zeppelin has started successfully, go to <kbd class="calibre11">http://localhost:8080</kbd> with your web browser and you will see that Zeppelin is running. More specifically, you'll see the following view on your browser:</p>
<div class="cdpaligncenter"><img class="image-border299" src="../images/00069.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 2</strong>: Zeppelin is running on http://localhost:8080</div>
<p class="mce-root">Congratulations; you have successfully installed Apache Zeppelin! Now, let's move on to Zeppelin and get started with our data analytics once we have configured the preferred interpreter.</p>
<p class="mce-root">Now, to stop Zeppelin from the command line, issue the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ bin/zeppelin-daemon.sh stop</strong>
</pre>


            </article>

            
        </section>
    

        <section id="IDBRK1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Creating notebooks</h1>
                
            
            <article>
                
<p class="mce-root">Once you are on <kbd class="calibre11">http://localhost:8080/</kbd>, you can explore different options and menus that help you understand how to get familiar with Zeppelin. You can find more on Zeppelin and its user-friendly UI at <a href="https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html" class="calibre10">https://zeppelin.apache.org/docs/0.7.1/quickstart/explorezeppelinui.html</a> (you can refer to the latest quick start documentation too, based on the available versions).</p>
<p class="mce-root">Now, let's first create a sample notebook and get started. As shown in the following figure, you can create a new notebook by clicking on the <span>Create new note</span> option:</p>
<div class="cdpaligncenter"><img class="image-border300" src="../images/00082.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 3</strong>: Creating a sample Zeppelin notebook</div>
<p class="mce-root">As shown in the previous figure, the default interpreter is selected as Spark. In the drop-down list, you will also see only Spark, since we have download the Spark-only binary package for Zeppelin.</p>


            </article>

            
        </section>
    

        <section id="IEAC61-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Configuring the interpreter</h1>
                
            
            <article>
                
<p class="mce-root">Every interpreter belongs to an interpreter group. An interpreter group is a unit of start/stop interpreters. By default, every interpreter belongs to a single group, but the group might contain more interpreters. For example, the Spark interpreter group includes Spark support, pySpark, Spark SQL, and the dependency loader. If you want to execute an SQL statement on Zeppelin, you should specify the interpreter type using the <kbd class="calibre11">%</kbd> sign; for example, for using SQL, you should use <kbd class="calibre11">%sql</kbd>; for mark-down, use <kbd class="calibre11">%md</kbd>, and so on.</p>
<p class="mce-root">For more information, refer to the following image:</p>
<div class="cdpaligncenter"><img class="image-border301" src="../images/00086.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 4</strong>: The interpreter properties for using Spark on Zeppelin Data ingestion</div>
<p class="mce-root">Well, once you have created the notebook, you can start writing Spark code directly in the code section. For this simple example, we will use the bank dataset, which is publicly available for research and can be downloaded from <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/00222/" class="calibre10">https://archive.ics.uci.edu/ml/machine-learning-databases/00222/</a>, courtesy of S. Moro, R. Laureano, and P. Cortez, Using Data Mining for Bank Direct Marketing: An Application of the CRISP-DM Methodology. The dataset contains data such as age, job title, marital status, education, if s/he is a defaulter, bank balance, housing, if borrower loaned from the bank, and so on, about the customer of the bank in a CSV format. A sample of the dataset is given as follows:</p>
<div class="cdpaligncenter"><img class="image-border302" src="../images/00094.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 5</strong>: A sample of the bank dataset</div>
<p class="mce-root">Now, let's first load the data on the Zeppelin notebook:</p>
<pre class="calibre19">
valbankText = sc.textFile("/home/asif/bank/bank-full.csv")
</pre>
<p class="mce-root">Upon the execution of this line of code, create a new paragraph and name it as the data ingestion paragraph:</p>
<div class="cdpaligncenter"><img class="image-border303" src="../images/00098.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 6</strong>: Data ingesting paragraph</div>
<p class="mce-root">If you see the preceding image carefully, the code worked and we did not need to define the Spark context. The reason is that it is already defined there as <kbd class="calibre11">sc</kbd>. You don't even need to define Scala implicitly. We will see an example of this later.</p>


            </article>

            
        </section>
    

        <section id="IF8SO1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data processing and visualization</h1>
                
            
            <article>
                
<p class="mce-root">Now, let's create a case class that will tell us how to pick the selected fields from the dataset:</p>
<pre class="calibre19">
case class Bank(age:Int, job:String, marital : String, education : String, balance : Integer)
</pre>
<p class="mce-root">Now, split each line, filter out the header (starts with <kbd class="calibre11">age</kbd>), and map it into the <kbd class="calibre11">Bank</kbd> case class, as follows:</p>
<pre class="calibre19">
val bank = bankText.map(s=&gt;s.split(";")).filter(s =&gt; (s.size)&gt;5).filter(s=&gt;s(0)!="\"age\"").map( <br class="title-page-name"/>  s=&gt;Bank(s(0).toInt,  <br class="title-page-name"/>  s(1).replaceAll("\"", ""), <br class="title-page-name"/>  s(2).replaceAll("\"", ""), <br class="title-page-name"/>  s(3).replaceAll("\"", ""), <br class="title-page-name"/>  s(5).replaceAll("\"", "").toInt <br class="title-page-name"/>        ) <br class="title-page-name"/>) 
</pre>
<p class="mce-root">Finally, convert to DataFrame and create a temporal table:</p>
<pre class="calibre19">
bank.toDF().createOrReplaceTempView("bank")
</pre>
<p class="cdpalignleft1">The following screenshot shows that all the code snippets were executed successfully without showing any errors:</p>
<div class="cdpaligncenter"><img class="image-border304" src="../images/00102.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 7</strong>: Data process paragraph</div>
<p class="mce-root">To make it more transparent, let's see the status marked in green color (in the top-right corner of the image), as follows, after the code has been executed for each case:</p>
<div class="cdpaligncenter"><img class="image-border305" src="../images/00114.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 8</strong>: A successful execution of Spark code in each paragraph</div>
<p class="mce-root">Now let's load some data to play with the following SQL command:</p>
<pre class="calibre19">
<strong class="calibre1">%sql select age, count(1) from bank where age &gt;= 45 group by age order by age</strong>
</pre>
<p class="mce-root">Note that the preceding line of code is a pure SQL statement that selects the names of all the customers whose age is greater than or equal to 45 (that is, age distribution). Finally, it counts the number for the same customer group.</p>
<p class="mce-root">Now let's see how the preceding SQL statement works on the temp view (that is, <kbd class="calibre11">bank</kbd>):</p>
<div class="cdpaligncenter"><img class="image-border306" src="../images/00126.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 9</strong>: SQL query that selects the names of all the customers with age distribution [Tabular]</div>
<p class="mce-root">Now you can select graph options, such as histogram, pie-chart, bar chart, and so on, from the tab near the table icon (in the result section). For example, using histogram, you can see the corresponding count for <kbd class="calibre11">age group &gt;=45</kbd>.</p>
<div class="cdpaligncenter"><img class="image-border307" src="../images/00092.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 10</strong>: SQL query that selects the names of all the customers with age distribution [Histogram]</div>
<p class="mce-root">This is how it looks using a pie-chart:</p>
<div class="cdpaligncenter"><img class="image-border308" src="../images/00328.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 11</strong>: SQL query that selects the names all the customers with age distribution [pie-chart]</div>
<p class="mce-root">Fantastic! We are now almost ready to do more complex data analytics problems using Zeppelin.</p>


            </article>

            
        </section>
    

        <section id="IG7DA1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Complex data analytics with Zeppelin</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will see how to perform more complex analytics using Zeppelin. At first, we will formalize the problem, and then, will explore the dataset that will be used. Finally, we will apply some visual analytics and machine learning techniques.</p>


            </article>

            
        </section>
    

        <section id="IH5TS1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">The problem definition</h1>
                
            
            <article>
                
<p class="mce-root">In this section, we will build a spam classifier for classifying the raw text as spam or ham. We will also show how to evaluate such a model. We will try to focus using and working with the DataFrame API. In the end, the spam classifier model will help you distinguish between spam and ham messages. The following image shows a conceptual view of two messages (spam and ham respectively):</p>
<div class="cdpaligncenter"><a href="https://blog.codecentric.de/files/2016/06/ham-vs-spam.png" class="calibre3"><img class="image-border309" src="../images/00333.jpeg"/></a></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 12</strong>: Spam and Ham example</div>
<p class="mce-root">We power some basic machine learning techniques to build and evaluate such a classifier for this kind of problem. In particular, the logistic regression algorithm will be used for this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header id="II4EE2-21aec46d8593429cacea59dbdcd64e1c">
                    </header><h1 class="header-title" id="calibre_pb_0">Dataset descripting and exploration</h1>
                
            
            <article>
                
<p class="mce-root">The spam data set that we downloaded from <a href="https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection" class="calibre10">https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</a> consists of 5,564 SMS, which have been classified by hand as either ham or spam. Only 13.4% of these SMSes are spam. This means that the dataset is skewed and provides only a few examples of spam. This is something to keep in mind, as it can introduce bias while training models:</p>
<div class="cdpaligncenter"><img class="image-border310" src="../images/00336.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 13</strong>: A snap of the SMS dataset</div>
<p class="mce-root">So, what does this data look like? As you might have seen, social media text can really get dirty, containing slang words, misspelled words, missing whitespaces, abbreviated words, such as <em class="calibre8">u</em>, <em class="calibre8">urs</em>, <em class="calibre8">yrs</em>, and so on, and, often, a violation of grammar rules. It sometimes even contains trivial words in the messages. Thus, we need to take care of these issues as well. In the following steps, we will encounter these issues for a better interpretation of the analytics.</p>
<p class="mce-root"><strong class="calibre1">Step 1. Load the required packages and APIs on Zeppelin</strong> - Let's load the required packages and APIs and create the first paragraph, before we ingest the dataset on Zeppelin:</p>
<div class="cdpaligncenter"><img class="image-border311" src="../images/00345.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 14</strong>: Package/APIs load paragraph</div>
<p class="mce-root"><strong class="calibre1">Step 2. Load and parse the dataset</strong> - We'll use the CSV parsing library by Databricks (that is, <kbd class="calibre11">com.databricks.spark.csv</kbd>) to read the data into the DataFrame:</p>
<div class="cdpaligncenter"><img class="image-border312" src="../images/00351.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 15</strong>: Data ingesting/load paragraph</div>
<p class="mce-root"><strong class="calibre1">Step 3. Using</strong> <kbd class="calibre11">StringIndexer</kbd> <strong class="calibre1">to create numeric labels</strong> - Since the labels in the original DataFrame are categorical, we will have to convert them back so that we can feed them to or use them in the machine learning models:</p>
<div class="cdpaligncenter"><img class="image-border313" src="../images/00357.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 16</strong>: The StringIndexer paragraph, and the output shows the raw labels, original texts, and corresponding labels.</div>
<p class="mce-root"><strong class="calibre1">Step 4. Using</strong> <kbd class="calibre11">RegexTokenizer</kbd> <strong class="calibre1">to create a bag of words</strong> - We'll use <kbd class="calibre11">RegexTokenizer</kbd> to remove unwanted words and create a bag of words:</p>
<div class="cdpaligncenter"><img class="image-border314" src="../images/00363.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 17</strong>: The RegexTokenizer paragraph, and the output shows the raw labels, original texts, corresponding labels, and tokens</div>
<p class="mce-root"><strong class="calibre1">Step 5. Removing stop words and creating a filtered</strong> <strong class="calibre1">DataFrame</strong> - We'll remove stop words and create a filtered DataFrame for visual analytics. Finally, we show the DataFrame:</p>
<div class="cdpaligncenter"><img class="image-border315" src="../images/00329.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 18</strong>: StopWordsRemover paragraph and the output shows the raw labels, original texts, corresponding labels, tokens, and filtered tokens without the stop words</div>
<p class="mce-root"><strong class="calibre1">Step 6. Finding spam messages/words and their frequency</strong> - Let's try to create a DataFrame containing only the spam words, along with their respective frequency, to understand the context of the messages in the dataset. We can create a paragraph on Zeppelin:</p>
<div class="cdpaligncenter"><img class="image-border316" src="../images/00349.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 19</strong>: Spam tokens with a frequency paragraph</div>
<p class="mce-root">Now, let's see them in the graph using SQL queries. The following query selects all the tokens with frequencies of more than 100. Then, we sort the tokens in a descending order of their frequency. Finally, we use the dynamic forms to limit the number of records. The first one is just a raw tabular format:</p>
<div class="cdpaligncenter"><img class="image-border317" src="../images/00128.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 20</strong>: Spam tokens with a frequency visualization paragraph [Tabular]</div>
<p class="mce-root">Then, we'll use a bar diagram, which provides more visual insights. We can now see that the most frequent words in the spam messages are <span>call</span> and <span>free,</span> with a frequency of 355 and 224 respectively:</p>
<div class="cdpaligncenter"><img class="image-border318" src="../images/00096.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 21</strong>: Spam tokens with a frequency visualization paragraph [Histogram]</div>
<p class="mce-root">Finally, using the pie chart provides much better and wider visibility, especially if you specify the column range:</p>
<div class="cdpaligncenter"><img class="image-border31" src="../images/00145.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 22</strong>: Spam tokens with a frequency visualization paragraph [Pie chart]</div>
<p class="mce-root"><strong class="calibre1">Step 7. Using HashingTF for term frequency</strong> - Use <kbd class="calibre11">HashingTF</kbd> to generate the term frequency of each filtered token, as follows:</p>
<div class="cdpaligncenter"><img class="image-border319" src="../images/00251.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 23</strong>: HashingTF paragraph, and the output shows the raw labels, original texts, corresponding labels, tokens, filtered tokens, and corresponding term-frequency for each row</div>
<p class="mce-root"><strong class="calibre1">Step 8. Using IDF for Term frequency-inverse document frequency (TF-IDF)</strong> - TF-IDF is a feature vectorization method widely used in text mining to reflect the importance of a term to a document in the corpus:</p>
<div class="cdpaligncenter"><img class="image-border320" src="../images/00085.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 24</strong>: IDF paragraph, and the output shows the raw labels, original texts, corresponding labels, tokens, filtered tokens, term-frequency, and the corresponding IDFs for each row</div>
<div class="cdpaligncenter3">
<p class="calibre49"><strong class="calibre27">Bag of words:</strong> The bag of words assigns a value of <kbd class="calibre22">1</kbd> for every occurrence of a word in a sentence. This is probably not ideal, as each category of the sentence, most likely, has the same frequency of <em class="calibre25">the</em>, <em class="calibre25">and</em>, and other words; whereas words such as <em class="calibre25">viagra</em> and <em class="calibre25">sale</em> probably should have an increased importance in figuring out whether or not the text is spam.</p>
<p class="calibre49"><strong class="calibre27">TF-IDF:</strong> This is the acronym for Text Frequency – Inverse Document Frequency. This term is essentially the product of text frequency and inverse document frequency for each word. This is commonly used in the bag of words methodology in NLP or text analytics.</p>
<p class="calibre49"><strong class="calibre27">Using TF-IDF:</strong> Let's take a look at word frequency. Here, we consider the frequency of a word in an individual entry, that is, term. The purpose of calculating text frequency (TF) is to find terms that appear to be important in each entry. However, words such as <em class="calibre25">the</em> and <em class="calibre25">and</em> may appear very frequently in every entry. We want to downweigh the importance of these words, so we can imagine that multiplying the preceding TF by the inverse of the whole document frequency might help find important words. However, since a collection of texts (a corpus) may be quite large, it is common to take the logarithm of the inverse document frequency. In short, we can imagine that high values of TF-IDF might indicate words that are very important to determining what a document is about. Creating the TF-IDF vectors requires us to load all the text into memory and count the occurrences of each word before we can start training our model.</p>
</div>
<p class="mce-root"><strong class="calibre1">Step 9. Using VectorAssembler to generate raw features for the Spark ML pipeline</strong> - As you saw in the previous step, we have only the filtered tokens, labels, TF, and IDF. However, there are no associated features that can be fed into any ML models. Thus, we need to use the Spark VectorAssembler API to create features based on the properties in the previous DataFrame, as follows:</p>
<div class="cdpaligncenter"><img class="image-border321" src="../images/00101.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 25</strong>: The VectorAssembler paragraph that shows using VectorAssembler for feature creations</div>
<p class="mce-root"><strong class="calibre1">Step 10. Preparing the training and test set</strong> - Now we need to prepare the training and test set. The training set will be used to train the Logistic Regression model in Step 11<em class="calibre8">,</em> and the test set will be used to evaluate the model in Step 12. Here, I make it 75% for the training and 25% for the test. You can adjust it accordingly:</p>
<div class="cdpaligncenter"><img class="image-border322" src="../images/00117.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 26</strong>: Preparing training/test set paragraph</div>
<p class="mce-root"><strong class="calibre1">Step 11. Training binary logistic regression model</strong> - Since, the problem itself is a binary classification problem, we can use a binary logistic regression classifier, as follows:</p>
<div class="cdpaligncenter"><img class="image-border323" src="../images/00133.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 27</strong>: LogisticRegression paragraph that shows how to train the logistic regression classifier with the necessary labels, features, regression parameters, elastic net param, and maximum iterations</div>
<p class="mce-root">Note that, here, for better results, we have iterated the training for 200 times. We have set the regression parameter and elastic net params a very low -i.e. 0.0001 for making the training more intensive.</p>
<p class="mce-root"><strong class="calibre1">Step 12. Model evaluation</strong> - Let's compute the raw prediction for the test set. Then, we instantiate the raw prediction using the binary classifier evaluator, as follows:</p>
<div class="cdpaligncenter"><strong class="calibre1"><img class="image-border324" src="../images/00335.jpeg"/></strong></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 28</strong>: Model evaluator paragraph</div>
<p class="mce-root">Now let's compute the accuracy of the model for the test set, as follows:</p>
<div class="cdpaligncenter"><img class="image-border325" src="../images/00346.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 29</strong>: Accuracy calculation paragraph</div>
<p class="mce-root">This is pretty impressive. However, if you were to go with the model tuning using cross-validation, for example, you could gain even higher accuracy. Finally, we will compute the confusion matrix to get more insight:</p>
<div class="cdpaligncenter"><img class="image-border326" src="../images/00350.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 30</strong>: Confusion paragraph shows the number of correct and incorrect predictions summarized with count values and broken down by each class</div>


            </article>

            
        </section>
    

        <section id="IJ2V01-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Data and results collaborating</h1>
                
            
            <article>
                
<p class="mce-root">Furthermore, Apache Zeppelin provides a feature for publishing your notebook paragraph results. Using this feature, you can show the Zeppelin notebook paragraph results on your own website. It's very straightforward; just use the <kbd class="calibre11">&lt;iframe&gt;</kbd> tag on your page. If you want to share the link of your Zeppelin notebook, the first step to publish your paragraph result is <span>Copy a paragraph link</span>. After running a paragraph in your Zeppelin notebook, click the gear button located on the right-hand. Then, click <span>Link this paragraph</span> in the menu, as shown in the following image:</p>
<div class="cdpaligncenter"><img class="image-border327" src="../images/00355.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 31</strong>: Linking the paragraph</div>
<p class="mce-root">Then, just copy the provided link, as shown here:</p>
<div class="cdpaligncenter"><img class="image-border328" src="../images/00358.jpeg"/></div>
<div class="cdpaligncenter1"><strong class="calibre1">Figure 32</strong>: Getting the link for paragraph sharing with collaborators</div>
<p class="mce-root">Now, even if you want to publish the copied paragraph, you may use the <kbd class="calibre11">&lt;iframe&gt;</kbd> tag on your website. Here is an example:</p>
<pre class="calibre19">
&lt;iframe src="http://&lt;ip-address &gt;:&lt; port &gt;/#/notebook/2B3QSZTKR/paragraph/...?asIframe" height="" width="" &gt;&lt;/iframe&gt;<br class="title-page-name"/>  
</pre>
<p class="mce-root">Now, you can show off your beautiful visualization results on your website. This is more or less the end of our data analytics journey with Apache Zeppelin. For more inforamtion and related updates, you should visit the official website of Apache Zeppelin at <a href="https://zeppelin.apache.org/" class="calibre10">https://zeppelin.apache.org/</a>; you can even subscribe to Zeppelin users at <a href="mailto:users-subscribe@zeppelin.apache.org" class="calibre10">users-subscribe@zeppelin.apache.org</a>.</p>


            </article>

            
        </section>
    

        <section id="IK1FI1-21aec46d8593429cacea59dbdcd64e1c">

                            <header>
                    </header><h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            <article>
                
<p class="mce-root">Apache Zeppelin is a web-based notebook that enables you to do data analytics in an interactive way. Using Zeppelin, you can make beautiful data-driven, interactive, and collaborative documents with SQL, Scala, and more. It is gaining more popularity by the day, since more features are being added to recent releases. However, due to page limitations, and to make you more focused on using Spark only, we have shown examples that are only suitable for using Spark with Scala. However, you can write your Spark code in Python and test your notebook with similar ease.</p>
<p class="mce-root">In this chapter, we discussed how to use Apache Zeppelin for large-scale data analytics using Spark in the backend as the interpreter. We saw how to install and get started with Zeppelin. We then saw how to ingest your data and parse and analyse it for better visibility. Then, we saw how to visualize it for better insights. Finally, we saw how to share the Zeppelin notebook with collaborators.</p>


            </article>

            
        </section>
    
</body></html>