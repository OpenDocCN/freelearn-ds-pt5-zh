- en: Decision Trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A decision tree is the arrangement of data in a tree structure where, at each
    node, data is separated into different branches according to the value of the
    attribute at the node.
  prefs: []
  type: TYPE_NORMAL
- en: To construct a decision tree, we will use a standard ID3 learning algorithm
    that chooses an attribute that classifies data samples in the best possible way
    to maximize the information gain—a measure based on information entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What a decision tree is and how to represent data in a decision tree through
    the swim preference example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concepts of information entropy and information gain, theoretically in the
    first instance, before applying the swim preference example in practical terms
    in the *Information theory* section
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use a ID3 algorithm to construct a decision tree from the training data,
    and its implementation in Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to classify new data items using the constructed decision tree through the
    swim preference example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to carry out an alternative analysis of the chess playing problem in [Chapter
    2](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml), Naive Bayes, using decision trees
    and how the results of the two algorithms may differ
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will verify your understanding in the *Problems* section and look at when
    to use decision trees as a method of analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to deal with data inconsistencies during decision tree construction with
    the *Going shopping* example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swim preference – representing data using a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We may have certain preferences that determine whether or not we would swim.
    These can be recorded in a table, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Swimming suit** | **Water temperature** | **Swim preference** |'
  prefs: []
  type: TYPE_TB
- en: '| None | Cold | No |'
  prefs: []
  type: TYPE_TB
- en: '| None | Warm | No |'
  prefs: []
  type: TYPE_TB
- en: '| Small | Cold | No |'
  prefs: []
  type: TYPE_TB
- en: '| Small | Warm | No |'
  prefs: []
  type: TYPE_TB
- en: '| Good | Cold | No |'
  prefs: []
  type: TYPE_TB
- en: '| Good | Warm | Yes |'
  prefs: []
  type: TYPE_TB
- en: 'The data in this table can alternatively be presented in the following decision
    tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b974fd83-2b36-4328-8c77-02a7deccd42d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Decision tree for the Swim preference example'
  prefs: []
  type: TYPE_NORMAL
- en: At the root node, we ask the question—do you have a swimming suit? The response
    to the question separates the available data into three groups, each with two
    rows. If the attribute is `swimming suit = none`, then two rows have the swim
    preference attribute as `no`. Therefore, there is no need to ask a question about
    the temperature of the water, as all the samples with the `swimming suit = none`
    attribute would be classified as `no`. This is also true for the `swimming suit
    = small` attribute. In the case of `swimming suit = good`, the remaining two rows
    can be divided into two classes – `no` and `yes`.
  prefs: []
  type: TYPE_NORMAL
- en: Without further information, we would not be able to classify each row correctly.
    Fortunately, there is one more question that can be asked about each row that
    classifies it correctly. For the row with the `water=cold` attribute, the swimming
    preference is `no`. For the row with the `water=warm` attribute, the swimming
    preference is `yes`.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, starting with the root node, we ask a question at every node and,
    based on the answer, we move down the tree until we reach a leaf node where we
    find the class of the data item corresponding to those answers.
  prefs: []
  type: TYPE_NORMAL
- en: This is how we can use a ready-made decision tree to classify samples of data.
    But it is also important to know how to construct a decision tree from data.
  prefs: []
  type: TYPE_NORMAL
- en: Which attribute has a question at which node? How does this reflect on the construction
    of a decision tree? If we change the order of the attributes, can the resulting
    decision tree classify better than another tree?
  prefs: []
  type: TYPE_NORMAL
- en: Information theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information theory studies the quantification of information, its storage, and
    communication. We introduce concepts of information entropy and information gain,
    which are used to construct a decision tree using the ID3 algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Information entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The information entropy of any given piece data is a measure of the smallest
    amount of information necessary to represent a data item from that data. The units
    of information entropy are familiar - bits, bytes, kilobytes, and so on. The lower
    the information entropy, the more regular the data is, and the more patterns occur
    in the data, thus, the smaller the quantity of information required to represent
    it. That is how compression tools on computers can take large text files and compress
    them to a much smaller size, as words and word expressions keep reoccurring, forming
    a pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Coin flipping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine we flip an unbiased coin. We would like to know whether the result is
    heads or tails. How much information do we need to represent the result? Both
    words, head, and tail, consist of four characters, and if we represent one character
    with one byte (8 bits), as is standard in the ASCII table, then we would need
    four bytes, or 32 bits, to represent the result.
  prefs: []
  type: TYPE_NORMAL
- en: But information entropy is the smallest amount of data necessary to represent
    the result. We know that there are only two possible results—heads or tails. If
    we agree to represent head with 0 and tail with 1, then 1 bit would be sufficient
    to communicate the result efficiently. Here, the data is the space of the possibilities
    of the result of the coin throw. It is the set `{head,tail}` that can be represented
    as a set `{0,1}`. The actual result is a data item from this set. It turns out
    that the entropy of the set is 1\. This is owing to the probability of head and
    tail both being 50%.
  prefs: []
  type: TYPE_NORMAL
- en: Now imagine that the coin is biased and throws heads 25% of the time and tails
    75% of the time. What would the entropy of the probability space `{0,1}` be this
    time? We could certainly represent the result with one bit of information. But
    can we do better? One bit is, of course, indivisible, but maybe we could generalize
    the concept of information to indiscrete amounts.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we knew nothing about the previous result of the coin
    flip unless we looked at the coin. But in the example with the biased coin, we
    know that the result is more likely to be tails. If we recorded *n* results of
    coin flips in a file representing heads with 0 and tails with 1, then about 75 percent
    of the bits there would have the value 1, and 25 percent of them would have the
    value 0\. The size of such a file would be *n* bits. But since it is more regular
    (a pattern of 1s prevails in it), a good compression tool should be able to compress
    it to less than *n* bits.
  prefs: []
  type: TYPE_NORMAL
- en: To learn the theory behind to compression and how much information is necessary
    to represent a data item, let's take a look at a precise definition of information
    entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Definition of information entropy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose that we are given a probability space, *S*, with the elements *1, 2,
    ..., n*. The probability that an element, *i*, will be chosen from the probability
    space is *p[i]*. The information entropy of the probability space is then defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e5fdefb-a6bf-44f2-8797-bb12161e973b.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *log[2]* is a binary logarithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hence, the information entropy of the probability space of unbiased coin throws
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/25c77162-1f29-42fb-9d8a-3aacab40d80e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When the coin is biased, with a 25% chance of heads and a 75% chance of tails,
    then the information entropy of such a space is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd1627da-fe47-48b2-8b52-dde0f8070f10.png)'
  prefs: []
  type: TYPE_IMG
- en: This is less than 1\. Thus, for example, if we had a large file with about 25%
    of 0 bits and 75% of 1 bits, a good compression tool should be able to compress
    it down to about 81.12% of its size.
  prefs: []
  type: TYPE_NORMAL
- en: Information gain
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Information gain is the amount of information entropy gained as a result of
    a certain procedure. For example, if we would like to know the results of three
    fair coins, then the information entropy is 3\. But if we could look at the third
    coin, then the information entropy of the result for the remaining two coins would
    be 2\. Thus, by looking at the third coin, we gained one bit of information, so
    the information gain is 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'We may also gain the information entropy by dividing the whole set, *S*, into
    sets, grouping them by a similar pattern. If we group elements by their value
    of an attribute, *A*, then we define the information gain as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d65d98-1431-4e05-9704-64d62f7dc1a5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *S[v]*, is a set with the elements of *S* that have the value, *v*, for
    the attribute, *A*.
  prefs: []
  type: TYPE_NORMAL
- en: Swim preference – information gain calculation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s calculate the information gain for the six rows in the swim preference
    example by taking a `swimming suit` as an attribute. Because we are interested
    in whether a given row of data is classified as `no` or `yes` in response to the
    question as to whether you should go for a swim, we will use the swim preference
    to calculate the entropy and information gain. We partition the set *S* using
    the `swimming suit` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[none]={(none,cold,no),(none,warm,no)}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[small]={(small,cold,no),(small,warm,no)}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[good]={(good,cold,no),(good,warm,yes)}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The information entropy of *S* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S)=![](img/36bd8ff7-5288-4b68-aa7e-e2ae36444bce.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The information entropy of the partitions is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S[none])=-(2/2)*log[2](2/2)=-log[2](1)=0,* since all instances have the
    class `no`.'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S[small])=0* for a similar reason.'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S[good])=![](img/c6aad04d-c94c-4ac3-8bcb-009ec62772bc.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the information gain is:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IG(S,swimming suit)=E(S)-[(2/6)*E(S[none])+(2/6)*E(S[small])+(2/6)*E(S[good])]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*=0.65002242164-(1/3)=0.3166890883*'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we chose the `water temperature` attribute to partition the set, *S*, what
    would be the information gain, *IG(S,water temperature)*? The water temperature
    partitions the set, *S*, into the following sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[cold]={(none,cold,no),(small,cold,no),(good,cold,no)}*'
  prefs: []
  type: TYPE_NORMAL
- en: '*S[warm]={(none,warm,no),(small,warm,no),(good,warm,yes)}*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Their entropies are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S[cold])=0,* since all instances are classified as no.'
  prefs: []
  type: TYPE_NORMAL
- en: '*E(S[warm])=-(2/3)*log[2](2/3)-(1/3)*log[2](1/3)~0.91829583405*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, the information gain from partitioning the set, *S*, using the *water
    temperature* attribute is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*IG(S,water temperature)=E(S)-[(1/2)*E(S[cold])+(1/2)*E(S[warm])]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*= 0.65002242164-0.5*0.91829583405=0.19087450461*'
  prefs: []
  type: TYPE_NORMAL
- en: This is less than *IG(S,swimming suit)*. Therefore, we can gain more information
    about the set, *S*, (the classification of its instances) by partitioning it as
    per the `swimming suit` attribute instead of the `water temperature` attribute.
    This finding will be the basis of the ID3 algorithm when constructing a decision
    tree in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ID3 algorithm – decision tree construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ID3 algorithm constructs a decision tree from data based on the information
    gain. In the beginning, we start with the set, *S*. The data items in the set, *S*,
    have various properties, according to which we can partition the set, *S*. If
    an attribute, *A*, has the values *{v[1], ..., v[n]}*, then we partition the set,
    *S*, into the sets *S[1]*, ..., *S[n]*, where the set, S[i], is a subset of the
    set, *S*, where the elements have the value, *v[i]*, for the attribute, *A*.
  prefs: []
  type: TYPE_NORMAL
- en: If each element in the set, *S*, has the attributes *A[1], ..., A[m]*, then
    we can partition the set, *S*, according to any of the possible attributes. The
    ID3 algorithm partitions the set, *S*, according to the attribute that yields
    the highest information gain. Now suppose that it has the attribute, *A[1]*. Then,
    for the set, *S*, we have the partitions *S[1], ..., S[n]*, where *A[1]* has the
    possible values *{v[1],..., v[n]}*.
  prefs: []
  type: TYPE_NORMAL
- en: Since we have not constructed a tree yet, we first place a root node. For every
    partition of *S*, we place a new branch from the root. Every branch represents
    one value of the selected attributes. A branch has data samples with the same
    value for that attribute. For every new branch, we can define a new node that
    will have data samples from its ancestor branch.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have defined a new node, we choose another of the remaining attributes
    with the highest information gain for the data at that node to partition the data
    further at that node, and then define new branches and nodes. This process can
    be repeated until we run out of all the attributes for the nodes, or even earlier,
    until all the data at the node has the same class as our interest. In the case
    of the swim preference example, there are only two possible classes for the swimming
    preference—class `no` and class `yes`. The last node is called a **leaf node**,
    and decides the class of a data item from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Swim preference – decision tree construction by the ID3 algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we describe, step by step, how an ID3 algorithm would construct a decision
    tree from the given data samples in the swim preference example. The initial set
    consists of six data samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous sections, we calculated the information gains for both, and
    the only non- classifying attributes, `swimming suit`, and `water temperature`,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, we would choose the `swimming suit` attribute as it has a higher information
    gain. There is no tree drawn yet, so we start from the root node. As the `swimming
    suit` attribute has three possible values – `{none, small, good}`, we draw three
    possible branches out of it for each. Each branch will have one partition from
    the partitioned set *S: S[none]*, *S[small]*, and *S[good]*. We add nodes to the
    ends of the branches. *S*[*none*] data samples have the same swimming preference
    class = `no`, so we do not need to branch that node with a further attribute and
    partition the set. Thus, the node with the data, *S[none]*, is already a leaf
    node. The same is true for the node with the data, *S[small]*.'
  prefs: []
  type: TYPE_NORMAL
- en: But the node with the data, *S[good]*, has two possible classes for swimming
    preference. Therefore, we will branch the node further. There is only one non-classifying
    attribute left—`water temperature`. So there is no need to calculate the information
    gain for that attribute with the data, *S[good]*. From the node, *S[good]*, we
    will have two branches, each with a partition from the set, *S[good]*. One branch
    will have the set of the data sample, *S[good,] [cold]={(good,cold,no)};* the
    other branch will have the partition, *S[good,] [warm]={(good,warm,yes)}*. Each
    of these two branches will end with a node. Each node will be a leaf node, because
    each node has data samples of the same value for the classifying swimming preference
    attribute.
  prefs: []
  type: TYPE_NORMAL
- en: The resulting decision tree has four leaf nodes and is the tree in *Figure 3.1
    – Decision tree for the swim preference example*.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We implement an ID3 algorithm that constructs a decision tree for the data
    given in a CSV file. All sources are in the chapter directory. The most important
    parts of the source code are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Program input**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We input the data from the swim preference example into the program to construct
    a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Program output**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We construct a decision tree from the `swim.csv` data file, with the verbosity
    set to `0`. The reader is encouraged to set the verbosity to `2` to see a detailed
    explanation of how exactly the decision tree is constructed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Classifying with a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we have constructed a decision tree from the data with the attributes *A[1],
    ..., A[m]* and the classes *{c[1], ..., c[k]}*, we can use this decision tree
    to classify a new data item with the attributes *A[1], ..., A[m]* into one of
    the classes *{c[1], ..., c[k]}*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given a new data item that we would like to classify, we can think of each
    node, including the root, as a question for the data sample: *What value does
    that data sample have for the selected attribute, A[i]?* Then, based on the answer,
    we select a branch of the decision tree and move on to the next node. Then, another
    question is answered about the data sample, and another, until the data sample
    reaches the leaf node. A leaf node has one of the classes *{c[1], ..., c[k]}* associated
    with it; for example, *c[i]*. Then, the decision tree algorithm would classify
    the data sample into the class, *c[i]*.'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying a data sample with the swimming preference decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's construct a decision tree for the swimming preference example using the
    ID3 algorithm. Now that we have constructed the decision tree, we would like to
    use it to classify a data sample, *(good,cold,?)* into one of the two classes
    in the set *{no,yes}*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with a data sample at the root of the tree. The first attribute that
    branches from the root is `swimming suit`, so we ask for the value of the `swimming
    suit` attribute of the sample *(good, cold,?)*. We learn that the value of the
    attribute is `swimming suit=good`; therefore, move down the rightmost branch with
    that value for its data samples. We arrive at the node with the `water temperature` attribute
    and ask the question: *What is the value of the water temperature attribute for
    the data sample (good, cold,?)?* We learn that for that data sample, we have `water
    temperature=cold`; therefore, we move down the left-hand branch into the leaf
    node. This leaf is associated with the `swimming preference=no` class. Therefore,
    the decision tree would classify the data sample *(good, cold,?)* to be in that
    swimming preference class; in other words, to complete it (by replacing the question
    mark with the exact data) to the data sample *(**good, cold, no)*.'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the decision tree says that if you have a good swimming suit, but
    the water temperature is cold, then you would still not want to swim based on
    the data collected in the table.
  prefs: []
  type: TYPE_NORMAL
- en: Playing chess – analysis with a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take an example from Chapter 2, *Naive Bayes*, again:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature** | **Wind** | **Sunshine** | **Play** |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Strong | Cloudy | No |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Strong | Cloudy | No |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | Sunny | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | None | Sunny | No |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | Breeze | Cloudy | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Breeze | Sunny | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Breeze | Cloudy | No |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | None | Sunny | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | Strong | Cloudy | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | Cloudy | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Strong | Sunny | ? |'
  prefs: []
  type: TYPE_TB
- en: We would like to find out whether our friend would like to play chess with us
    in the park. But this time, we would like to use decision trees to find the answer.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the initial set, *S*, of the data samples, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we determine the information gain for each of the three non-classifying
    attributes: `temperature`, `wind`, and `sunshine`. The possible values for `temperature`
    are `Cold`, `Warm`, and `Hot`. Therefore, we will partition the set, *S*, into
    three sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We calculate the information entropies for the sets first:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b6d23d69-5c62-455a-808f-83f5f3a7ba2b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9d5fdd96-20e6-42be-a66f-168df9b6fba5.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/73ac61fc-b892-4050-9542-6cae5c95cf0d.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/55f6d338-236f-4860-bcfc-a6afa7c374e3.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/04be8669-816b-4abf-b8a9-84f08fa5d441.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/1bc93be0-e80e-4478-a7eb-7770febac121.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/5cc2ba99-5f78-406c-9384-04a1f5897775.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The possible values for the `wind` attribute are `None`, `Breeze`, and `Strong`.
    Thus, we will split the set, *S*, into the three partitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/913e09cf-1f4a-4838-a44d-7e386857de33.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/21c1f8f6-ef95-446f-8f5b-0ef3181db4af.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/050cedb3-7fc0-4d70-aced-ccd53cc97b6a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The information entropies of the sets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29b98f94-6d23-4d88-be49-85cb9528fa23.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/25ec3d85-19a3-4732-9cbe-b70e8bc0361b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/03d4f9db-4723-4d5f-b1c6-603be3bcef37.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/66ec446d-ffd5-4028-b7ed-485fabe7551b.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7f61d2d0-fb0c-4ac0-a6bf-c9dd108c8bf0.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/c30c697d-c7f2-4338-86c5-74c992d534b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the third attribute, `Sunshine`, has two possible values, `Cloudy`
    and `Sunny`. Hence, it splits the set, *S*, into two sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db8b17d8-8329-4731-9ca0-520393b753d8.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/9f9c929a-dd35-49a6-8e98-3646356ffa2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The entropies of the sets are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/74517c8e-9378-421b-841a-ba76bc3fbd31.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/0013e3ef-8416-470d-a331-41774c28103f.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b022dcbf-097b-4106-b005-385de8861495.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/b5b3caaa-c2e7-4042-849b-82707ebfb928.png)'
  prefs: []
  type: TYPE_IMG
- en: '*IG(S,wind)* and *IG(S,temperature)* are greater than *IG(S,sunshine)*. Both
    of them are equal; therefore, we can choose any of the attributes to form the
    three branches; for example, the first one, `Temperatur``e`. In that case, each
    of the three branches would have the data samples *S[cold]*, *S[warm]*, and *S[hot]*.
    At those branches, we could apply the algorithm further to form the rest of the
    decision tree. Instead, we will use the program to complete it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Output**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have constructed the decision tree, we would like to use it to classify
    a data sample *(warm,strong,sunny,?)* into one of the two classes in the set *{no,yes}*.
  prefs: []
  type: TYPE_NORMAL
- en: We start at the root. What value does the `temperature` attribute have in that
    instance? `Warm`, so we go to the middle branch. What value does the `wind` attribute have
    in that instance? `Strong`, so the instance would fall into the class `No` since
    we have already arrived at the leaf node.
  prefs: []
  type: TYPE_NORMAL
- en: So, our friend would not want to play chess with us in the park, according to
    the decision tree classification algorithm. Please note that the Naive Bayes algorithm
    stated otherwise. An understanding of the problem is required to choose the best
    possible method. At other times, a method with greater accuracy is one that takes
    into consideration the results of several algorithms or several classifiers, as
    in the case of the random forest algorithm in [Chapter 4](0a6e5d42-ab32-49c4-934f-7f1954eb1a25.xhtml), *Random
    Forests*.
  prefs: []
  type: TYPE_NORMAL
- en: Going shopping – dealing with data inconsistencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have the following data about the shopping preferences of our friend, Jane:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature** | **Rain** | **Shopping** |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | None | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | No |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Strong | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | None | No |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Strong | No |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | None | ? |'
  prefs: []
  type: TYPE_TB
- en: We would like to find out, using a decision tree, whether Jane would go shopping
    if the outside temperature was cold with no rain.
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we should be careful, as there are instances of the data that have the
    same values for the same attributes, but different classes; that is, `(cold,none,yes)`
    and `(cold,none,no)`. The program we made would form the following decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: But at the leaf node `[Rain=None]` with the parent `[Temperature=Cold]`, there
    are two data samples with both classes, `no` and `yes`. We cannot, therefore,
    classify an instance `(cold,none,?)` accurately. For the decision tree algorithm
    to work better, we would have to provide a class at the leaf node with the greatest
    weight—that is, the majority class. An even better approach would be to collect
    values for more attributes for the data samples so that we can make a decision
    more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, given the available data, we are uncertain whether Jane would go
    shopping or not.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how a decision tree ID3 algorithm first constructs
    a decision tree from the input data and then classifies a new data instance using
    the constructed tree. The decision tree was constructed by selecting the attribute
    for branching with the highest information gain. We studied how information gain
    measures the amount of information that can be learned in terms of the gain in
    information entropy.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned that the decision tree algorithm can achieve a different result
    from other algorithms, such as Naive Bayes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to combine various algorithms or classifiers
    into a decision forest (called **random forest**) in order to achieve a more accurate
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Problem 1**: What is the information entropy of the following multisets?'
  prefs: []
  type: TYPE_NORMAL
- en: a) {1,2}, b) {1,2,3}, c) {1,2,3,4}, d) {1,1,2,2}, e) {1,1,2,3}
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 2**: What is the information entropy of the probability space induced
    by the biased coin that shows head with a probability of 10%, and tail with a
    probability of 90%?'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 3**: Let''s take another example of playing chess from [Chapter 2](4f54d05d-6791-47c5-a260-c2fd563a55cf.xhtml),
    *Naive Bayes*:'
  prefs: []
  type: TYPE_NORMAL
- en: a) What is the information gain for each of the non-classifying attributes in
    the table?
  prefs: []
  type: TYPE_NORMAL
- en: b) What is the decision tree constructed from the given table?
  prefs: []
  type: TYPE_NORMAL
- en: c) How would you classify a data sample `(Warm,Strong,Spring,?)` according to
    the constructed decision tree?
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature** | **Wind** | **Season** | **Play** |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Strong | Winter | No |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Strong | Autumn | No |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | Summer | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | None | Spring | No |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | Breeze | Autumn | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Breeze | Spring | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | Breeze | Winter | No |'
  prefs: []
  type: TYPE_TB
- en: '| Cold | None | Spring | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Hot | Strong | Summer | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | None | Autumn | Yes |'
  prefs: []
  type: TYPE_TB
- en: '| Warm | Strong | Spring | ? |'
  prefs: []
  type: TYPE_TB
- en: '**Problem 4**: **Mary and temperature preferences**: Let''s take the example
    from [Chapter 1](e0824a1e-65dc-4fee-a0e5-56170fdb36b9.xhtml), *Classification
    Using K Nearest Neighbors*, regarding Mary''s temperature preferences:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Temperature in °C** | **Wind speed in kmph** | **Mary''s perception** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 0 | Cold |'
  prefs: []
  type: TYPE_TB
- en: '| 25 | 0 | Warm |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 5 | Cold |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 3 | Warm |'
  prefs: []
  type: TYPE_TB
- en: '| 18 | 7 | Cold |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 10 | Cold |'
  prefs: []
  type: TYPE_TB
- en: '| 22 | 5 | Warm |'
  prefs: []
  type: TYPE_TB
- en: '| 24 | 6 | Warm |'
  prefs: []
  type: TYPE_TB
- en: We would like to use decision trees to decide whether our friend, Mary, would
    feel warm or cold in a room with a temperature of 16°C and a wind speed of 3 km/h.
  prefs: []
  type: TYPE_NORMAL
- en: Can you please explain how a decision tree algorithm could be used here and
    how beneficial it would be to use it for this example?
  prefs: []
  type: TYPE_NORMAL
- en: Analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Problem 1**: Here are entropies of the multisets:'
  prefs: []
  type: TYPE_NORMAL
- en: a) ![](img/7f1d570b-442a-4461-bcf2-a3969e95bac8.png)
  prefs: []
  type: TYPE_NORMAL
- en: b) ![](img/cdb58fb4-3f36-4ea9-84ae-99a1d32285d3.png)
  prefs: []
  type: TYPE_NORMAL
- en: c) ![](img/7912ec03-379f-4fc1-9d4d-d9e98db19757.png)
  prefs: []
  type: TYPE_NORMAL
- en: d) ![](img/20279a9c-d349-4c0b-bf66-d277eb2ff08c.png)
  prefs: []
  type: TYPE_NORMAL
- en: e) ![](img/4b287548-1003-4188-aced-deabb5645e5b.png)
  prefs: []
  type: TYPE_NORMAL
- en: Note here that the information entropy of the multisets that have more than
    two classes is greater than 1, so we need more than one bit of information to
    represent the result. But is this true for every multiset that has more than two
    classes of elements?
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 2**: *![](img/bcd665ff-2e2f-444c-9b5f-bc9e17a3eae3.png)*'
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 3**: a) The information gains for the three attributes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a94df6b-1916-46b4-a830-dc6ce3356092.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/43db9692-58e3-423a-9431-040c14eee8ac.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/a1267b6c-2d2e-450c-a578-38aec73de774.png)'
  prefs: []
  type: TYPE_IMG
- en: 'b) Therefore, we would choose the `season` attribute to branch from the root
    node as it has the highest information gain. Alternatively, we can put all the
    input data into the program to construct a decision tree, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: c) According to the decision tree constructed, we would classify the data sample
    `(warm,strong,spring,?)` to the class `Play=Yes` by going to the bottom-most branch
    from the root node and then arriving at the leaf node by taking the middle branch.
  prefs: []
  type: TYPE_NORMAL
- en: '**Problem 4**: Here, the decision tree algorithm may not perform that well
    without any processing of the data. If we considered every class of a temperature,
    then 25°C would still not occur in the decision tree as it is not in the input
    data, so we would not be able to classify how Mary would feel at 16°C and a wind
    speed of 3 km/h.'
  prefs: []
  type: TYPE_NORMAL
- en: We could alternatively divide the temperature and wind speed into intervals
    in order to reduce the classes so that the resulting decision tree could classify
    the input instance. But it is this division, the intervals into which 25°C and
    3 km/h should be classified, that is the fundamental part of the analysis procedure
    for this type of problem. Thus, decision trees without any serious modification
    could not analyze the problem well.
  prefs: []
  type: TYPE_NORMAL
