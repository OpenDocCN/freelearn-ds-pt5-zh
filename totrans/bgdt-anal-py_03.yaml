- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with Big Data Frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Explain the HDFS and YARN Hadoop components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform file operations with HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compare a pandas DataFrame with a Spark DataFrame
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read files from a local filesystem and HDFS using Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write files in Parquet format using Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write partitioned files in Parquet for fast analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulate non-structured data with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will explore big data tools such as Hadoop and Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw in the previous chapters how to work with data using pandas and Matplotlib
    for visualization and the other tools in the Python data science stack. So far,
    the datasets that we have used have been relatively small and with a relatively
    simple structure. Real-life datasets can be orders of magnitude larger than can
    fit into the memory of a single machine, the time to process these datasets can
    be long, and the usual software tools may not be up to the task. This is the usual
    definition of what big data is: an amount of data that does not fit into memory
    or cannot be processed or analyzed in a reasonable amount of time by common software
    methods. What is big data for some may not be big data for others, and this definition
    can vary depending on who you ask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Big Data is also associated with the 3 V’s (later extended to 4 V’s):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Volume**: Big data, as the name suggests, is usually associated with very
    large volumes of data. What is large depends on the context: for one system, gigabytes
    can be large, while for another, we have to go to petabytes of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variety**: Usually, big data is associated with different data formats and
    types, such as text, video, and audio. Data can be structured, like relational
    tables, or unstructured, like text and video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Velocity**: The speed at which data is generated and stored is faster than
    other systems and produced more continuously. Streaming data can be generated
    by platforms such as telecommunications operators or online stores, or even Twitter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Veracity**: This was added later and tries to show that knowing the data
    that is being used and its meaning is important in any analysis work. We need
    to check whether the data corresponds with what we expect the data to be, that
    the transformation process didn’t change the data, and whether it reflects what
    was collected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But one aspect that makes big data compelling is the analysis component: big
    data platforms are created to allow analysis and information extraction over these
    large datasets. This is where this chapter starts: we will learn how to manipulate,
    store, and analyze large datasets using two of the most common and versatile frameworks
    for big data: Hadoop and Spark.'
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apache Hadoop is a set of software components created for the parallel storage
    and computation of large volumes of data. The main idea at the time of its inception
    was to use commonly available computers in a distributed fashion, with high resiliency
    against failure and distributed computation. With its success, more high-end computers
    started to be used on Hadoop clusters, although commodity hardware is still a
    common use case.
  prefs: []
  type: TYPE_NORMAL
- en: By parallel storage, we mean any system that stores and retrieves stored data
    in a parallel fashion, using several nodes interconnected by a network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hadoop is composed of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hadoop Common**: the basic common Hadoop items'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop YARN**: a resource and job manager'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop MapReduce**: a large-scale parallel processing engine'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hadoop Distributed File System** (**HDFS**): as the name suggests, HDFS is
    a file system that can be distributed over several machines, using local disks,
    to create a large storage pool:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.1: Architecture of HDFS](img/C12913_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Architecture of HDFS'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Another important component is **YARN** (**Yet Another Resource Negotiator**),
    a resource manager and job scheduler for Hadoop. It is responsible for managing
    jobs submitted to the Hadoop cluster, allocating memory and CPU based on the required
    and available resources.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop popularized a parallel computation model called MapReduce, a distributed
    computation paradigm first developed by Google. It’s possible to run a program
    using MapReduce in Hadoop directly. But since Hadoop’s creation, other parallel
    computation paradigms and frameworks have been developed (such as Spark), so MapReduce
    is not commonly used for data analysis. Before diving into Spark, let’s see how
    we can manipulate files on the HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating Data with the HDFS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The HDFS is a distributed file system with an important distinction: it was
    designed to run on thousands of computers that were not built specially for it—so-called
    **commodity hardware**. It doesn’t require any special networking gear or special
    disks, it can run on common hardware. Another idea that permeates HDFS is that
    it is resilient: hardware will always fail, so instead of trying to prevent failure,
    the HDFS works around this by being extremely fault-tolerant. It assumes that
    failures will occur considering its scale, so the HDFS implements fault detection
    for fast and automatic recovery. It is also portable, running in diverse platforms,
    and can hold single files with terabytes of data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the big advantages from a user perspective is that the HDFS supports
    traditional hierarchical file structure organization (folders and files in a tree
    structure), so users can create folders inside folders and files inside folders
    on each level, simplifying its use and operation. Files and folders can be moved
    around, deleted, and renamed, so users do not need to know about data replication
    or **NameNode**/**DataNode** architecture to use the HDFS; it would look similar
    to a Linux filesystem. Before demonstrating how to access files, we need to explain
    a bit about the addresses used to access Hadoop data. For example, the URI for
    accessing files in HDFS has the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Where `namenode.domainname` is the address configured in Hadoop. Hadoop user
    guide ([https://exitcondition.com/install-hadoop-windows/](https://exitcondition.com/install-hadoop-windows/))
    details a bit more on how to access different parts of the Hadoop system. Let’s
    look at a few examples to better understand how all this works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 16: Manipulating Files in the HDFS'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An analyst just received a large dataset to analyze and it’s stored on an HDFS
    system. How would this analyst list, copy, rename, and move these files? Let’s
    assume that the analyst received a file with raw data, named `new_data.csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start checking the current directories and files using the following
    command on the terminal if you are on Linux based system or command prompt if
    you are on Windows system:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have a local file on disk, called `new_data.csv`, which we want to copy
    to the HDFS data folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that the last part of the command is the path inside HDFS. Now, create
    a folder in HDFS using the command `mkdir`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And move the file to a data folder in HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the name of the CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to check whether the file is present in the current
    location or not:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Commands after the HDFS part have the same name as commands in the Linux shell.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Knowing how to manipulate files and directories with the HDFS is an important
    part of big data analysis, but usually, direct manipulation is done only on ingestion.
    To analyze data, HDFS is not directly used, and tools such as Spark are more powerful.
    Let’s see how to use Spark in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Spark** ([https://spark.apache.org](https://spark.apache.org)) is a unified
    analytics engine for large-scale data processing. Spark started as a project by
    the University of California, Berkeley, in 2009, and moved to the Apache Software
    Foundation in 2013.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark was designed to tackle some problems with the Hadoop architecture when
    used for analysis, such as data streaming, SQL over files stored on HDFS and machine
    learning. It can distribute data over all computing nodes in a cluster in a way
    that decreases the latency of each computing step. Another Spark difference is
    its flexibility: there are interfaces for Java, Scala, SQL, R and Python, and
    libraries for different problems, such as MLlib for machine learning, GraphX for
    graph computation, and Spark Streaming, for streaming workloads.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark uses the worker abstraction, having a driver process that receives user
    input to start parallel executions, and worker processes that reside on the cluster
    nodes, executing tasks. It has a built-in cluster management tool and supports
    other tools, such as Hadoop YARN and Apache Mesos (and even Kubernetes), integrating
    into different environments and resource distribution scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can also be very fast, because it first tries to distribute data over
    all nodes and keep it in memory instead of relying only on data on disk. It can
    handle datasets larger than the total available memory, shifting data between
    memory and disk, but making the process slower than if the entire dataset fitted
    in the total available memory of all nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: Working mechanism of Spark](img/C12913_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Working mechanism of Spark'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Other great advantages are that Spark has interfaces for a large variety of
    local and distributed storage systems, such as HDFS, Amazon S3, Cassandra, and
    others; can connect to RDBMS such as PostgreSQL and MySQL via JDBC or ODBC connectors;
    and can use the **Hive Metastore** to run SQL directly over a HDFS file. File
    formats such as CSV, Parquet, and ORC can also be read directly by Spark.
  prefs: []
  type: TYPE_NORMAL
- en: This flexibility can be a great help when working with big data sources, which
    can have varying formats.
  prefs: []
  type: TYPE_NORMAL
- en: Spark can be used either as an interactive shell with Scala, Python, and R,
    or as a job submission platform with the spark-submit command. The submit method
    is used to dispatch jobs to a Spark cluster coded in a script. The Spark shell
    interface for Python is called PySpark. It can be accessed directly from the terminal,
    where the Python version that is the default will be used; it can be accessed
    using the IPython shell or even inside a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL and Pandas DataFrames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **RDD**, or **Resilient Distributed Dataset**, is the base abstraction that
    Spark uses to work with data. Starting on Spark version 2.0, the recommended API
    to manipulate data is the DataFrame API. The DataFrame API is built on top of
    the RDD API, although the RDD API can still be accessed.
  prefs: []
  type: TYPE_NORMAL
- en: Working with RDDs is considered low-level and all operations are available in
    the DataFrame API, but it doesn’t hurt learning a bit more about the RDD API.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL module enables users to query the data in Spark using SQL queries, similar
    to common relational databases. The DataFrame API is part of the SQL module, which
    works with structured data. This interface for data helps to create extra optimizations,
    with the same execution engine being used, independently of the API or language
    used to express such computations.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API is similar to the **Pandas DataFrame**. In Spark, a DataFrame
    is a distributed collection of data, organized into columns, with each column
    having a name. With Spark 2.0, the DataFrame is a part of the more general Dataset
    API, but as this API is only available for the Java and Scala languages, we will
    discuss only the DataFrame API (called **Untyped Dataset Operations** in the documentation).
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface for Spark DataFrames is similar to the pandas interface, but
    there are important differences:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first difference is that Spark DataFrames are **immutable**: after being
    created, they cannot be altered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second difference is that Spark has two different kinds of operations:
    **transformations** and **actions**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformations** are operations that are applied over the elements of a
    DataFrame and are queued to be executed later, not fetching data yet.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Only when an **action** is called is data fetched and all queued transformations
    are executed. This is called lazy evaluation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 17: Performing DataFrame Operations in Spark'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s start using Spark to perform input/output and simple aggregation operations.
    The Spark interface, as we said before, was inspired by the pandas interface.
    What we learned in *Chapter 2*, *Statistical Visualizations Using Matplotlib and
    Seaborn*, can be applied here, making it easier to carry out more complex analysis
    faster, including aggregations, statistics, computation, and visualization on
    aggregated data later on. We want to read a CSV file, as we did before, to perform
    some analysis on it:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s use the following command on Jupyter notebook to create a Spark
    session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s use the following command to read the data from the `mydata.csv`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we said before, Spark evaluation is lazy, so if we want to show what values
    are inside the DataFrame, we need to call the action, as illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This is not necessary with pandas: printing the DataFrame would work directly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Exercise 18: Accessing Data with Spark'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After reading our DataFrame and showing its contents, we want to start manipulating
    the data so that we can do an analysis. We can access data using the same NumPy
    selection syntax, providing the column name as `Column`:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s select one column from the DataFrame that we ingested in the previous
    exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is different from what we’ve seen with pandas. The method that selects
    values from columns in a Spark DataFrame is `select`. So, let’s see what happens
    when we use this method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using the same DataFrame again, use the `select` method to select the name
    column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it changed from `Column` to `DataFrame`. Because of that, we can use the
    methods for DataFrames. Use the `show` method for showing the results from the
    `select` method for `age`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s select more than one column. We can use the names of the columns to do
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is extensible for other columns, selecting by name, with the same syntax.
    We will look at more complex operations, such as **aggregations with GroupBy**,
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 19: Reading Data from the Local Filesystem and the HDFS'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw before, to read files from the local disk, just give Spark the path
    to it. We can also read several other file formats, located in different storage
    systems. Spark can read files in the following formats:'
  prefs: []
  type: TYPE_NORMAL
- en: CSV
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: JSON
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ORC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parquet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'And can read from the following storage systems:'
  prefs: []
  type: TYPE_NORMAL
- en: JDBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ODBC
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HDFS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on a URL scheme, as an exercise, let’s read data from different places
    and formats:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries on the Jupyter notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s assume that we have to get some data from a JSON file, which is common
    for data collected from APIs on the web. To read a file directly from HDFS, use
    the following URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that, with this kind of URL, we have to provide the full address of the
    HDFS endpoint. We could also use only the simplified path, assuming that Spark
    was configured with the right options.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, read the data into the Spark object using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, we choose the format on the `read` method and the storage system on the
    access URL. The same method is used to access JDBC connections, but usually, we
    have to provide a user and a password to connect. Let’s see how to connect to
    a PostgreSQL database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Exercise 20: Writing Data Back to the HDFS and PostgreSQL'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw with pandas, after performing some operations and transformations,
    let’s say that we want to write the results back to the local file system. This
    can be very useful when we finish an analysis and want to share the results with
    other teams, or we want to show our data and results using other tools:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the `write` method directly on the HDFS from the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the relational database, use the same URL and properties dictionary as
    illustrated here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This gives Spark great flexibility in manipulating large datasets and combining
    them for analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Spark can be used as an intermediate tool to transform data, including aggregations
    or fixing data issues, and saving in a different format for other applications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Writing Parquet Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Parquet data format ([https://parquet.apache.org/](https://parquet.apache.org/))
    is binary, columnar storage that can be used by different tools, including Hadoop
    and Spark. It was built to support compression, to enable higher performance and
    storage use. Its column-oriented design helps with data selection for performance,
    as only the data in the required columns are retrieved, instead of searching for
    the data and discarding values in rows that are not required, reducing the retrieval
    time for big data scenarios, where the data is distributed and on disk. Parquet
    files can also be read and written by external applications, with a C++ library,
    and even directly from pandas.
  prefs: []
  type: TYPE_NORMAL
- en: The Parquet library is currently being developed with the **Arrow project**
    ([https://arrow.apache.org/](https://arrow.apache.org/)).
  prefs: []
  type: TYPE_NORMAL
- en: When considering more complex queries in Spark, storing the data in Parquet
    format can increase performance, especially when the queries need to search a
    massive dataset. Compression helps to decrease the data volume that needs to be
    communicated when an operation is being done in Spark, decreasing the network
    I/O. It also supports schemas and nested schemas, similar to JSON, and Spark can
    read the schema directly from the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Parquet writer in Spark has several options, such as mode (append, overwrite,
    ignore or error, the default option) and compression, a parameter to choose the
    compression algorithm. The available algorithms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gzip`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lzo`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`brottli`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lz4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Snappy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncompressed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The default algorithm is **snappy**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 21: Writing Parquet Files'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s say that we received lots of CSV files and we need to do some analysis
    on them. We also need to reduce the data volume size. We can do that using Spark
    and Parquet. Before starting our analysis, let’s convert the CSV files to Parquet:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, read the CSV files from the HDFS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the CSV files in the DataFrame back to the HDFS, but now in Parquet format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now read the Parquet file to a new DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The `write.parquet` method creates a folder named `data_file` with a file with
    a long name such as `part-00000-1932c1b2-e776-48c8-9c96-2875bf76769b-c000.snappy.parquet`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Increasing Analysis Performance with Parquet and Partitions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An important concept that Parquet supports and that can also increase the performance
    of queries is partitioning. The idea behind partitioning is that data is split
    into divisions that can be accessed faster. The partition key is a column with
    the values used to split the dataset. Partitioning is useful when there are divisions
    in your data that are meaningful to work on separately. For example, if your data
    is based on time intervals, a partition column could be the year value. That way,
    when a query uses a filter value based on the year, only the data in the partition
    that matches the requested year is read, instead of the entire dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Partitions can also be nested and are represented by a directory structure
    in Parquet. So, let’s say that we also want to partition by the column month.
    The folder structure of the Parquet dataset would be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Partitioning allows better performance when partitions are filtered, as only
    the data in the chosen partition will be read, increasing performance. To save
    partitioned files, the `partitionBy` option should be used, either in the `parquet`
    command or as the previous command chained to the write operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The alternative method is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The latter format can be used with the previous operations. When reading partitioned
    data, Spark can infer the partition structure from the directory structure.
  prefs: []
  type: TYPE_NORMAL
- en: An analyst can considerably improve the performance of their queries if partitioning
    is used correctly. But partitioning can hinder performance if partition columns
    are not chosen correctly. For example, if there is only one year in the dataset,
    partitioning per year will not provide any benefits. If there is a column with
    too many distinct values, partitioning using this column could also create problems,
    creating too many partitions that would not improve speed and may even slow things
    down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 22: Creating a Partitioned Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We discovered in our preliminary analysis that the data has date columns, one
    for the year, one for the month, and one for the day. We will be aggregating this
    data to get the minimum, mean, and maximum values per year, per month, and per
    day. Let’s create a partitioned dataset saved in Parquet from our database:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define a PostgreSQL connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the data from PostgreSQL to a DataFrame, using the JDBC connector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And let’s convert this into partitioned Parquet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The use of Spark as an intermediary for different data sources, and considering
    its data processing and transformation capabilities, makes it an excellent tool
    for combining and analyzing data.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Unstructured Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unstructured data usually refers to data that doesn’t have a fixed format. CSV
    files are structured, for example, and JSON files can also be considered structured,
    although not tabular. Computer logs, on the other hand, don’t have the same structure,
    as different programs and daemons will output messages without a common pattern.
    Images are also another example of unstructured data, like free text.
  prefs: []
  type: TYPE_NORMAL
- en: We can leverage Spark’s flexibility for reading data to parse unstructured formats
    and extract the required information into a more structured format, allowing analysis.
    This step is usually called **pre-processing** or **data wrangling**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Parsing Text and Cleaning'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will read a text file, split it into lines and remove
    the words `the` and `a` from the string given string:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the text file `shake.txt` ([https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt](https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson03/data/shake.txt))
    into the Spark object using the `text` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the lines from the text using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This splits each line in the file as an entry in the list. To check the result,
    you can use the `collect` method, which gathers all data back to the driver process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s count the number of lines, using the `count` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Be careful when using the `collect` method! If the DataFrame or RDD being collected
    is larger than the memory of the local driver, Spark will throw an error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s first split each line into words, breaking it by the space around
    it, and combining all elements, removing words in uppercase:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s also remove the words `the` and `a`, and punctuations like ‘`.`’, ‘`,`’
    from the given string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to remove the stop words from our token list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can now process our token list and count the unique words. The idea is to
    generate a list of tuples, where the first element is the token and the second
    element is the count of that particular token.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s map our token to a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `reduceByKey` operation, which will apply the operation to each of
    the lists:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.3: Parsing Text and Cleaning](img/C12913_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Parsing Text and Cleaning'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Remember, `collect()`collects all data back to the driver node! Always check
    whether there is enough memory by using tools such as `top` and `htop`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Removing Stop Words from Text'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will read a text file, split it into lines and remove
    the `stopwords` from the text:'
  prefs: []
  type: TYPE_NORMAL
- en: Read the text file `shake.txt` as used in Exercise 8\.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the lines from the text and create a list with each line.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split each line into words, breaking it by the space around it and remove words
    in uppercase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Remove the stop words: ‘of’, ‘a’, ‘and’, ‘to’ from our token list.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Process the token list and count the unique words, generating list of tuples
    made up of the token and its count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Map our tokens to a list using the `reduceByKey` operation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4: Removing Stop Words from Text](img/C12913_03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Removing Stop Words from Text'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 213.
  prefs: []
  type: TYPE_NORMAL
- en: 'We get the list of tuples, where each tuple is a token and the count of the
    number of times that word appeared in the text. Notice that, before the final
    collect on count (an action), the operations that were transformations did not
    start running right away: we needed the action operation count to Spark start
    executing all the steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Other kinds of unstructured data can be parsed using the preceding example,
    and either operated on directly, such as in the preceding activity or transformed
    into a DataFrame later.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After a review of what big data is, we learned about some tools that were designed
    for the storage and processing of very large volumes of data. Hadoop is an entire
    ecosystem of frameworks and tools, such as HDFS, designed to store data in a distributed
    fashion in a huge number of commodity-computing nodes, and YARN, a resource and
    job manager. We saw how to manipulate data directly on the HDFS using the HDFS
    fs commands.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about Spark, a very powerful and flexible parallel processing
    framework that integrates well with Hadoop. Spark has different APIs, such as
    SQL, GraphX, and Streaming. We learned how Spark represents data in the DataFrame
    API and that its computation is similar to pandas’ methods. We also saw how to
    store data in an efficient manner using the Parquet file format, and how to improve
    performance when analyzing data using partitioning. To finish up, we saw how to
    handle unstructured data files, such as text.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go more deeply into how to create a meaningful
    statistical analysis using more advanced techniques with Spark and how to use
    Jupyter notebooks with Spark.
  prefs: []
  type: TYPE_NORMAL
