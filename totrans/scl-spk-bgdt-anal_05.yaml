- en: Tackle Big Data â€“ Spark Comes to the Party
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An approximate answer to the right problem is worth a good deal more than an
    exact answer to an approximate problem.
  prefs: []
  type: TYPE_NORMAL
- en: '- John Tukey'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learn about data analysis and big data; we see the challenges
    that big data provides and how they are dealt with. You will learn about distributed
    computing and the approach suggested by functional programming; we introduce Google's
    MapReduce, Apache Hadoop, and finally Apache Spark and see how they embrace this
    approach and these techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to data analytics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed computing using Apache Hadoop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here comes Apache Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to data analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Data analytics** is the process of applying qualitative and quantitative
    techniques when examining data with the goal of providing valuable insights. Using
    various techniques and concepts, data analytics can provide the means to explore
    the data **Exploratory Data Analysis** (**EDA**) as well as draw conclusions about
    the data **Confirmatory Data Analysis** (**CDA**). EDA and CDA are fundamental
    concepts of data analytics, and it is important to understand the difference between
    the two.'
  prefs: []
  type: TYPE_NORMAL
- en: EDA involves methodologies, tools, and techniques used to explore data with
    the intention of finding patterns in the data and relationships between various
    elements of the data. CDA involves methodologies, tools, and techniques used to
    provide an insight or conclusion on a specific question based on a hypothesis
    and statistical techniques or simple observation of the data.
  prefs: []
  type: TYPE_NORMAL
- en: A quick example to understand these ideas is that of a grocery store, which
    has asked you to give them ways to improve sales and customer satisfaction as
    well as keep the cost of operations low.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a grocery store with aisles of various products:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00107.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Assume that all sales at the grocery store are stored in some database and that
    you have access to the data for the last 3 months. Typically, businesses store
    data for years as you need sufficient data over a period of time to establish
    any hypothesis or observe any patterns. In this example, our goal is to perform
    better placement of products in various aisles based on how customers are buying
    the products. One hypothesis is that customers often buy products, that are both
    at eye level and also close together. For instance, if Milk is on one corner of
    the store and Yogurt is in other corner of the store, some customers might just
    choose either Milk or Yogurt and just leave the store, causing a loss of business.
    More adverse affects might result in customers choosing another store where products
    are better placed because if the feeling that *things are hard to find at this
    store*. Once that feeling sets in, it also percolates to friends and family eventually
    causing a bad social presence. This phenomenon is not uncommon in the real world
    causing some businesses to succeed while others fail while both seem to be very
    similar in products and prices.
  prefs: []
  type: TYPE_NORMAL
- en: There are many ways to approach this problem starting from customer surveys
    to professional statisticians to machine learning scientists. Our approach will
    be to understand what we can from just the sales transactions alone.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of what the transactions might look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00111.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following are the steps you could follow as part of EDA:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate *Average number of products bought per day = Total of all products
    sold in a day / Total number of receipts for the* *day*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding step for last 1 week, month, and quarter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to understand if there is a difference between weekends and weekdays and
    also time of the day (morning, noon, and evening)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each product, create a list of all other products to see which products
    are usually bought together (same receipt)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the preceding step for 1 day, 1 week, month, and quarter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Try to determine which products should be placed closer together by the number
    of transactions (sorted in descending order).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once we have completed the preceding 6 steps, we can try to reach some conclusions
    for CDA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s assume this is the output we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item** | **Day Of Week** | **Quantity** |'
  prefs: []
  type: TYPE_TB
- en: '| Milk | Sunday | 1244 |'
  prefs: []
  type: TYPE_TB
- en: '| Bread | Monday | 245 |'
  prefs: []
  type: TYPE_TB
- en: '| Milk | Monday | 190 |'
  prefs: []
  type: TYPE_TB
- en: 'In this case, we could state that **Milk** is bought more on *weekends* so
    its better to increase the quantity and variety of Milk products over weekends.
    Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Item1** | **Item2** | **Quantity** |'
  prefs: []
  type: TYPE_TB
- en: '| Milk | Eggs | 360 |'
  prefs: []
  type: TYPE_TB
- en: '| Bread | Cheese | 335 |'
  prefs: []
  type: TYPE_TB
- en: '| Onions | Tomatoes | 310 |'
  prefs: []
  type: TYPE_TB
- en: In this case, we could state that **Milk** and **Eggs** are bought by *more*
    customers in one purchase followed by **Bread** and **Cheese.** So, we could recommend
    that the store realigns the aisles and shelves to move **Milk** and **Eggs** *closer*
    to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The two conclusions we have are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Milk** is bought more on *weekends,* so it''s better to increase the quantity
    and variety of Milk products over weekends.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Milk** and **Eggs** are bought by *more* customers in one purchase followed
    by **Bread** and **Cheese.** So, we could recommend that the store realigns the
    aisles and shelves to move **Milk** and **Eggs** *closer* to each other.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conclusions are usually tracked over a period of time to evaluate the gains.
    If there is no significant impact on sales even after adopting the preceding two
    recommendations for 6 months, we simply invested in the recommendations which
    are not able to give you a good Return On Investment (ROI).
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, you can also perform some analysis with respect to the Profit margin
    and pricing optimizations. This is why you will typically see a single item costing
    more than the average of multiple numbers of the same item bought. Buy one Shampoo
    for $7 or two bottles of Shampoo for $12.
  prefs: []
  type: TYPE_NORMAL
- en: Think about other aspects you can explore and recommend for the grocery store.
    For example, can you guess which products to position near checkout registers
    just based on fact that these have no affinity toward any particular product--chewing
    gum, magazines, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Data analytics initiatives support a wide variety of business uses. For example,
    banks and credit card companies analyze withdrawal and spending patterns to prevent
    fraud and identity theft. Advertising companies analyze website traffic to identify
    prospects with a high likelihood of conversion to a customer. Department stores
    analyze customer data to figure out if better discounts will help boost sales.
    Cell Phone operators can figure out pricing strategies. Cable companies are constantly
    looking for customers who are likely to churn unless given some offer or promotional
    rate to retain their customer. Hospitals and pharmaceutical companies analyze
    data to come up with better products and detect problems with prescription drugs
    or measure the performance of prescription drugs.
  prefs: []
  type: TYPE_NORMAL
- en: Inside the data analytics process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data analytics applications involve more than just analyzing data. Before any
    analytics can be planned, there is also a need to invest time and effort in collecting,
    integrating, and preparing data, checking the quality of the data and then developing,
    testing, and revising analytical methodologies. Once data is deemed ready, data
    analysts and scientists can explore and analyze the data using statistical methods
    such as SAS or machine learning models using Spark ML. The data itself is prepared
    by data engineering teams and the data quality team checks the data collected.
    Data governance becomes a factor too to ensure the proper collection and protection
    of the data. Another not commonly known role is that of a Data Steward who specializes
    in understanding data to the byte, exactly where it is coming from, all transformations
    that occur, and what the business really needs from the column or field of data.
  prefs: []
  type: TYPE_NORMAL
- en: Various entities in the business might be dealing with addresses differently,
    **123 N Main St** as opposed to **123 North Main Street.** But, our analytics
    depends on getting the correct address field; otherwise both the addresses mentioned
    above will be considered different and our analytics will not have the same accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: The analytics process starts with data collection based on what the analysts
    might need from the data warehouse, collecting all sorts of data in the organization
    (Sales, Marketing, Employee, Payroll, HR, and so on). Data stewards and the Governance
    team are important here to make sure the right data is collected and that any
    information deemed confidential or private is not accidentally exported out even
    if the end users are all employees.
  prefs: []
  type: TYPE_NORMAL
- en: Social Security Numbers or full addresses might not be a good idea to include
    in analytics as this can cause a lot of problems to the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Data quality processes must be established to make sure the data being collected
    and engineered is correct and will match the needs of the data scientists. At
    this stage, the main goal is to find and fix data quality problems that could
    affect the accuracy of analytical needs. Common techniques are profiling the data
    and cleansing the data to make sure that the information in a dataset is consistent,
    and also that any errors and duplicate records are removed.
  prefs: []
  type: TYPE_NORMAL
- en: Data from disparate source systems may need to be combined, transformed, and
    normalized using various data engineering techniques, such as distributed computing
    or MapReduce programming, Stream processing, or SQL queries, and then stored on
    Amazon S3, Hadoop cluster, NAS, or SAN storage devices or a traditional data warehouse
    such as Teradata. Data preparation or engineering work involves techniques to
    manipulate and organize the data for the planned analytics use.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the data prepared and checked for quality, and it is available
    for the Data scientists or analysts to use, the actual analytical work starts.
    A Data scientist can now build an analytical model using predictive modeling tools
    and languages such as SAS, Python, R, Scala, Spark, H2O, and so on. The model
    is initially run against a partial dataset to test its accuracy in the *training
    phase*. Several iterations of the training phase are common and expected in any
    analytical project. After adjustments at the model level, or sometimes going all
    the way to the Data Steward to get or fix some data being collected or prepared,
    the model output tends to get better and better. Finally, a stable state is reached
    when further tuning does not change the outcome noticeably; at this time, we can
    think of the model as being ready for production usage.
  prefs: []
  type: TYPE_NORMAL
- en: Now, the model can be run in production mode against the full dataset and generate
    outcomes or results based on how we trained the model. The choices made in building
    the analysis, either statistical or machine learning, directly affect the quality
    and the purpose of the model. You cannot look at the sales from groceries and
    figure out if Asians buy more milk than Mexicans as that needs additional elements
    from demographical data. Similarly, if our analysis was focused on customer experience
    (returns or exchanges of products) then it is based on different techniques and
    models than if we are trying to focus on revenue or up-sell customers.
  prefs: []
  type: TYPE_NORMAL
- en: You will see various machine learning techniques in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Analytical applications can thus be realized using several disciplines, teams,
    and skillsets. Analytical applications can be used to generate reports all the
    way to automatically triggering business actions. For example, you can simply
    create daily sales reports to be emailed out to all managers every day at 8 a.m.
    in the morning. But, you can also integrate with Business process management applications
    or some custom stock trading application to take action, such as buying, selling,
    or alerting on activities in the stock market. You can also think of taking in
    news articles or social media information to further influence the decisions to
    be made.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization is an important piece of data analytics and it's hard to
    understand numbers when you are looking at a lot of metrics and calculation. Rather,
    there is an increasing dependence on **Business Intelligence** (**BI**) tools,
    such as Tableau, QlikView, and so on, to explore and analyze data. Of course,
    large-scale visualization such as showing all Uber cars in the country or heat
    maps showing the water supply in New York City requires more custom applications
    or specialized tools to be built.
  prefs: []
  type: TYPE_NORMAL
- en: Managing and analyzing data has always been a challenge across many organizations
    of different sizes across all industries. Businesses have always struggled to
    find a pragmatic approach to capturing information about their customers, products,
    and services. When the company only had a handful of customers who bought a few
    of their items, it was not that difficult. It was not as big a challenge. But
    over time, companies in the markets started growing. Things have become more complicated.
    Now, we have branding Information and social media. We have things that are sold
    and bought over the Internet. We need to come up with different solutions. Web
    development, organizations, pricing, social networks, and segmentations; there's
    a lot of different data that we're dealing with that brings a lot more complexity
    when it comes to dealing, managing, organizing, and trying to gain some insight
    from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As seen in the preceding section, data analytics incorporates techniques, tools,
    and methodologies to explore and analyze data to produce quantifiable outcomes
    for the business. The outcome could be a simple choice of a color to paint the
    storefront or more complicated predictions of customer behavior. As businesses
    grow, more and more varieties of analytics are coming into the picture. In 1980s
    or 1990s , all we could get was what was available in a SQL Data Warehouse; nowadays
    a lot of external factors are all playing an important role in influencing the
    way businesses run.
  prefs: []
  type: TYPE_NORMAL
- en: Twitter, Facebook, Amazon, Verizon, Macy's, and Whole Foods are all companies
    that run their business using data analytics and base many of the decisions on
    it. Think about what kind of data they are collecting, how much data they might
    be collecting, and then how they might be using the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at our grocery store example seen earlier. What if the store starts
    expanding its business to set up 100s of stores. Naturally, the sales transactions
    will have to be collected and stored on a scale that is 100s of times more than
    the single store. But then, no business works independently any more. There is
    a lot of information out there starting from local news, tweets, yelp reviews,
    customer complaints, survey activities, competition from other stores, changing
    demographics, or the economy of the local area, and so on. All such additional
    data can help in better understanding customer behavior and revenue models.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we see increasing negative sentiment regarding the store parking
    facility, then we could analyze this and take corrective action such as validated
    parking or negotiating with the city public transportation department to provide
    more frequent trains or buses for better reach.
  prefs: []
  type: TYPE_NORMAL
- en: Such increasing quantity and a variety of data while provides better analytics
    also poses challenges to the business IT organization trying to store, process,
    and analyze all the data. It is, in fact, not uncommon to see TBs of data.
  prefs: []
  type: TYPE_NORMAL
- en: Every day, we create more than 2 quintillion bytes of data (2 Exa Bytes), and
    it is estimated that more than 90% of the data has been generated in the last
    few years alone.
  prefs: []
  type: TYPE_NORMAL
- en: '**1 KB = 1024 Bytes**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 MB = 1024 KB**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 GB = 1024 MB**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 TB = 1024 GB ~ 1,000,000 MB**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 PB = 1024 TB ~ 1,000,000 GB ~ 1,000,000,000 MB**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 EB = 1024 PB ~ 1,000,000 TB ~ 1,000,000,000 GB ~ 1,000,000,000,000 MB**'
  prefs: []
  type: TYPE_NORMAL
- en: Such large amounts of data since the 1990s, and the need to understand and make
    sense of the data, gave rise to the term *big data*.
  prefs: []
  type: TYPE_NORMAL
- en: The term big data, which spans computer science and statistics/econometrics,
    probably originated in the lunch-table conversations at Silicon Graphics in the
    mid-1990s, in which John Mashey figured prominently.
  prefs: []
  type: TYPE_NORMAL
- en: In 2001, Doug Laney, then an analyst at consultancy Meta Group Inc (which got
    acquired by Gartner) introduced the idea of 3Vs (variety, velocity, and volume).
    Now, we refer to 4 Vs instead of 3Vs with the addition of Veracity of data to
    the 3Vs.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Vs of big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following are the 4 Vs of big data used to describe the properties of big
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Variety of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be from weather sensors, car sensors, census data, Facebook updates,
    tweets, transactions, sales, and marketing. The data format is both structured
    and unstructured as well. Data types can also be different; binary, text, JSON,
    and XML.
  prefs: []
  type: TYPE_NORMAL
- en: Velocity of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be obtained from a data warehouse, batch mode file archives, near real-time
    updates, or instantaneous real-time updates from the Uber ride you just booked.
  prefs: []
  type: TYPE_NORMAL
- en: Volume of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be collected and stored for an hour, a day, a month, a year, or 10
    years. The size of data is growing to 100s of TBs for many companies.
  prefs: []
  type: TYPE_NORMAL
- en: Veracity of Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data can be analyzed for actionable insights, but with so much data of all types
    being analyzed from across data sources, it is very difficult to ensure correctness
    and proof of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the 4 Vs of big data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00115.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To make sense of all the data and apply data analytics to big data, we need
    to expand the concept of data analytics to operate at a much larger scale dealing
    with the 4 Vs of big data. This changes not only the tools, technologies, and
    methodologies used in analyzing data, but also the way we even approach the problem.
    If a SQL database was used for data in a business in 1999, now to handle the data
    for the same business we will need a distributed SQL database scalable and adaptable
    to the nuances of the big data space.
  prefs: []
  type: TYPE_NORMAL
- en: Big data analytics applications often include data from both internal systems
    and external sources, such as weather data or demographic data on consumers compiled
    by third-party information services providers. In addition, streaming analytics
    applications are becoming common in big data environments, as users look to do
    real-time analytics on data fed into Hadoop systems through Spark's Spark streaming
    module or other open source stream processing engines, such as Flink and Storm.
  prefs: []
  type: TYPE_NORMAL
- en: Early big data systems were mostly deployed on-premises particularly in large
    organizations that were collecting, organizing, and analyzing massive amounts
    of data. But cloud platform vendors, such as **Amazon Web Services** (**AWS**)
    and Microsoft, have made it easier to set up and manage Hadoop clusters in the
    cloud, as have Hadoop suppliers such as Cloudera and Hortonworks, which support
    their distributions of the big data framework on the AWS and Microsoft Azure clouds.
    Users can now spin up clusters in the cloud, run them for as long as needed, and
    then take them offline, with usage-based pricing that doesn't require ongoing
    software licenses.
  prefs: []
  type: TYPE_NORMAL
- en: Potential pitfalls that can trip up organizations on big data analytics initiatives
    include a lack of internal analytics skills and the high cost of hiring experienced
    data scientists and data engineers to fill the gaps.
  prefs: []
  type: TYPE_NORMAL
- en: The amount of data that's typically involved, and its variety, can cause data
    management issues in areas including data quality, consistency, and governance;
    also, data silos can result from the use of different platforms and data stores
    in a big data architecture. In addition, integrating Hadoop, Spark, and other
    big data tools into a cohesive architecture that meets an organization's big data
    analytics needs is a challenging proposition for many IT and analytics teams,
    which have to identify the right mix of technologies and then put the pieces together.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed computing using Apache Hadoop
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Our world is filled with devices starting from the smart refrigerator, smart
    watch, phone, tablet, laptops, kiosks at the airport, ATM dispensing cash to you,
    and many many more. We are able to do things we could not even imagine just a
    few years ago. Instagram, Snapchat, Gmail, Facebook, Twitter, and Pinterest are
    a few of the applications we are now so used to; it is difficult to imagine a
    day without access to such applications.
  prefs: []
  type: TYPE_NORMAL
- en: With the advent of Cloud computing, using a few clicks we are able to launch
    100s if not, 1000s of machines in AWS, Azure (Microsoft), or Google Cloud among
    others and use immense resources to realize our business goals of all sorts.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing has introduced us to the concepts of IaaS, PaaS, and SaaS, which
    gives us the ability to build and operate scalable infrastructures serving all
    types of use cases and business needs.
  prefs: []
  type: TYPE_NORMAL
- en: '**IaaS** (**Infrastructure as a Service**) - Reliable-managed hardware is provided
    without the need for a Data center, power cords, Airconditioning, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**PaaS** (**Platform as a Service**) - On top of IaaS, managed platforms such
    as Windows, Linux , Databases and so on are provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '**SaaS** (**Software as a Service**) - On top of SaaS, managed services such
    as SalesForce, [Kayak.com](https://www.kayak.co.in/?ispredir=true) and so on are
    provided to everyone.'
  prefs: []
  type: TYPE_NORMAL
- en: Behind the scenes is the world of highly scalable distributed computing, which
    makes it possible to store and process PB (PetaBytes) of data.
  prefs: []
  type: TYPE_NORMAL
- en: 1 ExaByte = 1024 PetaBytes (50 Million Blue Ray Movies)
  prefs: []
  type: TYPE_NORMAL
- en: 1 PetaByte = 1024 Tera Bytes (50,000 Blue Ray Movies)
  prefs: []
  type: TYPE_NORMAL
- en: 1 TeraByte = 1024 Giga Bytes (50 Blue Ray Movies)
  prefs: []
  type: TYPE_NORMAL
- en: Average size of 1 Blue Ray Disc for a Movie is ~ 20 GB
  prefs: []
  type: TYPE_NORMAL
- en: Now, the paradigm of Distributed Computing is not really a genuinely new topic
    and has been pursued in some shape or form over decades primarily at research
    facilities as well as by a few commercial product companies. **Massively Parallel
    Processing** (**MPP**) is a paradigm that was in use decades ago in several areas
    such as Oceanography, Earthquake monitoring, and Space exploration. Several companies
    such as Teradata also implemented MPP platforms and provided commercial products
    and applications. Eventually, tech companies such as Google and Amazon among others
    pushed the niche area of scalable distributed computing to a new stage of evolution,
    which eventually led to the creation of Apache Spark by Berkeley University.
  prefs: []
  type: TYPE_NORMAL
- en: Google published a paper on **Map Reduce** (**MR**) as well as **Google File
    System** (**GFS**), which brought the principles of distributed computing to everyone.
    Of course, due credit needs to be given to Doug Cutting, who made it possible
    by implementing the concepts given in the Google white papers and introducing
    the world to Hadoop.
  prefs: []
  type: TYPE_NORMAL
- en: The Apache Hadoop Framework is an open source software framework written in
    Java. The two main areas provided by the framework are storage and processing.
    For Storage, the Apache Hadoop Framework uses **Hadoop Distributed File System**
    (**HDFS**), which is based on the Google File System paper released on October
    2003\. For processing or computing, the framework depends on MapReduce, which
    is based on a Google paper on MR released in December 2004.
  prefs: []
  type: TYPE_NORMAL
- en: The MapReduce framework evolved from V1 (based on Job Tracker and Task Tracker)
    to V2 (based on YARN).
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop Distributed File System (HDFS)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS is a software-based filesystem implemented in Java and sits on top of the
    native file system. The main concept behind HDFS is that it divides a file into
    blocks (typically 128 MB) instead of dealing with a file as a whole. This allowed
    many features such as distribution, replication, failure recovery, and more importantly
    distributed processing of the blocks using multiple machines.
  prefs: []
  type: TYPE_NORMAL
- en: Block sizes can be 64 MB, 128 MB, 256 MB, or 512 MB, whatever suits the purpose.
    For a 1 GB file with 128 MB blocks, there will be 1024 MB / 128 MB = 8 blocks.
    If you consider replication factor of 3, this makes it 24 blocks.
  prefs: []
  type: TYPE_NORMAL
- en: 'HDFS provides a distributed storage system with fault tolerance and failure
    recovery. HDFS has two main components: name node and data node(s). Name node
    contains all the metadata of all content of the file system. Data nodes connect
    to the Name Node and rely on the name node for all metadata information regarding
    the content in the file system. If the name node does not know any information,
    data node will not be able to serve it to any client who wants to read/write to
    the HDFS.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the HDFS architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00120.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: NameNode and DataNode are JVM processes so any machine that supports Java can
    run the NameNode or the DataNode process. There is only one NameNode (the second
    NameNode will be there too if you count the HA deployment) but 100s if not 1000s
    of DataNodes.
  prefs: []
  type: TYPE_NORMAL
- en: It is not advisable to have 1000s of DataNodes because all operations from all
    the DataNodes will tend to overwhelm the NameNode in a real production environment
    with a lot of data-intensive applications.
  prefs: []
  type: TYPE_NORMAL
- en: The existence of a single NameNode in a cluster greatly simplifies the architecture
    of the system. The NameNode is the arbitrator and repository for all HDFS metadata
    and any client, that wants to read/write data first contacts the NameNode for
    the metadata information. The data never flows directly through the NameNode,
    which allows 100s of DataNodes (PBs of data) to be managed by 1 NameNode.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS supports a traditional hierarchical file organization with directories
    and files similar to most other filesystems. You can create, move, and delete
    files, and directories. The NameNode maintains the filesystem namespace and records
    all changes and the state of the filesystem. An application can specify the number
    of replicas of a file that should be maintained by HDFS and this information is
    also stored by the NameNode.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS is designed to reliably store very large files in a distributed manner
    across machines in a large cluster of data nodes. To deal with replication, fault
    tolerance, as well as distributed computing, HDFS stores each file as a sequence
    of blocks.
  prefs: []
  type: TYPE_NORMAL
- en: The NameNode makes all decisions regarding the replication of blocks. This is
    mainly dependent on a Block report from each of the DataNodes in the cluster received
    periodically at a heart beat interval. A block report contains a list of all blocks
    on a DataNode, which the NameNode then stores in its metadata repository.
  prefs: []
  type: TYPE_NORMAL
- en: The NameNode stores all metadata in memory and serves all requests from clients
    reading from/writing to HDFS. However, since this is the master node maintaining
    all the metadata about the HDFS, it is critical to maintain consistent and reliable
    metadata information. If this information is lost, the content on the HDFS cannot
    be accessed.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, HDFS NameNode uses a transaction log called the EditLog, which
    persistently records every change that occurs to the metadata of the filesystem.
    Creating a new file updates EditLog, so does moving a file or renaming a file,
    or deleting a file. The entire filesystem namespace, including the mapping of
    blocks to files and filesystem properties, is stored in a file called the `FsImage`.
    The **NameNode** keeps everything in memory as well. When a NameNode starts up,
    it loads the EditLog and the `FsImage` initializes itself to set up the HDFS.
  prefs: []
  type: TYPE_NORMAL
- en: The DataNodes, however, have no idea about the HDFS, purely relying on the blocks
    of data stored. DataNodes rely entirely on the NameNode to perform any operations.
    Even when a client wants to connect to read a file or write to a file, it's the
    NameNode that tells the client where to connect to.
  prefs: []
  type: TYPE_NORMAL
- en: HDFS High Availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: HDFS is a Master-Slave cluster with the NameNode as the master and the 100s,
    if not 1000s of DataNodes as slaves, managed by the master node. This introduces
    a **Single Point of Failure** (**SPOF**) in the cluster as if the Master NameNode
    goes down for some reason, the entire cluster is going to be unusable. HDFS 1.0
    supports an additional Master Node known as the **Secondary NameNode** to help
    with recovery of the cluster. This is done by maintaining a copy of all the metadata
    of the filesystem and is by no means a Highly Available System requiring manual
    interventions and maintenance work. HDFS 2.0 takes this to the next level by adding
    support for full **High Availability** (**HA**).
  prefs: []
  type: TYPE_NORMAL
- en: HA works by having two Name Nodes in an active-passive mode such that one Name
    Node is active and other is passive. When the primary NameNode has a failure,
    the passive Name Node will take over the role of the Master Node.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the active-passive pair of NameNodes will be
    deployed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00123.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS Federation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'HDFS Federation is a way of using multiple name nodes to spread the filesystem
    namespace over. Unlike the first HDFS versions, which simply managed entire clusters
    using a single NameNode, which does not scale that well as the size of the cluster
    grows, HDFS Federation can support significantly larger clusters and horizontally
    scales the NameNode or name service using multiple federated name nodes. Take
    a look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00127.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS Snapshot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hadoop 2.0 also added a new capability: taking a snapshot (read-only copy and
    copy-on-write) of the filesystem (data blocks) stored on the data nodes. Using
    Snapshots, you can take a copy of directories seamlessly using the NameNode''s
    metadata of the data blocks. Snapshot creation is instantaneous and doesn''t require
    interference with other regular HDFS operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an illustration of how snapshot works on specific directories:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00131.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS Read
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Client connects to the NameNode and ask about a file using the name of the file.
    NameNode looks up the block locations for the file and returns the same to the
    client. The client can then connect to the DataNodes and read the blocks needed.
    NameNode does not participate in the data transfer.
  prefs: []
  type: TYPE_NORMAL
- en: The following is the flow of a read request from a client. First, the client
    gets the locations and then pulls the blocks from the DataNodes. If a DataNode
    fails in the middle, then the client gets the replica of the block from another
    DataNode.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00264.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: HDFS Write
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The client connects to the NameNode and asks the NameNode to let it write to
    the HDFS. The NameNode looks up information and plans the blocks, the Data Nodes
    to be used to store the blocks, and the replication strategy to be used. The NameNode
    does not handle any data and only tells the client where to write. Once the first
    DataNode receives the block, based on the replication strategy, the NameNode tells
    the first DataNode where else to replicate. So, the DataNode that is received
    from client sends the block over to the second DataNode (where the copy of the
    block is supposed to be written to) and then the second DataNode sends it to a
    third DataNode (if replication-factor is 3).
  prefs: []
  type: TYPE_NORMAL
- en: The following is the flow of a write request from a client. First, the client
    gets the locations and then writes to the first DataNode. The DataNode that receives
    the block replicates the block to the DataNodes that should hold the replica copy
    of the block. This happens for all the blocks being written to from the client.
    If a DataNode fails in the middle, then the block gets replicated to another DataNode
    as determined by the NameNode.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00267.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: So far, we have seen how HDFS provides a distributed filesystem using blocks,
    the NameNode, and DataNodes. Once data is stored at a PB scale, it is also important
    to actually process the data to serve the various use cases of the business.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce framework was created in the Hadoop framework to perform distributed
    computation. We will look at this further in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MapReduce** (**MR**) framework enables you to write distributed applications
    to process large amounts of data from a filesystem such as HDFS in a reliable
    and fault-tolerant manner. When you want to use the MapReduce Framework to process
    data, it works through the creation of a job, which then runs on the framework
    to perform the tasks needed.'
  prefs: []
  type: TYPE_NORMAL
- en: A MapReduce job usually works by splitting the input data across worker nodes
    running **Mapper** tasks in a parallel manner. At this time, any failures that
    happen either at the HDFS level or the failure of a Mapper task are handled automatically
    to be fault-tolerant. Once the Mappers are completed, the results are copied over
    the network to other machines running **Reducer** tasks.
  prefs: []
  type: TYPE_NORMAL
- en: An easy way to understand this concept is to imagine that you and your friends
    want to sort out piles of fruit into boxes. For that, you want to assign each
    person the task of going through one raw basket of fruit (all mixed up) and separate
    out the fruit into various boxes. Each person then does the same with this basket
    of fruit.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, you end up with a lot of boxes of fruit from all your friends. Then,
    you can assign a group to put the same kind of fruit together in a box, weight
    the box, and seal the box for shipping.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following depicts the idea of taking fruit baskets and sorting the fruit
    by the type of fruit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00270.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: MapReduce framework consists of a single resource manager and multiple node
    managers (usually Node Managers coexist with the data nodes of HDFS). When an
    application wants to run, the client launches the application master, which then
    negotiates with the resource manager to get resources in the cluster in form of
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: A container represents CPUs (cores) and memory allocated on a single node to
    be used to run tasks and processes. Containers are supervised by the node manager
    and scheduled by the resource manager.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of containers:'
  prefs: []
  type: TYPE_NORMAL
- en: 1 core + 4 GB RAM
  prefs: []
  type: TYPE_NORMAL
- en: 2 cores + 6 GB RAM
  prefs: []
  type: TYPE_NORMAL
- en: 4 cores + 20 GB RAM
  prefs: []
  type: TYPE_NORMAL
- en: Some Containers are assigned to be Mappers and other to be Reducers; all this
    is coordinated by the application master in conjunction with the resource manager.
    This framework is called **Yet Another Resource Negotiator** (**YARN**)
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a depiction of YARN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00276.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'A classic example showing the MapReduce framework at work is the word count
    example. The following are the various stages of processing the input data, first
    splitting the input across multiple worker nodes and then finally generating the
    output counts of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00279.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Though MapReduce framework is very successful all across the world and has been
    adopted by most companies, it does run into issues mainly because of the way it
    processes data. Several technologies have come into existence to try and make
    MapReduce easier to use such as Hive and Pig but the complexity remains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Hadoop MapReduce has several limitations such as:'
  prefs: []
  type: TYPE_NORMAL
- en: Performance bottlenecks due to disk-based processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch processing doesn't serve all needs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Programming can be verbose and complex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling of the tasks is slow as there is not much reuse of resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No good way to do real-time event processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning takes too long as usually ML involves iterative processing
    and MR is too slow for this
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hive was created by Facebook as a SQL-like interface to MR. Pig was created
    by Yahoo with a scripting interface to MR. Moreover, several enhancements such
    as Tez (Hortonworks) and LLAP (Hive2.x) are in use, which makes use of in-memory
    optimizations to circumvent the limitations of MapReduce.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at Apache Spark, which has already solved
    some of the limitations of Hadoop technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Here comes Apache Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apache Spark is a unified distributed computing engine across different workloads
    and platforms. Spark can connect to different platforms and process different
    data workloads using a variety of paradigms such as Spark streaming, Spark ML,
    Spark SQL, and Spark GraphX.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark is a fast in-memory data processing engine with elegant and expressive
    development APIs to allow data workers to efficiently execute streaming machine
    learning or SQL workloads that require fast interactive access to data sets. Apache
    Spark consists of Spark core and a set of libraries. The core is the distributed
    execution engine and the Java, Scala, and Python APIs offer a platform for distributed
    application development. Additional libraries built on top of the core allow workloads
    for streaming, SQL, Graph processing, and machine learning. Spark ML, for instance,
    is designed for data science and its abstraction makes data science easier.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides real-time streaming, queries, machine learning, and graph processing.
    Before Apache Spark, we had to use different technologies for different types
    of workloads, one for batch analytics, one for interactive queries, one for real-time
    streaming processing and another for machine learning algorithms. However, Apache
    Spark can do all of these just using Apache Spark instead of using multiple technologies
    that are not always integrated.
  prefs: []
  type: TYPE_NORMAL
- en: Using Apache Spark, all types of workload can be processed and Spark also supports
    Scala, Java, R, and Python as a means of writing client programs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apache Spark is an open-source distributed computing engine which has key advantages
    over the MapReduce paradigm:'
  prefs: []
  type: TYPE_NORMAL
- en: Uses in-memory processing as much as possible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General purpose engine to be used for batch, real-time workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compatible with YARN and also Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integrates well with HBase, Cassandra, MongoDB, HDFS, Amazon S3, and other file
    systems and data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spark was created in Berkeley back in 2009 and was a result of the project
    to build Mesos, a cluster management framework to support different kinds of cluster
    computing systems. Take a look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Version | Release date | Milestones |'
  prefs: []
  type: TYPE_TB
- en: '| 0.5 | 2012-10-07 | First available version for non-production usage |'
  prefs: []
  type: TYPE_TB
- en: '| 0.6 | 2013-02-07 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.7 | 2013-07-16 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.8 | 2013-12-19 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 0.9 | 2014-07-23 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 2014-08-05 | First production ready, backward-compatible release. Spark
    Batch, Streaming, Shark, MLLib, GraphX |'
  prefs: []
  type: TYPE_TB
- en: '| 1.1 | 2014-11-26 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 1.2 | 2015-04-17 | Structured Data, SchemaRDD (subsequently evolved into
    DataFrames) |'
  prefs: []
  type: TYPE_TB
- en: '| 1.3 | 2015-04-17 | API to provide a unified API to read from structured and
    semi-structured sources |'
  prefs: []
  type: TYPE_TB
- en: '| 1.4 | 2015-07-15 | SparkR, DataFrame API, Tungsten improvements |'
  prefs: []
  type: TYPE_TB
- en: '| 1.5 | 2015-11-09 | Point release with various changes |'
  prefs: []
  type: TYPE_TB
- en: '| 1.6 | 2016-11-07 | Dataset DSL introduced |'
  prefs: []
  type: TYPE_TB
- en: '| 2.0 | 2016-11-14 | DataFrames and Datasets API as fundamental layer for ML,
    Structured Streaming,SparkR improvements. |'
  prefs: []
  type: TYPE_TB
- en: '| 2.1 | 2017-05-02 | Event time watermarks, ML, GraphX improvements |'
  prefs: []
  type: TYPE_TB
- en: 2.2 has been released 2017-07-11 which has several improvements especially Structured
    Streaming which is now GA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark is a platform for distributed computing that has several features:'
  prefs: []
  type: TYPE_NORMAL
- en: Transparently processes data on multiple nodes via a simple API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resiliently handles failures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spills data to disk as necessary though predominantly uses memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Java, Scala, Python, R, and SQL APIs are supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same Spark code can run standalone, in Hadoop YARN, Mesos, and the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scala features such as implicits, higher-order functions, structured types,
    and so on allow us to easily build DSL's and integrate them with the language.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Spark does not provide a Storage layer and relies on HDFS or Amazon S3
    and so on. Hence, even if Apache Hadoop technologies are replaced with Apache
    Spark, HDFS is still needed to provide a reliable storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: Apache Kudu provides an alternative to HDFS and there is already integration
    between Apache Spark and Kudu Storage layer, further decoupling Apache Spark and
    the Hadoop ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Hadoop and Apache Spark are both popular big data frameworks, but they don't
    really serve the same purposes. While Hadoop provides distributed storage and
    a MapReduce distributed computing framework, Spark on the other hand is a data
    processing framework that operates on the distributed data storage provided by
    other technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Spark is generally a lot faster than MapReduce because of the way it processes
    data. MapReduce operates on splits using Disk operations, Spark operates on the
    dataset much more efficiently than MapReduce, with the main reason behind the
    performance improvement in Apache Spark being the efficient off-heap in-memory
    processing rather than solely relying on disk-based computations.
  prefs: []
  type: TYPE_NORMAL
- en: MapReduce's processing style can be sufficient if you were data operations and
    reporting requirements are mostly static and it is okay to use batch processing
    for your purposes, but if you need to do analytics on streaming data or your processing
    requirements need multistage processing logic, you will probably want to want
    to go with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: There are three layers in the Spark stack. The bottom layer is the cluster manager,
    which can be standalone, YARN, or Mesos.
  prefs: []
  type: TYPE_NORMAL
- en: Using local mode, you don't need a cluster manager to process.
  prefs: []
  type: TYPE_NORMAL
- en: In the middle, above the cluster manager, is the layer of Spark core, which
    provides all the underlying APIs to perform task scheduling and interacting with
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: At the top are modules that run on top of Spark core such as Spark SQL to provide
    interactive queries, Spark streaming for real-time analytics, Spark ML for machine
    learning, and Spark GraphX for graph processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three layers are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00283.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As seen in the preceding diagram, the various libraries such as Spark SQL, Spark
    streaming, Spark ML, and GraphX all sit on top of Spark core, which is the middle
    layer. The bottom layer shows the various cluster manager options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at each of the component briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark core is the underlying general execution engine for the Spark platform
    that all other functionality is built upon. Spark core contains basic Spark functionalities
    required for running jobs and needed by other components. It provides in-memory
    computing and referencing datasets in external storage systems, the most important
    being the **Resilient Distributed Dataset** (**RDD**).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Spark core contains logic for accessing various filesystems, such
    as HDFS, Amazon S3, HBase, Cassandra, relational databases, and so on. Spark core
    also provides fundamental functions to support networking, security, scheduling,
    and data shuffling to build a high scalable, fault-tolerant platform for distributed
    computing.
  prefs: []
  type: TYPE_NORMAL
- en: We cover Spark core in detail in [Chapter 6](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),
    *Start Working with Spark - REPL* and RDDs and [Chapter 7](part0212.html#6A5N81-21aec46d8593429cacea59dbdcd64e1c),
    *Special RDD Operations*.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames and datasets built on top of RDDs and introduced with Spark SQL are
    becoming the norm now over RDDs in many use cases. RDDs are still more flexible
    in terms of handling totally unstructured data, but in future datasets, API might
    eventually become the core API.
  prefs: []
  type: TYPE_NORMAL
- en: Spark SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark SQL is a component on top of Spark core that introduces a new data abstraction
    called **SchemaRDD**, which provides support for structured and semi-structured
    data. Spark SQL provides functions for manipulating large sets of distributed,
    structured data using an SQL subset supported by Spark and Hive QL. Spark SQL
    simplifies the handling of structured data through DataFrames and datasets at
    a much more performant level as part of the Tungsten initative. Spark SQL also
    supports reading and writing data to and from various structured formats and data
    sources, files, parquet, orc, relational databases, Hive, HDFS, S3, and so on.
    Spark SQL provides a query optimization framework called **Catalyst** to optimize
    all operations to boost the speed (compared to RDDs Spark SQL is several times
    faster). Spark SQL also includes a Thrift server, which can be used by external
    systems to query data through Spark SQL using classic JDBC and ODBC protocols.
  prefs: []
  type: TYPE_NORMAL
- en: We cover Spark SQL in detail in [Chapter 8](part0241.html#75QNI1-21aec46d8593429cacea59dbdcd64e1c),
    *Introduce a Little Structure - Spark SQL*.
  prefs: []
  type: TYPE_NORMAL
- en: Spark streaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming leverages Spark core's fast scheduling capability to perform
    streaming analytics by ingesting real-time streaming data from various sources
    such as HDFS, Kafka, Flume, Twitter, ZeroMQ, Kinesis, and so on. Spark streaming
    uses micro-batches of data to process the data in chunks and, uses a concept known
    as DStreams, Spark streaming can operate on the RDDs, applying transformations
    and actions as regular RDDs in the Spark core API. Spark streaming operations
    can recover from failure automatically using various techniques. Spark streaming
    can be combined with other Spark components in a single program, unifying real-time
    processing with machine learning, SQL, and graph operations.
  prefs: []
  type: TYPE_NORMAL
- en: We cover Spark streaming in detail in the [Chapter 9](part0288.html#8IL201-21aec46d8593429cacea59dbdcd64e1c),
    *Stream Me Up, Scotty - Spark Streaming*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the new Structured Streaming API makes Spark streaming programs
    more similar to Spark batch programs and also allows real-time querying on top
    of streaming data, which is complicated with the Spark streaming library before
    Spark 2.0+.
  prefs: []
  type: TYPE_NORMAL
- en: Spark GraphX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GraphX is a distributed graph processing framework on top of Spark. Graphs are
    data structures comprising vertices and the edges connecting them. GraphX provides
    functions for building graphs, represented as Graph RDDs. It provides an API for
    expressing graph computation that can model user-defined graphs by using the Pregel
    abstraction API. It also provides an optimized runtime for this abstraction. GraphX
    also contains implementations of the most important algorithms of graph theory,
    such as page rank, connected components, shortest paths, SVD++, and others.
  prefs: []
  type: TYPE_NORMAL
- en: We cover Spark Graphx in detail in [Chapter 10](part0326.html#9MSNC1-21aec46d8593429cacea59dbdcd64e1c),
    *Everything is Connected - GraphX*.
  prefs: []
  type: TYPE_NORMAL
- en: A newer module known as GraphFrames is in development, which makes it easier
    to do Graph processing using DataFrame-based Graphs. GraphX is to RDDs what GraphFrames
    are to DataFrames/datasets. Also, this is currently separate from GraphX and is
    expected to support all the functionality of GraphX in the future, when there
    might be a switch over to GraphFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MLlib is a distributed machine learning framework above Spark core and handles
    machine-learning models used for transforming datasets in the form of RDDs. Spark
    MLlib is a library of machine-learning algorithms providing various algorithms
    such as logistic regression, Naive Bayes classification, **Support Vector Machines**
    (**SVMs**), decision trees, random forests, linear regression, **Alternating Least
    Squares** (**ALS**), and k-means clustering. Spark ML integrates very well with
    Spark core, Spark streaming, Spark SQL, and GraphX to provide a truly integrated
    platform where data can be real-time or batch.
  prefs: []
  type: TYPE_NORMAL
- en: We cover Spark ML in detail in [Chapter 11](part0170.html#523VK1-21aec46d8593429cacea59dbdcd64e1c),
    *Learning Machine Learning - Spark MLlib and ML*.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, PySpark and SparkR are also available as means to interact with
    Spark clusters and use the Python and R APIs. Python and R integrations truly
    open up Spark to a population of Data scientists and Machine learning modelers
    as the most common languages used by Data scientists in general are Python and
    R. This is the reason why Spark supports Python integration and also R integration,
    so as to avoid the costly process of learning a new language of Scala. Another
    reason is that there might be a lot of existing code written in Python and R,
    and if we can leverage some of the code, that will improve the productivity of
    the teams rather than building everything again from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: There is increasing popularity for, and usage of, notebook technologies such
    as Jupyter and Zeppelin, which make it significantly easier to interact with Spark
    in general, but particularly very useful in Spark ML where a lot of hypotheses
    and analysis are expected.
  prefs: []
  type: TYPE_NORMAL
- en: PySpark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PySpark uses Python-based `SparkContext` and Python scripts as tasks and then
    uses sockets and pipes to executed processes to communicate between Java-based
    Spark clusters and Python scripts. PySpark also uses `Py4J`, which is a popular
    library integrated within PySpark that lets Python interface dynamically with
    Java-based RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: Python must be installed on all worker nodes running the Spark executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how PySpark works by communicating between Java processed
    and Python scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00286.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: SparkR
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`SparkR` is an R package that provides a light-weight frontend to use Apache
    Spark from R. SparkR provides a distributed data frame implementation that supports
    operations such as selection, filtering, aggregation, and so on. SparkR also supports
    distributed machine learning using MLlib. SparkR uses R-based `SparkContext` and
    R scripts as tasks and then uses JNI and pipes to executed processes to communicate
    between Java-based Spark clusters and R scripts.'
  prefs: []
  type: TYPE_NORMAL
- en: R must be installed on all worker nodes running the Spark executors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is how SparkR works by communicating between Java processed and
    R scripts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00289.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We explored the evolution of the Hadoop and MapReduce frameworks and discussed
    YARN, HDFS concepts, HDFS Reads and Writes, and key features as well as challenges.
    Then, we discussed the evolution of Apache Spark, why Apache Spark was created
    in the first place, and the value it can bring to the challenges of big data analytics
    and processing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we also took a peek at the various components in Apache Spark, namely,
    Spark core, Spark SQL, Spark streaming, Spark GraphX, and Spark ML as well as
    PySpark and SparkR as a means of integrating Python and R language code with Apache
    Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen big data analytics, the space and the evolution of the
    Hadoop Distributed computing platform, and the eventual development of Apache
    Spark along with a high-level overview of how Apache Spark might solve some of
    the challenges, we are ready to start learning Spark and how to use it in our
    use cases.
  prefs: []
  type: TYPE_NORMAL
- en: "In the next chapter, we will delve more deeply into Apache Spark and start\
    \ to look under the hood of how it all works in [Chapter 6\uFEFF](part0174.html#55U1S1-21aec46d8593429cacea59dbdcd64e1c),\
    \ *Start Working with Spark - REPL and RDDs*."
  prefs: []
  type: TYPE_NORMAL
