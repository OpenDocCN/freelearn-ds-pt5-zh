- en: '*Chapter 3*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Machine Learning via Scikit-Learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Prepare data for different types of supervised learning models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tune model hyperparameters using a grid search.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract feature importance from a tuned model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluate performance of classification and regression models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will be covering the important concepts of handling data
    and making the data ready for analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**scikit-learn** is a free, open source library built for Python that contains
    an assortment of supervised and unsupervised machine learning algorithms. Additionally,
    scikit-learn provides functions for data preprocessing, hyperparameter tuning,
    and model evaluation, which we will be covering in the upcoming chapters. It streamlines
    the model-building process and is easy to install on a wide variety of platforms.
    scikit-learn started in 2007 as a Google Summer of Code project by David Corneapeau,
    and after a series of developments and releases, scikit-learn has evolved into
    one of the premier tools used by academics and professionals for machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn to build a variety of widely used modeling algorithms,
    namely, linear and logistic regression, support vector machines (SVMs), decision
    trees, and random forests. First, we will cover linear and logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Linear and Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In regression, a single dependent, or outcome variable is predicted using one
    or more independent variables. Use cases for regression are included, but are
    not limited to predicting:'
  prefs: []
  type: TYPE_NORMAL
- en: The win percentage of a team, given a variety of team statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The risk of heart disease, given family history and a number of physical and
    psychological characteristics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The likelihood of snowfall, given several climate measurements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Linear and logistic regression are popular choices for predicting such outcomes
    due to the ease and transparency of interpretability, as well as the ability to
    extrapolate to values not seen in the training data. The end goal of linear regression
    is to draw a straight line through the observations that minimizes the absolute
    distance between the line and observations (that is, the line of best fit). Therefore,
    in linear regression, it is assumed that the relationship between the feature(s)
    and the continuous dependent variable follows a straight line. Lines are defined
    in slope-intercept form (that is, *y = a + bx*), where *a* is the intercept (that
    is, the value of *y* when *x* is 0), *b* is the slope, and *x* is the independent
    variable. There are two types of linear regression that will be covered in this
    chapter: simple linear regression and multiple linear regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Simple Linear Regression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simple linear regression models define the relationship between one feature
    and the continuous outcome variable using *y =* *α* *+* *β**x*. This equation
    is like the slope-intercept form, where *y* denotes the predicted value of the
    dependent variable, *α* denotes the intercept, *β* (beta) represents the slope,
    and *x* is the value of the independent variable. Given *x*, regression models
    compute the values for *α* and *β* that minimize the absolute difference between
    predicted *y* values (that is, *ŷ*) and actual *y* values.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if we are predicting the weight of an individual in kilograms (kg)
    using height in meters (m) as the lone predictor variable, and the simple linear
    regression model computes 1.5 as the value for *α* and 50 as the coefficient for
    *β*, this model can be interpreted as for every 1 m increase in height, weight
    increases by 50 kg. Thus, we can predict that the weight of an individual who
    is 1.8 m is 91.5 kg using y = 1.5 + (50 x 1.8). In the following exercises, we
    will demonstrate simple linear regression using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 21: Preparing Data for a Linear Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To prepare our data for a simple linear regression model, we will use a random
    subset of the Weather in Szeged 2006-2016 dataset, which consists of hourly weather
    measurements from April 1, 2006, to September 9, 2016, in Szeged, Hungary. The
    adapted data is provided as a `.csv` file (https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter02/weather.csv)
    and consists of 10,000 observations of 8 variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Temperature_c`: The temperature in Celsius'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Humidity`: The proportion of humidity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_Speed_kmh`: The wind speed in kilometers per hour'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Wind_Bearing_Degrees`: The wind direction in degrees clockwise from due north'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Visibility_km`: The visibility in kilometers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Pressure_millibars`: The atmospheric pressure as measured in millibars'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Rain`: rain = 1, snow = 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Description`: Warm, normal, or cold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Import the `weather.csv` dataset using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Explore the data using `df.info()`:![Figure 3.1: Information describing df'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.1: Information describing df'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The `Description` column is the lone categorical variable in `df`. Check the
    number of levels in `Description` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The number of levels is shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.2: Number of levels in the ''Description'' column](img/C13322_03_02.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 3.2: Number of levels in the ''Description'' column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Multi-class, categorical variables must be converted into dummy variables via
    a process termed "dummy coding." Dummy coding a multi-class, categorical variable
    creates n-1 new binary features, which correspond to the levels within the categorical
    variable. For example, a multi-class, categorical variable with three levels will
    create two binary features. After the multi-class, categorical feature has been
    dummy coded, the original feature must be dropped.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To dummy code all multi-class, categorical variables, refer to the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The original DataFrame, `df`, consisted of eight columns, one of which (that
    is, Description) was a multi-class, categorical variable with three levels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In step 4, we transformed this feature into n-1 (that is, 2), separated dummy
    variables, and dropped the original feature, `Description`. Thus, `df_dummies`
    should now contain one more column than df (that is, 9 columns). Check this out
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.3: Number of columns after dummy coding'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_03.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.3: Number of columns after dummy coding'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To remove any possible order effects in the data, it is good practice to first
    shuffle the rows of the data prior to splitting the data into features (`X`) and
    outcome (`y`). To shuffle the rows in `df_dummies`, refer to the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the data has been shuffled, we will split the rows in our data into
    features (`X`) and the dependent variable (`y`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: Linear regression is used for predicting a continuous outcome. Thus, in this
    exercise, we will pretend that the continuous variable `Temperature_c` (the temperature
    in Celsius) is the dependent variable, and that we are preparing data to fit a
    linear regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Split `df_shuffled` into `X` and `y` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split `X` and `y` into testing and training data using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the data has been dummy coded, shuffled, split into `X` and `y`, and
    further divided into testing and training datasets, it is ready to be used in
    a linear or logistic regression model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The screenshot here shows the first five rows of `X_train`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.4: The first five rows of X_train'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: The first five rows of X_train'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Exercise 22: Fitting a Simple Linear Regression Model and Determining the Intercept
    and Coefficient'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will continue using the data we prepared in Exercise 21
    to fit a simple linear regression model to predict the temperature in Celsius
    from the humidity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 21, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To instantiate a linear regression model, refer to the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the `Humidity` column in the training data using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.5: The output from fitting the simple linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.5: The output from fitting the simple linear regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract the value for the intercept using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the value for the `coefficient` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.6: A formula to predict temperature in Celsius from humidity using
    simple linear regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Great work! According to this simple linear regression model, a day with a 0.78
    humidity value has a predicted a temperature of 10.56 degrees Celsius. Now that
    we are familiar with extracting the intercept and coefficients of our simple linear
    regression model, it is time to generate predictions and subsequently evaluate
    how the model performs on unseen, test data.
  prefs: []
  type: TYPE_NORMAL
- en: Teaching tip
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Practice calculating temperature at various levels of humidity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 23: Generating Predictions and Evaluating the Performance of a Simple
    Linear Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The very purpose of supervised learning is to use existing, labeled data to
    generate predictions. Thus, this exercise will demonstrate how to generate predictions
    on the test feature and generate model performance metrics by comparing the predictions
    to the actual values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 22*, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate predictions on the test data using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: A common way to evaluate model performance is to examine the correlation between
    the predicted and actual values using a scatterplot. The scatterplot displays
    the relationship between the actual and predicted values. A perfect regression
    model will display a straight, diagonal line between predicted and actual values.
    The relationship between the predicted and actual values can be quantified using
    the Pearson r correlation coefficient. In the following step, we will create a
    scatterplot of the predicted and actual values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is helpful if the correlation coefficient is displayed in the plot''s title.
    The following code will show us how to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the resultant output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.7: Predicted versus actual values from a simple linear regression
    model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: With a Pearson r value of 0.62, there is a moderate, positive, linear correlation
    between the predicted and actual values. A perfect model would have all points
    on the plot in a straight line and an r value of 1.0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'A model that fits the data very well will have normally distributed residuals.
    To create a density plot of the residuals, refer to the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.8: A histogram of residuals from a simple linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.8: A histogram of residuals from a simple linear regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The histogram shows us that the residuals are negatively skewed and the value
    of the Shapiro W p-value in the title tells us that the distribution is not normal.
    This gives us further evidence that our model has room for improvement.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Lastly, we will compute metrics for mean absolute error, mean squared error,
    root mean squared error, and R-squared, and put them into a DataFrame using the
    code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Please refer to the resultant output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.9: Model evaluation metrics from a simple linear regression model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.9: Model evaluation metrics from a simple linear regression model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Mean absolute error** (**MAE**) is the average absolute difference between
    the predicted values and the actual values. **Mean squared error** (**MSE**) is
    the average of the squared differences between the predicted and actual values.
    **Root mean squared error** (**RMSE**) is the square root of the MSE. R-squared
    tells us the proportion of variance in the dependent variable that can be explained
    by the model. Thus, in this simple linear regression model, humidity explained
    only 38.9% of the variance in temperature. Additionally, our predictions were
    within ± 6.052 degrees Celsius.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, we have successfully used scikit-learn to fit and evaluate a simple linear
    regression model. This is the first step in a very exciting journey to becoming
    a machine learning guru. Next, we will continue expanding our knowledge of regression
    and improving this model by exploring multiple linear regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple Linear Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multiple linear regression models define the relationship between two or more
    features and the continuous outcome variable using *y =* *α* *+* *β**1**x**i1*
    *+* *β**2**x**i2* *+ … +* *β**p-1**x**i,p-1*. Again, *α* represents the intercept
    and *β* denotes the slope for each feature (*x*) in the model. Thus, if we are
    predicting the weight of an individual in kg using height in *m*, total cholesterol
    in milligrams per deciliter (*mg/dL)*, and minutes of cardiovascular exercise
    per day, and the multiple linear regression model computes 1.5 as the value for
    *α*, 50 as the coefficient for *β**1*, 0.1 as the coefficient for *β**2*, and
    -0.4 as the coefficient for *β**3*, this model can be interpreted as for every
    1 *m* increase in height, weight increases by 50 kg, controlling for all other
    features in the model. Additionally, for every 1 mg/dL increase in total cholesterol,
    weight increases by 0.1 kg, controlling for all other features in the model. Lastly,
    for every minute of cardiovascular exercise per day, weight decreases by 0.4 kg,
    controlling for all other features in the model. Thus, we can predict the weight
    of an individual who is 1.8 m tall, with total cholesterol of 200 mg/dL, and completes
    30 minutes of cardiovascular exercise per day as 99.5 kg using *y = 1.5 + (0.1
    x 50) + (200 x 0.5) + (30 x -0.4)*. In the following exercise, we will demonstrate
    conducting multiple linear regression using scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 24: Fitting a Multiple Linear Regression Model and Determining the
    Intercept and Coefficients'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this exercise, we will continue using the data we prepared in *Exercise 21*,
    *Preparing Data for a Linear Regression Model*, to fit a multiple linear regression
    model to predict the temperature in Celsius from all the features in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 23, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To instantiate a linear regression model, refer to the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model to the training data using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.10: The output from fitting the multiple linear regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: The output from fitting the multiple linear regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract the value for the intercept using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the value for the coefficients as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can print a message with the formula for predicting temperature in
    Celsius using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our output should look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: A formula to predict temperature in Celsius from humidity using
    multiple linear regression'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Nice job! According to this multiple regression model, a day with 0.78 humidity,
    5.0 km/h wind speed, wind direction at 81 degrees clockwise from due north, 3
    km of visibility, 1000 millibars of pressure, no rain, and is described as normal,
    has a predicted temperature in Celsius of 5.72 degrees. Now that we are familiar
    with extracting the intercept and coefficients of our multiple linear regression
    model, we can generate predictions and evaluate how the model performs on the
    test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 5: Generating Predictions and Evaluating the Performance of a Multiple
    Linear Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 23*, *Generating Predictions and Evaluating the Performance of
    a Simple Linear Regression Model*, we learned how to generate predictions and
    evaluate the performance of a simple linear regression model using a variety of
    methods. To reduce the code redundancy, we will evaluate the performance of our
    multiple linear regression model using the metrics in *step 4* of *Exercise 23*,
    and we will determine if the multiple linear regression model performed better
    or worse in relation to the simple linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 24, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate predictions on the test data using all the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot predictions versus actual using a scatterplot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the distribution of the residuals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the metrics for mean absolute error, mean squared error, root mean
    squared error, and R-squared and put them into a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine if the multiple linear regression model performed better or worse
    in relation to the simple linear regression model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 343.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should find that the multiple linear regression model performed better on
    every metric relative to the simple linear regression model. Most notably, in
    the simple linear regression model, only 38.9% of the variance in temperature
    was described by the model. However, in the multiple linear regression model,
    86.6% of the variance in temperature was explained by the combination of features.
    Additionally, our simple linear regression model predicted temperatures, on average,
    within ± 6.052 degrees, while our multiple linear regression model predicted temperatures,
    on average, within ± 2.861 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: The transparent nature of the intercept and beta coefficients make linear regression
    models very easy to interpret. In business, it is commonly requested that data
    scientists explain the effect of a certain feature on an outcome. Thus, linear
    regression provides metrics allowing a reasonable response to the business inquiry
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: However, much of the time, a problem requires the data scientist to predict
    an outcome measure that is not continuous, but categorical. For example, in insurance,
    given certain features of a customer, what is the probability that this customer
    will not renew their policy? In this case, there is not a linear relationship
    between the features in the data and the outcome variable, so linear regression
    will falter. A viable option for conducting regression analysis on a categorical
    dependent variable is logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Logistic regression uses categorical and continuous variables to predict a categorical
    outcome. When the dependent variable of choice has two categorical outcomes, the
    analysis is termed binary logistic regression. However, if the outcome variable
    consists of more than two levels, the analysis is referred to as multinomial logistic
    regression. For the purposes of this chapter, we will focus our learning on the
    former.
  prefs: []
  type: TYPE_NORMAL
- en: When predicting a binary outcome, we do not have a linear relationship between
    the features and the outcome variable; an assumption of linear regression. Thus,
    to express a nonlinear relationship in a linear way, we must transform the data
    using logarithmic transformation. As a result, logistic regression allows us to
    predict the probability of the binary outcome occurring given the feature(s) in
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'For logistic regression with 1 predictor, the logistic regression equation
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12: Logistic regression formula with 1 predictor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.12: Logistic regression formula with 1 predictor'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding figure, *P(Y)* is the probability of the outcome occurring,
    *e* is the base of natural logarithms, *α* is the intercept, *β* is the beta coefficient,
    and *x* is the value of the predictor. This equation can be extended to multiple
    predictors using the formula here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Logistic regression formula with more than one predictor'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Logistic regression formula with more than one predictor'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Thus, using logistic regression to model the probability of an event occurring
    is the same as fitting a linear regression model, except the continuous outcome
    variable has been replaced by the log odds (an alternate way of expressing probabilities)
    of success for a binary outcome variable. In linear regression, we assumed a linear
    relationship between the predictor variable(s) and the outcome variable. Logistic
    regression, on the other hand, assumes a linear relationship between the predictor
    variable(s) and the natural log of *p/(1-p)*, where *p* is the probability of
    the event occurring.
  prefs: []
  type: TYPE_NORMAL
- en: In the following exercise, we will use the `weather.csv` dataset to demonstrate
    building a logistic regression model to predict the probability of rain using
    all the features in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 25: Fitting a Logistic Regression Model and Determining the Intercept
    and Coefficients'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To model the probability of rain (as opposed to snow) using all the features
    in our data, we will use the `weather.csv` file and store the dichotomous variable
    `Rain` as the outcome measure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dummy code the `Description` variable as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Shuffle `df_dummies` using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the features and outcome into `X` and `y`, respectively, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the features and outcome into training and testing data using the code
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a logistic regression model using this code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the logistic regression model to the training data using `model.fit(X_train,
    y_train`). We should get the following output:![Figure 3.14: The output from fitting
    a logistic regression model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.14: The output from fitting a logistic regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Get the intercept using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the coefficients using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Place the coefficients into a list as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Match features to their coefficients, place them in a DataFrame, and print
    the DataFrame to the console as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.15: Features and their coefficients from the logistic regression
    model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Features and their coefficients from the logistic regression model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The coefficient for temperature can be interpreted as for every 1-degree increase
    in temperature, the log odds of rain increase by 5.69, controlling for all other
    features in the model. To generate predictions, we could convert the log odds
    to odds and the odds to probability. However, scikit-learn has functionality to
    generate predicted probability, as well as predicted classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 26: Generating Predictions and Evaluating the Performance of a Logistic
    Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 25*, we learned how to fit a logistic regression model and extract
    the elements necessary to generate predictions. However, scikit-learn makes our
    lives much easier by providing us with functions to predict the probability of
    an outcome, as well as the classes of an outcome. In this exercise, we will learn
    to generate predicted probabilities and classes, as well as evaluating a model
    performance using a confusion matrix and a classification report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 25, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate predicted probabilities using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Generate predicted classes using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate a performance using a confusion matrix as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.16: The confusion matrix from our logistic regression model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.16: The confusion matrix from our logistic regression model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: From the confusion matrix, we can see that, of the 383 observations that were
    not classified as rainy, 377 of them were correctly classified, and of the 2917
    observations that were classified as rainy, 2907 of them were correctly classified.
    To further inspect our model's performance using metrics such as precision, recall,
    and f1-score, we will generate a classification report.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Generate a classification report using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the resultant output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.17: The classification report generated from our logistic regression
    model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: The classification report generated from our logistic regression
    model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see from our confusion matrix and classification report, our model
    is performing very well and may be difficult to improve upon. However, machine
    learning models including logistic regression consist of numerous hyperparameters
    that can be adjusted to further improve model performance. In the next exercise,
    we will learn to find the optimal combination of hyperparameters to maximize model
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 27: Tuning the Hyperparameters of a Multiple Logistic Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *step 7* of *Exercise 25*, we fit a logistic regression model and the subsequent
    output from that model is displayed in Figure 3.14\. Each of those arguments inside
    the `LogisticRegression()` function is set to a default hyperparameter. To tune
    the model, we will use scikit-learn's grid search function, which fits a model
    for every combination of possible hyperparameter values and determines the value
    for each hyperparameter resulting in the best model. In this exercise, we will
    learn how to use grid search to tune models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 26*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The data has already been prepared for us (see Exercise 26); thus, we can jump
    right into instantiating a grid of possible hyperparameter values as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a grid search model to find the model with the greatest `f1` score
    (that is, the harmonic average of precision and recall) as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model on the training using `model.fit(X_train, y_train)` (keep in
    mind, this may take a while) and find the resultant output here:![Figure 3.18:
    The output from our logistic regression grid search model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.18: The output from our logistic regression grid search model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'We can return the optimal combination of hyperparameters as a dictionary as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Refer to the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.19: The tuned hyperparameters from our logistic regression grid
    search model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.19: The tuned hyperparameters from our logistic regression grid search
    model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We have found the combination of hyperparameters that maximizes the `f1` score.
    Remember, simply using the default hyperparameters in *Exercise 25* resulted in
    a model that performed very well on the test data. Thus, in the following activity,
    we will evaluate how the model with tuned hyperparameters performed on the test
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6: Generating Predictions and Evaluating Performance of a Tuned Logistic
    Regression Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the best combination of hyperparameters has been converged upon, we need
    to evaluate model performance much like we did in *Exercise 25*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 27:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the predicted probabilities of rain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the predicted class of rain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate performance with a confusion matrix and store it as a DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print a classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 346.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By tuning the hyperparameters of the logistic regression model, we were able
    to improve upon a logistic regression model that was already performing very well.
    We will continue to expand upon tuning different types of models in the following
    exercises and activities.
  prefs: []
  type: TYPE_NORMAL
- en: Max Margin Classification Using SVMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVM is an algorithm for supervised learning that solves both classification
    and regression problems. However, SVM is most commonly used in classification
    problems, so, for the purposes of this chapter, we will focus on SVM as a binary
    classifier. The goal of SVM is to determine the best location of a hyperplane
    that create a class boundary between data points plotted on a multidimensional
    space. To help clarify this concept, refer to Figure 3.20.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20: Hyperplane (blue) separating the circles from the squares in
    three dimensions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.20: Hyperplane (blue) separating the circles from the squares in three
    dimensions'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In Figure 3.20, the squares and circles are observations in the same DataFrame
    that represent different classes. In this figure, the hyperplane is depicted by
    a semi-transparent blue boundary lying between the circles and squares that separate
    the observations into two distinct classes. In this example, the observations
    are said to be linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The location of the hyperplane is determined by finding the position that creates
    the maximum separation (that is, margin) between the two classes. Thus, this is
    referred to as the **Maximum Margin Hyperplane** (MMH) and improves the likelihood
    that the points will remain on the correct side of the hyperplane boundary. It
    is possible to express the MMH using the points from each class that are closest
    to the MMH. These points are termed support vectors and each class has at least
    1\. Figure 3.21 visually depicts the support vectors in relation to the MMH in
    2 dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21: Support vectors in relation to the MMH'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_21.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.21: Support vectors in relation to the MMH'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In reality, most data is not linearly separable. In this case, SVM makes use
    of a slack variable, which creates a soft margin (as opposed to a maximum margin),
    allowing some observations to fall on the incorrect side of the line. See the
    following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_22.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.22: 2 observations (as denoted with grey shading and the Greek letter
    Χi) fall on the incorrect side of the soft margin line'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A cost value is applied to the misclassified data points and, instead of finding
    the maximum margin, the algorithm minimizes the total cost. As the cost parameter
    increases, a harder SVM optimization will go for 100% separation and may overfit
    the training data. Conversely, lower cost parameters emphasize a wider margin
    and may underfit the training data. Thus, to create SVM models that perform well
    on the test data, it is important to determine a cost parameter that balances
    overfitting and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, data that is not linearly separable can be transformed into a
    higher-dimension space using the kernel trick. After this mapping to a higher-dimensional
    space, a nonlinear relationship can appear linear. By transforming the original
    data, SVM can discover associations not explicitly apparent in the original features.
    scikit-learn uses the Gaussian RBF kernel by default, but comes equipped with
    common kernels such as linear, polynomial, and sigmoid as well. In order to maximize
    the performance of an SVM classifier model, the optimal combination of the kernel
    and cost function must be determined. Luckily, this can be easily achieved using
    grid search hyperparameter tuning, as introduced in Exercise 27\. In the following
    exercises and activities, we will learn how this feat is accomplished.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 28: Preparing Data for the Support Vector Classifier (SVC) Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before fitting an SVM classifier model to predict a binary outcome variable;
    in this case, rain or snow, we must prepare our data. Since SVM is a black box,
    meaning the processes between input and output are not explicit, we do not need
    to worry about interpretability. Thus, we will transform the features in our data
    into z-scores prior to fitting the model. The following steps will show how to
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import `weather.csv` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dummy code the categorical feature, `Description`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Shuffle `df_dummies` to remove any ordering effects using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split `X` and `y` into testing and training data using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To prevent any data leakage, scale `X_train` and `X_test` by fitting a scaler
    model to `X_train` and transforming them to z-scores separately, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that our data has been properly divided into features and outcome variables,
    split into testing and training data, and scaled separately, we can tune the hyperparameters
    of our SVC model using a grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 29: Tuning the SVC Model Using Grid Search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we discussed the importance of determining the optimal cost function
    and kernel for SVM classifier models. In Exercise 27, we learned how to find the
    optimal combination of hyperparameters using scikit-learn's grid search function.
    In this exercise, we will demonstrate using grid search to find the best combination
    of the cost function and kernel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 28*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the grid for which to search using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `GridSearchCV` model with the `gamma` hyperparameter set to
    `auto` to avoid warnings, and set probability to `True` so we can extract probability
    of rain as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the grid search model using `model.fit(X_train_scaled, y_train)`:![Figure
    3.23: The output from fitting the SVC grid search model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/C13322_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.23: The output from fitting the SVC grid search model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the best parameters using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.24: Tuned hyperparameters for our SVC grid search model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Once the optimal combination of hyperparameters has been determined, it is time
    to generate predictions and subsequently evaluate how our model performed on the
    unseen test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7: Generating Predictions and Evaluating the Performance of the SVC
    Grid Search Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In previous exercises/activities, we learned to generate predictions and evaluate
    classifier model performance. In this activity we will, again, evaluate the performance
    of our model by generating predictions, creating a confusion matrix, and printing
    a classification report.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 29:'
  prefs: []
  type: TYPE_NORMAL
- en: Extract the predicted classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create and print a confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate and print a classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 348.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we demonstrated how to tune the hyperparameters of an SVC model using
    grid search. After tuning the SVC model, it did not perform as well as the tuned
    logistic regression model in predicting rain/snow. Additionally, SVC models are
    a **black box** in that they do not provide insight into the contribution of features
    on the outcome measure. In the upcoming *Decision Trees* section, we will introduce
    a different algorithm known as a decision tree, which uses a "*divide and conquer*"
    approach to generate predictions and offers a feature importance attribute for
    determining the importance of each feature on the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Decision Trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine we are considering changing jobs. We are weighing the pros and cons
    of prospective job opportunities and, after a few years of being in our current
    position, we start to realize the things that are important to us. However, not
    all aspects of a career are of equal importance. In fact, after being in the job
    for a few years, we decide that the most important aspect of a position is our
    interest in the projects we will be doing, followed by compensation, then work-related
    stress, trailed by commute time, and, lastly, benefits. We have just created the
    scaffolding of a cognitive decision tree. We can go into further detail by saying
    that we want a job where we are very interested in the allocated projects, paying
    at least $55k/year, with low work-related stress, a commute of under 30 minutes,
    and good dental insurance. Creating mental decision trees is a decision-making
    process we all utilize by nature and is one of the reasons why decision trees
    are one of the most widely used machine learning algorithms today.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, decision trees use either *gini* impurity or *entropy*
    information gain as the criterion to measure the quality of a split. First, the
    decision tree algorithm determines the feature that maximizes the value indicating
    quality of a split. This becomes referred to as the root node, as it is the most
    important feature in the data. In the job offer mentioned earlier, being very
    interested in the prospective projects would be considered the root node. Taking
    into consideration the root node, the job opportunities are divided into those
    with very interesting projects and those without very interesting projects.
  prefs: []
  type: TYPE_NORMAL
- en: Next, each of these two categories are divided into the next most important
    feature, given the previous feature(s), and so on and so forth, until the potential
    jobs are identified as being of interest or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach is termed recursive partitioning, or "*divide and conquer*",
    because it continues the process of splitting and subsetting the data until the
    algorithm determines the subsets in the data as sufficiently homogenous, or:'
  prefs: []
  type: TYPE_NORMAL
- en: Nearly all the observations at the corresponding node have the same class (that
    is, purity).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are no further features in the data for which to split.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tree has reached the size limit decided upon a priori.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, if purity is determined by entropy, we must understand that entropy
    is a measure of randomness within a set of values. Decision trees operate by choosing
    the splits that minimize entropy (randomness) and, in turn, maximize information
    gain. Information gain is calculated as the difference in entropy between the
    split and all other following splits. The total entropy is then computed by taking
    the sum of the entropy in each partition, weighted by the proportion of observations
    in the partition. Luckily, scikit-learn provides us with a function that does
    all of this for us. In the following exercises and activities, we will implement
    the decision tree classifier model to predict whether it is raining or snowing,
    using the familiar `weather.csv` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 8: Preparing Data for a Decision Tree Classifier'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this activity, we will prepare our data for a decision tree classifier model.
    Perform the following steps to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Import `weather.csv` and store it as a DataFrame
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dummy code the multi-level, categorical feature `Summary`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shuffle the data to remove any possible order effects
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into features and outcome
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further divide the features and outcome into testing and training data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Scale `X_train` and `X_test` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following exercise, we will learn to tune and fit a decision tree classifier
    model..
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 30: Tuning a Decision Tree Classifier Using Grid Search'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the current exercise, we will instantiate a hyperparameter space and tune
    the hyperparameters of a decision tree classifier using a grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Activity 8*, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specify the hyperparameter space as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a grid search model using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit to the training set using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output displayed here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.25: The output from fitting our decision tree classifier grid search
    model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the tuned parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13322_03_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.26: The tuned hyperparameters for our decision tree classifier grid
    search model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see from Figure 3.26 that it used **gini** impurity as the criterion
    to measure the quality of a split. Further explanations of the hyperparameters
    are outside the scope of this chapter but can be found in the decision tree classifier
    scikit-learn documentation.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, in practice, it is common for decision makers to ask how various features
    are affecting the predictions. In linear and logistic regression, the intercept
    and coefficient(s) make model predictions very transparent.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Decision trees can also be very easy to interpret, as we can see where the decisions
    were made, but this requires an installation and proper configuration of Graphviz,
    as well as unscaled features.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of plotting the tree in the following exercise, we will explore an attribute
    found in scitkit-learn's tree-based model algorithms, '`feature_importances_`',
    which returns an array containing values of relative feature importance for each
    feature. It is important to note that this attribute is unavailable from a grid
    search model. As a result, in the next exercise, we will learn to programmatically
    extract values from the `best_parameters` dictionary and re-fit the tuned decision
    tree model, allowing us to access the attributes provided by the decision tree
    classifier function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 31: Programmatically Extracting Tuned Hyperparameters from a Decision
    Tree Classifier Grid Search Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous exercise, we saved the tuned hyperparameters as key value pairs
    in the `best_parameters` dictionary. This allows us to programmatically access
    the values and assign them to the appropriate hyperparameters of a decision tree
    classifier model. By fitting the tuned decision tree model, we will be able to
    access the attributes made available from the scikit-learn decision tree classifier
    function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 30*, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prove that we can access the value for ''`Tree_criterion`'' using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.27: The value assigned to the ‘Tree_criterion’ key in the best_parameters
    dictionary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.27: The value assigned to the ''Tree_criterion'' key in the best_parameters
    dictionary'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Instantiate decision tree classifier model and assign the values to the corresponding
    hyperparameters as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the grid search model to the scaled training data using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.28: The output from fitting the decision tree classifier model with
    tuned hyperparameters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Extract `feature_importances` attribute using:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 3.29: An array of feature importance from our tuned decision tree
    classifier model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.29: An array of feature importance from our tuned decision tree classifier
    model'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: From the array in Figure 3.29, we can see that the first feature completely
    dominated the other variables in terms of feature importance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize this using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.30: Feature importance from a tuned decision tree classifier model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.30: Feature importance from a tuned decision tree classifier model'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: It looks like temperature in Celsius was the sole driver in this classification
    problem. With the outcome measure being `rain ('Rain'=1)` or `snow ('Rain'=0)`
    and the way in which decision trees make split decisions via "*divide and conquer*,"
    it makes sense that the algorithm used temperature to determine if there was rainfall
    or snowfall at the time of measurement. In the upcoming activity, we will evaluate
    how the model performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 9: Generating Predictions and Evaluating the Performance of a Decision
    Tree Classifier Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have generated predictions and evaluated the model performance in previous
    exercises and activities. We will be taking the same approach in this activity
    to evaluate the performance of our tuned decision tree classifier model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 31, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate the predicted probabilities of rain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate the predicted classes of rain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate and print a confusion matrix.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print a classification report.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 350.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should find that there was only one misclassified observation. Thus, by
    tuning a decision tree classifier model on our `weather.csv` dataset, we were
    able to predict rain (or snow) with great accuracy. We can see that the sole driving
    feature was temperature in Celsius. This makes sense due to the way in which decision
    trees use recursive partitioning to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, after evaluation, a single model is a weak learner and does not perform
    well. However, by combining weak learners, we create a stronger learner. The approach
    of combining numerous weak learners to create a stronger learner is termed ensemble.
    Random forest models combine numerous decision tree models to create a stronger
    ensemble model. Random forests can be used for classification or regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Random Forests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As briefly mentioned earlier, random forests are ensembles of decision trees
    that can be used to solve classification or regression problems. Random forests
    use a small portion of the data to fit each tree, so they can handle very large
    datasets, and they are less prone to the "*curse of dimensionality*" relative
    to other algorithms. The curse of dimensionality is a situation in which an abundance
    of features in the data diminishes the performance of the model. Predictions of
    the random forest are then determined by combining the predictions of each tree.
    Like SVM, random forests are a **black box** with inputs and outputs which cannot
    be interpreted.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming exercises and activities, we will tune and fit a random forest
    regressor using grid search to predict the temperature in Celsius. Then, we will
    evaluate the performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 32: Preparing Data for a Random Forest Regressor'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will prepare the data for the random forest regressor with ''`Temperature_c`''
    as the dependent variable, just as we did in Exercise 21:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import ''`weather.csv`'' and save it as `df` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Dummy code the multi-class, categorical variable, Description, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove any possible ordering effects by shuffling `df_dummies` using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split `df_shuffled` into `X` and `y` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split `X` and `y` into testing and training data as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scale `X_train` and `X_test` using the code here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have imported, shuffled, separated our data into features (`X`)
    and dependent variable (`y`), split `X` and `y` into testing and training data,
    and scaled `X_train` and `X_test`, we will tune a random forest regressor model
    using grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 10: Tuning a Random Forest Regressor'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The data has been prepared for inclusion in a random forest regressor. Now,
    we must set up the hyperparameter space and find the optimal combination of hyperparameters
    using a grid search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from Exercise 32, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the hyperparameter space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate the `GridSearchCV` model optimizing the explained variance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fit the grid search model to the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the tuned parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 351.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After performing a grid search of our random forest regressor hyperparameters,
    we need to fit a random forest regressor model with the tuned hyperparameters.
    We will programmatically extract the values in the `best_parameters` dictionary
    and assign them to the corresponding hyperparameters in the random forest regressor
    function, so we can access the attributes from the random forest regressor function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 33: Programmatically Extracting Tuned Hyperparameters and Determining
    Feature Importance from a Random Forest Regressor Grid Search Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: By extracting the value from the key-value pairs in the `best_parameters` dictionary,
    we eliminate the possibility of manual errors, as well as make our code more automated.
    In this exercise, we will replicate the steps from *Exercise 31*, but will adapt
    our code for the random forest regressor model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Activity 10*, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate a random forest regressor model with the values for each key from
    the `best_parameters` dictionary assigned to the corresponding hyperparameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model on the training data using the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_03_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.31: The output from fitting the random forest regressor model with
    tuned hyperparameters'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Plot feature importance in descending order using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'See the resultant output here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.32: Feature importance from a random forest regressor model with
    tuned hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_03_32.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.32: Feature importance from a random forest regressor model with tuned
    hyperparameters'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: From Figure 3.32, we can see that the '`Description_Warm`' dummy variable and
    '`Humidity`' are the main drivers of temperature in Celsius. Meanwhile, '`Visibility_km`'
    and '`Wind_Bearing_degrees`' have a small effect on the temperature. Let's now
    check to see how our model performs on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 11: Generating Predictions and Evaluating the Performance of a Tuned
    Random Forest Regressor Model'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Exercise 23* and *Activity 5*, we learned to generate predictions and evaluate
    the performance of regression models that predict a continuous outcome. In this
    activity, we will be taking the same approach to evaluate the performance of our
    random forest regressor model to predict temperature in Celsius.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continuing from *Exercise 33*, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate predictions on the test data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the correlation of predicted and actual values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Plot the distribution of residuals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute metrics, then place them in a DataFrame and print it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 352.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The random forest regressor model seems to underperform compared to the multiple
    linear regression, as evidenced by greater MAE, MSE, and RMSE values, as well
    as less explained variance. Additionally, there was a weaker correlation between
    the predicted and actual values, and the residuals were further from being normally
    distributed. Nevertheless, by leveraging ensemble methods using a random forest
    regressor, we constructed a model that explains 75.8% of the variance in -temperature
    and predicts temperature in Celsius ± 3.781 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to the open source machine learning library
    for Python, scikit-learn. You learned to preprocess data, as well as how to tune
    and fit a few different regression and classification algorithms. Lastly, you
    learned how to quickly and effectively evaluate the performance of classification
    and regression models. This was a very comprehensive introduction to the scikit-learn
    library, and the strategies employed here can be applied to building numerous
    additional algorithms provided by scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn about dimensionality reduction and unsupervised
    learning.
  prefs: []
  type: TYPE_NORMAL
