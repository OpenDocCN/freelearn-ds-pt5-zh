["```py\n//Create vectors\nscala> import org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\n\n//Create dense vector\nscala> val dense_v: Vector = Vectors.dense(10.0,0.0,20.0,30.0,0.0)\ndense_v: org.apache.spark.ml.linalg.Vector = [10.0,0.0,20.0,30.0,0.0]\nscala>\n\n//Create sparse vector: pass size, position index array and value array\nscala> val sparse_v1: Vector = Vectors.sparse(5,Array(0,2,3),\n       Array(10.0,20.0,30.0))\nsparse_v1: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])\nscala>\n\n//Another way to create sparse vector with position, value tuples\nscala> val sparse_v2: Vector = Vectors.sparse(5,\n        Seq((0,10.0),(2,20.0),(3,30.0)))\nsparse_v2: org.apache.spark.ml.linalg.Vector = (5,[0,2,3],[10.0,20.0,30.0])\nscala>  \n Compare vectors \n--------------- cala> sparse_v1 == sparse_v2\nres0: Boolean = true\nscala> sparse_v1 == dense_v\nres1: Boolean = true      //All three objects are equal but...\nscala> dense_v.toString()\nres2: String = [10.0,0.0,20.0,30.0,0.0]\nscala> sparse_v2.toString()\nres3: String = (5,[0,2,3],[10.0,20.0,30.0]) //..internal representation\ndiffers\nscala> sparse_v2.toArray\nres4: Array[Double] = Array(10.0, 0.0, 20.0, 30.0, 0.0)\n\nInterchangeable ---------------\nscala> dense_v.toSparse\nres5: org.apache.spark.mllib.linalg.SparseVector = (5,[0,2,3]\n[10.0,20.0,30.0])\nscala> sparse_v1.toDense\nres6: org.apache.spark.mllib.linalg.DenseVector = [10.0,0.0,20.0,30.0,0.0]\nscala>\n\nA common operation ------------------\nscala> Vectors.sqdist(sparse_v1,\n        Vectors.dense(1.0,2.0,3.0,4.0,5.0))\nres7: Double = 1075.0\n```", "```py\n//Create vectors\n>>> from pyspark.ml.linalg import Vector, Vectors\n//Create vectors\n>>> dense_v = Vectors.dense(10.0,0.0,20.0,30.0,0.0)\n//Pass size, position index array and value array\n>>> sparse_v1 = Vectors.sparse(5,[0,2,3],\n                    [10.0,20.0,30.0])\n>>> \n\n//Another way to create sparse vector with position, value tuples\n>>> sparse_v2 = Vectors.sparse(5,\n                  [[0,10.0],[2,20.0],[3,30.0]])\n>>> \n\nCompare vectors \n--------------- >>> sparse_v1 == sparse_v2\nTrue\n>>> sparse_v1 == dense_v\nTrue      //All three objects are equal but...\n>>> dense_v\nDenseVector([10.0, 0.0, 20.0, 30.0, 0.0])\n>>> sparse_v1\nSparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0}) //..internal representation\ndiffers\n>>> sparse_v2\nSparseVector(5, {0: 10.0, 2: 20.0, 3: 30.0})\n\nInterchangeable \n---------------- //Note: as of Spark 2.0.0, toDense and toSparse are not available in pyspark\n A common operation \n------------------- >>> Vectors.squared_distance(sparse_v1,\n        Vectors.dense(1.0,2.0,3.0,4.0,5.0))\n1075.0\n```", "```py\nscala> import org.apache.spark.ml.linalg.{Matrix,Matrices}\nimport org.apache.spark.ml.linalg.{Matrix, Matrices}\n\nCreate dense matrix \n------------------- //Values in column major order\nMatrices.dense(3,2,Array(9.0,0,0,0,8.0,6))\nres38: org.apache.spark.mllib.linalg.Matrix =\n9.0  0.0\n0.0  8.0\n0.0  6.0\n Create sparse matrix \n-------------------- //1.0 0.0 4.0\n0.0 3.0 5.0\n2.0 0.0 6.0//\nval sm: Matrix = Matrices.sparse(3,3,\n        Array(0,2,3,6), Array(0,2,1,0,1,2),\n        Array(1.0,2.0,3.0,4.0,5.0,6.0))\nsm: org.apache.spark.mllib.linalg.Matrix =\n3 x 3 CSCMatrix\n(0,0) 1.0\n(2,0) 2.0\n(1,1) 3.0\n(0,2) 4.0\n(1,2) 5.0\n(2,2) 6.0\n Sparse matrix, a column of all zeros \n------------------------------------ //third column all zeros\nMatrices.sparse(3,4,Array(0,2,3,3,6),\n    Array(0,2,1,0,1,2),values).toArray\nres85: Array[Double] = Array(1.0, 0.0, 2.0, 0.0, 3.0, 0.0, 0.0, 0.0, 0.0,\n4.0, 5.0, 6.0)\n\n```", "```py\n//Create dense matrix\n>>> from pyspark.ml.linalg import Matrix, Matrices\n\n//Values in column major order\n>>> Matrices.dense(3,2,[9.0,0,0,0,8.0,6])\nDenseMatrix(3, 2, [9.0, 0.0, 0.0, 0.0, 8.0, 6.0], False)\n>>> \n\n//Create sparse matrix\n//1.0 0.0 4.0\n0.0 3.0 5.0\n2.0 0.0 6.0//\n>>> sm = Matrices.sparse(3,3,\n        [0,2,3,6], [0,2,1,0,1,2],\n        [1.0,2.0,3.0,4.0,5.0,6.0])\n>>> \n\n//Sparse matrix, a column of all zeros\n//third column all zeros\n>>> Matrices.sparse(3,4,[0,2,3,3,6],\n        [0,2,1,0,1,2],\n    values=[1.0,2.0,3.0,4.0,5.0,6.0]).toArray()\narray([[ 1.,  0.,  0.,  4.],\n       [ 0.,  3.,  0.,  5.],\n       [ 2.,  0.,  0.,  6.]])\n>>> \n```", "```py\nscala> import org.apache.spark.mllib.linalg.{Vector,Vectors}\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nscala> import org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n\nscala>val dense_vlist: Array[Vector] = Array(\n    Vectors.dense(11.0,12,13,14),\n    Vectors.dense(21.0,22,23,24),\n    Vectors.dense(31.0,32,33,34))\ndense_vlist: Array[org.apache.spark.mllib.linalg.Vector] =\nArray([11.0,12.0,13.0,14.0], [21.0,22.0,23.0,24.0], [31.0,32.0,33.0,34.0])\nscala>\n\n//Distribute the vector list\nscala> val rows  = sc.parallelize(dense_vlist)\nrows: org.apache.spark.rdd.RDD[org.apache.spark.mllib.linalg.Vector] =\nParallelCollectionRDD[0] at parallelize at <console>:29\nscala> val m: RowMatrix = new RowMatrix(rows)\nm: org.apache.spark.mllib.linalg.distributed.RowMatrix =\norg.apache.spark.mllib.linalg.distributed.RowMatrix@5c5043fe\nscala> print(\"Matrix size is \" + m.numRows()+\"X\"+m.numCols())\nMatrix size is 3X4\nscala>\n```", "```py\n>>> from pyspark.mllib.linalg import Vector,Vectors\n>>> from pyspark.mllib.linalg.distributed import RowMatrix\n\n>>> dense_vlist = [Vectors.dense(11.0,12,13,14),\n         Vectors.dense(21.0,22,23,24), Vectors.dense(31.0,32,33,34)]\n>>> rows  = sc.parallelize(dense_vlist)\n>>> m = RowMatrix(rows)\n>>> \"Matrix size is {0} X {1}\".format(m.numRows(), m.numCols())\n'Matrix size is 3 X 4'\n```", "```py\n//Pipeline example with single stage to illustrate syntax\nscala> import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.Pipeline\nscala> import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.feature._\n\n//Create source data frame\nscala> val df = spark.createDataFrame(Seq(\n         (\"Oliver Twist\",\"Charles Dickens\"),\n        (\"Adventures of Tom Sawyer\",\"Mark Twain\"))).toDF(\n        \"Title\",\"Author\")\n\n//Split the Title to tokens\nscala> val tok = new Tokenizer().setInputCol(\"Title\").\n          setOutputCol(\"words\")\ntok: org.apache.spark.ml.feature.Tokenizer = tok_2b2757a3aa5f\n\n//Define a pipeline with a single stage\nscala> val p = new Pipeline().setStages(Array(tok))\np: org.apache.spark.ml.Pipeline = pipeline_f5e0de400666\n\n//Run an Estimator (fit) using the pipeline\nscala> val model = p.fit(df)\nmodel: org.apache.spark.ml.PipelineModel = pipeline_d00989625bb2\n\n//Examine stages\nscala> p.getStages   //Returns a list of stage objects\nres1: Array[org.apache.spark.ml.PipelineStage] = Array(tok_55af0061af6d)\n\n// Examine the results\nscala> val m = model.transform(df).select(\"Title\",\"words\")\nm: org.apache.spark.sql.DataFrame = [Title: string, words: array<string>]\nscala> m.select(\"words\").collect().foreach(println)\n[WrappedArray(oliver, twist)]\n[WrappedArray(adventures, of, tom, sawyer)]\n```", "```py\n//Pipeline example with single stage to illustrate syntax\n//Create source data frame\n>>> from pyspark.ml.pipeline import Pipeline\n>>> from pyspark.ml.feature import Tokenizer\n>>>  df = sqlContext.createDataFrame([\n    (\"Oliver Twist\",\"Charles Dickens\"),\n    (\"Adventures of Tom Sawyer\",\"Mark Twain\")]).toDF(\"Title\",\"Author\")\n>>> \n\n//Split the Title to tokens\n>>> tok = Tokenizer(inputCol=\"Title\",outputCol=\"words\")\n\n//Define a pipeline with a single stage\n>>> p = Pipeline(stages=[tok])\n\n//Run an Estimator (fit) using the pipeline\n>>> model = p.fit(df)\n\n//Examine stages\n>>> p.getStages()  //Returns a list of stage objects\n[Tokenizer_4f35909c4c504637a263]\n\n// Examine the results\n>>> m = model.transform(df).select(\"Title\",\"words\")\n>>> [x[0] for x in m.select(\"words\").collect()]\n[[u'oliver', u'twist'], [u'adventures', u'of', u'tom', u'sawyer']]\n>>> \n```", "```py\nscala> import org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.Pipeline\nscala> import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.feature._\nscala> \n\n//Create a dataframe\nscala> val df2 = spark.createDataset(Array(\n         (1,\"Here is some text to illustrate pipeline\"),\n         (2, \"and tfidf, which stands for term frequency inverse document\nfrequency\"\n         ))).toDF(\"LineNo\",\"Text\")\n\n//Define feature transformations, which are the pipeline stages\n// Tokenizer splits text into tokens\nscala> val tok = new Tokenizer().setInputCol(\"Text\").\n             setOutputCol(\"Words\")\ntok: org.apache.spark.ml.feature.Tokenizer = tok_399dbfe012f8\n\n// HashingTF maps a sequence of words to their term frequencies using hashing\n// Larger value of numFeatures reduces hashing collision possibility\nscala> val tf = new HashingTF().setInputCol(\"Words\").setOutputCol(\"tf\").setNumFeatures(100)\ntf: org.apache.spark.ml.feature.HashingTF = hashingTF_e6ad936536ea\n// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces weightage of commonly occuring words\nscala> val idf = new IDF().setInputCol(\"tf\").setOutputCol(\"tf_idf\")\nidf: org.apache.spark.ml.feature.IDF = idf_8af1fecad60a\n// VectorAssembler merges multiple columns into a single vector column\nscala> val va = new VectorAssembler().setInputCols(Array(\"tf_idf\")).setOutputCol(\"features\")\nva: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_23205c3f92c8\n//Define pipeline\nscala> val tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))\nval tfidf_pipeline = new Pipeline().setStages(Array(tok,tf,idf,va))\nscala> tfidf_pipeline.getStages\nres2: Array[org.apache.spark.ml.PipelineStage] = Array(tok_399dbfe012f8, hashingTF_e6ad936536ea, idf_8af1fecad60a, vecAssembler_23205c3f92c8)\nscala>\n\n//Now execute the pipeline\nscala> val result = tfidf_pipeline.fit(df2).transform(df2).select(\"words\",\"features\").first()\nresult: org.apache.spark.sql.Row = [WrappedArray(here, is, some, text, to, illustrate, pipeline),(100,[0,3,35,37,69,81],[0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644,0.4054651081081644])]\n```", "```py\n//A realistic, multi-step pipeline that converts text to TF_ID\n>>> from pyspark.ml.pipeline import Pipeline\n>>> from pyspark.ml.feature import Tokenizer, HashingTF, IDF, VectorAssembler, \\\n               StringIndexer, VectorIndexer\n\n//Create a dataframe\n>>> df2 = sqlContext.createDataFrame([\n    [1,\"Here is some text to illustrate pipeline\"],\n    [2,\"and tfidf, which stands for term frequency inverse document\nfrequency\"\n    ]]).toDF(\"LineNo\",\"Text\")\n\n//Define feature transformations, which are the pipeline stages\n//Tokenizer splits text into tokens\n>>> tok = Tokenizer(inputCol=\"Text\",outputCol=\"words\")\n\n// HashingTF maps a sequence of words to their term frequencies using\nhashing\n\n// Larger the numFeatures, lower the hashing collision possibility\n>>> tf = HashingTF(inputCol=\"words\", outputCol=\"tf\",numFeatures=1000)\n\n// IDF, Inverse Docuemnt Frequency is a statistical weight that reduces\nweightage of commonly occuring words\n>>> idf = IDF(inputCol = \"tf\",outputCol=\"tf_idf\")\n\n// VectorAssembler merges multiple columns into a single vector column\n>>> va = VectorAssembler(inputCols=[\"tf_idf\"],outputCol=\"features\")\n\n//Define pipeline\n>>> tfidf_pipeline = Pipeline(stages=[tok,tf,idf,va])\n>>> tfidf_pipeline.getStages()\n[Tokenizer_4f5fbfb6c2a9cf5725d6, HashingTF_4088a47d38e72b70464f, IDF_41ddb3891541821c6613, VectorAssembler_49ae83b800679ac2fa0e]\n>>>\n\n//Now execute the pipeline\n>>> result = tfidf_pipeline.fit(df2).transform(df2).select(\"words\",\"features\").collect()\n>>> [(x[0],x[1]) for x in result]\n[([u'here', u'is', u'some', u'text', u'to', u'illustrate', u'pipeline'], SparseVector(1000, {135: 0.4055, 169: 0.4055, 281: 0.4055, 388: 0.4055, 400: 0.4055, 603: 0.4055, 937: 0.4055})), ([u'and', u'tfidf,', u'which', u'stands', u'for', u'term', u'frequency', u'inverse', u'document', u'frequency'], SparseVector(1000, {36: 0.4055, 188: 0.4055, 333: 0.4055, 378: 0.4055, 538: 0.4055, 597: 0.4055, 727: 0.4055, 820: 0.4055, 960: 0.8109}))]\n>>> \n```", "```py\nscala> import org.apache.spark.ml.feature._\nimport org.apache.spark.ml.feature._\nscala>\n\n//Basic examples illustrating features usage\n//Look at model examples for more feature examples\n//Binarizer converts continuous value variable to two discrete values based on given threshold\nscala> import scala.util.Random\nimport scala.util.Random\nscala> val nums = Seq.fill(10)(Random.nextDouble*100)\n...\nscala> val numdf = spark.createDataFrame(nums.map(Tuple1.apply)).toDF(\"raw_nums\")\nnumdf: org.apache.spark.sql.DataFrame = [raw_nums: double]\nscala> val binarizer = new Binarizer().setInputCol(\"raw_nums\").\n            setOutputCol(\"binary_vals\").setThreshold(50.0)\nbinarizer: org.apache.spark.ml.feature.Binarizer = binarizer_538e392f56db\nscala> binarizer.transform(numdf).select(\"raw_nums\",\"binary_vals\").show(2)\n+------------------+-----------+\n|          raw_nums|binary_vals|\n+------------------+-----------+\n|55.209245003482884|        1.0|\n| 33.46202184060426|        0.0|\n+------------------+-----------+\nscala>\n\n//Bucketizer to convert continuous value variables to desired set of discrete values\nscala> val split_vals:Array[Double] = Array(0,20,50,80,100) //define intervals\nsplit_vals: Array[Double] = Array(0.0, 20.0, 50.0, 80.0, 100.0)\nscala> val b = new Bucketizer().\n           setInputCol(\"raw_nums\").\n           setOutputCol(\"binned_nums\").\n           setSplits(split_vals)\nb: org.apache.spark.ml.feature.Bucketizer = bucketizer_a4dd599e5977\nscala> b.transform(numdf).select(\"raw_nums\",\"binned_nums\").show(2)\n+------------------+-----------+\n|          raw_nums|binned_nums|\n+------------------+-----------+\n|55.209245003482884|        2.0|\n| 33.46202184060426|        1.0|\n+------------------+-----------+\nscala>\n\n//Bucketizer is effectively equal to binarizer if only two intervals are\ngiven \nscala> new Bucketizer().setInputCol(\"raw_nums\").\n        setOutputCol(\"binned_nums\").setSplits(Array(0,50.0,100.0)).\n        transform(numdf).select(\"raw_nums\",\"binned_nums\").show(2)\n+------------------+-----------+\n|          raw_nums|binned_nums|\n+------------------+-----------+\n|55.209245003482884|        1.0|\n| 33.46202184060426|        0.0|\n+------------------+-----------+\nscala>\n```", "```py\n//Some more features\n>>> from pyspark.ml import feature, pipeline\n>>> \n\n//Basic examples illustrating features usage\n//Look at model examples for more examples\n//Binarizer converts continuous value variable to two discrete values based on given threshold\n>>> import random\n>>> nums = [random.random()*100 for x in range(1,11)]\n>>> numdf = sqlContext.createDataFrame(\n             [[x] for x in nums]).toDF(\"raw_nums\")\n>>> binarizer = feature.Binarizer(threshold= 50,\n       inputCol=\"raw_nums\", outputCol=\"binary_vals\")\n>>> binarizer.transform(numdf).select(\"raw_nums\",\"binary_vals\").show(2)\n+------------------+-----------+\n|          raw_nums|binary_vals|\n+------------------+-----------+\n| 95.41304359504672|        1.0|\n|41.906045589243405|        0.0|\n+------------------+-----------+\n>>> \n\n//Bucketizer to convert continuous value variables to desired set of discrete values\n>>> split_vals = [0,20,50,80,100] //define intervals\n>>> b =\nfeature.Bucketizer(inputCol=\"raw_nums\",outputCol=\"binned_nums\",splits=split\nvals)\n>>> b.transform(numdf).select(\"raw_nums\",\"binned_nums\").show(2)\n+------------------+-----------+\n|          raw_nums|binned_nums|\n+------------------+-----------+\n| 95.41304359504672|        3.0|\n|41.906045589243405|        1.0|\n+------------------+-----------+\n\n//Bucketizer is effectively equal to binarizer if only two intervals are\ngiven \n>>> feature.Bucketizer(inputCol=\"raw_nums\",outputCol=\"binned_nums\",                  \n                       splits=[0,50.0,100.0]).transform(numdf).select(\n                       \"raw_nums\",\"binned_nums\").show(2)\n+------------------+-----------+\n|          raw_nums|binned_nums|\n+------------------+-----------+\n| 95.41304359504672|        1.0|\n|41.906045589243405|        0.0|\n+------------------+-----------+\n>>> \n```", "```py\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.regression.LinearRegressionModel\nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD\nscala> import org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}\nimport org.apache.spark.ml.regression.{LinearRegression,LinearRegressionModel}\n// Load the data\nscala> val data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_linear_regression_data.txt\")\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n// Build the model\nscala> val lrModel = new LinearRegression().fit(data)\n\n//Note: You can change ElasticNetParam, MaxIter and RegParam\n// Defaults are 0.0, 100 and 0.0\nlrModel: org.apache.spark.ml.regression.LinearRegressionModel = linReg_aa788bcebc42\n\n//Check Root Mean Squared Error\nscala> println(\"Root Mean Squared Error = \" + lrModel.summary.rootMeanSquaredError)\nRoot Mean Squared Error = 10.16309157133015\n```", "```py\n>>> from pyspark.ml.regression import LinearRegression, LinearRegressionModel\n>>>\n\n// Load the data\n>>> data = spark.read.format(\"libsvm\").load(\"data/mllib/sample_linear_regression_data.txt\")\n>>> \n\n// Build the model\n>>> lrModel = LinearRegression().fit(data)\n\n//Note: You can change ElasticNetParam, MaxIter and RegParam\n// Defaults are 0.0, 100 and 0.0\n//Check Root Mean Squared Error\n>>> print \"Root Mean Squared Error = \", lrModel.summary.rootMeanSquaredError\nRoot Mean Squared Error = 10.16309157133015\n>>> \n```", "```py\nscala> import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nimport org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\nscala> import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\nscala>\n\n// Load training data in LIBSVM format.\nscala> val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_libsvm_data.txt\")\ndata: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[6] at map at MLUtils.scala:84\nscala>\n\n// Split data into training (60%) and test (40%).\nscala> val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\nsplits: Array[org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint]] = Array(MapPartitionsRDD[7] at randomSplit at <console>:29, MapPartitionsRDD[8] at randomSplit at <console>:29)\nscala> val training = splits(0).cache()\ntraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[7] at randomSplit at <console>:29\nscala> val test = splits(1)\ntest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[8] at randomSplit at <console>:29\nscala>\n\n// Run training algorithm to build the model\nscala> val model = SVMWithSGD.train(training, numIterations=100)\nmodel: org.apache.spark.mllib.classification.SVMModel = org.apache.spark.mllib.classification.SVMModel: intercept = 0.0, numFeatures = 692, numClasses = 2, threshold = 0.0\nscala>\n\n// Clear the default threshold.\nscala> model.clearThreshold()\nres1: model.type = org.apache.spark.mllib.classification.SVMModel: intercept =\n0.0, numFeatures = 692, numClasses = 2, threshold = None\nscala>\n\n// Compute raw scores on the test set.\nscala> val scoreAndLabels = test.map { point =>\n       val score = model.predict(point.features)\n      (score, point.label)\n      }\nscoreAndLabels: org.apache.spark.rdd.RDD[(Double, Double)] =\nMapPartitionsRDD[213] at map at <console>:37\nscala>\n\n// Get evaluation metrics.\nscala> val metrics = new BinaryClassificationMetrics(scoreAndLabels)\nmetrics: org.apache.spark.mllib.evaluation.BinaryClassificationMetrics = org.apache.spark.mllib.evaluation.BinaryClassificationMetrics@3106aebb\nscala> println(\"Area under ROC = \" + metrics.areaUnderROC())\nArea under ROC = 1.0\nscala>\n```", "```py\n//Assuming ml.Pipeline and ml.features are already imported\nscala> import org.apache.spark.ml.classification.{\n        DecisionTreeClassifier, DecisionTreeClassificationModel}\nimport org.apache.spark.ml.classification.{DecisionTreeClassifier,\nDecisionTreeClassificationModel}\nscala>\n/prepare train data\nscala> val f:String = \"<Your path>/simple_file1.csv\"\nf: String = <your path>/simple_file1.csv\nscala> val trainDF = spark.read.options(Map(\"header\"->\"true\",\n            \"inferSchema\"->\"true\")).csv(f)\ntrainDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]\n\nscala>\n\n //define DecisionTree pipeline\n//StringIndexer maps labels(String or numeric) to label indices\n//Maximum occurrence label becomes 0 and so on\nscala> val lblIdx = new StringIndexer().\n                setInputCol(\"Label\").\n                setOutputCol(\"indexedLabel\")\nlblIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_3a7bc9c1ed0d\nscala>\n\n// Create labels list to decode predictions\nscala> val labels = lblIdx.fit(trainDF).labels\nlabels: Array[String] = Array(2, 1, 3)\nscala>\n\n//Define Text column indexing stage\nscala> val fIdx = new StringIndexer().\n                setInputCol(\"Text\").\n              setOutputCol(\"indexedText\")\nfIdx: org.apache.spark.ml.feature.StringIndexer = strIdx_49253a83c717\n\n// VectorAssembler\nscala> val va = new VectorAssembler().\n              setInputCols(Array(\"indexedText\")).\n              setOutputCol(\"features\")\nva: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_764720c39a85\n\n//Define Decision Tree classifier. Set label and features vector\nscala> val dt = new DecisionTreeClassifier().\n            setLabelCol(\"indexedLabel\").\n            setFeaturesCol(\"features\")\ndt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_84d87d778792\n\n//Define label converter to convert prediction index back to string\nscala> val lc = new IndexToString().\n                setInputCol(\"prediction\").\n                setOutputCol(\"predictedLabel\").\n                setLabels(labels)\nlc: org.apache.spark.ml.feature.IndexToString = idxToStr_e2f4fa023665\nscala>\n\n//String the stages together to form a pipeline\nscala> val dt_pipeline = new Pipeline().setStages(\n          Array(lblIdx,fIdx,va,dt,lc))\ndt_pipeline: org.apache.spark.ml.Pipeline = pipeline_d4b0e884dcbf\nscala>\n//Apply pipeline to the train data\nscala> val resultDF = dt_pipeline.fit(trainDF).transform(trainDF)\n\n//Check results. Watch Label and predictedLabel column values match\nresultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 6 more\nfields]\nscala>\nresultDF.select(\"Text\",\"Label\",\"features\",\"prediction\",\"predictedLabel\").show()\n+----+-----+--------+----------+--------------+\n|Text|Label|features|prediction|predictedLabel|\n+----+-----+--------+----------+--------------+\n|   A|    1|   [1.0]|       1.0|             1|\n|   B|    2|   [0.0]|       0.0|             2|\n|   C|    3|   [2.0]|       2.0|             3|\n|   A|    1|   [1.0]|       1.0|             1|\n|   B|    2|   [0.0]|       0.0|             2|\n+----+-----+--------+----------+--------------+\nscala>\n\n//Prepare evaluation data\nscala> val eval:String = \"<Your path>/simple_file2.csv\"\neval: String = <Your path>/simple_file2.csv\nscala> val evalDF = spark.read.options(Map(\"header\"->\"true\",\n            \"inferSchema\"->\"true\")).csv(eval)\nevalDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int]\nscala>\n\n//Apply the same pipeline to the evaluation data\nscala> val eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)\neval_resultDF: org.apache.spark.sql.DataFrame = [Text: string, Label: int ... 7\nmore fields]\n\n//Check evaluation results\nscala>\neval_resultDF.select(\"Text\",\"Label\",\"features\",\"prediction\",\"predictedLabel\").sh\nw()\n+----+-----+--------+----------+--------------+\n|Text|Label|features|prediction|predictedLabel|\n+----+-----+--------+----------+--------------+\n|   A|    1|   [0.0]|       1.0|             1|\n|   A|    1|   [0.0]|       1.0|             1|\n|   A|    2|   [0.0]|       1.0|             1|\n|   B|    2|   [1.0]|       0.0|             2|\n|   C|    3|   [2.0]|       2.0|             3|\n+----+-----+--------+----------+--------------+\n//Note that predicted label for the third row is 1 as against Label(2) as\nexpected\n\nPython:\n\n//Model training example\n>>> from pyspark.ml.pipeline import Pipeline\n>>> from pyspark.ml.feature import StringIndexer, VectorIndexer, VectorAssembler,\nIndexToString\n>>> from pyspark.ml.classification import DecisionTreeClassifier,\nDecisionTreeClassificationModel\n>>> \n\n//prepare train data\n>>> file_location = \"../work/simple_file1.csv\"\n>>> trainDF = spark.read.csv(file_location,header=True,inferSchema=True)\n\n //Read file\n>>>\n\n//define DecisionTree pipeline\n//StringIndexer maps labels(String or numeric) to label indices\n//Maximum occurrence label becomes 0 and so on\n>>> lblIdx = StringIndexer(inputCol = \"Label\",outputCol = \"indexedLabel\")\n\n// Create labels list to decode predictions\n>>> labels = lblIdx.fit(trainDF).labels\n>>> labels\n[u'2', u'1', u'3']\n>>> \n\n//Define Text column indexing stage\n>>> fidx = StringIndexer(inputCol=\"Text\",outputCol=\"indexedText\")\n\n// Vector assembler\n>>> va = VectorAssembler(inputCols=[\"indexedText\"],outputCol=\"features\")\n\n//Define Decision Tree classifier. Set label and features vector\n>>> dt = DecisionTreeClassifier(labelCol=\"indexedLabel\",featuresCol=\"features\")\n\n//Define label converter to convert prediction index back to string\n>>> lc = IndexToString(inputCol=\"prediction\",outputCol=\"predictedLabel\",\n                       labels=labels)\n\n//String the stages together to form a pipeline\n>>> dt_pipeline = Pipeline(stages=[lblIdx,fidx,va,dt,lc])\n>>>\n>>> \n\n//Apply decision tree pipeline\n>>> dtModel = dt_pipeline.fit(trainDF)\n>>> dtDF = dtModel.transform(trainDF)\n>>> dtDF.columns\n['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction',\n'probability', 'prediction', 'predictedLabel']\n>>> dtDF.select(\"Text\",\"Label\",\"indexedLabel\",\"prediction\",\n\"predictedLabel\").show()\n+----+-----+------------+----------+--------------+\n|Text|Label|indexedLabel|prediction|predictedLabel|\n+----+-----+------------+----------+--------------+\n|   A|    1|         1.0|       1.0|             1|\n|   B|    2|         0.0|       0.0|             2|\n|   C|    3|         2.0|       2.0|             3|\n|   A|    1|         1.0|       1.0|             1|\n|   B|    2|         0.0|       0.0|             2|\n+----+-----+------------+----------+--------------+\n\n>>>\n\n>>> //prepare evaluation dataframe\n>>> eval_file_path = \"../work/simple_file2.csv\"\n>>> evalDF = spark.read.csv(eval_file_path,header=True, inferSchema=True) \n\n//Read eval file\n>>> eval_resultDF = dt_pipeline.fit(evalDF).transform(evalDF)\n>>> eval_resultDF.columns\n['Text', 'Label', 'indexedLabel', 'indexedText', 'features', 'rawPrediction', 'probability', 'prediction', 'predictedLabel']\n>>> eval_resultDF.select(\"Text\",\"Label\",\"indexedLabel\",\"prediction\",\n\"predictedLabel\").show()\n+----+-----+------------+----------+--------------+\n|Text|Label|indexedLabel|prediction|predictedLabel|\n+----+-----+------------+----------+--------------+\n|   A|    1|         1.0|       1.0|             1|\n|   A|    1|         1.0|       1.0|             1|\n|   A|    2|         0.0|       1.0|             1|\n|   B|    2|         0.0|       0.0|             2|\n|   C|    3|         2.0|       2.0|             3|\n+----+-----+------------+----------+--------------+\n>>> \n\nAccompanying data files:\nsimple_file1.csv Text,Label\nA,1\nB,2\nC,3\nA,1\nB,2simple_file2.csv Text,Label\nA,1\nA,1\nA,2\nB,2\nC,3\n```", "```py\n// Marks < 40 = Fail\n// Attendence == Poor => Fail\n// Marks >40 and attendence Full => Pass\n// Marks > 60 and attendence Enough or Full => Pass\n// Two exceptions were studentId 1009 and 1020 who were granted Pass\n//This example also emphasizes the reuse of pipeline stages\n// Initially the code trains a DecisionTreeClassifier\n// Then, same stages are reused to train a GBT classifier\n```", "```py\nscala> import org.apache.spark.ml.feature._\nscala> import org.apache.spark.ml.Pipeline\nscala> import org.apache.spark.ml.classification.{DecisionTreeClassifier,\n                                   DecisionTreeClassificationModel}\nscala> case class StResult(StudentId:String, Avg_Marks:Double,\n        Attendance:String, Result:String)\nscala> val file_path = \"../work/StudentsPassFail.csv\"\nscala> val source_ds = spark.read.options(Map(\"header\"->\"true\",\n            \"inferSchema\"->\"true\")).csv(file_path).as[StResult]\nsource_ds: org.apache.spark.sql.Dataset[StResult] = [StudentId: int, Avg_Marks:\ndouble ... 2 more fields]\nscala>\n//Examine source data\nscala> source_ds.show(4)\n+---------+---------+----------+------+\n|StudentId|Avg_Marks|Attendance|Result|\n+---------+---------+----------+------+\n|     1001|     48.0|      Full|  Pass|\n|     1002|     21.0|    Enough|  Fail|\n|     1003|     24.0|    Enough|  Fail|\n|     1004|      4.0|      Poor|  Fail|\n+---------+---------+----------+------+\n\nscala>           \n//Define preparation pipeline\nscala> val marks_bkt = new Bucketizer().setInputCol(\"Avg_Marks\").\n        setOutputCol(\"Mark_bins\").setSplits(Array(0,40.0,60.0,100.0))\nmarks_bkt: org.apache.spark.ml.feature.Bucketizer = bucketizer_5299d2fbd1b2\nscala> val att_idx = new StringIndexer().setInputCol(\"Attendance\").\n        setOutputCol(\"Att_idx\")\natt_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_2db54ba5200a\nscala> val label_idx = new StringIndexer().setInputCol(\"Result\").\n        setOutputCol(\"Label\")\nlabel_idx: org.apache.spark.ml.feature.StringIndexer = strIdx_20f4316d6232\nscala>\n\n//Create labels list to decode predictions\nscala> val resultLabels = label_idx.fit(source_ds).labels\nresultLabels: Array[String] = Array(Fail, Pass)\nscala> val va = new VectorAssembler().setInputCols(Array(\"Mark_bins\",\"Att_idx\")).\n                  setOutputCol(\"features\")\nva: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_5dc2dbbef48c\nscala> val dt = new DecisionTreeClassifier().setLabelCol(\"Label\").\n         setFeaturesCol(\"features\")\ndt: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_e8343ae1a9eb\nscala> val lc = new IndexToString().setInputCol(\"prediction\").\n             setOutputCol(\"predictedLabel\").setLabels(resultLabels)\nlc: org.apache.spark.ml.feature.IndexToString = idxToStr_90b6693d4313\nscala>\n\n//Define pipeline\nscala>val dt_pipeline = new\nPipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,dt,lc))\ndt_pipeline: org.apache.spark.ml.Pipeline = pipeline_95876bb6c969\nscala> val dtModel = dt_pipeline.fit(source_ds)\ndtModel: org.apache.spark.ml.PipelineModel = pipeline_95876bb6c969\nscala> val resultDF = dtModel.transform(source_ds)\nresultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ...\n10 more fields]\nscala> resultDF.filter(\"Label != prediction\").select(\"StudentId\",\"Label\",\"prediction\",\"Result\",\"predictedLabel\").show()\n+---------+-----+----------+------+--------------+\n|StudentId|Label|prediction|Result|predictedLabel|\n+---------+-----+----------+------+--------------+\\\n|     1009|  1.0|       0.0|  Pass|          Fail|\n|     1020|  1.0|       0.0|  Pass|          Fail|\n+---------+-----+----------+------+--------------+\n\n//Note that the difference is in the student ids that were granted pass\n\n//Same example using Gradient boosted tree classifier, reusing the pipeline stages\nscala> import org.apache.spark.ml.classification.GBTClassifier\nimport org.apache.spark.ml.classification.GBTClassifier\nscala> val gbt = new GBTClassifier().setLabelCol(\"Label\").\n              setFeaturesCol(\"features\").setMaxIter(10)\ngbt: org.apache.spark.ml.classification.GBTClassifier = gbtc_cb55ae2174a1\nscala> val gbt_pipeline = new\nPipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,gbt,lc))\ngbt_pipeline: org.apache.spark.ml.Pipeline = pipeline_dfd42cd89403\nscala> val gbtResultDF = gbt_pipeline.fit(source_ds).transform(source_ds)\ngbtResultDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]\nscala> gbtResultDF.filter(\"Label !=\nprediction\").select(\"StudentId\",\"Label\",\"Result\",\"prediction\",\"predictedLabel\").show()\n+---------+-----+------+----------+--------------+\n|StudentId|Label|Result|prediction|predictedLabel|\n+---------+-----+------+----------+--------------+\n|     1009|  1.0|  Pass|       0.0|          Fail|\n|     1020|  1.0|  Pass|       0.0|          Fail|\n+---------+-----+------+----------+--------------+\n```", "```py\n>>> from pyspark.ml.pipeline import Pipeline\n>>> from pyspark.ml.feature import Bucketizer, StringIndexer, VectorAssembler, IndexToString\n>>> from pyspark.ml.classification import DecisionTreeClassifier,\nDecisionTreeClassificationModel\n>>> \n\n//Get source file\n>>> file_path = \"../work/StudentsPassFail.csv\"\n>>> source_df = spark.read.csv(file_path,header=True,inferSchema=True)\n>>> \n\n//Examine source data\n>>> source_df.show(4)\n+---------+---------+----------+------+\n|StudentId|Avg_Marks|Attendance|Result|\n+---------+---------+----------+------+\n|     1001|     48.0|      Full|  Pass|\n|     1002|     21.0|    Enough|  Fail|\n|     1003|     24.0|    Enough|  Fail|\n|     1004|      4.0|      Poor|  Fail|\n+---------+---------+----------+------+\n\n//Define preparation pipeline\n>>> marks_bkt = Bucketizer(inputCol=\"Avg_Marks\",\n        outputCol=\"Mark_bins\", splits=[0,40.0,60.0,100.0])\n>>> att_idx = StringIndexer(inputCol = \"Attendance\",\n        outputCol=\"Att_idx\")\n>>> label_idx = StringIndexer(inputCol=\"Result\",\n                   outputCol=\"Label\")\n>>> \n\n//Create labels list to decode predictions\n>>> resultLabels = label_idx.fit(source_df).labels\n>>> resultLabels\n[u'Fail', u'Pass']\n>>> \n>>> va = VectorAssembler(inputCols=[\"Mark_bins\",\"Att_idx\"],\n                         outputCol=\"features\")\n>>> dt = DecisionTreeClassifier(labelCol=\"Label\", featuresCol=\"features\")\n>>> lc = IndexToString(inputCol=\"prediction\",outputCol=\"predictedLabel\",\n             labels=resultLabels)\n>>> dt_pipeline = Pipeline(stages=[marks_bkt, att_idx, label_idx,va,dt,lc])\n>>> dtModel = dt_pipeline.fit(source_df)\n>>> resultDF = dtModel.transform(source_df)\n>>>\n\n//Look for obervatiuons where prediction did not match\n>>> resultDF.filter(\"Label != prediction\").select(\n         \"StudentId\",\"Label\",\"prediction\",\"Result\",\"predictedLabel\").show()\n+---------+-----+----------+------+--------------+\n|StudentId|Label|prediction|Result|predictedLabel|\n+---------+-----+----------+------+--------------+\n|     1009|  1.0|       0.0|  Pass|          Fail|\n|     1020|  1.0|       0.0|  Pass|          Fail|\n+---------+-----+----------+------+--------------+\n\n//Note that the difference is in the student ids that were granted pass\n>>> \n//Same example using Gradient boosted tree classifier, reusing the pipeline\nstages\n>>> from pyspark.ml.classification import GBTClassifier\n>>> gbt = GBTClassifier(labelCol=\"Label\", featuresCol=\"features\",maxIter=10)\n>>> gbt_pipeline = Pipeline(stages=[marks_bkt,att_idx,label_idx,va,gbt,lc])\n>>> gbtResultDF = gbt_pipeline.fit(source_df).transform(source_df)\n>>> gbtResultDF.columns\n['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx',\n'Label', 'features', 'prediction', 'predictedLabel']\n>>> gbtResultDF.filter(\"Label !=\nprediction\").select(\"StudentId\",\"Label\",\"Result\",\"prediction\",\"predictedLabel\").show()\n+---------+-----+------+----------+--------------+\n|StudentId|Label|Result|prediction|predictedLabel|\n+---------+-----+------+----------+--------------+\n|     1009|  1.0|  Pass|       0.0|          Fail|\n|     1020|  1.0|  Pass|       0.0|          Fail|\n+---------+-----+------+----------+--------------+\n```", "```py\nscala> import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nscala> import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nscala> import org.apache.spark.mllib.util.MLUtils\nimport org.apache.spark.mllib.util.MLUtils\n\n// Load training data\nscala> val data = MLUtils.loadLibSVMFile(sc,\n\"data/mllib/sample_multiclass_classification_data.txt\").toDF()\ndata: org.apache.spark.sql.DataFrame = [label: double, features: vector]\n\n//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions\nscala> val data2 = MLUtils.convertVectorColumnsToML(data)\ndata2: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\n// Split the data into train and test\nscala> val splits = data2.randomSplit(Array(0.6, 0.4), seed = 1234L)\nsplits: Array[org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]] = Array([label: double, features: vector], [label: double, features: vector])\nscala> val train = splits(0)\ntrain: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\nscala> val test = splits(1)\ntest: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [label: double, features: vector]\n\n// specify layers for the neural network:\n// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\nscala> val layers = Array[Int](4, 5, 4, 3)\nlayers: Array[Int] = Array(4, 5, 4, 3)\n\n// create the trainer and set its parameters\nscala> val trainer = new MultilayerPerceptronClassifier().\n           setLayers(layers).setBlockSize(128).\n           setSeed(1234L).setMaxIter(100)\ntrainer: org.apache.spark.ml.classification.MultilayerPerceptronClassifier = mlpc_edfa49fbae3c\n\n// train the model\nscala> val model = trainer.fit(train)\nmodel: org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel = mlpc_edfa49fbae3c\n\n// compute accuracy on the test set\nscala> val result = model.transform(test)\nresult: org.apache.spark.sql.DataFrame = [label: double, features: vector ... 1 more field]\nscala> val predictionAndLabels = result.select(\"prediction\", \"label\")\npredictionAndLabels: org.apache.spark.sql.DataFrame = [prediction: double, label: double]\nscala> val evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_a4f43d85f261\nscala> println(\"Accuracy:\" + evaluator.evaluate(predictionAndLabels))\nAccuracy:0.9444444444444444\n\nPython: >>> from pyspark.ml.classification import MultilayerPerceptronClassifier\n>>> from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n>>> from pyspark.mllib.util import MLUtils\n>>>\n\n  //Load training data\n>>> data = spark.read.format(\"libsvm\").load(      \"data/mllib/sample_multiclass_classification_data.txt\")\n\n//Convert mllib vectors to ml Vectors for spark 2.0+. Retain data for previous versions\n>>> data2 = MLUtils.convertVectorColumnsToML(data)\n>>>\n\n // Split the data into train and test\n>>> splits = data2.randomSplit([0.6, 0.4], seed = 1234L)\n>>> train, test = splits[0], splits[1]\n>>>\n\n // specify layers for the neural network:\n // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n>>> layers = [4,5,4,3] \n\n// create the trainer and set its parameters\n>>> trainer = MultilayerPerceptronClassifier(layers=layers, blockSize=128,\n                 seed=1234L, maxIter=100)\n// train the model\n>>> model = trainer.fit(train)\n>>>\n\n// compute accuracy on the test set\n>>> result = model.transform(test)\n>>> predictionAndLabels = result.select(\"prediction\", \"label\")\n>>> evaluator = MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\n>>> print \"Accuracy:\",evaluator.evaluate(predictionAndLabels)\nAccuracy: 0.901960784314\n>>> \n```", "```py\nscala> import org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nimport org.apache.spark.ml.clustering.{KMeans, KMeansModel}\nscala> import org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.linalg.Vectors\nscala>\n\n//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES\nscala> val km = new KMeans()\nkm: org.apache.spark.ml.clustering.KMeans = kmeans_b34da02bd7c8\nscala> val kmeans_pipeline = new\nPipeline().setStages(Array(marks_bkt,att_idx,label_idx,va,km,lc))\nkmeans_pipeline: org.apache.spark.ml.Pipeline = pipeline_0cd64aa93a88\n\n//Train and transform\nscala> val kmeansDF = kmeans_pipeline.fit(source_ds).transform(source_ds)\nkmeansDF: org.apache.spark.sql.DataFrame = [StudentId: int, Avg_Marks: double ... 8 more fields]\n\n//Examine results\nscala> kmeansDF.filter(\"Label != prediction\").count()\nres17: Long = 13\n\n```", "```py\n>>> from pyspark.ml.clustering import KMeans, KMeansModel\n>>> from pyspark.ml.linalg import Vectors\n>>> \n\n//Define pipeline for kmeans. Reuse the previous stages in ENSEMBLES\n>>> km = KMeans()\n>>> kmeans_pipeline = Pipeline(stages = [marks_bkt, att_idx, label_idx,va,km,lc])\n\n//Train and transform\n>>> kmeansDF = kmeans_pipeline.fit(source_df).transform(source_df)\n>>> kmeansDF.columns\n['StudentId', 'Avg_Marks', 'Attendance', 'Result', 'Mark_bins', 'Att_idx', 'Label', 'features', 'prediction', 'predictedLabel']\n>>> kmeansDF.filter(\"Label != prediction\").count()\n4\n```"]