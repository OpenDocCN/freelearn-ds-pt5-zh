<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch06"/>Chapter 6. Analytics Study: AI and Image Recognition with TensorFlow</h1></div></div></div><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote" summary="Block quote"><tr><td valign="top"> </td><td valign="top"><p><em>"Artificial Intelligence, deep learning, machine learning — whatever you're doing if you don't understand it — learn it. Because otherwise, you're going to be a dinosaur within 3 years."</em></p></td><td valign="top"> </td></tr><tr><td valign="top"> </td><td colspan="2" align="right" valign="top" style="text-align: center">--<em>Mark Cuban</em></td></tr></table></div><p>This is the first chapter of a series of sample applications covering popular industry use cases, and it is no coincidence that I start with a use case related to machine learning and, more specifically, deep learning through a image recognition sample application. We're seeing <a id="id336" class="indexterm"/>accelerated growth in the field of <strong>Artificial Intelligence</strong> (<strong>AI</strong>) over the last few years, to the point where many practical applications are becoming a reality, such as self-driving cars, and chatbots with advanced automated speech recognition that, for some tasks, are perfectly able to replace human operators, while more and more people, from academia to industry, are starting to get involved. However, there is a perception that the cost of entry is very high and that mastering the underlying mathematical concepts of machine learning is a prerequisite. In this chapter, we try to demonstrate, through the use of examples, that this is not the case.</p><p>We will start this chapter with a quick introduction to machine learning, and a subset of it called deep learning. We will then introduce a very popular deep learning framework called TensorFlow that we'll use to build an image recognition model. In the second part of this chapter, we'll show how to operationalize the model we've built by implementing a sample PixieApp that lets the user enter a link to a website, have all the images scraped, and use as input to the model to categorize them.</p><p>At the end of this chapter, you should be convinced that it is possible to build meaningful applications and operationalize them without a Ph.D. in machine learning.</p><div><div><div><div><h1 class="title"><a id="ch06lvl1sec43"/>What is machine learning?</h1></div></div></div><p>One definition <a id="id337" class="indexterm"/>that I think captures very well the intuition behind machine learning comes from Andrew Ng, adjunct professor at Stanford University, in his <em>Machine Learning</em> class on Coursera (<a class="ulink" href="https://www.coursera.org/learn/machine-learning">https://www.coursera.org/learn/machine-learning</a>):</p><div><blockquote class="blockquote"><p>Machine learning is the science of getting computers to learn, without being explicitly programmed.</p></blockquote></div><p>The key word from the preceding definition is <em>learn,</em> which, in this context, has a meaning that is very similar to how, we, humans learn. To continue with this parallel, from a young age, we were taught how to accomplish a task either by example, or on our own by trial and error. Broadly speaking, machine learning algorithms can be categorized into two types that correspond to the two ways in which humans learn:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><strong>Supervised</strong>: The algorithm <a id="id338" class="indexterm"/>learns from example data that has been properly labeled. This data is also called training data, or sometimes referred to as <em>ground truth</em>.</li><li class="listitem" style="list-style-type: disc"><strong>Unsupervised</strong>: The algorithm <a id="id339" class="indexterm"/>is able to learn on its own from data that has not been labeled.</li></ul></div><p>For each of the two categories described here, the following table gives a high-level overview of the most commonly used machine learning algorithms and the type of problem they solve:</p><div><img src="img/B09699_06_01.jpg" alt="What is machine learning?" width="986" height="474"/><div><p>List of machine learning algorithms</p></div></div><p>The output of these algorithms is called a <strong>model</strong> and is used to make predictions on new input data that has not been seen before. The overall end-to-end process for building and deploying <a id="id340" class="indexterm"/>these models is very consistent across the different types of algorithms.</p><p>The following diagram shows a high-level workflow of this process:</p><div><img src="img/B09699_06_02.jpg" alt="What is machine learning?" width="1000" height="511"/><div><p>Machine learning model workflow</p></div></div><p>As always, the workflow starts with data. In the case of supervised learning, the data will be used as <a id="id341" class="indexterm"/>an example and therefore must be correctly labeled with the correct answers. The input data is then processed to extract intrinsic properties called <strong>features,</strong> which we <a id="id342" class="indexterm"/>can think of as numerical values representing the input data. Subsequently, these features are fed into a machine learning algorithm that builds a model. In typical settings, the original data is split between training, test, and blind data. The test and blind data are used during the model building phase to validate and optimize the model to make sure that it doesn't overfit the training data. Overfitting happens when the model parameters are such that they follow too closely the training data, leading to errors when unseen data is used. When the model produces the desired accuracy level, it is then deployed in production and used against new data as needed by the host application.</p><p>In this section, we will provide a very high-level introduction to machine learning with a simplified data pipeline workflow, just enough to give the intuition of how a model is built and deployed. Once again, if you are a beginner, I highly recommend Andrew Ng's <em>Machine Learning</em> class on Coursera (which I still revisit from time to time). In the next section, we will introduce a branch of machine learning called deep learning, which we'll use to build the image recognition sample application.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec44"/>What is deep learning?</h1></div></div></div><p>Getting <a id="id343" class="indexterm"/>computers to learn, reason, and think (make decisions) is a science that is commonly called <strong>cognitive computing,</strong> of which machine learning and deep learning are a big part. The following Venn diagram shows how these fields are related to <a id="id344" class="indexterm"/>the overarching field of AI:</p><div><img src="img/B09699_06_03.jpg" alt="What is deep learning?" width="770" height="431"/><div><p>How deep learning fits in to AI</p></div></div><p>As the diagram suggests, deep learning is one type of machine learning algorithm. What is perhaps not widely known is that the field of deep learning has existed for quite some time, but hasn't really been widely used until very recently. The rekindling in interest is due to the extraordinary advances in computer, cloud, and storage technologies observed in the last few years that have fuelled exponential growth in AI with the development of many new deep learning algorithms, each best suited to solve a particular problem.</p><p>As we'll discuss <a id="id345" class="indexterm"/>later in this chapter, deep learning algorithms are especially good at learning complex non-linear hypotheses. Their design is actually inspired by how the human brain works, for example, the input data flows through multiple <a id="id346" class="indexterm"/>layers of computation units in order to decompose complex model representations (such as an image, for example) into simpler ones, before passing the results to the next layer, and so on and so forth, until reaching the final layer that is responsible for outputting the results. The assembly of these layers is also referred to as <strong>neural networks</strong>, and the computation units that compose a layer are called <strong>neurons</strong>. In essence, a neuron is responsible for taking multiple inputs and transforming them into a single output that can then be fed into other neurons in the next layers.</p><p>The following diagram represents a multilayer neural network for image classification:</p><div><img src="img/B09699_06_04.jpg" alt="What is deep learning?" width="1000" height="551"/><div><p>High-level representation of a neural network for image classification</p></div></div><p>The preceding <a id="id347" class="indexterm"/>neural network is also called <strong>feed-forward</strong> because the output of each computation unit is used as input to the next layer, starting with the input layer. The intermediary layers are called the <strong>hidden layers</strong> and contain intermediary <a id="id348" class="indexterm"/>features that are automatically learned by the network. In our image example, certain neurons could be responsible for detecting corners, while <a id="id349" class="indexterm"/>certain others might focus on edges, and so on. The final output layer is responsible for assigning a confidence level (score) to each of the output classes.</p><p>One important question is how does the neuron output get generated from its input? Without diving too deeply in to the mathematics involved, each artificial neuron applies an activation function <img src="img/B09699_06_26.jpg" alt="What is deep learning?" width="47" height="34"/> on the weighted sum of its inputs to decide whether it should <em>fire</em> or not.</p><p>The following formula calculates the weighted sum:</p><div><img src="img/B09699_06_27.jpg" alt="What is deep learning?" width="204" height="37"/></div><p>Where <img src="img/B09699_06_28.jpg" alt="What is deep learning?" width="22" height="27"/> is the matrix of weights between the layer <em>i</em> and <em>i + 1</em>. These weights are computed during the training phase that we will discuss briefly a little later.</p><div><div><h3 class="title"><a id="note0100"/>Note</h3><p><strong>Note</strong>: The bias in the preceding formula represents the weight of the bias neuron, which is an extra neuron added to each layer with an x value of +1. The bias neuron is special because it contributes to the input for the next layer, but it is not connected to the previous one. Its weight, however, is still normally learned like any other neuron. The intuition behind the bias neuron is that it provides the constant term b in the linear regression equation: </p><div><img src="img/B09699_06_29.jpg" alt="What is deep learning?" width="93" height="24"/></div></div></div><p>Of course, applying <a id="id350" class="indexterm"/>the neuron activation function <img src="img/B09699_06_30.jpg" alt="What is deep learning?" width="47" height="34"/> on <em>A</em> cannot simply produce a binary (0 or 1) value, because we wouldn't be able to correctly rank the final candidate answers if multiple classes are given the score of 1. Instead, we use activation functions that provide a non-discrete score between 0 and 1 and set a threshold value (for example, 0.5) to decide whether to activate the neuron or not.</p><p>One of the most popular activation functions is the sigmoid function:</p><div><img src="img/B09699_06_31.jpg" alt="What is deep learning?" width="112" height="40"/></div><p>The following diagram shows how a neuron output is calculated from its input and its weight using a sigmoid activation function:</p><div><img src="img/B09699_06_05.jpg" alt="What is deep learning?" width="969" height="630"/><div><p>Neuron output calculation using the sigmoid function</p></div></div><p>Other popular activation functions include the hyperbolic tangent <img src="img/B09699_06_32.jpg" alt="What is deep learning?" width="70" height="34"/> and the <strong>Rectified Linear Unit</strong> (<strong>ReLu</strong>): <img src="img/B09699_06_33.jpg" alt="What is deep learning?" width="87" height="34"/>. ReLu works better when there are a lot of layers because <a id="id351" class="indexterm"/>it provides sparsity of <em>firing</em> neurons, thereby reducing noise and resulting in faster learning.</p><p>Feed-forward <a id="id352" class="indexterm"/>propagation is used during scoring of the model, but when it comes to training the weight matrix of the neural network, a popular method used is called <strong>backpropagation</strong> (<a class="ulink" href="https://en.wikipedia.org/wiki/Backpropagation">https://en.wikipedia.org/wiki/Backpropagation</a>).</p><p>The following <a id="id353" class="indexterm"/>high-level steps describe how the training works:</p><div><ol class="orderedlist arabic"><li class="listitem">
Randomly initialize the weight matrix (preferably using small values, for example, <img src="img/B09699_06_34.jpg" alt="What is deep learning?" width="58" height="24"/>.
</li><li class="listitem">Use the forward propagation described earlier on all the training examples to compute the outputs of each neuron using the activation function of your choice.</li><li class="listitem">Implement a cost function for your neural network. A <strong>cost function</strong> quantifies the error with respect to the training examples. There are multiple cost functions <a id="id354" class="indexterm"/>that can be used with the backpropagation algorithm, such as a mean-square error (<a class="ulink" href="https://en.wikipedia.org/wiki/Mean_squared_error">https://en.wikipedia.org/wiki/Mean_squared_error</a>) and cross-entropy (<a class="ulink" href="https://en.wikipedia.org/wiki/Cross_entropy">https://en.wikipedia.org/wiki/Cross_entropy</a>).</li><li class="listitem">Use backpropagation <a id="id355" class="indexterm"/>to minimize your cost function and compute the weight matrix. The idea behind backpropagation is to start with the activation values of the output layer, compute <a id="id356" class="indexterm"/>the error with respect to the training data, and pass their errors backward to the hidden layers. These errors are then adjusted to minimize the cost function implemented in step 3.</li></ol></div><div><div><h3 class="title"><a id="note123"/>Note</h3><p>
<strong>Note</strong>: Explaining in detail these cost functions and how they are being optimized is beyond the scope of this book. For a deeper dive, I highly recommend looking at the <em>Deep Learning</em> book from MIT press (Ian Goodfellow, Yoshua Bengio, and Aaron Courville)</p></div></div><p>In this section, we've discussed at a high level how neural networks work and how they are trained. Of course, we've only touched the surface of this exciting technology, but you hopefully should <a id="id357" class="indexterm"/>have an idea as to how they work. In the next section, we start looking at TensorFlow, which is a programming framework that helps abstract the underlying complexity of implementing a neural network.</p></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec45"/>Getting started with TensorFlow</h1></div></div></div><p>There are <a id="id358" class="indexterm"/>multiple open source deep learning frameworks besides TensorFlow (<a class="ulink" href="https://www.tensorflow.org">https://www.tensorflow.org</a>) that I could have chosen for this sample application.</p><p>Some of <a id="id359" class="indexterm"/>the most popular frameworks are as follows:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">PyTorch (<a class="ulink" href="http://pytorch.org">http://pytorch.org</a>)</li><li class="listitem" style="list-style-type: disc">Caffee2 (<a class="ulink" href="https://caffe2.ai">https://caffe2.ai</a>)</li><li class="listitem" style="list-style-type: disc">MXNet (<a class="ulink" href="https://mxnet.apache.org">https://mxnet.apache.org</a>)</li><li class="listitem" style="list-style-type: disc">Keras (<a class="ulink" href="https://keras.io">https://keras.io</a>): A high-level neural network abstraction API capable of running other deep learning frameworks such as TensorFlow, CNTK (<a class="ulink" href="https://github.com/Microsoft/cntk">https://github.com/Microsoft/cntk</a>), and Theano (<a class="ulink" href="https://github.com/Theano/Theano">https://github.com/Theano/Theano</a>)</li></ul></div><p>TensorFlow APIs are available in multiple languages: Python, C++, Java, Go, and, more recently, JavaScript. We can distinguish two categories of APIs: high level and low level, represented by this diagram:</p><div><img src="img/B09699_06_06.jpg" alt="Getting started with TensorFlow" width="1000" height="439"/><div><p>TensorFlow high-level API architecture</p></div></div><p>To get <a id="id360" class="indexterm"/>started with the TensorFlow API, let's build a simple neural network that will learn the XOR transformation.</p><p>As a reminder, the XOR operator has only four training examples:</p><div><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/><col style="text-align: left"/></colgroup><tbody><tr><td style="text-align: left" valign="top">
<p>
<strong>X</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>
<strong>Y</strong>
</p>
</td><td style="text-align: left" valign="top">
<p>
<strong>Result</strong>
</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>1</p>
</td><td style="text-align: left" valign="top">
<p>0</p>
</td></tr></tbody></table></div><p>It's interesting to note that linear classifiers (<a class="ulink" href="https://en.wikipedia.org/wiki/Linear_classifier">https://en.wikipedia.org/wiki/Linear_classifier</a>) are not able to learn the XOR transformation. However, we can solve this problem with a simple neural network with two <a id="id361" class="indexterm"/>neurons in the input layer, one hidden layer with two neurons, and an output layer with one neuron (binary classification), demonstrated as follows:</p><div><img src="img/B09699_06_07.jpg" alt="Getting started with TensorFlow" width="703" height="643"/><div><p>XOR neural network</p></div></div><div><div><h3 class="title"><a id="note124"/>Note</h3><p>
<strong>Note</strong>: You can install TensorFlow directly from the Notebook by using the following command:</p><div><pre class="programlisting">
<strong>!pip install tensorflow</strong>
</pre></div><p>As always, don't forget to restart the kernel after any successful install.</p></div></div><p>To create <a id="id362" class="indexterm"/>the input and output layer tensors, we use the <code class="literal">tf.placeholder</code> API, as shown in the following code:</p><div><pre class="programlisting">import tensorflow as tf
x_input = tf.placeholder(tf.float32)
y_output = tf.placeholder(tf.float32)</pre></div><p>Then, we use the <code class="literal">tf.Variable</code> API (<a class="ulink" href="https://www.tensorflow.org/programmers_guide/variables">https://www.tensorflow.org/programmers_guide/variables</a>) to initialize the random value for the matrices <img src="img/B09699_new_01.jpg" alt="Getting started with TensorFlow" width="25" height="29"/>, and <img src="img/B09699_new_02.jpg" alt="Getting started with TensorFlow" width="25" height="32"/> corresponding to the hidden layer and the output layer:</p><div><pre class="programlisting">eps = 0.01
W1 = tf.Variable(tf.random_uniform([2,2], -eps, eps))
W2 = tf.Variable(tf.random_uniform([2,1], -eps, eps))</pre></div><p>For the activation function, we use the sigmoid function:</p><div><div><h3 class="title"><a id="note126"/>Note</h3><p>
<strong>Note</strong>: For simplicity, we omit to introduce the bias.</p></div></div><div><pre class="programlisting">layer1 = tf.sigmoid(tf.matmul(x_input, W1))
output_layer = tf.sigmoid(tf.matmul(layer1, W2))</pre></div><p>For the cost function, we use the <strong>MSE</strong> (short for, <strong>mean square error</strong>):</p><div><pre class="programlisting">cost = tf.reduce_mean(tf.square(y_output - output_layer))</pre></div><p>With all the <a id="id363" class="indexterm"/>tensors in place in the graph, we can now proceed with the training by using the <code class="literal">tf.train.GradientDescentOptimizer</code> with a learning rate of <code class="literal">0.05</code> to minimize our cost function:</p><div><pre class="programlisting">train = tf.train.GradientDescentOptimizer(0.05).minimize(cost)
training_data = ([[0,0],[0,1],[1,0],[1,1]], [[0],[1],[1],[0]])
<strong>with tf.Session() as sess:</strong>
    sess.run(tf.global_variables_initializer())
    for i in range(5000):
        sess.run(train,
            feed_dict={x_input: training_data[0], y_output: training_data[1]})</pre></div><div><div><h3 class="title"><a id="note127"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode1.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode1.py</a>
</p></div></div><p>The preceding <a id="id364" class="indexterm"/>code introduces the concept of a TensorFlow <code class="literal">Session</code> for the first time, which is a foundational part of the framework. In essence, any TensorFlow operation must be executed within the context of <code class="literal">Session</code> by using its <code class="literal">run</code> method. Sessions also maintain resources that need to be explicitly released using the <code class="literal">close</code> method. For convenience, the <code class="literal">Session</code> class supports the context management protocol by providing an <code class="literal">__enter__</code> and <code class="literal">__exit__</code> method. This allows the caller to call TensorFlow operations using the <code class="literal">with</code> statement (<a class="ulink" href="https://docs.python.org/3/whatsnew/2.6.html#pep-343-the-with-statement">https://docs.python.org/3/whatsnew/2.6.html#pep-343-the-with-statement</a>) and have the resources automatically freed.</p><p>The following pseudo-code shows a typical structure of a TensorFlow execution:</p><div><pre class="programlisting">with tf.Session() as sess:
    with-block statement with TensorFlow operations</pre></div><p>In this section, we quickly explored the low-level TensorFlow APIs to build a simple neural network that learned the XOR transformation. In the next section, we'll explore the higher level estimator APIs that provide an abstraction layer on top of the low-level API.</p><div><div><div><div><h2 class="title"><a id="ch06lvl2sec28"/>Simple classification with DNNClassifier</h2></div></div></div><div><div><h3 class="title"><a id="note128"/>Note</h3><p>
<strong>Note</strong>: This section discusses the source code for a sample PixieApp. If you want to follow along, it might be easier to download the complete Notebook at this location:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/TensorFlow%20classification.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/TensorFlow%20classification.ipynb</a>
</p></div></div><p>Before we <a id="id365" class="indexterm"/>look at using Tensors, Graphs, and Sessions from the low-level TensorFlow APIs, it would be good to get familiar with the high-level API provided in the <code class="literal">Estimators</code> package. In this section, we build a simple PixieApp that takes a pandas DataFrame as input and trains a classification model with the categorical output.</p><div><div><h3 class="title"><a id="note129"/>Note</h3><p>
<strong>Note</strong>: There are essentially two types of classification output: categorical and continuous. In a categorical classifier model, the output can only be chosen from a list of finite predefined values with or without a logical order. We commonly call binary classification a classification model with only two classes. On the other hand, the continuous output can have any numerical values.</p></div></div><p>The user is first asked to choose a numerical column to predict on, and a classification model is trained on all the other numerical columns present in the DataFrame.</p><div><div><h3 class="title"><a id="note130"/>Note</h3><p>
<strong>Note</strong>: Some of the code of this sample app is adapted from <a class="ulink" href="https://github.com/tensorflow/models/tree/master/samples/core/get_started">https://github.com/tensorflow/models/tree/master/samples/core/get_started</a>.</p></div></div><p>For this example, we'll use built-in sample dataset #7: Boston Crime data, two-week sample, but you could use any other dataset as long it has sufficient data and numerical columns.</p><p>As a reminder, you can browse the PixieDust built-in datasets using the following code:</p><div><pre class="programlisting">import pixiedust
pixiedust.sampleData()</pre></div><div><img src="img/B09699_06_08.jpg" alt="Simple classification with DNNClassifier" width="1000" height="553"/><div><p>List of built-in datasets in PixieDust</p></div></div><p>The following <a id="id366" class="indexterm"/>code loads the <em>Boston Crime</em> dataset using the <code class="literal">sampleData()</code> API:</p><div><pre class="programlisting">import pixiedust
crimes = pixiedust.sampleData(7, forcePandas=True)</pre></div><p>As always, we first start by exploring the data using the <code class="literal">display()</code> command. The goal here is to look for a suitable column to predict on:</p><div><pre class="programlisting">display(crimes)</pre></div><div><img src="img/B09699_06_09.jpg" alt="Simple classification with DNNClassifier" width="890" height="634"/><div><p>Table view of the crime dataset</p></div></div><p>It looks <a id="id367" class="indexterm"/>like <code class="literal">nonviolent</code> is a good candidate for binary classification. Let's now bring up a bar chart to make sure we have a good data distribution in this column:</p><div><img src="img/B09699_06_10.jpg" alt="Simple classification with DNNClassifier" width="677" height="782"/><div><p>Select the nonviolent column in the option dialog</p></div></div><p>Clicking <strong>OK</strong> produces <a id="id368" class="indexterm"/>the following chart:</p><div><img src="img/B09699_06_11.jpg" alt="Simple classification with DNNClassifier" width="909" height="588"/><div><p>Distribution of nonviolent crimes</p></div></div><p>Unfortunately, the data is skewed toward nonviolent crimes, but we have close to 2,000 data points <a id="id369" class="indexterm"/>for violent crimes, which, for the purpose of this sample application, should be OK.</p><p>We are now ready to create the <code class="literal">do_training</code> method that will use a <code class="literal">tf.estimator.DNNClassifier</code> to create a classification model.</p><div><div><h3 class="title"><a id="note131"/>Note</h3><p>
<strong>Note</strong>: You can find more information on <code class="literal">DNNClassifier</code> and other high-level TensorFlow estimators here:</p><p>
<a class="ulink" href="https://www.tensorflow.org/api_docs/python/tf/estimator">https://www.tensorflow.org/api_docs/python/tf/estimator</a>
</p></div></div><p>The <code class="literal">DNNClassifier</code> constructor takes a lot of optional parameters. In our sample application, we'll only use three of them, but I encourage you to take a look at the other parameters in the documentation:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">feature_columns</code>: An iterable of <code class="literal">feature_column._FeatureColumn</code> model inputs. In our case, we can just create an array from the numerical columns of the pandas DataFrame using Python comprehension.</li><li class="listitem" style="list-style-type: disc"><code class="literal">hidden_units</code>: An iterable of a number of hidden layers per unit. Here, we'll use only two layers with 10 nodes each.</li><li class="listitem" style="list-style-type: disc"><code class="literal">n_classes</code>: The number of label classes. We'll infer this number by grouping the DataFrame on the predictor columns and count the rows.</li></ul></div><p>Here's the <a id="id370" class="indexterm"/>code for the <code class="literal">do_training</code> method:</p><div><pre class="programlisting">def do_training(train, train_labels, test, test_labels, num_classes):
    #set TensorFlow logging level to INFO
    tf.logging.set_verbosity(tf.logging.INFO)

    # Build 2 hidden layer DNN with 10, 10 units respectively.
    classifier = tf.estimator.DNNClassifier(
        # Compute feature_columns from dataframe keys using a list comprehension
        feature_columns =
            [tf.feature_column.numeric_column(key=key) for key in train.keys()],
        hidden_units=[10, 10],
        n_classes=num_classes)

    # Train the Model
    classifier.train(
        input_fn=lambda:train_input_fn(train, train_labels,100),
        steps=1000
    )

    # Evaluate the model
    eval_result = classifier.evaluate(
        input_fn=lambda:eval_input_fn(test, test_labels,100)
    )

    return (classifier, eval_result)</pre></div><div><div><h3 class="title"><a id="note132"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode2.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode2.py</a>
</p></div></div><p>The <code class="literal">classifier.train</code> method uses a <code class="literal">train_input_fn</code> method that is responsible for providing training input data (a.k.a ground truth) as minibatches, returning either a <code class="literal">tf.data.Dataset</code> or a tuple of <code class="literal">(features, labels)</code>. Our code is also performing a model evaluation using <code class="literal">classifier.evaluate</code> to validate the accuracy by scoring the model against the test dataset and comparing the results in the given label. The results are then returned as part of the function output. </p><p>This method requires an <code class="literal">eval_input_fn</code> method that is similar to the <code class="literal">train_input_fn</code>, with the exception that we do not make the dataset <a id="id371" class="indexterm"/>repeatable during evaluation. Since the two methods share most of the same code, we use a helper method called <code class="literal">input_fn</code> that is called by both methods with the appropriate flag:</p><div><pre class="programlisting">def input_fn(features, labels, batch_size, train):
    # Convert the inputs to a Dataset and shuffle.
    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels)).shuffle(1000)
    if train:
        #repeat only for training
<strong>        dataset = dataset.repeat()</strong>
    # Return the dataset in batch
    return <strong>dataset.batch(batch_size)</strong>

def train_input_fn(features, labels, batch_size):
    return input_fn(features, labels, batch_size, train=True)

def eval_input_fn(features, labels, batch_size):
    return input_fn(features, labels, batch_size, train=False)</pre></div><div><div><h3 class="title"><a id="note133"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode3.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode3.py</a>
</p></div></div><p>The next step is to build the PixieApp that will create the classifier from a pandas DataFrame passed as input to the <code class="literal">run</code> method. The main screen builds a list of all the numerical columns into a drop-down control and asks the user to select a column that will be used as the classifier output. This is done in the following code using a Jinja2 <code class="literal">{%for ...%}</code> loop iterating over the DataFrame passed as input that is referenced using the <code class="literal">pixieapp_entity</code> variable.</p><div><div><h3 class="title"><a id="note134"/>Note</h3><p>
<strong>Note</strong>: The following code uses the <code class="literal">[[SimpleClassificationDNN]]</code> notation to denote that it is incomplete code from the specified class. Do not try to run this code yet until the full implementation is provided.</p></div></div><div><pre class="programlisting">[[SimpleClassificationDNN]]
from pixiedust.display.app import *
@PixieApp
class SimpleClassificationDNN():
    @route()
    def main_screen(self):
        return """
&lt;h1 style="margin:40px"&gt;
    &lt;center&gt;The classificiation model will be trained on all the numeric columns of the dataset&lt;/center&gt;
&lt;/h1&gt;
&lt;style&gt;
    div.outer-wrapper {
        display: table;width:100%;height:300px;
    }
    div.inner-wrapper {
        display: table-cell;vertical-align: middle;height: 100%;width: 100%;
    }
&lt;/style&gt;
&lt;div class="outer-wrapper"&gt;
    &lt;div class="inner-wrapper"&gt;
        &lt;div class="col-sm-3"&gt;&lt;/div&gt;
        &lt;div class="input-group col-sm-6"&gt;
          &lt;select id="cols{{prefix}}" style="width:100%;height:30px" pd_options="<strong>predictor=$val(cols{{prefix}})</strong>"&gt;
              &lt;option value="0"&gt;Select a predictor column&lt;/option&gt;
              <strong>{%for col in this.pixieapp_entity.columns.values.tolist()%}</strong>
<strong>              &lt;option value="{{col}}"&gt;{{col}}&lt;/option&gt;</strong>
<strong>              {%endfor%}</strong>
          &lt;/select&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;     
        """</pre></div><div><div><h3 class="title"><a id="note135"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode4.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode4.py</a>
</p></div></div><p>Using the <code class="literal">crimes</code> dataset, we run <a id="id372" class="indexterm"/>the PixieApp with the following code:</p><div><pre class="programlisting">app = SimpleClassificationDNN()
app.run(crimes)</pre></div><div><div><h3 class="title"><a id="note136"/>Note</h3><p>
<strong>Note</strong>: The PixieApp code is incomplete at this time, but we can still see the results of the welcome page, as shown in the following screenshot:</p><div><img src="img/B09699_06_12.jpg" alt="Simple classification with DNNClassifier" width="955" height="478"/><div><p>The main screen showing the list of columns in the input pandas DataFrame</p></div></div></div></div><p>When the <a id="id373" class="indexterm"/>user selects the prediction column (for example, <code class="literal">nonviolent</code>), a new <code class="literal">prepare_training</code> route is triggered by the attribute: <code class="literal">pd_options="predictor=$val(cols{{prefix}})"</code>. This route will show two bar charts showing the output class distribution for both the training and test sets that are randomly selected using an 80/20 split from the original dataset.</p><div><div><h3 class="title"><a id="note137"/>Note</h3><p>
<strong>Note</strong>: We use an 80/20 split between training and test sets, which, from my experience, is quite common. Of course, this is not an absolute rule and could be adjusted depending on the use case</p></div></div><p>The screen fragment also includes a button to start training the classifier.</p><p>The code for the <code class="literal">prepare_training</code> route is shown here:</p><div><pre class="programlisting">[[SimpleClassificationDNN]]
@route(predictor="*")
@templateArgs
def prepare_training(self, predictor):
        #select only numerical columns
        self.dataset = self.pixieapp_entity.dropna(axis=1).select_dtypes(
            include=['int16', 'int32', 'int64', 'float16', 'float32', 'float64']
        )
        #Compute the number of classed by counting the groups
        self.num_classes = self.dataset.groupby(predictor).size().shape[0]
        #Create the train and test feature and labels
        self.train_x=self.dataset.sample(frac=0.8)
        self.full_train = self.train_x.copy()
        self.train_y = self.train_x.pop(predictor)
        self.test_x=self.dataset.drop(self.train_x.index)
        self.full_test = self.test_x.copy()
        self.test_y=self.test_x.pop(predictor)
        
        bar_chart_options = {
          "rowCount": "100",
          "keyFields": predictor,
          "handlerId": "barChart",
          "noChartCache": "true"
        }
        
        return """
&lt;div class="container" style="margin-top:20px"&gt;
    &lt;div class="row"&gt;
        &lt;div class="col-sm-5"&gt;
            &lt;h3&gt;&lt;center&gt;Train set class distribution&lt;/center&gt;&lt;/h3&gt;
            &lt;div pd_entity="full_train" pd_render_onload&gt;
                &lt;pd_options&gt;{{bar_chart_options|tojson}}&lt;/pd_options&gt;
            &lt;/div&gt;
        &lt;/div&gt;
        &lt;div class="col-sm-5"&gt;
            &lt;h3&gt;&lt;center&gt;Test set class distribution&lt;/center&gt;&lt;/h3&gt;
            &lt;div pd_entity="full_test" pd_render_onload&gt;
                &lt;pd_options&gt;{{bar_chart_options|tojson}}&lt;/pd_options&gt;
            &lt;/div&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;

<strong>&lt;div style="text-align:center"&gt;</strong>
<strong>    &lt;button class="btn btn-default" type="submit" pd_options="do_training=true"&gt;</strong>
<strong>        Start Training</strong>
<strong>    &lt;/button&gt;</strong>
<strong>&lt;/div&gt;</strong>
"""</pre></div><div><div><h3 class="title"><a id="note138"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode5.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode5.py</a>
</p><p>
<strong>Note</strong>: <code class="literal">@templateArgs</code> is used due to the fact that we compute the <code class="literal">bar_chart_options</code> variable once and then use it in the Jinja2 template.</p></div></div><p>Selecting the <code class="literal">nonviolent</code> prediction column gives us the following screenshot result:</p><div><img src="img/B09699_06_13.jpg" alt="Simple classification with DNNClassifier" width="1000" height="454"/><div><p>Pretraining screen</p></div></div><p>The <strong>Start Training</strong> button invokes the <code class="literal">do_training</code> route using the attribute <code class="literal">pd_options="do_training=true",</code> which invokes the <code class="literal">do_training</code> method we created earlier. Note that we use the <code class="literal">@captureOutput</code> decorator because, since we set the TensorFlow log level to <code class="literal">INFO</code>, we want to capture the log messages and display them to the user. These <a id="id374" class="indexterm"/>log messages are sent back to the browser using the <em>stream</em> mode, and PixieDust will automatically display them as a specially created <code class="literal">&lt;div&gt;</code> element that will append the data to it as it arrives. When the training is done, the route returns an HTML fragment that generates a table with the evaluation metrics returned by the <code class="literal">do_training</code> method, as shown in the following code:</p><div><pre class="programlisting">[[SimpleClassificationDNN]]
@route(do_training="*")
   @captureOutput
def do_training_screen(self):
<strong>       self.classifier, self.eval_results = \</strong>
<strong>      do_training(</strong>
<strong>self.train_x, self.train_y, self.test_x, self.test_y, self.num_classes</strong>
<strong>      )</strong>
        return """
&lt;h2&gt;Training completed successfully&lt;/h2&gt;
&lt;table&gt;
    &lt;thead&gt;
        &lt;th&gt;Metric&lt;/th&gt;
        &lt;th&gt;Value&lt;/th&gt;
    &lt;/thead&gt;
    &lt;tbody&gt;
{%for key,value in <strong>this.eval_results.items</strong>()%}
&lt;tr&gt;
    &lt;td&gt;{{key}}&lt;/td&gt;
    &lt;td&gt;{{value}}&lt;/td&gt;
&lt;/tr&gt;
{%endfor%}
    &lt;/tbody&gt;
&lt;/table&gt;
        """</pre></div><div><div><h3 class="title"><a id="note139"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode6.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode6.py</a>
</p></div></div><p>The following <a id="id375" class="indexterm"/>screenshot shows the results after the model has been successfully created and includes the evaluation metrics table for the classification model with an accuracy of 87%:</p><div><img src="img/B09699_06_14.jpg" alt="Simple classification with DNNClassifier" width="1000" height="652"/><div><p>Final screen showing the result of successful training</p></div></div><p>This PixieApp <a id="id376" class="indexterm"/>was run using the <code class="literal">crimes</code> dataset as an argument, as shown in the following code:</p><div><pre class="programlisting">app = SimpleClassificationDNN()
app.run(crimes)</pre></div><p>Once the model is successfully trained, you can access it to classify new data by calling the <code class="literal">predict</code> method on the <code class="literal">app.classifier</code> variable. Similar to the <code class="literal">train</code> and <code class="literal">evaluate</code> method, <code class="literal">predict</code> also takes an <code class="literal">input_fn</code> that constructs the input features.</p><div><div><h3 class="title"><a id="note140"/>Note</h3><p>
<strong>Note</strong>: More details on the <code class="literal">predict</code> method are provided here:</p><p>
<a class="ulink" href="https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#predict">https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier#predict</a>
</p></div></div><p>This sample application provides a good starting point for getting familiar with the TensorFlow framework by using the high-level estimator APIs.</p><div><div><h3 class="title"><a id="note141"/>Note</h3><p>
<strong>Note</strong>: The complete Notebook for this sample application can be found here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/TensorFlow%20classification.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/TensorFlow%20classification.ipynb</a>
</p></div></div><p>In the next section, we'll start building our image recognition sample application using the low-level TensorFlow APIs, including Tensors, Graphs, and Sessions.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec46"/>Image recognition sample application</h1></div></div></div><p>When it comes <a id="id377" class="indexterm"/>to building an open-ended application, you want to start by defining the requirements for an <strong>MVP</strong> (short for, <strong>Minimum Viable Product</strong>) version that contains just enough functionalities to make it usable and <a id="id378" class="indexterm"/>valuable to your users. When it comes to making technical decisions for your implementation, making sure that you get a working end-to-end implementation as quickly as possible, without investing too much time, is a very important criteria. The idea is that you want to start small so that you can quickly iterate and improve your application.</p><p>For the MVP of our image recognition sample application, we'll use the following requirements:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Don't build a <a id="id379" class="indexterm"/>model from scratch; instead, reuse one of the pretrained generic <strong>convolutional neural network</strong> (<strong>CNN</strong>: <a class="ulink" href="https://en.wikipedia.org/wiki/Convolutional_neural_network">https://en.wikipedia.org/wiki/Convolutional_neural_network</a>) models that are publicly available, such as MobileNet. We can always retrain these models later with custom training images using transfer learning (<a class="ulink" href="https://en.wikipedia.org/wiki/Transfer_learning">https://en.wikipedia.org/wiki/Transfer_learning</a>).</li><li class="listitem" style="list-style-type: disc">For MVP, while we are focusing on scoring only and not training, we should still make it interesting for the users. So let's build a PixieApp that allows the user to input the URL of a web page and display all the images scraped from the page, including the classification output inferred by our model.</li><li class="listitem" style="list-style-type: disc">Since we are learning about deep learning neural networks and TensorFlow, it would be great if we could display the TensorBoard Graph Visualization (<a class="ulink" href="https://www.tensorflow.org/programmers_guide/graph_viz">https://www.tensorflow.org/programmers_guide/graph_viz</a>) in the Jupyter Notebook directly without forcing the user to use another tool. This will provide a better user experience and increase their engagement with the application.</li></ul></div><div><div><h3 class="title"><a id="note142"/>Note</h3><p>
<strong>Note</strong>: The implementation of the application in this section is adapted from the tutorial:</p><p>
<a class="ulink" href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets">https://codelabs.developers.google.com/codelabs/tensorflow-for-poets</a>
</p></div></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec29"/>Part 1 – Load the pretrained MobileNet model</h2></div></div></div><div><div><h3 class="title"><a id="note143"/>Note</h3><p>
<strong>Note</strong>: You can download the completed Notebook to follow this section discussion here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%201.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%201.ipynb</a>
</p></div></div><p>There are <a id="id380" class="indexterm"/>plenty of publicly available image classification models, using CNNs, that are pretrained on large image databases such as ImageNet (<a class="ulink" href="http://www.image-net.org">http://www.image-net.org</a>). ImageNet has started multiple public challenges, such as the <strong>ImageNet Large Scale Visual Recognition Challenge</strong> (<strong>ILSVRC</strong>) or the <em>ImageNet Object Localization Challenge</em> on Kaggle (<a class="ulink" href="https://www.kaggle.com/c/imagenet-object-localization-challenge">https://www.kaggle.com/c/imagenet-object-localization-challenge</a>), with very interesting results.</p><p>These challenges <a id="id381" class="indexterm"/>have produced multiple models, such as ResNet, Inception, SqueezeNet, VGGNet, or Xception, each using a different neural network architecture. Going over each of these architectures is beyond the scope of this book, but even if you are not yet an expert in machine learning (which I am definitely not), I encourage you to read about them online. The model I've selected for this sample application is MobileNet because it is small, fast, and very accurate. It provides an image classification model for 1,000 categories of images, which is sufficient for this sample application.</p><p>To ensure the stability of the code, I've made a copy of the model in the GitHub repo: <a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/tree/master/chapter%206/Visual%20Recognition/mobilenet_v1_0.50_224">https://github.com/DTAIEB/Thoughtful-Data-Science/tree/master/chapter%206/Visual%20Recognition/mobilenet_v1_0.50_224</a>.</p><p>In this directory, you can find the following files:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">frozen_graph.pb</code>: A serialized binary version of the TensorFlow graph</li><li class="listitem" style="list-style-type: disc"><code class="literal">labels.txt</code>: A text file that includes a description of the 1,000 image categories and their index</li><li class="listitem" style="list-style-type: disc"><code class="literal">quantized_graph.pb</code>: A compressed form of the model graph that used an 8-bit fixed point representation</li></ul></div><p>Loading the model consists of building a <code class="literal">tf.graph</code> object and associated labels. Since we may want to load multiple models in the future, we first define a dictionary that provides metadata about the model:</p><div><pre class="programlisting">models = {
    "mobilenet": {
        "base_url":"https://github.com/DTAIEB/Thoughtful-Data-Science/raw/master/chapter%206/Visual%20Recognition/mobilenet_v1_0.50_224",
        "model_file_url": "frozen_graph.pb",
        "label_file": "labels.txt",
        "output_layer": "MobilenetV1/Predictions/Softmax"
    }
}</pre></div><div><div><h3 class="title"><a id="note144"/>Note</h3><p>You can find the file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode7.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode7.py</a>
</p></div></div><p>Each key in <a id="id382" class="indexterm"/>the preceding <code class="literal">models</code> dictionary represents the metadata of a particular model:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">base_url</code>: Points to the URL where the files are stored</li><li class="listitem" style="list-style-type: disc"><code class="literal">model_file_url</code>: The name of the model file that is assumed to be relative to <code class="literal">base_url</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">label_file</code>: The name of the labels that are assumed to be relative to <code class="literal">base_url</code></li><li class="listitem" style="list-style-type: disc"><code class="literal">output_layer</code>: The name of the output layer that provides final scoring for each category</li></ul></div><p>We implement a <code class="literal">get_model_attribute</code> helper method to facilitate reading from the <code class="literal">model</code> metadata, which will be very useful throughout our application:</p><div><pre class="programlisting"># helper method for reading attributes from the model metadata
def get_model_attribute(model, key, default_value = None):
    if key not in model:
        if default_value is None:
            raise Exception("Require model attribute {} not found".format(key))
        return default_value
    return model[key]</pre></div><div><div><h3 class="title"><a id="note145"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode8.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode8.py</a>
</p></div></div><p>To load the graph, we download the binary file, load it into a <code class="literal">tf.GraphDef</code> object using the <code class="literal">ParseFromString</code> method, and we then invoke the <code class="literal">tf.import_graph_def</code> method using the graph as the current content manager:</p><div><pre class="programlisting">import tensorflow as tf
import requests
# Helper method for resolving url relative to the selected model
def get_url(model, path):
    return model["base_url"] + "/" + path
    
# Download the serialized model and create a TensorFlow graph
def load_graph(model):
    graph = tf.Graph()
    graph_def = tf.GraphDef()
    graph_def.ParseFromString(
        requests.get( get_url( model, model["model_file_url"] ) ).content
    )
    with graph.as_default():
        tf.import_graph_def(graph_def)
    return graph</pre></div><div><div><h3 class="title"><a id="note146"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode9.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode9.py</a>
</p></div></div><p>The method <a id="id383" class="indexterm"/>that loads the labels returns either a JSON object or an array (we'll see later that both are needed). The following code uses a Python list comprehension to iterate over the lines returned by the <code class="literal">requests.get</code> call. It then uses the <code class="literal">as_json</code> flag to format the data as appropriate:</p><div><pre class="programlisting"># Load the labels
def load_labels(model, as_json = False):
    labels = [line.rstrip() \
      for line in requests.get(get_url(model, model["label_file"]) ).text.split("\n") if line != ""]
    if as_json:
        return [{"index": item.split(":")[0],"label":item.split(":")[1]} for item in labels]
    return labels</pre></div><div><div><h3 class="title"><a id="note147"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode10.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode10.py</a>
</p></div></div><p>The next step is to invoke the model to classify images. To make it simpler and perhaps more valuable, we ask the user to provide a URL to an HTML page that contains the images to be classified. We'll use the BeautifulSoup4 library to help parsing the page. To install BeautifulSoup4, simply run the following command:</p><div><pre class="programlisting">
<strong>!pip install beautifulsoup4</strong>
</pre></div><div><div><h3 class="title"><a id="note148"/>Note</h3><p>
<strong>Note</strong>: As always, don't forget to restart the kernel once installation is complete.</p></div></div><p>The following <code class="literal">get_image_urls</code> method takes a URL as an input, downloads the HTML, instantiates a BeautifulSoup parser and extracts all the images found in any <code class="literal">&lt;img&gt;</code> elements and <code class="literal">background-image</code> styles. BeautifulSoup has a very elegant and easy-to-use API for parsing HTML. Here, we simply use the <code class="literal">find_all</code> method to find all <code class="literal">&lt;img&gt;</code> elements and the <code class="literal">select</code> method to select all elements with an inline style. The reader will be quick to notice that <a id="id384" class="indexterm"/>there are many other ways to create images using HTML that we are not discovering, such as, for example, images declared as CSS classes. As always, if you have the interest and time to improve it, I strongly welcome pull requests in the GitHub repo (see here for instructions on how to create a pull request: <a class="ulink" href="https://help.github.com/articles/creating-a-pull-request">https://help.github.com/articles/creating-a-pull-request</a>).</p><p>The code for <code class="literal">get_image_urls</code> looks like this:</p><div><pre class="programlisting">from bs4 import BeautifulSoup as BS
import re

# return an array of all the images scraped from an html page
def get_image_urls(url):
    # Instantiate a BeautifulSoup parser
    soup = BS(requests.get(url).text, "html.parser")
    
    # Local helper method for extracting url
    def extract_url(val):
        m = <strong>re.match(r"url\((.*)\)", val)</strong>
        val = m.group(1) if m is not None else val
        return "http:" + val if val.startswith("//") else val
    
    # List comprehension that look for &lt;img&gt; elements and backgroud-image styles
    return [extract_url(imgtag['src']) for imgtag in <strong>soup.find_all('img')</strong>] + [ \
        extract_url(val.strip()) for key,val in \
        [tuple(selector.split(":")) for elt in soup.select("[style]") \
            for selector in elt["style"].strip(" ;").split(";")] \
            if <strong>key.strip().lower()=='background-image'</strong> \
        ]</pre></div><div><div><h3 class="title"><a id="note149"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode11.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode11.py</a>
</p></div></div><p>For each of the images discovered, we'll also need a helper function to download the images that <a id="id385" class="indexterm"/>will be passed as input to the model for classification.</p><p>The following <code class="literal">download_image</code> method downloads the image into a temporary file:</p><div><pre class="programlisting">import tempfile
def download_image(url):
   response = requests.get(url, stream=True)
   if response.status_code == 200:
      with <strong>tempfile.NamedTemporaryFile(delete=False) as f:</strong>
<strong>         for chunk in response.iter_content(2048):</strong>
<strong>            f.write(chunk)</strong>
         return f.name
   else:
      raise Exception("Unable to download image: {}".format(response.status_code))</pre></div><div><div><h3 class="title"><a id="note150"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode12.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode12.py</a>
</p></div></div><p>Given a local path to an image, we now need to decode it into a tensor by calling the right decode method from the <code class="literal">tf.image</code> package, that is, the <code class="literal">decode_png</code> for <code class="literal">.png</code> files.</p><div><div><h3 class="title"><a id="note151"/>Note</h3><p>
<strong>Note</strong>: In mathematics, a tensor is a generalization of a vector, which is defined by a direction and a size, to support higher dimensionality. Vectors are tensors of order 1, similarly, scalars are tensors of order 0. Intuitively, we can think of order 2 tensors as a two-dimensional array with values defined as a result of multiplying two vectors. In TensorFlow, tensors are arrays of n-dimensions.</p></div></div><p>After a few transformations on the image reader tensor (casting to the right decimal representation, resizing, and normalization), we call <code class="literal">tf.Session.run</code> on the normalizer tensor to <a id="id386" class="indexterm"/>execute the steps defined earlier, as shown in the following code:</p><div><pre class="programlisting"># decode a given image into a tensor
def read_tensor_from_image_file(model, file_name):
    file_reader = tf.read_file(file_name, "file_reader")
    if file_name.endswith(".png"):
        image_reader = tf.image.decode_png(file_reader, channels = 3,name='png_reader')
    elif file_name.endswith(".gif"):
        image_reader = tf.squeeze(tf.image.decode_gif(file_reader,name='gif_reader'))
    elif file_name.endswith(".bmp"):
        image_reader = tf.image.decode_bmp(file_reader, name='bmp_reader')
    else:
        image_reader = tf.image.decode_jpeg(file_reader, channels = 3, name='jpeg_reader')
    float_caster = tf.cast(image_reader, tf.float32)
    dims_expander = tf.expand_dims(float_caster, 0);
    
    # Read some info from the model metadata, providing default values
    input_height = get_model_attribute(model, "input_height", 224)
    input_width = get_model_attribute(model, "input_width", 224)
    input_mean = get_model_attribute(model, "input_mean", 0)
    input_std = get_model_attribute(model, "input_std", 255)
        
    resized = tf.image.resize_bilinear(dims_expander, [input_height, input_width])
    normalized = tf.divide(tf.subtract(resized, [input_mean]), [input_std])
    sess = tf.Session()
    result = sess.run(normalized)
    return result</pre></div><div><div><h3 class="title"><a id="note152"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode13.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode13.py</a>
</p></div></div><p>With all the pieces in place, we are now ready to implement the <code class="literal">score_image</code> method that takes a <code class="literal">tf.graph</code>, a model metadata, and a URL to an image as input parameters, and returns the top five candidate classifications based on their confidence score, including their labels:</p><div><pre class="programlisting">import numpy as np

# classify an image given its url
def score_image(graph, model, url):
    # Get the input and output layer from the model
    input_layer = get_model_attribute(model, "input_layer", "input")
    output_layer = get_model_attribute(model, "output_layer")
    
    # Download the image and build a tensor from its data
    <strong>t = read_tensor_from_image_file(model, download_image(url))</strong>
    
    # Retrieve the tensors corresponding to the input and output layers
    input_tensor = graph.get_tensor_by_name("import/" + input_layer + ":0");
    output_tensor = graph.get_tensor_by_name("import/" + output_layer + ":0");

    with tf.Session(graph=graph) as sess:
        results = sess.run(output_tensor, {input_tensor: t})
    results = np.squeeze(results)
    # select the top 5 candidate and match them to the labels
    <strong>top_k = results.argsort()[-5:][::-1]</strong>
<strong>    labels = load_labels(model)</strong>
<strong>    return [(labels[i].split(":")[1], results[i]) for i in top_k]</strong>
</pre></div><div><div><h3 class="title"><a id="note153"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode14.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode14.py</a>
</p></div></div><p>We can now <a id="id387" class="indexterm"/>test the code using the following steps:</p><div><ol class="orderedlist arabic"><li class="listitem">Pick the <code class="literal">mobilenet</code> model and load the corresponding graph</li><li class="listitem">Get a list of image URLs scraped from the Flickr website</li><li class="listitem">Call the <code class="literal">score_image</code> method for each image URL and print the result</li></ol></div><p>The code is shown here:</p><div><pre class="programlisting">model = models['mobilenet']
graph = load_graph(model)
image_urls = <strong>get_image_urls("https://www.flickr.com/search/?text=cats")</strong>
for url in image_urls:
    results = score_image(graph, model, url)
    print("Result for {}: \n\t{}".format(url, results))</pre></div><div><div><h3 class="title"><a id="note154"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode15.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode15.py</a>
</p></div></div><p>The results are <a id="id388" class="indexterm"/>pretty accurate (except for the first image that is a blank image) as shown in the following screenshot:</p><div><img src="img/B09699_06_15.jpg" alt="Part 1 – Load the pretrained MobileNet model" width="1000" height="517"/><div><p>Classification of the images found on a Flickr page related to cats</p></div></div><p>Part 1 of our image recognition sample application is now complete; you can find the full Notebook at the following location: <a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%201.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%201.ipynb</a>.</p><p>In the next section, we will build a more user-friendly experience by building a user interface with a PixieApp.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec30"/>Part 2 – Create a PixieApp for our image recognition sample application</h2></div></div></div><div><div><h3 class="title"><a id="note155"/>Note</h3><p>
<strong>Note</strong>: You can download the completed Notebook to follow this section discussion here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%202.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%202.ipynb</a>
</p></div></div><p>As a reminder, the <code class="literal">setup</code> method of a PixieApp, if defined, is executed before the app starts running. We use <a id="id389" class="indexterm"/>it to select our model and initialize the graph:</p><div><pre class="programlisting">from pixiedust.display.app import *

@PixieApp
class ScoreImageApp():
    def setup(self):
        self.model = models["mobilenet"]
        self.graph = load_graph( self.model )
    ...</pre></div><div><div><h3 class="title"><a id="note156"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode16.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode16.py</a>
</p></div></div><p>In the main screen of the PixieApp, we use an input box to let the user enter the URL to the web page, as shown in the following code snippet:</p><div><pre class="programlisting">[[ScoreImageApp]]
@route()
def main_screen(self):
   return """
&lt;style&gt;
    div.outer-wrapper {
        display: table;width:100%;height:300px;
    }
    div.inner-wrapper {
        display: table-cell;vertical-align: middle;height: 100%;width: 100%;
    }
&lt;/style&gt;
&lt;div class="outer-wrapper"&gt;
    &lt;div class="inner-wrapper"&gt;
        &lt;div class="col-sm-3"&gt;&lt;/div&gt;
        &lt;div class="input-group col-sm-6"&gt;
          &lt;input id="url{{prefix}}" type="text" class="form-control"
              value="https://www.flickr.com/search/?text=cats"
              placeholder="Enter a url that contains images"&gt;
          &lt;span class="input-group-btn"&gt;
            &lt;button class="btn btn-default" type="button" <strong>pd_options="image_url=$val(url{{prefix}})"</strong>&gt;Go&lt;/button&gt;
          &lt;/span&gt;
        &lt;/div&gt;
    &lt;/div&gt;
&lt;/div&gt;
"""</pre></div><div><div><h3 class="title"><a id="note157"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode17.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode17.py</a>
</p></div></div><p>For convenience, we initialize the input text with a default value of <code class="literal">https://www.flickr.com/search/?text=cats</code>.</p><p>We can already <a id="id390" class="indexterm"/>run the code to test the main screen by using the following code:</p><div><pre class="programlisting">app = ScoreImageApp()
app.run()</pre></div><p>The main screen looks like this:</p><div><img src="img/B09699_06_16.jpg" alt="Part 2 – Create a PixieApp for our image recognition sample application" width="1000" height="363"/><div><p>The main screen for the image recognition PixieApp</p></div></div><div><div><h3 class="title"><a id="note158"/>Note</h3><p>
<strong>Note</strong>: This is good for testing, but we should keep in mind that the <code class="literal">do_process_url</code> route has not yet been implemented and, therefore, clicking on the <strong>Go</strong> button will fall back to the default route again.</p></div></div><p>Let's now implement the <code class="literal">do_process_url</code> route, which is triggered when the user clicks on the <strong>Go</strong> button. This route first calls the <code class="literal">get_image_urls</code> method to get the list of image URLs. Using Jinja2, we then build an HTML fragment that displays all the images. For each image, we asynchronously invoke the <code class="literal">do_score_url</code> route that runs the model and displays the results.</p><p>The following <a id="id391" class="indexterm"/>code shows the implementation of the <code class="literal">do_process_url</code> route:</p><div><pre class="programlisting">[[ScoreImageApp]]
@route(image_url="*")
@templateArgs
def do_process_url(self, image_url):
    image_urls = <strong>get_image_urls(image_url)</strong>
    return """
&lt;div&gt;
{%for url in image_urls%}
&lt;div style="float: left; font-size: 9pt; text-align: center; width: 30%; margin-right: 1%; margin-bottom: 0.5em;"&gt;
&lt;img src="img/{{url}}" style="width: 100%"&gt;
  &lt;div style="display:inline-block" pd_render_onload pd_options="<strong>score_url={{url}}</strong>"&gt;
  &lt;/div&gt;
&lt;/div&gt;
{%endfor%}
&lt;p style="clear: both;"&gt;
&lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note159"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode18.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode18.py</a>
</p><p>Notice the use of the <code class="literal">@templateArgs</code> decorator, which allows the Jinja2 fragment to reference the local <code class="literal">image_urls</code> variable.</p></div></div><p>Finally, in the <code class="literal">do_score_url</code> route, we call the <code class="literal">score_image</code> and display the results as a list:</p><div><pre class="programlisting">[[ScoreImageApp]]
@route(score_url="*")
@templateArgs
def do_score_url(self, score_url):
    results = score_image(self.graph, self.model, score_url)
    return """
&lt;ul style="text-align:left"&gt;
{%for label, confidence in results%}
&lt;li&gt;&lt;b&gt;{{label}}&lt;/b&gt;: {{confidence}}&lt;/li&gt;
{%endfor%}
&lt;/ul&gt;
"""</pre></div><div><div><h3 class="title"><a id="note160"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode19.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode19.py</a>
</p></div></div><p>The following <a id="id392" class="indexterm"/>screenshot shows the results for the Flickr page that contains images of cats:</p><div><img src="img/B09699_06_17.jpg" alt="Part 2 – Create a PixieApp for our image recognition sample application" width="1000" height="935"/><div><p>Results of the image classification for cats</p></div></div><div><div><h3 class="title"><a id="note161"/>Note</h3><p>As a reminder, you can find the complete Notebook at this location:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%202.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%202.ipynb</a>
</p></div></div><p>Our MVP application <a id="id393" class="indexterm"/>is almost complete. In the next section, we will integrate the TensorBoard graph visualization directly in the Notebook.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec31"/>Part 3 – Integrate the TensorBoard graph visualization</h2></div></div></div><div><div><h3 class="title"><a id="note162"/>Note</h3><p>
<strong>Note</strong>: Part of the code described in this section is adapted from the <code class="literal">deepdream</code> notebook located here:</p><p>
<a class="ulink" href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/deepdream/deepdream.ipynb</a></p><p>You can download the completed Notebook to follow this section discussion here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%203.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%203.ipynb</a>
</p></div></div><p>TensorFlow comes with a very powerful suite of visualizations that help with debugging and performance <a id="id394" class="indexterm"/>optimization of your application. Please take a moment to explore the TensorBoard capabilities here: <a class="ulink" href="https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard">https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard</a>.</p><p>One issue here is that configuring the TensorBoard server to work with your Notebook could be difficult, especially if your Notebooks are hosted on the cloud, and you have little to no access to the underlying operating systems. In this case, configuring and starting the TensorBoard server could prove to be an impossible task. In this section, we show how to work around this problem by integrating the model graph visualization directly in your Notebook with zero configuration required. To provide a better user experience, we want to add the TensorBoard visualization to our PixieApp. We do that by changing the main layout to a tab layout and assign the TensorBoard visualization to its own tab. Conveniently, PixieDust provides a base PixieApp called <code class="literal">TemplateTabbedApp</code> that takes care of building a tabbed layout. When using <code class="literal">TemplateTabbedApp</code> as the base class, we need to configure the tab in the <code class="literal">setup</code> method as follows:</p><div><pre class="programlisting">[[ImageRecoApp]]
from pixiedust.apps.template import TemplateTabbedApp
@PixieApp
class ImageRecoApp(TemplateTabbedApp):
    def setup(self):
        self.apps = [
            {"title": "Score", "app_class": "ScoreImageApp"},
            {"title": "Model", "app_class": "TensorGraphApp"},
            {"title": "Labels", "app_class": "LabelsApp"}
        ]
        self.model = models["mobilenet"]
        self.graph = self.load_graph(self.model)
        
app = ImageRecoApp()
app.run()</pre></div><div><div><h3 class="title"><a id="note163"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode20.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode20.py</a>
</p></div></div><p>It should be noted that in the preceding code, we have added the <code class="literal">LabelsApp</code> child PixieApp to the list of tabs even though it hasn't yet been implemented. Therefore, as expected, if you run the code as is, the <code class="literal">Labels</code> tab will fail.</p><p>
<code class="literal">self.apps</code> contains an array of objects that define the tabs:</p><div><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">title</code>: Tab title</li><li class="listitem" style="list-style-type: disc"><code class="literal">app_class</code>: PixieApp to run when the tab is selected</li></ul></div><p>In <code class="literal">ImageRecoApp</code>, we configure three tabs associated with three child PixieApps: the <code class="literal">ScoreImageApp</code> that we've already created in <em>Part 2 – Create a PixieApp for our image recognition sample application</em>, the <code class="literal">TensorGraphApp</code> for displaying the model graph, and the <code class="literal">LabelsApp</code> to display a table of all the labeled categories used in the model.</p><p>The results <a id="id395" class="indexterm"/>are shown in the following screenshot:</p><div><img src="img/B09699_06_18.jpg" alt="Part 3 – Integrate the TensorBoard graph visualization" width="976" height="392"/><div><p>Tabbed layout that includes Score, Model, and Labels</p></div></div><p>What's also nice about using <code class="literal">TemplateTabbedApp</code> superclass is that the sub-PixieApps are defined separately, which makes the code more maintainable and reusable.</p><p>Let's first look at the <code class="literal">TensorGraphApp</code> PixieApp. Its main route returns an HTML fragment that loads the <code class="literal">tf-graph-basic.build.html</code> into an Iframe from <code class="literal">https://tensorboard.appspot.com,</code> and using a JavaScript load listener applies the serialized graph definition that was computed using the <code class="literal">tf.Graph.as_graph_def</code> method. To make sure the graph definition remains at a reasonable size, and to avoid unnecessary performance degradation on the browser client, we call the <code class="literal">strip_consts</code> method to <a id="id396" class="indexterm"/>remove tensors with constant values that have a large size.</p><p>The code for <code class="literal">TensorGraphApp</code> is shown here:</p><div><pre class="programlisting">@PixieApp
class TensorGraphApp():
    """Visualize TensorFlow graph."""
    def setup(self):
        self.graph = self.parent_pixieapp.graph

    @route()
    @templateArgs
    def main_screen(self):
        strip_def = self.strip_consts(self.graph.as_graph_def())
        code = """
            &lt;script&gt;
              function load() {{
                <strong>document.getElementById("{id}").pbtxt = {data};</strong>
              }}
            &lt;/script&gt;
            &lt;link rel="import" href="https://tensorboard.appspot.com/tf-graph-basic.build.html" onload=load()&gt;
            &lt;div style="height:600px"&gt;
              &lt;tf-graph-basic id="{id}"&gt;&lt;/tf-graph-basic&gt;
            &lt;/div&gt;
        """.format(data=repr(str(strip_def)), id='graph'+ self.getPrefix()).replace('"', '&amp;quot;')

        return """
&lt;iframe seamless style="width:1200px;height:620px;border:0" srcdoc=<strong>"{{code}}"</strong>&gt;&lt;/iframe&gt;
"""

    def strip_consts(self, graph_def, max_const_size=32):
        """Strip large constant values from graph_def."""
        strip_def = tf.GraphDef()
        for n0 in graph_def.node:
            n = strip_def.node.add() 
            n.MergeFrom(n0)
            if n.op == 'Const':
                tensor = n.attr['value'].tensor
                size = len(tensor.tensor_content)
                if size &gt; max_const_size:
                    tensor.tensor_content = "&lt;stripped {} bytes&gt;".format(size).encode("UTF-8")
        return strip_def</pre></div><div><div><h3 class="title"><a id="note164"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode21.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode21.py</a></p><p><strong>Note</strong>: Child PixieApps have access to their parent PixieApp through the <code class="literal">self.parent_pixieapp</code> variables.</p></div></div><p>The resulting <a id="id397" class="indexterm"/>screen for the <code class="literal">TensorGraphApp</code> child PixieApp is shown in the following screenshot. It provides an interactive visualization of the TensorFlow graph for the selected model, allowing the user to navigate through the different nodes and to drill down deeper into the model. However, it is important to note that the visualization runs entirely within the browser, without the TensorBoard server. Therefore, some of the functions available in the full TensorBoard, such as runtime statistics, are disabled.</p><div><img src="img/B09699_06_19.jpg" alt="Part 3 – Integrate the TensorBoard graph visualization" width="1000" height="650"/><div><p>Displaying the model graph for MobileNet V1</p></div></div><p>In the <code class="literal">LabelsApp</code> PixieApp, we simply load the labels as JSON format, and display it in a PixieDust <a id="id398" class="indexterm"/>table, using the <code class="literal">handlerId=tableView</code> option:</p><div><pre class="programlisting">[[LabelsApp]]
@PixieApp
class LabelsApp():
    def setup(self):
        self.labels = self.parent_pixieapp.load_labels(
            self.parent_pixieapp.model, as_json=True
        )
    
    @route()
    def main_screen(self):
        return """
&lt;div pd_render_onload pd_entity="labels"&gt;
    &lt;pd_options&gt;
    {
        "table_noschema": "true",
<strong>        "handlerId": "tableView",</strong>
        "rowCount": "10000"
    }
    &lt;/pd_options&gt;
&lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note165"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode22.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode22.py</a>
</p><p>
<strong>Note</strong>: We configure the table to not show the schema by setting <code class="literal">table_noschema</code> to <code class="literal">true</code>, but we keep the search bar for convenience.</p></div></div><p>The results <a id="id399" class="indexterm"/>are shown in the following screenshot:</p><div><img src="img/B09699_06_20.jpg" alt="Part 3 – Integrate the TensorBoard graph visualization" width="1000" height="659"/><div><p>Searchable table for the model categories</p></div></div><p>Our MVP image recognition sample application is now complete; you can find the full Notebook here: <a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%203.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%203.ipynb</a>.</p><p>In the next <a id="id400" class="indexterm"/>section, we will improve the application by allowing the user to retrain the model using custom images.</p></div><div><div><div><div><h2 class="title"><a id="ch06lvl2sec32"/>Part 4 – Retrain the model with custom training data</h2></div></div></div><div><div><h3 class="title"><a id="note166"/>Note</h3><p>
<strong>Note</strong>: You can download the completed Notebook to follow this section discussion here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%204.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%204.ipynb</a></p><p>The code in this section is quite extensive, and some helper functions that are not directly related to the topic will be omitted. However, as always, refer to the complete Notebook on GitHub for more information on the code.</p></div></div><p>In this section, we want to retrain the MobileNet model with custom training data and use it to classify <a id="id401" class="indexterm"/>images that would have had a low score on the generic model otherwise.</p><div><div><h3 class="title"><a id="note167"/>Note</h3><p>
<strong>Note</strong>: The code in this section is adapted from the <em>TensorFlow for poets</em> tutorial:</p><p>
<a class="ulink" href="https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py">https://github.com/googlecodelabs/tensorflow-for-poets-2/blob/master/scripts/retrain.py</a>
</p></div></div><p>As is the case most of the time, getting quality training data can be one of the most daunting and time-consuming tasks. In our example, we need images in large quantities for each of the classes we want to train. For the sake of simplicity and reproducibility, we are using the ImageNet databases that conveniently provide APIs for getting URLs and associated labels. We also limit the downloaded files to <code class="literal">.jpg</code> files. Of course, feel free to acquire your own training data if needed.</p><p>We first download the list of all the image URLs from the Fall 2011 release that is available here: <a class="ulink" href="http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz">http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz</a>, and unpack the file into a local directory of your choice (for example, I chose <code class="literal">/Users/dtaieb/Downloads/fall11_urls.txt</code>).We also need to download the mapping between WordNet ID and words for all <code class="literal">synsets</code> available at <a class="ulink" href="http://image-net.org/archive/words.txt">http://image-net.org/archive/words.txt</a>, which we'll use to find the WordNet IDs containing the URLs that we need to download.</p><p>The following code will load both files into a pandas DataFrame respectively:</p><div><pre class="programlisting">import pandas
wnid_to_urls = pandas.read_csv('/Users/dtaieb/Downloads/fall11_urls.txt',
                sep='\t', names=["wnid", "url"],
                header=0, error_bad_lines=False,
                warn_bad_lines=False, encoding="ISO-8859-1")
wnid_to_urls['wnid'] = wnid_to_urls['wnid'].apply(<strong>lambda x: x.split("_")[0]</strong>)
wnid_to_urls = wnid_to_urls.dropna()

wnid_to_words = pandas.read_csv(<strong>'/Users/dtaieb/Downloads/words.txt'</strong>,
                sep='\t', names=["wnid", "description"],
                header=0, error_bad_lines=False,
                warn_bad_lines=False, encoding="ISO-8859-1")
wnid_to_words = wnid_to_words.dropna()</pre></div><div><div><h3 class="title"><a id="note168"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode23.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode23.py</a>
</p><p>Notice that we needed to clean the <code class="literal">wnid</code> column in the <code class="literal">wnid_to_urls</code> dataset because it contains a suffix corresponding to the index of the image in the category.</p></div></div><p>We can <a id="id402" class="indexterm"/>then define a method <code class="literal">get_url_for_keywords</code> that returns a dictionary containing the categories as keys and an array of URLs as values:</p><div><pre class="programlisting">def get_url_for_keywords(keywords):
    results = {}
    for keyword in keywords:
        df = wnid_to_words.loc[wnid_to_words['description'] == keyword]
        row_list = df['wnid'].values.tolist()
        descriptions = df['description'].values.tolist()
        if len(row_list) &gt; 0:
            results[descriptions[0]] = \
            wnid_to_urls.loc[wnid_to_urls['wnid'] == \
            row_list[0]]["url"].values.tolist()
    return results</pre></div><div><div><h3 class="title"><a id="note169"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode24.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode24.py</a>
</p></div></div><p>We can easily glance at the data distribution by using PixieDust <code class="literal">display</code>. As always, feel free to do more exploration on your own:</p><div><img src="img/B09699_06_21.jpg" alt="Part 4 – Retrain the model with custom training data" width="1000" height="685"/><div><p>Distribution of images by categories</p></div></div><p>We can now <a id="id403" class="indexterm"/>build the code that will download the images corresponding to a list of categories of our choice. In our case, we chose fruits: <code class="literal">["apple", "orange", "pear", "banana"]</code>. The images will be downloaded in a subdirectory of the PixieDust home directory (using the PixieDust <code class="literal">Environment</code> helper class from the <code class="literal">pixiedust.utils</code> package), limiting the number of images to <code class="literal">500</code> for speed:</p><div><div><h3 class="title"><a id="note170"/>Note</h3><p>
<strong>Note</strong>: The following code uses methods and imports defined earlier in the Notebook. Make sure to run the corresponding cell before attempting to run the following code.</p></div></div><div><pre class="programlisting">from pixiedust.utils.environment import Environment
root_dir = ensure_dir_exists(os.path.join(<strong>Environment.pixiedustHome</strong>, "imageRecoApp")
image_dir = root_dir
image_dict = get_url_for_keywords(["apple", "orange", "pear", "banana"])
with open(os.path.join(image_dir, "retrained_label.txt"), "w") as f_label:
    for key in image_dict:
        f_label.write(key + "\n")
        path = ensure_dir_exists(os.path.join(image_dir, key))
        count = 0
        for url in image_dict[key]:
            download_image_into_dir(url, path)
            count += 1
            if count &gt; 500:
                break;</pre></div><div><div><h3 class="title"><a id="note171"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode25.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode25.py</a>
</p></div></div><p>The next part <a id="id404" class="indexterm"/>of the code processes each of the images in the training set using the following steps:</p><div><div><h3 class="title"><a id="note172"/>Note</h3><p>
<strong>Note</strong>: As mentioned before, the code is quite extensive, and part of it is omitted; only the important parts are explained here. Please do not attempt to run the following code as is and refer to the complete Notebook for full implementation.</p></div></div><div><ol class="orderedlist arabic"><li class="listitem">Decode the <code class="literal">.jpeg</code> file using the following code:<div><pre class="programlisting">def add_jpeg_decoding(model):
    input_height = get_model_attribute(model,
                   "input_height")
    input_width = get_model_attribute(model, "input_width")
    input_depth = get_model_attribute(model, "input_depth")
    input_mean = get_model_attribute(model, "input_mean",
                 0)
    input_std = get_model_attribute(model, "input_std",
                255)
    
    jpeg_data = tf.placeholder(tf.string,
                name='DecodeJPGInput')
    decoded_image = <strong>tf.image.decode_jpeg</strong>(jpeg_data,
                    channels=input_depth)
    decoded_image_as_float = tf.cast(decoded_image,
                             dtype=tf.float32)
    decoded_image_4d =  <strong>tf.expand_dims</strong>(
                       decoded_image_as_float,
                       0)
    resize_shape = tf.stack([input_height, input_width])
    resize_shape_as_int = tf.cast(resize_shape,
                          dtype=tf.int32)
    resized_image = <strong>tf.image.resize_bilinear</strong>(
                    decoded_image_4d,
                    resize_shape_as_int)
    offset_image = tf.subtract(resized_image, input_mean)
    mul_image = tf.multiply(offset_image, 1.0 / input_std)
    return jpeg_data, mul_image</pre></div><div><div><h3 class="title"><a id="note173"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode26.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode26.py</a>
</p></div></div></li><li class="listitem">Create the <a id="id405" class="indexterm"/>bottleneck values (caching them as appropriate) that normalize the image by resizing and rescaling it. This is done in the following code:<div><pre class="programlisting">def run_bottleneck_on_image(sess, image_data,
    image_data_tensor,decoded_image_tensor,
    resized_input_tensor,bottleneck_tensor):
    # First decode the JPEG image, resize it, and rescale the pixel values.
    resized_input_values = sess.run(decoded_image_tensor,
        {image_data_tensor: image_data})
    # Then run it through the recognition network.
    bottleneck_values = sess.run(
        bottleneck_tensor,
        {resized_input_tensor: resized_input_values})
    bottleneck_values = np.squeeze(bottleneck_values)
    return bottleneck_values</pre></div><div><div><h3 class="title"><a id="note174"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode27.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode27.py</a>
</p></div></div></li><li class="listitem">Add the final training operations using the <code class="literal">add_final_training_ops</code> method, under a common namespace, so that it's easier to manipulate when visualizing the graph. The training steps are as follows:<div><ol class="orderedlist arabic"><li class="listitem">Generate random weight with the <code class="literal">tf.truncated_normal</code> API:<div><pre class="programlisting">      initial_value = tf.truncated_normal(
          [bottleneck_tensor_size, class_count],
          stddev=0.001)
          layer_weights = tf.Variable(
              initial_value, name='final_weights')</pre></div></li><li class="listitem">Add the biases, initialized to zero:<div><pre class="programlisting">      layer_biases = tf.Variable(tf.zeros([class_count]),
          name='final_biases')</pre></div></li><li class="listitem">Compute the weighted sum:<div><pre class="programlisting">      logits = tf.matmul(bottleneck_input, layer_weights) +
          layer_biases</pre></div></li><li class="listitem">Add the <code class="literal">cross_entropy</code> cost function:<div><pre class="programlisting">      cross_entropy =
          tf.nn.softmax_cross_entropy_with_logits(
          labels=ground_truth_input, logits=logits)
      with tf.name_scope('total'):
          cross_entropy_mean = tf.reduce_mean(
          cross_entropy)</pre></div></li><li class="listitem">Minimize the cost function:<div><pre class="programlisting">      optimizer = tf.train.GradientDescentOptimizer(
          learning_rate)
      train_step = optimizer.minimize(cross_entropy_mean)</pre></div></li></ol></div></li></ol></div><p>To visualize <a id="id406" class="indexterm"/>the retrained graph, we first need to update the <code class="literal">TensorGraphApp</code> PixieApp to let the user select which model to visualize: generic MobileNet or custom. This is done by adding a <code class="literal">&lt;select&gt;</code> drop-down in the main route and attaching a <code class="literal">pd_script</code> element to update the state:</p><div><pre class="programlisting">[[TensorGraphApp]]
return """
{%if this.custom_graph%}
&lt;div style="margin-top:10px" pd_refresh&gt;
    &lt;pd_script&gt;
<strong>self.graph = self.custom_graph if self.graph is not self.custom_graph else self.parent_pixieapp.graph</strong>
    &lt;/pd_script&gt;
    &lt;span style="font-weight:bold"&gt;Select a model to display:&lt;/span&gt;
    &lt;select&gt;
<strong>        &lt;option {%if this.graph!=this.custom_graph%}selected{%endif%} value="main"&gt;MobileNet&lt;/option&gt;</strong>
<strong>        &lt;option {%if this.graph==this.custom_graph%}selected{%endif%} value="custom"&gt;Custom&lt;/options&gt;</strong>
    &lt;/select&gt;
{%endif%}
&lt;iframe seamless style="width:1200px;height:620px;border:0" srcdoc="{{code}}"&gt;&lt;/iframe&gt;
"""</pre></div><div><div><h3 class="title"><a id="note175"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode28.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode28.py</a>
</p></div></div><p>Rerunning our <code class="literal">ImageReco</code> PixieApp <a id="id407" class="indexterm"/>produces the following screenshot:</p><div><img src="img/B09699_06_22.jpg" alt="Part 4 – Retrain the model with custom training data" width="1000" height="697"/><div><p>Visualization of the retrained graph</p></div></div><p>Clicking on the train node will reveal the nested operations that run the backpropagation algorithms to minimize the <code class="literal">cross_entropy_mean</code> cost functions specified in the preceding <code class="literal">add_final_training_ops</code>:</p><div><pre class="programlisting">with tf.name_scope('cross_entropy'):
    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(
        labels=ground_truth_input, logits=logits)
    with tf.name_scope('total'):
        cross_entropy_mean = <strong>tf.reduce_mean(cross_entropy)</strong>
</pre></div><div><div><h3 class="title"><a id="note176"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode29.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode29.py</a>
</p></div></div><p>The following <a id="id408" class="indexterm"/>screenshot shows the details of the <strong>train</strong> namespace:</p><div><img src="img/B09699_06_23.jpg" alt="Part 4 – Retrain the model with custom training data" width="1000" height="654"/><div><p>Backpropagation during training</p></div></div><p>Similarly, we can add the drop-down toggle in the <code class="literal">LabelsApp</code> to switch the visualization between the <a id="id409" class="indexterm"/>generic MobileNet and custom model:</p><div><pre class="programlisting">[[LabelsApp]]
@PixieApp
class LabelsApp():
    def setup(self):
        ...
    
    @route()
    def main_screen(self):
        return """
{%if this.custom_labels%}
&lt;div style="margin-top:10px" pd_refresh&gt;
    &lt;pd_script&gt;
<strong>self.current_labels = self.custom_labels if self.current_labels is not self.custom_labels else self.labels</strong>
    &lt;/pd_script&gt;
    &lt;span style="font-weight:bold"&gt;
        Select a model to display:&lt;/span&gt;
    &lt;select&gt;
        &lt;option {%if this.current_labels!=this.labels%}selected{%endif%} value="main"&gt;MobileNet&lt;/option&gt;
        &lt;option {%if this.current_labels==this.custom_labels%}selected{%endif%} value="custom"&gt;Custom&lt;/options&gt;
    &lt;/select&gt;
{%endif%}
&lt;div pd_render_onload pd_entity="current_labels"&gt;
    &lt;pd_options&gt;
    {
        "table_noschema": "true",
        "handlerId": "tableView",
        "rowCount": "10000",
        "noChartCache": "true"
        
    }
    &lt;/pd_options&gt;
&lt;/div&gt;
        """</pre></div><div><div><h3 class="title"><a id="note177"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode30.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode30.py</a>
</p></div></div><p>The results are displayed in the following screenshot:</p><div><img src="img/B09699_06_24.jpg" alt="Part 4 – Retrain the model with custom training data" width="1000" height="329"/><div><p>Display labels information for each model</p></div></div><p>The last step <a id="id410" class="indexterm"/>for our Part 4 MVP is to update the <code class="literal">score_image</code> method to classify the image with both models and add the results in a dictionary with an entry for each model. We define a local method <code class="literal">do_score_image</code> that returns the top 5 candidates answers.</p><p>This method is called for each model, and the results populate a dictionary with the model name as the key:</p><div><pre class="programlisting"># classify an image given its url
def score_image(graph, model, url):
    # Download the image and build a tensor from its data
    t = read_tensor_from_image_file(model, download_image(url))

    def do_score_image(graph, output_layer, labels):
        # Retrieve the tensors corresponding to the input and output layers
        input_tensor = graph.get_tensor_by_name("import/" +
            input_layer + ":0");
        output_tensor = graph.get_tensor_by_name( output_layer +
            ":0");

        with tf.Session(graph=graph) as sess:
            # Initialize the variables
            sess.run(tf.global_variables_initializer())
            results = sess.run(output_tensor, {input_tensor: t})
        results = np.squeeze(results)
        # select the top 5 candidates and match them to the labels
        top_k = results.argsort()[-5:][::-1]
        return [(labels[i].split(":")[1], results[i]) for i in top_k]
    
    results = {}
    input_layer = get_model_attribute(model, "input_layer",
        "input")
    labels = load_labels(model)
    results["mobilenet"] = do_score_image(graph, "import/" +
        get_model_attribute(model, "output_layer"), labels)
    if "custom_graph" in model and "custom_labels" in model:
        with open(model["custom_labels"]) as f:
            labels = [line.rstrip() for line in f.readlines() if line != ""]
            custom_labels = ["{}:{}".format(i, label) for i,label in zip(range(len(labels)), labels)]
        results["custom"] = do_score_image(model["custom_graph"],
            "final_result", custom_labels)
    return results</pre></div><div><div><h3 class="title"><a id="note178"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode31.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode31.py</a>
</p></div></div><p>Since we <a id="id411" class="indexterm"/>modified the returned values for the <code class="literal">score_image</code> method, we need to adjust the HTML fragment returned in <code class="literal">ScoreImageApp</code> to loop over all the model entries of the <code class="literal">results</code> dictionary:</p><div><pre class="programlisting">@route(score_url="*")
@templateArgs
def do_score_url(self, score_url):
    scores_dict = score_image(self.graph, self.model, score_url)
    return """
{%for model, results in scores_dict.items()%}
&lt;div style="font-weight:bold"&gt;{{model}}&lt;/div&gt;
&lt;ul style="text-align:left"&gt;
{%for label, confidence in results%}
&lt;li&gt;&lt;b&gt;{{label}}&lt;/b&gt;: {{confidence}}&lt;/li&gt;
{%endfor%}
&lt;/ul&gt;
{%endfor%}
    """</pre></div><div><div><h3 class="title"><a id="note179"/>Note</h3><p>You can find the code file here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode32.py">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/sampleCode32.py</a>
</p></div></div><p>With these changes in place, the PixieApp will automatically invoke the custom models if available and, if that's the case, display the results for both models.</p><p>The following <a id="id412" class="indexterm"/>screenshot shows the results for images related to <em>banana</em>:</p><div><img src="img/B09699_06_25.jpg" alt="Part 4 – Retrain the model with custom training data" width="916" height="787"/><div><p>Score with generic MobileNet and custom-trained model</p></div></div><p>The reader will notice that the scores for the custom models are pretty low. One possible explanation is that the training data acquisition is fully automated and used without human curation. One possible enhancement to this sample application would be to move the training data acquisition and retraining steps into its own tab PixieApp. We should also give the user the opportunity to validate the images and reject the one that is of poor quality. It would also be great to let the user relabel the images that have been wrongly categorized.</p><div><div><h3 class="title"><a id="note180"/>Note</h3><p>The completed Notebook for Part 4 can be found here:</p><p>
<a class="ulink" href="https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%204.ipynb">https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%206/Tensorflow%20VR%20Part%204.ipynb</a>
</p></div></div><p>In this section, we've discussed the incremental approach of building an image recognition <a id="id413" class="indexterm"/>sample application in a Jupyter Notebook using TensorFlow, with a special focus on operationalizating the algorithms using PixieApps. We started with building a simple classification model from a pandas DataFrame using the TensorFlow <code class="literal">DNNClassifier</code> estimator. We then built an MVP version of the image recognition sample application in four parts:</p><div><ol class="orderedlist arabic"><li class="listitem">We loaded the pretrained MobileNet model</li><li class="listitem">We created a PixieApp for our image recognition sample application</li><li class="listitem">We integrated the TensorBoard graph visualization into the PixieApp</li><li class="listitem">We enabled users to retrain the model with custom training data from ImageNet</li></ol></div></div></div></div>



  
<div><div><div><div><div><h1 class="title"><a id="ch06lvl1sec47"/>Summary</h1></div></div></div><p>Machine learning is a vast topic that enjoys tremendous growth, both in research and development. In this chapter, we've explored only a tiny fraction of the state of the art in connection with machine learning algorithms, namely, using a deep learning neural network to perform image recognition. For some readers who are just beginning to get familiar with machine learning, the sample PixieApps and associated algorithms code may be too deep to digest at one time. However, the underlying aim was to demonstrate how to iteratively build an application that leverages a machine learning model. We happened to use a convolutional neural network model for image recognition, but any other model would do.</p><p>Hopefully, you got a good idea of how PixieDust and the PixieApp programming model can help you with your own project, and I strongly encourage you to use this sample application as a starting point to build your own custom application using the machine learning of your choice. I also recommend deploying your PixieApp as a web application with the PixieGateway microservice and exploring whether it's a viable solution.</p><p>In the next chapter, we will cover another important industry use case related to big data and natural language processing. We'll build a sample application that analyzes social media trends using a natural language understanding service.</p></div></div>



  </body></html>