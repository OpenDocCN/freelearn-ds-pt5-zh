["```py\n> magic <- read.csv(\"magic04.data\", header = FALSE) \n> names(magic) <- c(\"FLENGTH\", \"FWIDTH\", \"FSIZE\", \"FCONC\", \"FCONC1\", \n  \"FASYM\", \"FM3LONG\", \"FM3TRANS\", \"FALPHA\", \"FDIST\", \"CLASS\") \n> magic$CLASS <- as.factor(ifelse(magic$CLASS =='g', 1, -1)) \n```", "```py\n> library(caret) \n> set.seed(33711209) \n> magic_sampling_vector <- createDataPartition(magic$CLASS,  \n                             p = 0.80, list = FALSE) \n> magic_train <- magic[magic_sampling_vector, 1:10] \n> magic_train_output <- magic[magic_sampling_vector, 11] \n> magic_test <- magic[-magic_sampling_vector, 1:10] \n> magic_test_output <- magic[-magic_sampling_vector, 11] \n```", "```py\n> magic_pp <- preProcess(magic_train, method = c(\"center\",  \n                                                 \"scale\")) \n> magic_train_pp <- predict(magic_pp, magic_train) \n> magic_train_df_pp <- cbind(magic_train_pp,  \n                             CLASS = magic_train_output) \n> magic_test_pp <- predict(magic_pp, magic_test) \n```", "```py\n> library(nnet) \n> n_model <- nnet(CLASS ~ ., data = magic_train_df_pp, size = 1) \n> n_test_predictions <- predict(n_model, magic_test_pp, \n                                type = \"class\") \n> (n_test_accuracy <- mean(n_test_predictions ==   \n                           magic_test_output)) \n[1] 0.7948988 \n```", "```py\nAdaBoostNN <- function(training_data, output_column, M,   \n                       hidden_units) { \n  require(\"nnet\") \n  models <- list() \n  alphas <- list() \n  n <- nrow(training_data) \n  model_formula <- as.formula(paste(output_column, '~ .', sep = '')) \n  w <- rep((1/n), n) \n  for (m in 1:M) { \n    model <- nnet(model_formula, data = training_data,  \n                size = hidden_units, weights = w) \n    models[[m]] <- model \n    predictions <- as.numeric(predict(model,  \n                    training_data[, -which(names(training_data) == \n                    output_column)], type = \"class\")) \n    errors <- predictions != training_data[, output_column] \n    error_rate <- sum(w * as.numeric(errors)) / sum(w) \n    alpha <- 0.5 * log((1 - error_rate) / error_rate) \n    alphas[[m]] <- alpha \n    temp_w <- mapply(function(x, y) if (y) { x * exp(alpha) }  \n                    else { x * exp(-alpha)}, w, errors) \n    w <- temp_w / sum(temp_w) \n  } \n  return(list(models = models, alphas = unlist(alphas))) \n} \n```", "```py\nAdaBoostNN.predict <- function(ada_model, test_data) { \n  models <- ada_model$models \n  alphas <- ada_model$alphas \n  prediction_matrix <- sapply(models, function (x)  \n             as.numeric(predict(x, test_data, type = \"class\"))) \n  weighted_predictions <- t(apply(prediction_matrix, 1,  \n             function(x) mapply(function(y, z) y * z, x, alphas))) \n  final_predictions <- apply(weighted_predictions, 1, \n              function(x) sign(sum(x))) \n  return(final_predictions) \n} \n```", "```py\n> ada_model <- AdaBoostNN(magic_train_df_pp, 'CLASS', 10, 1) \n> predictions <- AdaBoostNN.predict(ada_model, magic_test_pp,  \n                                    'CLASS') \n> mean(predictions == magic_test_output) \n [1] 0.804365 \n```"]