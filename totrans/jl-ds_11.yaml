- en: Chapter 11. Introduction to Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Innovators have always longed to make machines that can think. At the point
    when programmable PCs were first considered, individuals pondered whether they
    might get to be wise, over a hundred years before one was constructed (Lovelace
    in 1842).
  prefs: []
  type: TYPE_NORMAL
- en: Today, **artificial intelligence** (**AI**) is a flourishing field with numerous
    reasonable applications and dynamic exploration points. We look to intelligent
    programming to automate routine work, process image and audio and extract meaning
    out of it, automate diagnoses of several diseases, and much more.
  prefs: []
  type: TYPE_NORMAL
- en: In the beginning, when artificial intelligence (AI) was picking up, the field
    handled and tackled issues that are mentally difficult for individuals, yet moderately
    straightforward for computers. These issues can be depicted by a rundown of formal,
    scientific principles. The genuine test for artificial intelligence turned out
    to be unraveling the undertakings that are simple for individuals to perform yet
    hard for individuals to depict formally. These issues we explain naturally, for
    example the ability of humans to understand speech (and sarcasm) and our ability
    to identify images, especially faces.
  prefs: []
  type: TYPE_NORMAL
- en: This arrangement is to permit computers to learn by gaining experience and to
    comprehend the world as far as a chain or a tree of facts, with every fact defined
    as far as its connection to more straightforward facts. By understanding these
    facts, this methodology maintains a strategic distance from the requirement for
    human administrators to formally indicate the greater part of the information
    that the computer needs.
  prefs: []
  type: TYPE_NORMAL
- en: The progressive system of facts permits the computer to learn convoluted ideas
    by building them out of more straightforward ones. In the event that we draw a
    diagram indicating how these ideas are based on top of each other, the chart is
    profound, with numerous layers. Thus, we call this way to deal with AI deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'A number of the early accomplishments of AI occurred in moderately sterile
    and formal situations and it was not necessary for computers to have much learning
    of the world. Let''s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: IBM's Deep Blue chess-playing framework in 1997 defeated Mr. Gary Kasparov,
    the world champion at the time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We should also consider these factors:'
  prefs: []
  type: TYPE_NORMAL
- en: Chess is obviously an extremely basic world.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It contains just 64 blocks and 32 elements that can only move in predefined
    ways.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although conceiving a fruitful chess system is a huge achievement, the test
    is not due to the difficulty of describing the arrangement of chess elements and
    passable moves to the computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chess can be totally portrayed by an extremely short rundown of totally formal
    principles, effortlessly given earlier by the software engineer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Computers perform better than human beings in some of the tasks and worse in
    others:'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract tasks that are among the most difficult mental endeavors for a person
    are among the simplest for a computer. Computers are much better suited for such
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An example of this is performing complex mathematical tasks.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Subjective and natural tasks are performed much better by the average human
    being than a computer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A man's ordinary life requires a tremendous measure of information about the
    world.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of this learning is subjective and natural, and accordingly difficult
    to express formally.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Computers need to catch this same information so as to act in a wise way. One
    of the key difficulties in artificial intelligence is the means by which you get
    this casual learning onto a computer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A few artificial intelligence ventures have looked to hard-code information
    about the world in formal dialects. A computer can reason about articulations
    in these formal dialects, consequently utilizing legitimate deduction rules. This
    is known as the information base way to deal with artificial intelligence. None
    of these activities have prompted a noteworthy success.
  prefs: []
  type: TYPE_NORMAL
- en: The difficulties confronted by frameworks depending on hard-coded information
    propose that AI frameworks require the capacity to obtain their own particular
    learning, by extracting patterns from crude information. This is known as machine
    learning, which we studied in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: The execution of these straightforward machine-learning calculations depends
    vigorously on the representation of the information they are given.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, when logistic regression is utilized to suggest the future weather,
    the AI framework does not look at the patient straightforwardly:'
  prefs: []
  type: TYPE_NORMAL
- en: The specialist tells the framework a few bits of important data, for example,
    the varying temperatures, wind direction and speed, humidity, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every bit of the data incorporated into the representation of the weather is
    known as a feature. Logistic regression figures out how each of these features
    of the weather relates to different weather in different seasons or in other locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In any case, it can't influence the way that the features are defined in any
    capacity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One answer for this issue is to utilize the machine, figuring out how to find
    the mapping from the representation to yield as well as the representation itself.
    This methodology is known as representation learning. Learned representations
    frequently bring about much-preferred execution over what can be acquired with
    hand-planned representations. They additionally permit AI frameworks to quickly
    adjust to new tasks, with negligible human intercession.
  prefs: []
  type: TYPE_NORMAL
- en: A representation learning calculation can find a decent arrangement of features
    for a straightforward undertaking in minutes, or for a complex assignment in hours
    to months. Physically outlining highlights for a complex work require a lot of
    human time and effort, much more than for computers.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we will go through multiple topics, starting with the basic
    introduction:'
  prefs: []
  type: TYPE_NORMAL
- en: Basic foundations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Differences between machine learning and deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep feed-forward networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single and multi-layer neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Practical methodology and applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Revisiting linear algebra
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linear algebra is a widely used branch of mathematics. Linear algebra is a part
    of discrete mathematics and not of continuous mathematics. A good understanding
    is needed to understand the machine learning and deep learning models. We will
    only revise the mathematical objects.
  prefs: []
  type: TYPE_NORMAL
- en: A gist of scalars
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A scalar is just a single number (as opposed to a large portion of alternate
    objects examined in linear algebra, which are generally arrays of various numbers).
  prefs: []
  type: TYPE_NORMAL
- en: A brief outline of vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A vector is an organized collection or an array of numbers. We can recognize
    every individual number by its index in that list. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x = [x1, x2, x3, x4 ..... xn]*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Vectors can also be thought of as identifying points in space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each element represents the value of coordinate along a different axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also index the positions of these values in the vector. Therefore, it
    makes it easier to access the specific value of the array.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The importance of matrices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A matrix is a two-dimensional array of numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every component is identified by two indexes rather than only one.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, a point in 2D space may be identified as (3,4). It means that the
    point is 3 points on the *x* axis and 4 points on the *y* axis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can also have arrays of such numbers as[(3,4), (2,4), (1,0)]. Such an array
    is called a matrix.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are tensors?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If more than two-dimensions are needed (matrix) then we use tensors.
  prefs: []
  type: TYPE_NORMAL
- en: This is an array of numbers without a defined number of axes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Such objects have a structure as follows: *T (x, y, z)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*[(1,3,5), (11,12,23), (34,32,1)]*'
  prefs: []
  type: TYPE_NORMAL
- en: Probability and information theory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability theory is a scientific system for speaking to questionable explanations.
    It gives a method for evaluating instability and adages for inferring new indeterminate
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In applications of AI, we utilize probability theory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The laws of probability define how AI frameworks ought to reason, so algorithms
    are designed to figure or approximate different expressions inferred on utilizing
    probability theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability and statistics can be utilized to hypothetically investigate the
    behavior of proposed AI frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While probability theory permits us to put forth indeterminate expressions and
    reason within the sight of uncertainty, data permits us to measure the degree
    of uncertainty in a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Why probability?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine learning makes substantial utilization of probability theory unlike
    other branches of computer science that are mainly dependent on the deterministic
    nature of the computer system:'
  prefs: []
  type: TYPE_NORMAL
- en: This is on the grounds that machine learning must dependably manage uncertain
    quantities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the time it may also be necessary to manage stochastic (non-deterministic)
    amounts. Uncertainty and stochasticity can emerge from numerous sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All exercises require some capacity to reason within the sight of uncertainty.
    Actually, with past numerical explanations that are valid by definition,, it is
    difficult to think about any suggestion that is completely valid or any occasion
    that is totally ensured to happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'Uncertainty has are three possible sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Existing stochasticity in the framework that is being modeled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, while playing a card game we make the assumption that the cards
    are truly shuffled in a random fashion.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fragmented observability. When a greater part of the variables that drive the
    conduct of the framework cannot be observed then even deterministic frameworks
    can seem stochastic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, in a question with multiple-choice options as answers, one choice
    leads to the correct answer while others will result in nothing. The result given
    the challenger's decision is deterministic, yet from the candidate's perspective,
    the result is indeterminate.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fragmented modeling. When we utilize a model that must dispose of a portion
    of the data we have observed, the disposed-of data results in instability in the
    model's expectations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For instance, assume we manufacture a robot that can precisely watch the area
    of each article around it. In the event that the robot discretizes space while
    anticipating the future area of these objects, then the discretization makes the
    robot quickly become dubious about the exact position of the articles: every item
    could be any place inside the discrete cell that it was seen to possess.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A probability can be seen as the augmentation of rationale to manage uncertainty.
    Rationale gives an arrangement of formal rules for figuring out what suggestions
    are inferred to be true or false given the suspicion that some other arrangement
    of recommendations is true or false.
  prefs: []
  type: TYPE_NORMAL
- en: Probability hypothesis gives an arrangement of formal principles for deciding
    the probability of a suggestion being genuine given the probability of the different
    recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: Differences between machine learning and deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning and deep learning intend to accomplish the same objective,
    but, they are distinctive and amount to various thoughts. Machine learning is
    the most major of the two and scientists and mathematicians have been doing research
    on it for a few decades now. Deep learning is a comparatively new idea. Deep learning
    is based on learning via neural networks (multiple layers) to achieve the goal.
    Understanding the difference between the two is important to know where we should
    apply deep learning and which problems can be solved using machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: It was understood that a more intense approach to construct pattern recognition
    algorithms is achieved by utilizing the information that can be effortlessly mined
    relying only upon the area and the deciding objective.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in image recognition we accumulate various pictures and expand
    the algorithm on that. Utilizing the information as a part of these pictures,
    our model can be trained to recognize creatures, human appearances, or different
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is connected to different areas and now it is not limited to
    image or character recognition. It is currently utilized intensely as a part of
    robotics, financial markets, self-driving cars, and genome analysis. We learned
    about machine learning in previous chapters and now we can go further to understand
    how different it is from deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: What is deep learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning started becoming popular in 2006\. It is also known as hierarchical
    learning. Its applications are wide and it has increased the scope of artificial
    intelligence and machine learning. There is a huge interest in deep learning from
    the community.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning refers to a class of machine learning techniques which:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform unsupervised or supervised feature extraction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform pattern analysis or classification by exploiting multiple layers of
    non-linear information processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It consists of a hierarchy of features or factors. In this hierarchy, lower-level
    features help in defining higher-level features. Artificial neural networks are
    typically used in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Conventional machine learning models learn patterns or clusters. Deep neural
    networks learn computations with a very small number of steps.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generally speaking, the deeper the neural network, the more powerful it gets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks are updated according to the new data made available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificial neural networks are fault tolerant, which means that if some part
    of the network is destroyed, then that may affect the performance of the network,
    but the key functioning of the network may still be retained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning algorithms learn multiple levels of representation and do the
    computations in parallel, which may be of increasing complexity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we fast forward to today, there is a widespread enthusiasm for something
    that many refer to as deep learning. The most prominent sorts of deep learning
    models, as they are utilized as a part of extensive scale image recognition tasks,
    are known as Convolutional Neural Nets, or essentially ConvNets.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning emphasizes the sort of model that we need to utilize (such as
    a deep convolutional multi-layer neural system) and that we can utilize information
    to fill in the missing parameters.
  prefs: []
  type: TYPE_NORMAL
- en: With deep learning comes incredible obligation. Since we are beginning with
    a model of the world which has a high dimensionality, we truly require a great
    deal of information which we also call big data, and a considerable measure of
    computational force (General Purpose GPUs/ High performance computing). Convolutions
    are utilized widely as a part of deep learning (particularly computer vision applications).
  prefs: []
  type: TYPE_NORMAL
- en: '![What is deep learning?](img/B05321_11_01-1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In the previous image we saw three layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output layer**: Here this predicts a supervised target'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: Abstract representations of the intermediary functions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input layer**: Raw inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Artificially simulated neurons stand for the building blocks of the multi-layer
    artificial neural systems. The essential idea is to simulate a human brain and
    how it solves a complex problem. The main idea to manufacture neural systems was
    based upon these theories and models.
  prefs: []
  type: TYPE_NORMAL
- en: Numerous more significant leaps were brought about in the last few decades with
    regards to deep-learning algorithms. These can be utilized to make feature indicators
    from unlabeled information and also to pre-train deep neural networks, which are
    the neural systems that are made out of numerous layers.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks are an interesting issue in scholastic exploration, as well
    as in huge innovation organizations, for example for companies such as Facebook,
    Microsoft, and Google, who are investing heavily in deep-learning research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Complex neural networks fueled by deep-learning calculations are considered
    as best in class with regards to critical problem solving. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google''s image search**: We can search images on the Internet using the
    Google image search tool. This can be done by uploading an image or giving the
    URL of the image to search for on the Internet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Translate**: This tool can read text in images and understand speech
    to translate or tell meaning in several languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One other very famous application is used in the self-driving cars, created
    by Google or Tesla. They are powered by deep learning to find out the best path,
    drive through the traffic in real time, and perform necessary tasks as they would
    if they were being driven by a human driver.
  prefs: []
  type: TYPE_NORMAL
- en: Deep feedforward networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep feedforward networks are the most famous deep learning models. These are
    also called the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward neural networks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-layer perceptrons** (**MLPs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Deep feedforward networks](img/B05321_11_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The aim of the feed-forward neural network is to learn by their parameters and
    define a function that maps to the output *y:*
  prefs: []
  type: TYPE_NORMAL
- en: '*y = f(x, theta)*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: As also depicted in the image, the feedforward neural networks are called such
    because of their data flows in one direction. It starts from the *x* and passes
    through the function for the intermediate calculations to generate *y*.
  prefs: []
  type: TYPE_NORMAL
- en: When such systems also include connections to the previous layer (feedback),
    then these are known as recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward systems are of great significance to machine learning experts. They
    frame the premise of numerous imperative business applications. For instance,
    the convolutional systems utilized for natural language processing from speech
    are a specific sort of feedforward system.
  prefs: []
  type: TYPE_NORMAL
- en: Feedforward systems are a reasonable stepping stone on the way to recurrent
    networks. These systems have numerous natural language applications. Feedforward
    neural networks are called networks since they are represented by forming together
    numerous different functions. The model is connected with a directed acyclic graph
    portraying how the functions are created together.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we have three functions - *f(1)*, *f(2)*, and *f(3)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are chained or associated together as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(x) = f(3)(f(2)(f(1)(x)))*'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'These chain structures are the most normally utilized structures of neural
    systems. For this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f(1)* is known as the first layer of the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(2)* is known as the second layer, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The general length of the chain gives the depth of the model. It is from this
    wording the name "deep learning" emerges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final layer of a feedforward network is known as the output or yield layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Amid neural network training, we follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Drive *f(x)* to coordinate *f∗(x)*. The training information has data with noise
    and inexact data off *∗(x)* assessed at different training sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every example of *x* is associated by a label *y ≈ f∗(x)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training cases determine straightforwardly what the yield layer must do
    at every point *x*. That is, it must create a value that is near *y*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the hidden layers in a neural network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The conduct of alternate layers is not straightforwardly specified by the training
    information. The learning algorithm must choose how to utilize those layers to
    create the desired yield, yet the training information does not say what every
    individual layer ought to do.
  prefs: []
  type: TYPE_NORMAL
- en: Rather, it is the learning algorithm which must choose how to utilize these
    layers to best execute an estimation off ∗. Since the training information does
    not demonstrate the desired yield for each of these layers, these layers are called
    hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: The motivation of neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These systems are called neural on the grounds that they are approximately motivated
    by neuroscience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each concealed or hidden layer of the system is generally vector-valued.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dimension of *y* of these hidden layers decides the width of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every component of the vector might be translated as assuming a part comparable
    to a neuron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As opposed to thinking about the layer as exhibiting a single vector-to-vector
    function, it should be thought that the layer comprises of numerous units that
    work in parallel, each exhibiting a vector-to-scalar function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each unit looks like a neuron in respect that it gets a contribution from numerous
    different units and registers its own activation value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using numerous layers of vector-valued representation is drawn from neuroscience.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decision of the function *f(i)(x)* used to figure out these representations
    is somewhat guided by neuroscientific observations about the functions that organic
    neurons process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We studied regularization in previous chapters. Let's study why this is important
    and required for deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main issue in machine learning is the means by which to make an algorithm
    that will perform well on the training information, as well as on new inputs.
    Numerous techniques utilized as a part of machine learning are expressly intended
    to diminish test errors, potentially to the detriment of increased training errors.
    These techniques are referred to aggregately as regularization.
  prefs: []
  type: TYPE_NORMAL
- en: There are many types of regularization accessible to the deep-learning specialist.
    More effective regularization systems have been one of the research efforts in
    the field.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous regularization systems.
  prefs: []
  type: TYPE_NORMAL
- en: Additional constraints on a machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For example, including constraints on the parameter values.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional terms in the target functions that can be taken as comparing to a
    delicate requirement on the parameter values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If done strategically and carefully, these additional requirements and constraints
    can result in enhanced performance on the testing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These constraints and restrictions can also be used to encode specific sorts
    of prior learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These constraints and restrictions can also lead to generalization of the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensemble methods also use regularization to generate better results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With regards to deep learning, most regularization procedures depend on regularizing
    estimators. To regulate the estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: We need to exchange increased bias for reduced variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An effective regularizer is one that makes a profitable exchange, which means
    it decreases the variance drastically whilst not excessively expanding the bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In overfitting and generalization we concentrate on these situations for the
    model that we are training:'
  prefs: []
  type: TYPE_NORMAL
- en: Avoid the true information on the producing process to take into account the
    overfitting and inducing bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include the true information on the producing process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Include information on the producing process and additionally numerous other
    information on producing processes to take into account the overfitting where
    variance instead of bias rules the estimation error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective of regularization is to take a model to the second process that
    is mentioned.
  prefs: []
  type: TYPE_NORMAL
- en: An excessively complex model family does not, as a matter of course, incorporate
    the target function or the genuine information producing process. In any case,
    most utilizations of deep-learning algorithms are where the genuine information
    producing procedure is in all likelihood outside the model family. Deep learning
    algorithms are normally connected, to a great degree, to complicated use cases
    such as image recognition, speech recognition, self-driving cars, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: This means that controlling the complexity of the nature of the model is not
    just a matter of finding a model of the appropriate size with the right set of
    parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing deep learning models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Optimization methods are vital in designing algorithms to extract desired knowledge
    from huge volumes of data. Deep learning is a rapidly evolving field where new
    optimization techniques are generated.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning algorithms include enhancement in numerous connections. For instance,
    performing deduction in models, for example, PCA, includes taking care of an improvement
    issue.
  prefs: []
  type: TYPE_NORMAL
- en: We regularly utilize diagnostic optimization to compose verifications or configuration
    calculations. Of the majority of the numerous optimization issues required in
    deep learning, the most difficult is preparing the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: It is very common to contribute days, or even months, of time to many machines
    with a specific end goal to solve even a single case of the neural system-training
    problem. Since this issue is so critical and thus expensive, a specific arrangement
    of optimization strategies has been produced for enhancing it.
  prefs: []
  type: TYPE_NORMAL
- en: The case of optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To find the parameters θ of a neural network that significantly lessen a cost
    function J(θ), commonly incorporates an execution measure assessed on the whole
    training set and additionally extra regularization terms.
  prefs: []
  type: TYPE_NORMAL
- en: An optimization used as a training algorithm for a machine learning task is
    different from immaculate optimization. More complex algorithms adjust their learning
    rates amid training or influence data contained in the second derivatives of the
    cost function. Finally, a few optimization methodologies are created by joining
    basic optimization algorithms into higher-level strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimization algorithms utilized for the training of deep learning models are
    different from conventional optimization algorithms in a few ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning typically acts in a roundabout way. In most machine-learning
    situations, we think about some execution measure *P*, that is defined as for
    the test set and may likewise be obstinate. We accordingly upgrade *P* just in
    a roundabout way. We decrease a different cost function *J(θ) *with the expectation
    that doing so will enhance *P*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is as opposed to pure optimization, where minimizing *J* is an objective
    all by itself. Optimization algorithms for preparing deep learning models likewise
    commonly incorporate some specialization on the specific structure of machine
    learning target functions.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many good and tested libraries for deep learning in popular programming
    languages:'
  prefs: []
  type: TYPE_NORMAL
- en: Theano (Python)  can utilize both CPU and GPU (from the MILA Lab at the University
    of Montreal)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Torch (Lua) is a Matlab-like environment (from Ronan Collobert, Clement Farabet,
    and Koray Kavukcuoglu)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensorflow (Python) makes use of data flow graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MXNet (Python, R, Julia, C++)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Caffe is the most popular and widely used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keras (Python) based on Theano
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mocha (Julia) by Chiyuan Zhang
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will mainly go through Mocha for Julia, which is an amazing package written
    by Chiyuan Zhang, a PhD student at MIT.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, add the package as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Network architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Network architecture in Mocha refers to a set of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The input of the `ip_layer` has the same name as the output of the `data_layer`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The same name connects them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A topological sort is carried out by Mocha on a collection of layers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These layers read information from the source and feed them to the top layers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Computation layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These take the input stream from the base layers, do the calculations, and feed
    the results generated to the top layers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These layers take processed results (and ground truth names/labels) from the
    base layers and figure a scalar loss value
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss values from all the layers and regularizers in a net are included to characterize
    the final loss function of the net
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The net parameters in the back propagation are trained with the help of the
    loss function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistics layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These take information from the base layers and generate valuable insights like
    classification accuracy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Insights are gathered all through the various iterations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reset_statistics` can be utilized to unequivocally reset the statistics aggregation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Utility Layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neurons (activation functions)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's understand real neural nets (brains). Neuroscience is the study of the
    functioning of the brain and has given us good evidence about the way in which
    it works. Neurons are the real information storage of the brain. It is also very
    important to understand their connection strengths, namely how strongly one neuron
    can influence those neurons connected to it.
  prefs: []
  type: TYPE_NORMAL
- en: Learning or the repetition of a task and exposure to new stimulating procedures
    or environment often leads to activity in the brain which is actually the neurons
    acting according to the new data being received.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons, and therefore the brain, behave very differently to different stimuli
    and environments. They may react or get excited more in some scenarios as compared
    to others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some understanding of this is important in getting to know about artificial
    neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: Neurons can be connected to any layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The neuron of every layer will influence the yield in the forward pass and the
    slope in the backward pass consequently, unless it is an identity neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, layers have an identity neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s go through the various types of neurons that we can utilize to make
    the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '`class Neurons.Identity`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is an activation function whose input is not changed.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class Neurons.ReLU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rectified Linear Unit. Amid the forward pass, this restrains all restraints
    underneath some limit *ϵ*, normally 0.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It processes point-wise *y=max(ϵ,x)*.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class Neurons.LreLU`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky Rectified Linear Unit. A Leaky ReLU can settle the "dying ReLU" issue.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU's can "die" if a sufficiently substantial gradient changes the weights
    such that the neuron never activates on new information.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class Neurons.Sigmoid`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid is a smoothed step function
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It produces roughly 0 for negative information with vast absolute values and
    estimated 1 for huge positive inputs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The point-wise equation is *y=1/(1+e−x)y=1/(1+e−x)*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class Neurons.Tanh`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tanh is a variation of Sigmoid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It takes values in *±1±1* rather than the unit interim
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The point-wise equation is *y=(1−e−2x)/(1+e−2x)*
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Neurons (activation functions)](img/image_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Understanding regularizers for ANN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We studied regularizers in our previous sections. Regularizers include additional
    penalties or restrictions for network parameters to confine the complexity of
    the model. In a popular deep-learning framework, caffe, it is known as decay.
  prefs: []
  type: TYPE_NORMAL
- en: Weight decay and regularization are comparable in back-propagation. The theoretical
    contrast in the forward pass is that when regarded as weight decay, they are not
    considered as being a piece of the objective function.
  prefs: []
  type: TYPE_NORMAL
- en: By default, Mocha similarly eliminates the forward calculation for regularizers
    with a specific end goal to decrease the quantity of calculations. We utilize
    the term regularization rather than weight decay since it is less demanding to
    comprehend.
  prefs: []
  type: TYPE_NORMAL
- en: 'class `NoRegu`: No regularization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class `L2Regu`: L2 regularizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class `L1Regu`:  L1 regularizer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Norm constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Norm restrictions are a more immediate method for limiting the complexity of
    the model by unequivocally contracting the parameters in each of the n cycles
    if the standard or norm of the parameters surpasses a given threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'class `NoCons`: No constraints'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class `L2Cons`: Restrict the Euclidean norm of parameters. Threshold and contracting
    are applied to every parameter. In particular, the threshold is applied to each
    filter for the filters parameter of a convolution layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using solvers in deep neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Mocha contains broadly useful stochastic (sub-) gradient-based solvers. These
    can be utilized to prepare deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A solver is developed by indicating a lexicon of solver parameters that give
    vital configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: General settings like stop conditions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameters particular to a specific calculation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, it is generally recommended to take some short breaks between
    training iterations to print progress or for saving a snapshot. These are called
    coffee breaks in Mocha.
  prefs: []
  type: TYPE_NORMAL
- en: '**Solver algorithms**'
  prefs: []
  type: TYPE_NORMAL
- en: 'class `SGD`: Stochastic Gradient Descent with momentum.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_policy`: Learning rate policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mom_policy`: Momentum policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class `Nesterov`: Stochastic Nesterov accelerated gradient method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_policy`: Learning rate policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mom_policy`: Momentum policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'class `Adam`: A Method for Stochastic Optimization'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_policy`: Learning rate policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta1`: Exponential decay factor for first order moment estimates. *0<=beta1<1*,
    default *0.9*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta2`: Exponential decay factor for second order moment estimates, *0<=beta1<1*,
    default *0.999*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epsilon`: Affects the scaling of the parameter updates for numerical conditioning,
    default *1e-8*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Coffee breaks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Training can become a very computationally intensive iteration of several loops.
    It is generally recommended to take some short breaks between training iterations
    to print progress or for saving a snapshot. These are called coffee breaks in
    Mocha. They are performed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This prints the training summary every 1,000 iterations and saves a snapshot
    every 5,000 iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Image classification with pre-trained Imagenet CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MNIST is a handwritten digit recognition dataset. It contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 60,000 training examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10,000 test examples
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 28 x 28 single channel grayscale images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use `get-mnist.sh` script to download the dataset
  prefs: []
  type: TYPE_NORMAL
- en: It calls `mnist.convert.jl` to convert the binary dataset into a HDF5 file that
    Mocha can read.
  prefs: []
  type: TYPE_NORMAL
- en: '`data/train.hdf5` and `data/test.hdf5` will be generated when the conversion
    finishes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are using Mocha''s native extension here to get faster convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This configures Mocha to use the native background and not the GPU (CUDA).
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will proceed with defining the network structure. We will start by defining
    a data layer that will read the HDF5 file. This will be the input for the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `source` contains a list of real data files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Mini-batches are formed to process the data. As the batch size increases, the
    variance decreases but it affects the computational performance.
  prefs: []
  type: TYPE_NORMAL
- en: Shuffling reduces the effect of ordering during the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we will proceed with defining the convolution layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '`name`: The name of the layer to identify it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_filter`: The number of convolution filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel`: The size of the filter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`bottoms`: An array to define where to get the input. (The HDF5 data layer
    that we defined.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tops`: The output of the convolution layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Define more convolution layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: These are two fully connected layers after convolution and pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: The computation to create the layer is an inner product between the input and
    the layer weights. These are also called an `InnerProductLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The layer weights are also learned, so we also give names to the two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The last inner product layer has the dimension of 10, which represents the number
    of classes (digits 0~9).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the basic structure of LeNet. To train this network, we will define
    a loss function by adding a loss layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now construct our network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Training the neural network with Stochastic Gradient Descent is performed as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The parameters used are:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_iter`: These are the maximum number of iterations the solver will execute
    to train the network'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regu_coef`: The regularization coefficient'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`mom_policy`: The momentum policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lr_policy`: The learning rate policy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`load_from`: Here we can load the saved model from a file or a directory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Add some coffee breaks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Performance is checked periodically on a separate validation set so we can see
    the progress. The validation dataset that we have will be used as the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform an evaluation, a new network is defined with the same architecture
    but a different data layer, which will get the input from the validation set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a coffee break to get the report of the validation performance, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, start the training, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the two networks we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Image classification with pre-trained Imagenet CNN](img/image_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we run the model generated on the test data that we have. We get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about deep learning and how different it is from
    machine learning. Deep learning refers to a class of machine learning techniques
    that perform unsupervised or supervised feature extraction and pattern analysis
    or classification by exploiting multiple layers of non-linear information processing.
  prefs: []
  type: TYPE_NORMAL
- en: We studied deep feedforward networks, regularization, and optimizing deep learning
    models. We also learned how to create a neural network to classify hand-written
    digits using Mocha in Julia.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[http://docs.julialang.org/en/release-0.4/manual/](http://docs.julialang.org/en/release-0.4/manual/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/pluskid/Mocha.jl](https://github.com/pluskid/Mocha.jl)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://psych.utoronto.ca/users/reingold/courses/ai/nn.html](http://psych.utoronto.ca/users/reingold/courses/ai/nn.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/DeepLearning-NowPublishing-Vol7-SIG-039.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.deeplearningbook.org/contents/intro.html](http://www.deeplearningbook.org/contents/intro.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://deeplearning.net/tutorial/deeplearning.pdf](http://deeplearning.net/tutorial/deeplearning.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
