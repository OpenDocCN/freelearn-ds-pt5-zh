- en: '*Chapter 1*'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Data Science and Data Pre-Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Learning Objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By the end of this chapter, you will be able to:'
  prefs: []
  type: TYPE_NORMAL
- en: Use various Python machine learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle missing data and deal with outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data integration to bring together data from different sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform data transformation to convert data into a machine-readable form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale data to avoid problems with values of different magnitudes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Split data into train and test datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the different types of machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Describe the different performance measures of a machine learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter introduces data science and covers the various processes included
    in the building of machine learning models, with a particular focus on pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We live in a world where we are constantly surrounded by data. As such, being
    able to understand and process data is an absolute necessity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Science is a field that deals with the description, analysis, and prediction
    of data. Consider an example from our daily lives: every day, we utilize multiple
    social media applications on our phones. These applications gather and process
    data in order to create a more personalized experience for each user – for example,
    showing us news articles that we may be interested in, or tailoring search results
    according to our location. This branch of data science is known as **machine learning**.'
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning is the methodical learning of procedures and statistical representations
    that computers use to accomplish tasks without human intervention. In other words,
    it is the process of teaching a computer to perform tasks by itself without explicit
    instructions, relying only on patterns and inferences. Some common uses of machine
    learning algorithms are in email filtering, computer vision, and computational
    linguistics.
  prefs: []
  type: TYPE_NORMAL
- en: This book will focus on machine learning and other aspects of data science using
    Python. Python is a popular language for data science, as it is versatile and
    relatively easy to use. It also has several ready-made libraries that are well
    equipped for processing data.
  prefs: []
  type: TYPE_NORMAL
- en: Python Libraries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout this book, we'll be using various Python libraries, including pandas,
    Matplotlib, Seaborn, and scikit-learn.
  prefs: []
  type: TYPE_NORMAL
- en: '**pandas**'
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas is an open source package that has many functions for loading and processing
    data in order to prepare it for machine learning tasks. It also has tools that
    can be used to analyze and manipulate data. Data can be read from many formats
    using pandas. We will mainly be using CSV data throughout this book. To read CSV
    data, you can use the `read_csv()` function by passing `filename.csv` as an argument.
    An example of this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, `pd` is an alias name given to pandas. It is not mandatory
    to give an alias. To visualize a pandas DataFrame, you can use the `head()` function
    to list the top five rows. This will be demonstrated in one of the following exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Please visit the following link to learn more about pandas: https://pandas.pydata.org/pandas-docs/stable/.'
  prefs: []
  type: TYPE_NORMAL
- en: '**NumPy**'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy is one of the main packages that Python has to offer. It is mainly used
    in practices related to scientific computing and when working on mathematical
    operations. It comprises of tools that enable us to work with arrays and array
    objects.
  prefs: []
  type: TYPE_NORMAL
- en: '**Matplotlib**'
  prefs: []
  type: TYPE_NORMAL
- en: Matplotlib is a data visualization package. It is useful for plotting data points
    in a 2D space with the help of NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Seaborn**'
  prefs: []
  type: TYPE_NORMAL
- en: Seaborn is also a data visualization library that is based on matplotlib. Visualizations
    created using Seaborn are far more attractive than ones created using matplotlib
    in terms of graphics.
  prefs: []
  type: TYPE_NORMAL
- en: '**scikit-learn**'
  prefs: []
  type: TYPE_NORMAL
- en: scikit-learn is a Python package used for machine learning. It is designed in
    such a way that it interoperates with other numeric and scientific libraries in
    Python to achieve the implementation of algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: These ready-to-use libraries have gained interest and attention from developers,
    especially in the data science space. Now that we have covered the various libraries
    in Python, in the next section we'll explore the roadmap for building machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Roadmap for Building Machine Learning Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The roadmap for building machine learning models is straightforward and consists
    of five major steps, which are explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Pre-processing**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is the first step in building a machine learning model. Data pre-processing
    refers to the transformation of data before feeding it into the model. It deals
    with the techniques that are used to convert unusable raw data into clean reliable
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since data collection is often not performed in a controlled manner, raw data
    often contains outliers (for example, age = 120), nonsensical data combinations
    (for example, model: bicycle, type: 4-wheeler), missing values, scale problems,
    and so on. Because of this, raw data cannot be fed into a machine learning model
    because it might compromise the quality of the results. As such, this is the most
    important step in the process of data science.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model Learning**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After pre-processing the data and splitting it into train/test sets (more on
    this later), we move on to modeling. Models are nothing but sets of well-defined
    methods called algorithms that use pre-processed data to learn patterns, which
    can later be used to make predictions. There are different types of learning algorithms,
    including supervised, semi-supervised, unsupervised, and reinforcement learning.
    These will be discussed later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model Evaluation**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this stage, the models are evaluated with the help of specific performance
    metrics. With these metrics, we can go on to tune the hyperparameters of a model
    in order to improve it. This process is called **hyperparameter optimization**.
    We will repeat this step until we are satisfied with the performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Prediction**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once we are happy with the results from the evaluation step, we will then move
    on to predictions. Predictions are made by the trained model when it is exposed
    to a new dataset. In a business setting, these predictions can be shared with
    decision makers to make effective business choices.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model Deployment**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The whole process of machine learning does not just stop with model building
    and prediction. It also involves making use of the model to build an application
    with the new data. Depending on the business requirements, the deployment may
    be a report, or it may be some repetitive data science steps that are to be executed.
    After deployment, a model needs proper management and maintenance at regular intervals
    to keep it up and running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This chapter will mainly focus on pre-processing. We will cover the different
    tasks involved in data pre-processing, such as data representation, data cleaning,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: Data Representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main objective of machine learning is to build models that understand data
    and find underlying patterns. In order to do so, it is very important to feed
    the data in a way that is interpretable by the computer. To feed the data into
    a model, it must be represented as a table or a matrix of the required dimensions.
    Converting your data into the correct tabular form is one of the first steps before
    pre-processing can properly begin.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Represented in a Table**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data should be arranged in a two-dimensional space made up of rows and columns.
    This type of data structure makes it easy to understand the data and pinpoint
    any problems. An example of some raw data stored as a CSV (**comma separated values**)
    file is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.1: Raw data in CSV format](img/C13322_01_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Raw data in CSV format'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The representation of the same data in a table is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.2: CSV data in table format](img/C13322_01_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: CSV data in table format'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: If you compare the data in CSV and table formats, you will see that there are
    missing values in both. We will cover what to do with these later in the chapter.
    To load a CSV file and work on it as a table, we use the pandas library. The data
    here is loaded into tables called DataFrames.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To learn more about pandas, visit the following link: http://pandas.pydata.org/pandas-docs/version/0.15/tutorials.html.'
  prefs: []
  type: TYPE_NORMAL
- en: Independent and Target Variables
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DataFrame that we use contains variables or features that can be classified
    into two categories. These are independent variables (also called **predictor
    variables**) and dependent variables (also called **target variables**). Independent
    variables are used to predict the target variable. As the name suggests, independent
    variables should be independent of each other. If they are not, this will need
    to be addressed in the pre-processing (cleaning) stage.
  prefs: []
  type: TYPE_NORMAL
- en: '**Independent Variables**'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are all the features in the DataFrame except the **target variable**.
    They are of size (m, n), where m is the number of observations and n is the number
    of features. These variables must be normally distributed and should NOT contain:'
  prefs: []
  type: TYPE_NORMAL
- en: Missing or NULL values
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly categorical data features or high cardinality (these terms will be covered
    in more detail later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outliers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data on different scales
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multicollinearity (independent variables that are correlated)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very large independent feature sets (too many independent variables to be manageable)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Special characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature Matrix and Target Vector**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A single piece of data is called a scalar. A group of scalars is called a vector,
    and a group of vectors is called a matrix. A matrix is represented in rows and
    columns. Feature matrix data is made up of independent columns, and the target
    vector depends on the feature matrix columns. To get a better understanding of
    this, let''s look at the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.3: Table containing car details](img/C13322_01_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Table containing car details'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As you can see in the table, there are various columns: Car Model, Car Capacity,
    Car Brand, and Car Price. All columns except Car Price are independent variables
    and represent the feature matrix. Car Price is the dependent variable that depends
    on the other columns (Car Model, Car Capacity, and Car Brand). It is a target
    vector because it depends on the feature matrix data. In the next section, we''ll
    go through an exercise based on features and a target matrix to get a thorough
    understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: All exercises and activities will be primarily developed in Jupyter Notebook.
    It is recommended to keep a separate notebook for different assignments unless
    advised not to. Also, to load a sample dataset, the pandas library will be used,
    because it displays the data as a table. Other ways to load data will be explained
    in further sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 1: Loading a Sample Dataset and Creating the Feature Matrix and Target
    Matrix'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be loading the `House_price_prediction` dataset into
    the pandas DataFrame and creating feature and target matrices. The `House_price_prediction`
    dataset is taken from the UCI Machine Learning Repository. The data was collected
    from various suburbs of the USA and consists of 5,000 entries and 6 features related
    to houses. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `House_price_prediction` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add the following code to import pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need to load the dataset into a pandas DataFrame. As the dataset is
    a CSV file, we''ll be using the `read_csv()` function to read the data. Add the
    following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see in the preceding code, the data is stored in a variable named
    `df`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To print all the column names of the DataFrame, we''ll use the `df.columns`
    command. Write the following code in the notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.4: List of columns present in the dataframe](img/C13322_01_04.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.4: List of columns present in the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The dataset contains n number of data points. We can find the total number
    of rows using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.5: Total Index in the dataframe](img/C13322_01_05.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.5: Total Index in the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see in the preceding figure, our dataset contains 5000 rows, from
    index 0 to 5000.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: You can use the `set_index()` function in pandas to convert a column into an
    index of rows in a DataFrame. This is a bit like using the values in that column
    as your row labels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Dataframe.set_index(''column name'', inplace = True'')''`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s set the `Address` column as an index and reset it back to the original
    DataFrame. The pandas library provides the `set_index()` method to convert a column
    into an index of rows in a DataFrame. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.6: DataFrame with an indexed Address column](img/C13322_01_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.6: DataFrame with an indexed Address column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The `inplace` parameter in the `set_index()` function is by default set to `False`.
    If the value is changed to `True`, then whatever operation we perform the content
    of the DataFrame changes directly without the copy being created.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In order to reset the index of the given object, we use the `reset_index()`
    function. Write the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.7: DataFrame with the index reset](img/C13322_01_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.7: DataFrame with the index reset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The index is like a name given to a row and column. Rows and columns both have
    an index. You can index by row/column number or row/column name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can retrieve the first four rows and the first three columns using a row
    number and column number. This can be done using the `iloc` indexer in pandas,
    which retrieves data using index positions. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.8: Dataset of four rows and three columns](img/C13322_01_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.8: Dataset of four rows and three columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To retrieve the data using labels, we use the `loc` indexer. Add the following
    code to retrieve the first five rows of the Income and Age columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.9: Dataset of five rows and two columns](img/C13322_01_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.9: Dataset of five rows and two columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now create a variable called `X` to store the independent features. In our
    dataset, we will consider all features except Price as independent variables,
    and we will use the `drop()` function to include them. Once this is done, we print
    out the top five instances of the `X` variable. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.10: Dataset showing the first five rows of the feature matrix](img/C13322_01_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.10: Dataset showing the first five rows of the feature matrix'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The default number of instances that will be taken for the head is five, so
    if you don't specify the number then it will by default output five observations.
    The axis parameter in the preceding screenshot denotes whether you want to drop
    the label from rows (axis = 0) or columns (axis = 1).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of your newly created feature matrix using the `X.shape` command.
    Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/C13322_01_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.11: Shape of the feature matrix'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, the first value indicates the number of observations
    in the dataset (**5000**), and the second value represents the number of features
    (**6**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Similarly, we will create a variable called `y` that will store the target
    values. We will use indexing to grab the target column. Indexing allows you to
    access a section of a larger element. In this case, we want to grab the column
    named Price from the `df` DataFrame. Then, we want to print out the top 10 values
    of the variable. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.12: Dataset showing the first 10 rows of the target matrix](img/C13322_01_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.12: Dataset showing the first 10 rows of the target matrix'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the shape of your new variable using the `y.shape` command. The shape
    should be one-dimensional, with a length equal to the number of observations (**5000**)
    only. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.13: Shape of the target matrix](img/C13322_01_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Shape of the target matrix'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully created the feature and target matrices of a dataset.
    You have completed the first step in the process of building a predictive model.
    This model will learn the patterns from the feature matrix (columns in `X`) and
    how they map to the values in the target vector (`y`). These patterns can then
    be used to predict house prices from new data based on the features of those new
    houses.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore more steps involved in pre-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Data Cleaning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data cleaning includes processes such as filling in missing values and handling
    inconsistencies. It detects corrupt data and replaces or modifies it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Missing Values**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of missing values is important to understand if you want to master
    the skill of successful management and understanding of data. Let''s take a look
    at the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.14: Bank customer credit data](img/C13322_01_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: Bank customer credit data'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As you can see, the data belongs to a bank; each row is a separate customer
    and each column contains their details, such as age and credit amount. There are
    some cells that have either **NA** or are just empty. This is missing data. Each
    piece of information about the customer is crucial for the bank. If any of the
    information is missing, then it will be difficult for the bank to predict the
    risk of providing a loan to the customer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Handling Missing Data**'
  prefs: []
  type: TYPE_NORMAL
- en: Intelligent handling of missing data will result in building a robust model
    capable of handling complex tasks. There are many ways to handle missing data.
    Let's now look at some of those ways.
  prefs: []
  type: TYPE_NORMAL
- en: '**Removing the Data**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Checking missing values is the first and the most important step in data pre-processing.
    A model cannot accept data with missing values. This is a very simple and commonly
    used method to handle missing values: we delete a row if the missing value corresponds
    to the places in the row, or we delete a column if it has more than 70%-75% of
    missing data. Again, the threshold value is not fixed and depends on how much
    you wish to fix.'
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of this approach is that it is quick and easy to do, and in many
    cases no data is better than bad data. The drawback is that you may end up losing
    important information, because you're deleting a whole feature based on a few
    missing values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2: Removing Missing Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be loading the `Banking_Marketing.csv` dataset into
    the pandas DataFrame and handling the missing data. This dataset is related to
    direct marketing campaigns of a Portuguese banking institution. The marketing
    campaigns involved phone calls to clients to try and get them to subscribe to
    a particular product. The dataset contains the details of each client contacted,
    and whether they subscribed to the product. Follow these steps to complete this
    exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Banking_Marketing.csv` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook. Insert a new cell and add the following code to import
    pandas and fetch the `Banking_Marketing.csv` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you have fetched the dataset, print the datatype of each column. To do
    so, use the `dtypes` attribute from the pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.15: Data types of each feature](img/C13322_01_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.15: Data types of each feature'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now we need to find the missing values for each column. In order to do that,
    we use the `isna()` function provided by pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.16: Missing values of each column in the dataset](img/C13322_01_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.16: Missing values of each column in the dataset'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, we can see that there is data missing from three columns,
    namely `age`, `contact`, and `duration`. There are two NAs in the **age** column,
    six NAs in **contact**, and seven NAs in **duration**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once you have figured out all the missing details, we remove all the missing
    rows from the DataFrame. To do so, we use the `dropna()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To check whether the missing vales are still present, use the `isna()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.17: Each column of the dataset with zero missing values](img/C13322_01_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.17: Each column of the dataset with zero missing values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully removed all missing data from the DataFrame. In the next
    section, we'll look at the second method of dealing with missing data, which uses
    imputation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Mean/Median/Mode Imputation**'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of numerical data, we can compute its mean or median and use the
    result to replace missing values. In the case of the categorical (non-numerical)
    data, we can compute its mode to replace the missing value. This is known as imputation.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of using imputation, rather than just removing data, is that it
    prevents data loss. The drawback is that you don't know how accurate using the
    mean, median, or mode is going to be in a given situation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at an exercise in which we will use imputation method to solve missing
    data problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3: Imputing Missing Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be loading the `Banking_Marketing.csv` dataset into
    the pandas DataFrame and handle the missing data. We''ll make use of the imputation
    method. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Banking_Marketing.csv` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Load the dataset into the pandas
    DataFrame. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Impute the numerical data of the `age` column with its mean. To do so, first
    find the mean of the `age` column using the `mean()` function of pandas, and then
    print it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.18: Mean of the age column](img/C13322_01_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.18: Mean of the age column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once this is done, impute the missing data with its mean using the `fillna()`
    function. This can be done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we impute the numerical data of the duration column with its median. To
    do so, first find the median of the duration column using the `median()` function
    of the pandas. Add the following code to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.19: Median of the duration](img/C13322_01_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.19: Median of the duration'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: Impute the missing data of the duration with its median using the `fillna()`
    function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Impute the categorical data of the contact column with its mode. To do so,
    first, find the mode of the contact column using the `mode()` function of pandas.
    Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/C13322_01_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.20: Mode of the contact'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Impute the missing data of the contact column with its mode using the `fillna()`
    function. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Unlike mean and median, there may be more than one mode in a column. So, we
    just take the first mode with index 0.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You have successfully imputed the missing data in different ways and made the
    data complete and clean.
  prefs: []
  type: TYPE_NORMAL
- en: Another part of data cleaning is dealing with outliers, which will be discussed
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**Outliers**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Outliers are values that are very large or very small with respect to the distribution
    of the other data. We can only find outliers in numerical data. Box plots are
    one good way to find the outliers in a dataset, as you can see in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.21: Sample of outliers in a box plot](img/C13322_01.21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.21: Sample of outliers in a box plot'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: An outlier is not always bad data! With the help of business understanding and
    client interaction, you can discern whether to remove or retain the outlier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s learn how to find outliers using a simple example. Consider a sample
    dataset of temperatures from a place at different times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll sort the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Next, we'll calculate the median (Q2). The median is the middle data after sorting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, the middle terms are 70 and 71 after sorting the list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The median is *(70 + 71) / 2 = 70.5*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we'll calculate the lower quartile (Q1). Q1 is the middle value (median)
    of the first half of the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First half of the data = `60, 69, 70, 70, 70, 70`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Points 3 and 4 of the bottom 6 are both equal to 70.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The average is *(70 + 70) / 2 = 70*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Q1 = 70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we calculate the upper quartile (Q3).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Q3 is the middle value (median) of the second half of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Second half of the data = `71, 71, 72, 72, 90, 320`
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Points 3 and 4 of the upper 6 are 72 and 72.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The average is *(72 + 72) / 2 = 72*
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Q3 = 72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then we find the interquartile range (IQR).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: IQR = Q3 – Q1 = 72 – 70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IQR = 2
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we find the upper and lower fences.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lower fence = Q1 – 1.5 (IQR) = 70 – 1.5(2) = 67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Upper fence = Q3 + 1.5 (IQR) = 71.5 + 1.5(2) = 74.5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Boundaries of our fences = 67 and 74.5
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Any data points lower than the lower fence and greater than the upper fence
    are outliers. Thus, the outliers from our example are 60, 90 and 320.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 4: Finding and Removing Outliers in Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be loading the `german_credit_data.csv` dataset into
    the pandas DataFrame and removing the outliers. The dataset contains 1,000 entries
    with 20 categorial/symbolic attributes prepared by Prof. Hofmann. In this dataset,
    each entry represents a person who takes credit from a bank. Each person is classified
    as a good or bad credit risk according to the set of attributes. Follow these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The link to the `german_credit_data.csv` dataset can be found here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/german_credit_data.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the following code to import
    the necessary libraries: pandas, NumPy, matplotlib, and seaborn. Fetch the dataset
    and load it into the pandas DataFrame. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `%matplotlib inline` is a magic function that is essential
    if we want the plot to be visible in the notebook.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This dataset contains an `Age` column. Let''s plot a boxplot of the `Age` column.
    To do so, use the `boxplot()` function from the seaborn library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.22: A box plot of the Age column](img/C13322_01_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.22: A box plot of the Age column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: We can see that some data points are outliers in the boxplot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The boxplot uses the IQR method to display the data and the outliers (the shape
    of the data). But in order to print an outlier, we use a mathematical formula
    to retrieve it. Add the following code to find the outliers of the `Age` column
    using the IQR method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, Q1 is the first quartile and Q3 is the third quartile.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we find the upper fence and lower fence by adding the following code, and
    print all the data above the upper fence and below the lower fence. Add the following
    code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To print all the data above the upper fence and below the lower fence, add
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.23: Outlier data based on the Age column](img/C13322_01_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.23: Outlier data based on the Age column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Filter out the outlier data and print only the potential data. To do so, just
    negate the preceding result using the `~` operator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.24: Potential data based on the Age column](img/C13322_01_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.24: Potential data based on the Age column'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully found the outliers using the IQR. In the next section,
    we will explore another method of pre-processing called data integration.
  prefs: []
  type: TYPE_NORMAL
- en: Data Integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've made sure to remove the impurities in data and make it clean.
    Now, the next step is to combine data from different sources to get a unified
    structure with more meaningful and valuable information. This is mostly used if
    the data is segregated into different sources. To make it simple, let's assume
    we have data in CSV format in different places, all talking about the same scenario.
    Say we have some data about an employee in a database. We can't expect all the
    data about the employee to reside in the same table. It's possible that the employee's
    personal data will be located in one table, the employee's project history will
    be in a second table, the employee's time-in and time-out details will be in another
    table, and so on. So, if we want to do some analysis about the employee, we need
    to get all the employee data in one common place. This process of bringing data
    together in one place is called data integration. To do data integration, we can
    merge multiple pandas DataFrames using the `merge` function.
  prefs: []
  type: TYPE_NORMAL
- en: Let's solve an exercise based on data integration to get a clear understanding
    of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 5: Integrating Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we''ll merge the details of students from two datasets, namely
    `student.csv` and `marks.csv`. The `student` dataset contains columns such as
    `Age`, `Gender`, `Grade`, and `Employed`. The `marks.csv` dataset contains columns
    such as `Mark` and `City`. The `Student_id` column is common between the two datasets.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `student.csv` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `marks.csv` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/mark.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the following code to import
    pandas and load the `student.csv` and `marks.csv` datasets into the `df1` and
    `df2` pandas DataFrames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To print the first five rows of the first DataFrame, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.25: The first five rows of the first DataFrame](img/C13322_01_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.25: The first five rows of the first DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To print the first five rows of the second DataFrame, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.26: The first five rows of the second DataFrame](img/C13322_01_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.26: The first five rows of the second DataFrame'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: '`Student_id` is common to both datasets. Perform data integration on both the
    DataFrames with respect to the `Student_id` column using the `pd.merge()` function,
    and then print the first 10 values of the new DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.27: First 10 rows of the merged DataFrame](img/C13322_01_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.27: First 10 rows of the merged DataFrame'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Here, the data of the `df1` DataFrame is merged with the data of the `df2` DataFrame.
    The merged data is stored inside a new DataFrame called `df`.
  prefs: []
  type: TYPE_NORMAL
- en: We have now learned how to perform data integration. In the next section, we'll
    explore another pre-processing task, data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Data Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we saw how we can combine data from different sources into a unified
    dataframe. Now, we have a lot of columns that have different types of data. Our
    goal is to transform the data into a machine-learning-digestible format. All machine
    learning algorithms are based on mathematics. So, we need to convert all the columns
    into numerical format. Before that, let's see all the different types of data
    we have.
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking a broader perspective, data is classified into numerical and categorical
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Numerical**: As the name suggests, this is numeric data that is quantifiable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Categorical**: The data is a string or non-numeric data that is qualitative
    in nature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Numerical data is further divided into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Discrete**: To explain in simple terms, any numerical data that is countable
    is called discrete, for example, the number of people in a family or the number
    of students in a class. Discrete data can only take certain values (such as 1,
    2, 3, 4, etc).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Continuous**: Any numerical data that is measurable is called continuous,
    for example, the height of a person or the time taken to reach a destination.
    Continuous data can take virtually any value (for example, 1.25, 3.8888, and 77.1276).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical data is further divided into the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ordered**: Any categorical data that has some order associated with it is
    called ordered categorical data, for example, movie ratings (excellent, good,
    bad, worst) and feedback (happy, not bad, bad). You can think of ordered data
    as being something you could mark on a scale.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nominal**: Any categorical data that has no order is called nominal categorical
    data. Examples include gender and country.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From these different types of data, we will focus on categorical data. In the
    next section, we'll discuss how to handle categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: Handling Categorical Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are some algorithms that can work well with categorical data, such as
    decision trees. But most machine learning algorithms cannot operate directly with
    categorical data. These algorithms require the input and output both to be in
    numerical form. If the output to be predicted is categorical, then after prediction
    we convert them back to categorical data from numerical data. Let''s discuss some
    key challenges that we face while dealing with categorical data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**High cardinality**: Cardinality means uniqueness in data. The data column,
    in this case, will have a lot of different values. A good example is User ID –
    in a table of 500 different users, the User ID column would have 500 unique values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rare occurrences**: These data columns might have variables that occur very
    rarely and therefore would not be significant enough to have an impact on the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Frequent occurrences**: There might be a category in the data columns that
    occurs many times with very low variance, which would fail to make an impact on
    the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Won''t fit**: This categorical data, left unprocessed, won''t fit our model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: To address the problems associated with categorical data, we can use encoding.
    This is the process by which we convert a categorical variable into a numerical
    form. Here, we will look at three simple methods of encoding categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Replacing**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a technique in which we replace the categorical data with a number.
    This is a simple replacement and does not involve much logical processing. Let's
    look at an exercise to get a better idea of this.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6: Simple Replacement of Categorical Data with a Number'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will use the `student` dataset that we saw earlier. We
    will load the data into a pandas dataframe and simply replace all the categorical
    data with numbers. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `student` dataset can be found at this location: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/student.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the following code to import
    pandas and then load the dataset into the pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Find the categorical column and separate it out with a different dataframe.
    To do so, use the `select_dtypes()` function from pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.28: Categorical columns of the dataframe](img/C13322_01_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.28: Categorical columns of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Find the distinct unique values in the `Grade` column. To do so, use the `unique()`
    function from pandas with the column name:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.29: Unique values in the Grade column](img/C13322_01_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.29: Unique values in the Grade column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Find the frequency distribution of each categorical column. To do so, use the
    `value_counts()` function on each column. This function returns the counts of
    unique values in an object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this step is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.30: Total count of each unique value in the Grade column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.30: Total count of each unique value in the Grade column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For the `Gender` column, write the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.31: Total count of each unique value in the Gender column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.31: Total count of each unique value in the Gender column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Similarly, for the `Employed` column, write the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.32: Total count of each unique value in the Employed column'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.32: Total count of each unique value in the Employed column'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Replace the entries in the `Grade` column. Replace `1st class` with `1`, `2nd
    class` with `2`, and `3rd class` with `3`. To do so, use the `replace()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace the entries in the `Gender` column. Replace `Male` with `0` and `Female`
    with `1`. To do so, use the `replace()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Replace the entries in the `Employed` column. Replace `no` with `0` and `yes`
    with `1`. To do so, use the `replace()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once all the replacements for three columns are done, we need to print the
    dataframe. Add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 1.33: Numerical data after replacement'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.33: Numerical data after replacement'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully converted the categorical data to numerical data using
    a simple manual replacement method. We will now move on to look at another method
    of encoding categorical data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Label Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: This is a technique in which we replace each value in a categorical column with
    numbers from 0 to N-1\. For example, say we've got a list of employee names in
    a column. After performing label encoding, each employee name will be assigned
    a numeric label. But this might not be suitable for all cases because the model
    might consider numeric values to be weights assigned to the data. Label encoding
    is the best method to use for ordinal data. The scikit-learn library provides
    `LabelEncoder()`, which helps with label encoding. Let's look at an exercise in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7: Converting Categorical Data to Numerical Data Using Label Encoding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the `Banking_Marketing.csv` dataset into a pandas
    dataframe and convert categorical data to numeric data using label encoding. Follow
    these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Banking_Marketing.csv` dataset can be found here: https://github.com/TrainingByPackt/Master-Data-Science-with-Python/blob/master/Chapter%201/Data/Banking_Marketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the code to import pandas
    and load the dataset into the pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before doing the encoding, remove all the missing data. To do so, use the `dropna()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select all the columns that are not numeric using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To understand how the selection looks, refer to the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.34: Non-numeric columns of the dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.34: Non-numeric columns of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the first five rows of the new dataframe. Add the following code to do
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.35: Non-numeric values for the columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.35: Non-numeric values for the columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Iterate through this `category` column and convert it to numeric data using
    `LabelEncoder()`. To do so, import the `sklearn.preprocessing` package and use
    the `LabelEncoder()` class to transform the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.36: Values of non-numeric columns converted into numeric form'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.36: Values of non-numeric columns converted into numeric form'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding screenshot, we can see that all the values have been converted
    from categorical to numerical. Here, the original values have been transformed
    and replaced with the newly encoded data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have successfully converted categorical data to numerical data using the
    `LabelEncoder` method. In the next section, we''ll explore another type of encoding:
    one-hot encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: '**One-Hot Encoding**'
  prefs: []
  type: TYPE_NORMAL
- en: In label encoding, categorical data is converted to numerical data, and the
    values are assigned labels (such as 1, 2, and 3). Predictive models that use this
    numerical data for analysis might sometimes mistake these labels for some kind
    of order (for example, a model might think that a label of 3 is "better" than
    a label of 1, which is incorrect). In order to avoid this confusion, we can use
    one-hot encoding. Here, the label-encoded data is further divided into n number
    of columns. Here, n denotes the total number of unique labels generated while
    performing label encoding. For example, say that three new labels are generated
    through label encoding. Then, while performing one-hot encoding, the columns will
    be divided into three parts. So, the value of n is 3\. Let's look at an exercise
    to get further clarification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 8: Converting Categorical Data to Numerical Data Using One-Hot Encoding'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the `Banking_Marketing.csv` dataset into a pandas
    dataframe and convert the categorical data into numeric data using one-hot encoding.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Banking_Marketing` dataset can be found here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the code to import pandas
    and load the dataset into a pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Before doing the encoding, remove all the missing data. To do so, use the `dropna()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select all the columns that are not numeric using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.37: Non-numeric columns of the dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.37: Non-numeric columns of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the first five rows of the new dataframe. Add the following code to do
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.38: Non-numeric values for the columns'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.38: Non-numeric values for the columns'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Iterate through these category columns and convert them to numeric data using
    `OneHotEncoder`. To do so, import the `sklearn.preprocessing` package and avail
    yourself of the `OneHotEncoder()` class do the transformation. Before performing
    one-hot encoding, we need to perform label encoding:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.39: Values of non-numeric columns converted into numeric data](img/C13322_01_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 1.39: Values of non-numeric columns converted into numeric data'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Once we have performed label encoding, we execute one-hot encoding. Add the
    following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create a new dataframe with the encoded data and print the first five
    rows. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.40: Columns with one-hot encoded values'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.40: Columns with one-hot encoded values'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Due to one-hot encoding, the number of columns in the new dataframe has increased.
    In order to view and print all the columns created, use the `columns` attribute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.41: List of new columns generated after one-hot encoding'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.41: List of new columns generated after one-hot encoding'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For every level or category, a new column is created. In order to prefix the
    category name with the column name you can use this alternate way to create one-hot
    encoding. In order to prefix the category name with the column name, write the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.42: List of new columns containing the categories'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_42.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.42: List of new columns containing the categories'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully converted categorical data to numerical data using the
    `OneHotEncoder` method.
  prefs: []
  type: TYPE_NORMAL
- en: We will now move onto another data preprocessing step – how to deal with a range
    of magnitudes in your data.
  prefs: []
  type: TYPE_NORMAL
- en: Data in Different Scales
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In real life, values in a dataset might have a variety of different magnitudes,
    ranges, or scales. Algorithms that use distance as a parameter may not weigh all
    these in the same way. There are various data transformation techniques that are
    used to transform the features of our data so that they use the same scale, magnitude,
    or range. This ensures that each feature has an appropriate effect on a model's
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Some features in our data might have high-magnitude values (for example, annual
    salary), while others might have relatively low values (for example, the number
    of years worked at a company). Just because some data has smaller values does
    not mean it is less significant. So, to make sure our prediction does not vary
    because of different magnitudes of features in our data, we can perform feature
    scaling, standardization, or normalization (these are three similar ways of dealing
    with magnitude issues in data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 9: Implementing Scaling Using the Standard Scaler Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the `Wholesale customer''s data.csv` dataset
    into the pandas dataframe and perform scaling using the standard scaler method.
    This dataset refers to clients of a wholesale distributor. It includes the annual
    spending in monetary units on diverse product categories. Follow these steps to
    complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Wholesale customer` dataset can be found here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the code to import pandas
    and load the dataset into the pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether there is any missing data. If there is, drop the missing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.43: Different columns of the dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.43: Different columns of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, there are eight columns present in the dataframe, all of type
    `int64`. Since the null value is `False`, it means there are no null values present
    in any of the columns. Thus, there is no need to use the `dropna()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now perform standard scaling and print the first five rows of the new dataset.
    To do so, use the `StandardScaler()` class from `sklearn.preprocessing` and implement
    the `fit_transorm()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.44: Data of the features scaled into a uniform unit'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_44.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.44: Data of the features scaled into a uniform unit'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the `StandardScaler` method, we have scaled the data into a uniform unit
    over all the columns. As you can see in the preceding table, the values of all
    the features have been converted into a uniform range of the same scale. Because
    of this, it becomes easier for the model to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: You have successfully scaled the data using the `StandardScaler` method. In
    the next section, we'll have a go at an exercise in which we'll implement scaling
    using the `MinMax` scaler method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 10: Implementing Scaling Using the MinMax Scaler Method'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will be loading the `Wholesale customers data.csv` dataset
    into a pandas dataframe and perform scaling using the `MinMax` scaler method.
    Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Whole customers data.csv` dataset can be found here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Wholesale%20customers%20data.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the following code to import
    the pandas library and load the dataset into a pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether there is any missing data. If there is, drop the missing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.45: Different columns of the dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.45: Different columns of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: As we can see, there are eight columns present in the dataframe, all of type
    `int64`. Since the null value is `False`, it means there are no null values present
    in any of the columns. Thus, there is no need to use the `dropna()` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Perform `MinMax` scaling and print the initial five values of the new dataset.
    To do so, use the `MinMaxScaler()` class from `sklearn.preprocessing` and implement
    the `fit_transorm()` method. Add the following code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.46: Data of the features scaled into a uniform unit'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_46.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.46: Data of the features scaled into a uniform unit'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Using the `MinMaxScaler` method, we have again scaled the data into a uniform
    unit over all the columns. As you can see in the preceding table, the values of
    all the features have been converted into a uniform range of the same scale. You
    have successfully scaled the data using the `MinMaxScaler` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we''ll explore another pre-processing task: data discretization.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Discretization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have done the categorical data treatment using encoding and numerical
    data treatment using scaling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data discretization** is the process of converting continuous data into discrete
    buckets by grouping it. Discretization is also known for easy maintainability
    of the data. Training a model with discrete data becomes faster and more effective
    than when attempting the same with continuous data. Although continuous-valued
    data contains more information, huge amounts of data can slow the model down.
    Here, discretization can help us strike a balance between both. Some famous methods
    of data discretization are **binning** and using a histogram. Although data discretization
    is useful, we need to effectively pick the range of each bucket, which is a challenge.'
  prefs: []
  type: TYPE_NORMAL
- en: The main challenge in discretization is to choose the number of intervals or
    bins and how to decide on their width.
  prefs: []
  type: TYPE_NORMAL
- en: Here we make use of a function called `pandas.cut()`. This function is useful
    to achieve the bucketing and sorting of segmented data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 11: Discretization of Continuous Data'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the `Student_bucketing.csv` dataset and perform
    bucketing. The dataset consists of student details such as `Student_id`, `Age`,
    `Grade`, `Employed`, and `marks`. Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Student_bucketing.csv` dataset can be found here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Student_bucketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell. Write the following code to import
    the required libraries and load the dataset into a pandas dataframe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we load the dataframe, display the first five rows of the dataframe. Add
    the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.47: First five rows of the dataframe'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_47.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.47: First five rows of the dataframe'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Perform bucketing using the `pd.cut()` function on the `marks` column and display
    the top 10 columns. The `cut()` function takes parameters such as `x`, `bins`,
    and `labels`. Here, we have used only three parameters. Add the following code
    to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.48: Marks column with five discrete buckets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_48.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.48: Marks column with five discrete buckets'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'In the preceding code, the first parameter represents an array. Here, we have
    selected the `marks` column as an array from the dataframe. `5` represents the
    number of bins to be used. As we have set bins to `5`, the labels need to be populated
    accordingly with five values: `Poor`, `Below_average`, `Average`, `Above_average`,
    and `Excellent`. In the preceding figure, we can see the whole of the continuous
    **marks** column is put into five discrete buckets. We have learned how to perform
    bucketing.'
  prefs: []
  type: TYPE_NORMAL
- en: We have now covered all the major tasks involved in pre-processing. In the next
    section, we'll look in detail at how to train and test your data.
  prefs: []
  type: TYPE_NORMAL
- en: Train and Test Data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once you've pre-processed your data into a format that's ready to be used by
    your model, you need to split up your data into train and test sets. This is because
    your machine learning algorithm will use the data in the training set to learn
    what it needs to know. It will then make a prediction about the data in the test
    set, using what it has learned. You can then compare this prediction against the
    actual target variables in the test set in order to see how accurate your model
    is. The exercise in the next section will give more clarity on this.
  prefs: []
  type: TYPE_NORMAL
- en: We will do the train/test split in proportions. The larger portion of the data
    split will be the train set and the smaller portion will be the test set. This
    will help to ensure that you are using enough data to accurately train your model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, we carry out the train-test split with an 80:20 ratio, as per the
    Pareto principle. The Pareto principle states that "for many events, roughly 80%
    of the effects come from 20% of the causes." But if you have a large dataset,
    it really doesn't matter whether it's an 80:20 split or 90:10 or 60:40\. (It can
    be better to use a smaller split set for the training set if our process is computationally
    intensive, but it might cause the problem of overfitting – this will be covered
    later in the book.)
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 12: Splitting Data into Train and Test Sets'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this exercise, we will load the `USA_Housing.csv` dataset (which you saw
    earlier) into a pandas dataframe and perform a train/test split. Follow these
    steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `USA_Housing.csv` dataset is available here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/USA_Housing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a Jupyter notebook and add a new cell to import pandas and load the dataset
    into pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `X` to store the independent features. Use the `drop()`
    function to include all the features, leaving out the dependent or the target
    variable, which in this case is named `Price`. Then, print out the top five instances
    of the variable. Add the following code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.49: Dataframe consisting of independent variables'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.49: Dataframe consisting of independent variables'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the shape of your new created feature matrix using the `X.shape` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.50: Shape of the X variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_50.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.50: Shape of the X variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: In the preceding figure, the first value indicates the number of observations
    in the dataset (**5000**), and the second value represents the number of features
    (**6**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Similarly, we will create a variable called `y` that will store the target
    values. We will use indexing to grab the target column. Indexing allows us to
    access a section of a larger element. In this case, we want to grab the column
    named Price from the `df` dataframe and print out the top 10 values. Add the following
    code to implement this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.51: Top 10 values of the y variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_51.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.51: Top 10 values of the y variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Print the shape of your new variable using the `y.shape` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.52: Shape of the y variable'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/C13322_01_52.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.52: Shape of the y variable'
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
- en: The shape should be one-dimensional, with a length equal to the number of observations
    (**5000**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Make train/test sets with an 80:20 split. To do so, use the `train_test_split()`
    function from the `sklearn.model_selection` package. Add the following code to
    do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `test_size` is a floating-point value that defines the
    size of the test data. If the value is 0.2, then it is an 80:20 split. `test_train_split`
    splits the arrays or matrices into train and test subsets in a random way. Each
    time we run the code without `random_state`, we will get a different result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Print the shape of `X_train`, `X_test`, `y_train`, and `y_test`. Add the following
    code to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code generates the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.53: Shape of train and test datasets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/C13322_01_53.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1.53: Shape of train and test datasets'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: You have successfully split the data into train and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will complete an activity wherein you'll perform pre-processing
    on a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 1: Pre-Processing Using the Bank Marketing Subscription Dataset'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this activity, we'll perform various pre-processing tasks on the `Bank Marketing
    Subscription` dataset. This dataset relates to the direct marketing campaigns
    of a Portuguese banking institution. Phone calls are made to market a new product,
    and the dataset records whether each customer subscribed to the product.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The `Bank Marketing Subscription` dataset is available here: https://github.com/TrainingByPackt/Data-Science-with-Python/blob/master/Chapter01/Data/Banking_Marketing.csv.'
  prefs: []
  type: TYPE_NORMAL
- en: Load the dataset from the link given into a pandas dataframe.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explore the features of the data by finding the number of rows and columns,
    listing all the columns, finding the basic statistics of all columns (you can
    use the `describe().transpose()` function), and listing the basic information
    of the columns (you can use the `info()` function).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check whether there are any missing (or NULL) values, and if there are, find
    how many missing values there are in each column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remove any missing values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the frequency distribution of the `education` column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `education` column of the dataset has many categories. Reduce the categories
    for better modeling.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select and perform a suitable encoding method for the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into train and test sets. The target data is in the `y` column
    and the independent data is in the remaining columns. Split the data with 80%
    for the train set and 20% for the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
- en: The solution for this activity can be found on page 324.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we've covered the various data pre-processing steps, let's look at
    the different types of machine learning that are available to data scientists
    in some more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Supervised learning is a learning system that trains using labeled data (data
    in which the target variables are already known). The model learns how patterns
    in the feature matrix map to the target variables. When the trained machine is
    fed with a new dataset, it can use what it has learned to predict the target variables.
    This can also be called predictive modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Supervised learning is broadly split into two categories. These categories
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classification** mainly deals with categorical target variables. A classification
    algorithm helps to predict which group or class a data point belongs to.'
  prefs: []
  type: TYPE_NORMAL
- en: When the prediction is between two classes, it is known as binary classification.
    An example is predicting whether or not a customer will buy a product (in this
    case, the classes are yes and no).
  prefs: []
  type: TYPE_NORMAL
- en: If the prediction involves more than two target classes, it is known as multi-classification;
    for example, predicting all the items that a customer will buy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Regression** deals with numerical target variables. A regression algorithm
    predicts the numerical value of the target variable based on the training dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression measures the link between one or more predictor variables and
    one outcome variable. For example, linear regression could help to enumerate the
    relative impacts of age, gender, and diet (the predictor variables) on height
    (the outcome variable).
  prefs: []
  type: TYPE_NORMAL
- en: '**Time series analysis**, as the name suggests, deals with data that is distributed
    with respect to time, that is, data that is in a chronological order. Stock market
    prediction and customer churn prediction are two examples of time series data.
    Depending on the requirement or the necessities, time series analysis can be either
    a regression or classification task.'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike supervised learning, the unsupervised learning process involves data
    that is neither classified nor labeled. The algorithm will perform analysis on
    the data without guidance. The job of the machine is to group unclustered information
    according to similarities in the data. The aim is for the model to spot patterns
    in the data in order to give some insight into what the data is telling us and
    to make predictions.
  prefs: []
  type: TYPE_NORMAL
- en: An example is taking a whole load of unlabeled customer data and using it to
    find patterns to cluster customers into different groups. Different products could
    then be marketed to the different groups for maximum profitability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsupervised learning is broadly categorized into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clustering**: A clustering procedure helps to discover the inherent patterns
    in the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Association**: An association rule is a unique way to find patterns associated
    with a large amount of data, such as the supposition that when someone buys product
    1, they also tend to buy product 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Reinforcement learning is a broad area in machine learning where the machine
    learns to perform the next step in an environment by looking at the results of
    actions already performed. Reinforcement learning does not have an answer, and
    the learning agent decides what should be done to perform the specified task.
    It learns from its prior knowledge. This kind of learning involves both a reward
    and a penalty.
  prefs: []
  type: TYPE_NORMAL
- en: No matter the type of machine learning you're using, you'll want to be able
    to measure how effective your model is. You can do this using various performance
    metrics. You will see how these are used in later chapters in the book, but a
    brief overview of some of the most common ones is given here.
  prefs: []
  type: TYPE_NORMAL
- en: Performance Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are different evaluation metrics in machine learning, and these depend
    on the type of data and the requirements. Some of the metrics are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1 score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **confusion matrix** is a table that is used to define the performance of
    the classification model on the test data for which the actual values are known.
    To understand this better, look at the following figure, showing predicted and
    actual values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_54.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.54: Predicted versus actual values'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s examine the concept of a confusion matrix and its metrics, TP, TN, FP,
    and FN, in detail. Assume you are building a model that predicts pregnancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '`True`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`True`, which cannot happen. This is a type of error called a Type 1 error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False`, which is also an error. This is called a Type 2 error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`False`; that is a **True Negative**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Type 1 error is a more dangerous error than the Type 2 error. Depending
    on the problem, we have to figure out whether we need to reduce Type 1 errors
    or Type 2 errors.
  prefs: []
  type: TYPE_NORMAL
- en: '**Precision**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision is the ratio of TP outcomes to the total number of positive outcomes
    predicted by a model. The precision looks at how precise our model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_55.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.55: Precision equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Recall**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall calculates what proportion of the TP outcomes our model has predicted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_56.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.56: Recall equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**Accuracy**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy calculates the ratio of the number of positive predictions made by
    a model out of the total number of predictions made:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_57.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.57: Accuracy equation'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '**F1 score**'
  prefs: []
  type: TYPE_NORMAL
- en: 'F1 score is another accuracy measure, but one that allows us to seek a balance
    between precision and recall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_58.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.58: F1-score'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When considering the performance of a model, we have to understand two other
    important concepts of prediction error: bias and variance.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is bias?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias** is how far a predicted value is from the actual value. High bias means
    the model is very simple and is not capable of capturing the data''s complexity,
    causing what''s called underfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**What is variance?**'
  prefs: []
  type: TYPE_NORMAL
- en: '**High variance** is when the model performs too well on the trained dataset.
    This causes overfitting and makes the model too specific to the train data, meaning
    the model does not perform well on test data.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/C13322_01_59.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.59: High variance'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Assume you are building a linear regression model to predict the market price
    of cars in a country. Let's say you have a large dataset about the cars and their
    prices, but there are still some more cars whose prices need to be predicted.
  prefs: []
  type: TYPE_NORMAL
- en: When we train our model with the dataset, we want our model to just find that
    pattern within the dataset, nothing more, because if it goes beyond that, it will
    start to memorize the train set.
  prefs: []
  type: TYPE_NORMAL
- en: We can improve our model by tuning its hyperparameters - there is more on this
    later in the book. We work towards minimizing the error and maximizing the accuracy
    by using another dataset, called the validation set. The first graph shows that
    the model has not learned enough to predict well in the test set. The third graph
    shows that the model has memorized the training dataset, which means the accuracy
    score will be 100, with 0 error. But if we predict on the test data, the middle
    model will outperform the third.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, we covered the basics of data science and explored the process
    of extracting underlying information from data using scientific methods, processes,
    and algorithms. We then moved on to data pre-processing, which includes data cleaning,
    data integration, data transformation, and data discretization.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how pre-processed data is split into train and test sets when building
    a model using a machine learning algorithm. We also covered supervised, unsupervised,
    and reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we went over the different metrics, including confusion matrices, precision,
    recall, and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover data visualization.
  prefs: []
  type: TYPE_NORMAL
