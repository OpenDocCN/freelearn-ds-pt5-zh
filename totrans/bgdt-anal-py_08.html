<html><head></head><body>
		<div><h1 id="_idParaDest-190"><em class="italics"><a id="_idTextAnchor214"/>Chapter 8</em></h1>
		</div>
		<div><h1 id="_idParaDest-191"><a id="_idTextAnchor215"/>Creating a Full Analysis Report</h1>
		</div>
		<div><h2>Learning Objectives</h2>
			<p>By the end of this chapter, you will be able to:</p>
			<ul>
				<li class="bullets">Read data from different sources in Spark</li>
				<li class="bullets">Perform SQL operations on a Spark DataFrame</li>
				<li class="bullets">Generate statistical measurements in a consistent way</li>
				<li class="bullets">Generate graphs and plots using Plotly</li>
				<li class="bullets">Compile an analysis report incorporating all the previous steps and data</li>
			</ul>
			<p>In this chapter, we will read the data using Spark, aggregating it, and extract the statistical measurements. We will also use the Pandas to generate graphs from aggregated data and form an analysis report.</p>
		</div>
		<div><h2 id="_idParaDest-192"><a id="_idTextAnchor216"/>Introduction</h2>
			<p>If you have been part of the data industry for a while, you will understand the challenge of working with different data sources, analyzing them, and presenting them in consumable business reports. When using Spark on Python, you may have to read data from various sources, such as flat files, REST APIs in JSON format, and so on. </p>
			<p>In the real world, getting data in the right format is always a challenge and several SQL operations are required to gather data. Thus, it is mandatory for any data scientist to know how to handle different file formats and different sources, and to carry out basic SQL operations and present them in a consumable format. </p>
			<p>This chapter provides common methods for reading different types of data, carrying out SQL operations on it, doing descriptive statistical analysis, and generating a full analysis report. We will start with understanding how to read different kinds of data into PySpark and will then generate various analyses and plots on it.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor217"/>Reading Data in Spark from Different Data Sources</h2>
			<p>One of the advantages of Spark is the ability to read data from various data sources. However, this is not consistent and keeps changing with each Spark version. This section of the chapter will explain how to read files in CSV and JSON.</p>
			<h3 id="_idParaDest-194"><a id="_idTextAnchor218"/>Exercise 47: Reading Data from a CSV File Using the PySpark Object</h3>
			<p>To read CSV data, you have to write the <code>spark.read.csv("the file name with .csv")</code> function. Here, we are reading the bank data that was used in the earlier chapters.</p>
			<h4>Note</h4>
			<p class="callout">The <code>sep</code> function is used here.</p>
			<p>We have to ensure that the right <code>sep</code> function is used based on how the data is separated in the source data.</p>
			<p>Now let's perform the following steps to read the data from the <code>bank.csv</code> file:</p>
			<ol>
				<li>First, let's import the required packages into the Jupyter notebook:<pre>import os
import pandas as pd
import numpy as np
import collections
from sklearn.base import TransformerMixin
import random
import pandas_profiling</pre></li>
				<li>Next, import all the required libraries, as illustrated:<pre>import seaborn as sns
import time
import re
import os
import matplotlib.pyplot as plt</pre></li>
			</ol>
			<p>Now, use the <code>tick</code> themes, which will make our dataset more visible and provide higher contrast:</p>
			<pre>sns.set(style="ticks")</pre>
			<ol>
				<li value="1">Now, change the working directory using the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Let's import the libraries required for Spark to build the Spark session:<pre>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ml-bank').getOrCreate()</pre></li>
				<li>Now, let's read the CSV data after creating the <code>df_csv</code> Spark object using the following command:<pre>df_csv = spark.read.csv('bank.csv', sep=';', header = True, inferSchema = True)</pre></li>
				<li>Print the schema using the following command:<pre>df_csv.printSchema()</pre><p>The output is as follows:</p><div><img alt="Figure 8.1: Bank schema" src="img/C12913_08_01.jpg"/></div></li>
			</ol>
			<h6>Figure 8.1: Bank schema</h6>
			<h3 id="_idParaDest-195"><a id="_idTextAnchor219"/>Reading JSON Data Using the PySpark Object</h3>
			<p>To read JSON data, you have to write the <code>read.json("the file name with .json")</code> function after setting the SQL context:</p>
			<div><div><img alt="Figure 8.2: Reading JSON file in PySpark" src="img/C12913_08_02.jpg"/>
				</div>
			</div>
			<h6>Figure 8.2: Reading JSON file in PySpark</h6>
			<h2 id="_idParaDest-196">SQ<a id="_idTextAnchor220"/>L Operations on a Spark DataFrame</h2>
			<p>A DataFrame in Spark is a distributed collection of rows and columns. It is the same as a table in a relational database or an Excel sheet. A Spark RDD/DataFrame is efficient at processing large amounts of data and has the ability to handle petabytes of data, whether structured or unstructured. </p>
			<p>Spark optimizes queries on data by organizing the DataFrame into columns, which helps Spark understand the schema. Some of the most frequently used SQL operations include subsetting the data, merging the data, filtering, selecting specific columns, dropping columns, dropping all null values, and adding new columns, among others.</p>
			<h3 id="_idParaDest-197">Ex<a id="_idTextAnchor221"/>ercise 48: Reading Data in PySpark and Carrying Out SQL Operations</h3>
			<p>For summary statistics of data, we can use the <code>spark_df.describe().show()</code> function, which will provide information on <code>count</code>, <code>mean</code>, <code>standard deviation</code>, <code>max</code>, and <code>min</code> for all the columns in the DataFrame. </p>
			<p>For example, in the dataset that we have considered—the bank marketing dataset (<a href="https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv">https://raw.githubusercontent.com/TrainingByPackt/Big-Data-Analysis-with-Python/master/Lesson08/bank.csv</a>)—the summary statistics data can be obtained as follows:</p>
			<ol>
				<li value="1">After creating a new Jupyter notebook, import all the required packages, as illustrated here:<pre>import os
import pandas as pd
import numpy as np</pre></li>
				<li>Now, change the working directory using the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Import all the libraries required for Spark to build the Spark session:<pre>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ml-bank').getOrCreate()</pre></li>
				<li>Create and read the data from the CSV file using the Spark object, as illustrated:<pre>spark_df = spark.read.csv('bank.csv', sep=';', header = True, inferSchema = True)</pre></li>
				<li>Now let's print the first five rows from the Spark object using the following command:<pre>spark_df.head(5)</pre><p>The output is as follows:</p><div><img alt="Figure 8.3: Bank data of first five rows (Unstructured)" src="img/C12913_08_03.jpg"/></div><h6>Figure 8.3: Bank data of first five rows (Unstructured)</h6></li>
				<li>The previous output is unstructured. Let's first identify the data types to proceed to get the structured data. Use the following command to print the datatype of each column:<pre>spark_df.printSchema()</pre><p>The output is as follows:</p><div><img alt="Figure 8.4: Bank datatypes (Structured)" src="img/C12913_08_04.jpg"/></div><h6>Figure 8.4: Bank datatypes (Structured)</h6></li>
				<li>Now let's calculate the total number of rows and columns with names to get a clear idea of the data we have:<pre>spark_df.count()</pre><p>The output is as follows:</p><pre>4521
len(spark_df.columns), spark_df.columns</pre><p>The output is as follows:</p><div><img alt="Figure 8.5: Total number of rows and columns names" src="img/Image38437.jpg"/></div><h6>Figure 8.5: Total number of rows and columns names</h6></li>
				<li>Print the summary statistics for the DataFrame using the following command:<pre>spark_df.describe().show()</pre><p>The output is as follows:</p><div><img alt="Figure 8.6: Summary statistics of numerical columns" src="img/C12913_08_06.jpg"/></div><h6>Figure 8.6: Summary statistics of numerical columns</h6><p>To select multiple columns from a DataFrame, we can use the <code>spark_df.select('col1', 'col2', 'col3')</code> function. For example, let's select the first five rows from the <code>balance</code> and <code>y</code> columns using the following command:</p><pre>spark_df.select('balance','y').show(5)</pre><p>The output is as follows:</p><div><img alt="Figure 8.7: Data of the balance and y columns" src="img/C12913_08_07.jpg"/></div><h6>Figure 8.7: Data of the balance and y columns</h6></li>
				<li>To identify the relation between two variables in terms of their frequency of levels, <code>crosstab</code> can be used. To derive crosstab between two columns, we can use the <code>spark_df.crosstab('col1', col2)</code> function. Crosstab is carried out between two categorical variables and not between numerical variables:<pre>spark_df.crosstab('y', 'marital').show()</pre><div><img alt="Figure 8.8: Pair wise frequency of categorical columns" src="img/C12913_08_08.jpg"/></div><h6>Figure 8.8: Pair wise frequency of categorical columns</h6></li>
				<li>Now, let's add a new column to the dataset:<pre># sample sets
sample1 = spark_df.sample(False, 0.2, 42)
sample2 = spark_df.sample(False, 0.2, 43)
# train set
train = spark_df.sample(False, 0.8, 44)
train.withColumn('balance_new', train.balance /2.0).select('balance','balance_new').show(5)</pre><p>The output is as follows:</p><div><img alt="Figure 8.9: Data of newly added column" src="img/C12913_08_09.jpg"/></div><h6>Figure 8.9: Data of newly added column</h6></li>
				<li>Drop the newly created column using the following command:<pre>train.drop('balance_new)</pre></li>
			</ol>
			<h3 id="_idParaDest-198"><a id="_idTextAnchor222"/>Exercise 49: Creating and Merging Two DataFrames</h3>
			<p>In this exercise, we will extract and use the bank marketing data (<a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>) from the UCI Machine Learning Repository. The objective is to carry out merge operations on a Spark DataFrame using PySpark.</p>
			<p>The data is related to the direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact for the same client was required, in order to access whether the product (<strong class="bold">bank term deposit</strong>) would be (<strong class="bold">yes</strong>) or would not be (<strong class="bold">no</strong>) subscribed.</p>
			<p>Now, let's create two DataFrames from the current bank marketing data and merge them on the basis of a primary key:</p>
			<ol>
				<li value="1">First, let's import the required header files in the Jupyter notebook:<pre>import os
import pandas as pd
import numpy as np
import pyspark</pre></li>
				<li>Now, change the working directory using the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Import all the libraries required for Spark to build the Spark session:<pre>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ml-bank').getOrCreate()</pre></li>
				<li>Read the data from the CSV files into a Spark object using the following command:<pre>spark_df = spark.read.csv('bank.csv', sep=';', header = True, inferSchema = True)</pre></li>
				<li>Print the first five rows from the Spark object:<pre>spark_df.head(5)</pre><p>The output is as follows:</p><div><img alt="" src="img/Image38484.jpg"/></div><h6>Figure 8.10: Bank data of first five rows (Unstructured)</h6></li>
				<li>Now, to merge the two DataFrames using the primary key (ID), first, we will have to split it into two DataFrames.</li>
				<li>First, add a new DataFrame with an <code>ID</code> column:<pre>from pyspark.sql.functions import monotonically_increasing_id
train_with_id = spark_df.withColumn("ID", monotonically_increasing_id())</pre></li>
				<li>Then, create another column, <code>ID2</code>:<pre>train_with_id = train_with_id.withColumn('ID2', train_with_id.ID)</pre></li>
				<li>Split the DataFrame using the following command:<pre>train_with_id1 = train_with_id.drop('balance', "ID2")
train_with_id2 = train_with_id.select('balance', "ID2")</pre></li>
				<li>Now, change the ID column names of <code>train_with_id2</code>:<pre>train_with_id2 = train_with_id2.withColumnRenamed("ID2", "ID")</pre></li>
				<li>Merge <code>train_with_id1</code> and <code>train_with_id2</code> using the following command:<pre>train_merged = train_with_id1.join(train_with_id2, on=['ID'], how='left_outer')</pre></li>
			</ol>
			<h3 id="_idParaDest-199">Exercise 50<a id="_idTextAnchor223"/>: Subsetting the DataFrame</h3>
			<p>In this exercise, we will extract and use the bank marketing data (<a href="https://archive.ics.uci.edu/ml/datasets/bank+marketing">https://archive.ics.uci.edu/ml/datasets/bank+marketing</a>) from the UCI Machine Learning Repository. The objective is to carry out filter/subsetting operations on the Spark DataFrame using PySpark.</p>
			<p>Let's subset the DataFrame where the balance is greater than <code>0</code> in the bank marketing data:</p>
			<ol>
				<li value="1">First, let's import the required header files in the Jupyter notebook:<pre>import os
import pandas as pd
import numpy as np
import pyspark</pre></li>
				<li>Now, change the working directory using the following command:<pre>os.chdir("/Users/svk/Desktop/packt_exercises")</pre></li>
				<li>Import all the libraries required for Spark to build the Spark session:<pre>from pyspark.sql import SparkSession
spark = SparkSession.builder.appName('ml-bank').getOrCreate()</pre></li>
				<li>Now, read the CSV data as a Spark object using the following command:<pre>spark_df = spark.read.csv('bank.csv', sep=';', header = True, inferSchema = True)</pre></li>
				<li>Let's run SQL queries to subset and filter the DataFrame:<pre>train_subsetted = spark_df.filter(spark_df.balance &gt; 0.0)
pd.DataFrame(train_subsetted.head(5))</pre><p>The output is as follows:</p><div><img alt="" src="img/Image38496.jpg"/></div></li>
			</ol>
			<h6>Figure 8.11: Filtered DataFrame</h6>
			<h2 id="_idParaDest-200">Generating <a id="_idTextAnchor224"/>Statistical Measurements</h2>
			<p>Python is a general-purpose language with statistical modules. A lot of statistical analysis, such as carrying out descriptive analysis, which includes identifying the distribution of data for numeric variables, generating a correlation matrix, the frequency of levels in categorical variables with identifying mode and so on, can be carried out in Python. The following is an example of correlation:</p>
			<div><div><img alt="Figure 8.12: Segment numeric data and correlation matrix output" src="img/C12913_08_12.jpg"/>
				</div>
			</div>
			<h6>Figure 8.12: Segment numeric data and correlation matrix output</h6>
			<p>Identifying the distribution of data and normalizing it is important for parametric models such as <code>yeo-johnson</code> method to normalize the data:</p>
			<div><div><img alt="Figure 8.13: Identifying the distribution of the data—Normality test" src="img/C12913_08_13.jpg"/>
				</div>
			</div>
			<h6>Figure 8.13: Identifying the distribution of the data—Normality test</h6>
			<p>The identified variables are then normalized using <code>yeo-johnson</code> or the <code>box-cox</code> method.</p>
			<p>Generating the importance of features is important in a data science project where predictive techniques are used. This broadly falls under statistical analysis as various statistical techniques are used to identify the important variables. One of the methods that's used here is <code>Boruta</code>, which is a wrap-around <code>RandomForest</code> algorithm for variable importance analysis. For this, we will be using the <code>BorutaPy</code> package:</p>
			<div><div><img alt="Figure 8.14: Feature importance" src="img/C12913_08_14.jpg"/>
				</div>
			</div>
			<h6>Figure 8.14: Feature importance</h6>
			<h3 id="_idParaDest-201"><a id="_idTextAnchor225"/>Activity 15: Generating Visualization Using Plotly</h3>
			<p>In this activity, we will extract and use the bank marketing data from the UCI Machine Learning Repository. The objective is to generate visualizations using Plotly in Python.</p>
			<h4>Note</h4>
			<p class="callout">Plotly's Python graphing library makes interactive, publication-quality graphs.</p>
			<p>Perform the following steps to generate the visualization using Plotly:</p>
			<ol>
				<li value="1">Import the required libraries and packages into the Jupyter notebook.</li>
				<li>Import the libraries required for Plotly to visualize the data visualization:<pre>import plotly.graph_objs as go
from plotly.plotly import iplot
import plotly as py</pre></li>
				<li>Read the data from the <code>bank.csv</code> file into the Spark DataFrame.</li>
				<li>Check the Plotly version that you are running on your system. Make sure you are running the updated version. Use the <code>pip install plotly --upgrade</code> command and then run the following code:<pre>from plotly import __version__
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
print(__version__) # requires version &gt;= 1.9.0</pre><p>The output is as follows:</p><pre>3.7.1</pre></li>
				<li>Now import the required libraries to plot the graphs using Plotly:<pre>import plotly.plotly as py
import plotly.graph_objs as go
from plotly.plotly import iplot
init_notebook_mode(connected=True)</pre></li>
				<li>Set the Plotly credentials in the following command, as illustrated here:<pre>plotly.tools.set_credentials_file(username='Your_Username', api_key='Your_API_Key')</pre><h4>Note</h4><p class="callout">To generate an API key for Plotly, sign up for an account and navigate to <a href="https://plot.ly/settings#/">https://plot.ly/settings#/</a>. Click on the <strong class="bold">API Keys</strong> option and then click on the <strong class="bold">Regenerate Key</strong> option.</p></li>
				<li>Now, plot each of the following graphs using Plotly:<p>Bar graph:</p><div><img alt="Figure 8.15: Bar graph of bank data" src="img/C12913_08_15.jpg"/></div></li>
			</ol>
			<h6>Figure 8.15: Bar graph of bank data</h6>
			<p>Scatter plot:</p>
			<div><div><img alt="Figure 8.16: Scatter plot of bank data" src="img/C12913_08_16.jpg"/>
				</div>
			</div>
			<h6>Figure 8.16: Scatter plot of bank data</h6>
			<p>Boxplot:</p>
			<div><div><img alt="Figure 8.17: Boxplot of bank data" src="img/C12913_08_17.jpg"/>
				</div>
			</div>
			<h6>Figure 8.17: Box<a id="_idTextAnchor226"/>plot of bank data</h6>
			<h4>Note</h4>
			<p class="callout">The solution for this activity can be found on page 248.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor227"/>Summary</h2>
			<p>In this c<a id="_idTextAnchor228"/>hapter,<a id="_idTextAnchor229"/> we learned how to import data from various sources into a Spark environment as a Spark DataFrame. In addition, we learned how to carry out various SQL operations on that DataFrame, and how to generate various statistical measures, such as correlation analysis, identifying the distribution of data, building a feature importance model, and so on. We also looked into how to generate effective graphs using Plotly offline, where you can generate various plots to develop an analysis report.</p>
			<p>This book has hopefully offered a stimulating journey through big data. We started with Python and  covered several libraries that are part of the Python data science stack: NumPy and Pandas, We also looked at home we can use Jupyter notebooks. We then saw how to create informative data visualizations, with some guiding principles on what is a good graph, and used Matplotlib and Seaborn to materialize the figures. Then we made a start with the Big Data tools - Hadoop and Spark, thereby understanding the principles and the basic operations. </p>
			<p>We have seen how we can use DataFrames in Spark to manipulate data, and have about utilize concepts such as correlation and dimensionality reduction to better understand our data. The book has also covered  reproducibility, so that the analysis created can be supported and better replicated when needed, and we finished our journey with a final report. We hope that the subjects covered, and the practical examples in this book will help you in all areas of your own data journey.</p>
		</div>
	</body></html>