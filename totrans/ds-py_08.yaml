- en: 'Chapter 8. Analytics Study: Prediction - Financial Time Series Analysis and
    Forecasting'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"When making important decisions, it''s ok to trust your instincts but always
    verify with data"'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – *David Taieb*
  prefs: []
  type: TYPE_NORMAL
- en: The study of time series is a very important field of data science with multiple
    applications in industry, including the weather, medicine, sales, and, of course,
    finance. It is a broad and complex subject and covering it in detail would be
    outside the scope of this book, but we'll try to touch upon a few of the important
    concepts in this chapter, staying sufficiently high level as not to require any
    particular specific knowledge from the reader. We also show how Python is particularly
    well adapted to time series analysis from data manipulation with libraries like
    pandas ([https://pandas.pydata.org](https://pandas.pydata.org)) for data analysis
    and NumPy ([http://www.numpy.org](http://www.numpy.org)) for scientific computation,
    to visualization with Matplotlib ([https://matplotlib.org](https://matplotlib.org))
    and Bokeh ([https://bokeh.pydata.org](https://bokeh.pydata.org)).
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with an introduction to the NumPy library and its most important
    APIs that will be put to good use when building descriptive analytics to analyze
    time series representing stock historical financial data. Using Python libraries
    such as `statsmodels` ([https://www.statsmodels.org/stable/index.html](https://www.statsmodels.org/stable/index.html)),
    we'll show how to do statistical exploration and find properties like stationarity,
    **autocorrelation function** (**ACF**), and **partial autocorrelation function**
    (**PACF**). which will be useful to find trends in the data and creating forecasting
    models. We'll then operationalize these analytics by building a PixieApp that
    summarizes all the important statistics and visualizations about stock historical
    financial data.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we'll attempt to build a time series forecasting model that
    predicts future trends of a stock. We'll use an autoregressive model with Integrated
    Moving Average called **ARIMA** where we use previous values in the time series
    to predict the next value. ARIMA is one of the most popular models currently used,
    although new models based on recurrent neural networks are starting to gain in
    popularity.
  prefs: []
  type: TYPE_NORMAL
- en: As usual, we'll conclude the chapter by incorporating the building of an ARIMA
    time series forecasting model in the `StockExplorer` PixieApp.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started with NumPy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The NumPy library is one of the main reasons why Python has gained so much traction
    in the data scientist community. It is a foundational library upon which a lot
    of the most popular libraries, such as pandas ([https://pandas.pydata.org](https://pandas.pydata.org)),
    Matplotlib ([https://matplotlib.org](https://matplotlib.org)), SciPy ([https://www.scipy.org](https://www.scipy.org)),
    and scikit-learn ([http://scikit-learn.org](http://scikit-learn.org)) are built.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key capabilities provided by NumPy are:'
  prefs: []
  type: TYPE_NORMAL
- en: A very powerful multidimensional NumPy array called ndarray with very high-performance
    mathematical operations (at least compared to regular Python lists and arrays)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Universal functions also called `ufunc` for short, for providing very efficient
    and easy-to-use element by element operations on one or more ndarray
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Powerful ndarray slicing and selection capabilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Broadcasting functions that make it possible to apply arithmetic operations
    on ndarray of different shapes provided that some rules are respected
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we start exploring the NumPy APIs, there is one API that is absolutely
    essential to know: `lookfor()`. With this method, you can find a function using
    a query string, which is very useful considering the hundreds of powerful APIs
    provided by NumPy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, I can look for a function that computes the average mean of an
    array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Within seconds, I can find a few candidate functions without having to leave
    my Notebook to consult the documentation. In the preceding case, I can spot a
    few functions that are interesting— `np.average` and `np.mean`—for which I still
    need to know their arguments. Again, instead of looking up the documentation which
    takes time and breaks the flow of what I was doing, I use a little-known capability
    of Jupyter Notebooks that provides me with the signature and docstring of the
    function inline. To invoke the inline help of a function, simply position the
    cursor at the end of the function and use the *Shift* + *Tab* combination. Calling
    *Shift* + *Tab* a second time will expand the pop-up window to show more of the
    text as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: *Shift* + *Tab* only applies to a function.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Getting started with NumPy](img/B09699_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Inline help in Jupyter Notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Using this method, I can rapidly iterate over the candidate functions until
    I find the one that fits my needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to note that `np.lookfor()` is not limited to querying the
    NumPy module; you could search in other modules as well. For example, the following
    code searches for `acf` (autocorrelation function) related methods in the `statsmodels`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode1.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode1.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Creating a NumPy array
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are many ways to create a NumPy array. Here are the methods most commonly
    used:'
  prefs: []
  type: TYPE_NORMAL
- en: From a Python list or tuple using `np.array()`, for example, `np.array([1, 2,
    3, 4])`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From one of the NumPy factory functions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`np.random`: A module that provides a very rich set of functions for randomly
    generating values. This module is composed of the following categories:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simple random data: `rand`, `randn`, `randint`, and so on'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Permutations: `shuffle`, `permutation`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Distributions: `geometric`, `logistic`, and so on'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find more information on the `np.random` module here:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html](https://docs.scipy.org/doc/numpy-1.14.0/reference/routines.random.html)'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`np.arange`: Return an ndarray with evenly spaced values within a given interval.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Signature: `numpy.arange([start, ]stop, [step, ]dtype=None)`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example: `np.arange(1, 100, 10)`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Results: `array([ 1, 11, 21, 31, 41, 51, 61, 71, 81, 91])`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`np.linspace`: Similar to `np.arange`, it returns an ndarray with evenly spaced
    values within a given interval, the difference being that with `linspace` you
    specify the number of samples you want instead of the number of steps.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example: `np.linspace(1,100,8, dtype=int)`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Results: `array([ 1, 15, 29, 43, 57, 71, 85, 100])`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`np.full`, `np.full_like`, `np.ones`, `np.ones_like`, `np.zeros`, `np.zeros_like`:
    Create an ndarray initialized with a constant value.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example: `np.ones( (2,2), dtype=int)`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Results: `array([[1, 1], [1, 1]])`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: '`np.eye`, `np.identity`, `np.diag`: Creates an ndarray with constant values
    in the diagonal:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example: `np.eye(3,3)`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Results: `array([[1, 0, 0],[0, 1, 0],[0, 0, 1]])`'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: When the `dtype` argument is not provided, NumPy tries to infer it
    from the input argument. However, it may happen that the type returned is not
    the correct one; for example, float is returned when it should be an integer.
    In this case, you should use the `dtype` argument to force the type. For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Why NumPy arrays are so much faster than their Python lists and arrays counterpart?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As mentioned before, operations on NumPy arrays run much faster than their Python
    counterpart. This is because Python is a dynamic language that doesn't know, a
    priori, the type it's dealing with and therefore has to constantly query the metadata
    associated with it to dispatch it to the right method. On the other hand, NumPy
    is highly optimized to deal with large multidimensional arrays of data by, among
    other things, delegating the execution of the CPU-intensive routine to external
    highly optimized C libraries that have been precompiled.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To be able to do that, NumPy places two important constraints on ndarrays:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**ndarrays are immutable**: Therefore, if you want to change the shape or the
    size of an ndarray or if you want to add/delete elements, you always must create
    a new one. For example, the following code creates an ndarray using the `arange()`
    function which returns a one-dimensional array with evenly spaced values, and
    then reshapes it to fit a 4 by 5 matrix:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode2.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode2.py)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The results are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Elements in an ndarray must be of the same type**: ndarray carries the element
    type in the `dtype` member. When creating a new ndarray using the `nd.array()`
    function, NumPy will automatically infer a type that is suitable for all elements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example: `np.array([1,2,3]).dtype` will be `dtype(''int64'')`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`np.array([1,2,''3'']).dtype` will be `dtype(''<U21'')` where `<` means little endian
    (see [https://en.wikipedia.org/wiki/Endianness](https://en.wikipedia.org/wiki/Endianness))
    and `U21` means a 21-character Unicode string.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: You can find detailed information about all the supported data types
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html](https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Operations on ndarray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most often, we have the need to summarize data over an ndarray. Fortunately,
    NumPy provides a very rich set of functions (also called **reduction functions**)
    that provide out-of-the-box summarization over an ndarray or an axis of the ndarray.
  prefs: []
  type: TYPE_NORMAL
- en: 'For reference, a NumPy axis corresponds to a dimension of the array. For example,
    a two-dimensional ndarray has two axes: one running across rows, which is referred
    to as axis 0 and one running across columns which is called axis 1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the axes in a two-dimensional array:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Operations on ndarray](img/B09699_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Axes in a two-dimensional array
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the reduction functions we''ll discuss next take an axis as an argument.
    They fall into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mathematical functions**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trigonometric: `np.sin`, `np.cos`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hyperbolic: `np.sinh`, `np.cosh`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rounding: `np.around`, `np.floor`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sums, products, differences: `np.sum`, `np.prod`, `np.cumsum`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Exponents and logarithms: `np.exp`, `np.log`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arithmetic: `np.add`, `np.multiply`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miscellaneous: `np.sqrt`, `np.absolute`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: All these unary functions (functions that take only one argument)
    work directly at the ndarray level. For example, we can use `np.square` to square
    all the values in an array at once:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Code: `np.square(np.arange(10))`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Results: `array([ 0, 1, 4, 9, 16, 25, 36, 49, 64, 81])`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can find more information on NumPy mathematical functions here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy/reference/routines.math.html](https://docs.scipy.org/doc/numpy/reference/routines.math.html)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Statistical functions**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Order statistics: `np.amin`, `np.amax`, `np.percentile`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Averages and variances: `np.median`, `np.var`, `np.std`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Correlating: `np.corrcoef`, `np.correlate`, `np.cov`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Histograms: `np.histogram`, `np.bincount`, and so on'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: pandas provides very tight integration with NumPy and lets you apply
    these NumPy operations on pandas DataFrames. We''ll use this capability quite
    a bit when analyzing time series in the rest of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example creates a pandas DataFrame and computes the square
    on all the columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Operations on ndarray](img/B09699_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Applying NumPy operations to pandas DataFrames
  prefs: []
  type: TYPE_NORMAL
- en: Selections on NumPy arrays
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'NumPy arrays support similar slicing operations as Python arrays and lists.
    So, using an ndarray created with the `np.arrange()` method, we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode3.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode3.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Which produces the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Selections using slices also work with NumPy arrays that have multiple dimensions.
    We can use slices for every dimension in the array. This is not the case for Python
    arrays and lists which only allow indexing using integers of slices.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: For reference a slice in Python has the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As an example, let''s create a NumPy array with the shape `(3,4)`, that is,
    3 rows * 4 columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose that I want to select only the middle of the matrix, that is, [5, 6].
    I can simply apply slices on rows and columns, for example, `[1:2]` to select
    the second row and `[1:3]` to select the second and third values in the second
    row:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Another interesting NumPy feature is that we can also use predicates to index
    an ndarray with Boolean values.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: We can then use the Boolean ndarray to select subsets of data with a simple
    and elegant syntax.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is only a small preview of all the selection capabilities of NumPy. For more
    information on NumPy selection, you can visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html](https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.indexing.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Broadcasting is a very convenient feature of NumPy. It lets you perform arithmetic
    operations on ndarrays having different shapes. The term **broadcasting** comes
    from the fact that the smaller array is automatically duplicated to fit the bigger
    array so that they have compatible shapes. There are however a set of rules that
    govern how broadcasting works.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find more information on broadcasting here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest form of NumPy broadcasting is **scalar broadcasting**, which lets
    you perform element-wise arithmetic operations between an ndarray and a scalar
    (that is, a number).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: In the following discussion, we assume that we want to operate on
    two ndarrays which do not have the same dimensions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Broadcasting with smaller arrays needs to follow only one rule: one of the
    arrays must have at least one of its dimensions equal to 1\. The idea is to duplicate
    the smaller array along the dimensions that don''t match until they do.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram, taken from the [http://www.scipy-lectures.org/](http://www.scipy-lectures.org/)
    website, illustrates very nicely the different cases for adding two arrays:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Broadcasting](img/B09699_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Broadcasting flow explained
  prefs: []
  type: TYPE_NORMAL
- en: 'Source: [http://www.scipy-lectures.org/_images/numpy_broadcasting.png](http://www.scipy-lectures.org/_images/numpy_broadcasting.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The three use cases demonstrated in the preceding diagram are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The array''s dimensions match**: Perform the sum element-wise as usual.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The smaller array has only 1 row**: Duplicate the rows until the dimensions
    fit the first array. The same algorithm would be used if the smaller array had only
    1 column.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The first array has only 1 column and the second array only 1 row**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate the columns in the first array until we have the same number of columns
    as the second array
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Duplicate the rows in the second array until we have the same number of rows
    as the first array
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code sample shows NumPy broadcasting in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this section, we provided a basic introduction to NumPy, at least enough
    to get us started and follow the code samples that we'll cover in the rest of
    this chapter. In the next section, we will start the discussion on time series
    with statistical data exploration to find patterns that will help us to identify
    underlying structures in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical exploration of time series
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the sample application, we'll use stock historical financial data provided
    by the Quandl data platform financial APIs ([https://www.quandl.com/tools/api](https://www.quandl.com/tools/api))
    and the `quandl` Python library ([https://www.quandl.com/tools/python](https://www.quandl.com/tools/python)).
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, we need to install the `quandl` library by running the following
    command in its own cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: As always, don''t forget to restart the kernel after the installation
    is complete.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Access to the Quandl data is free but limited to 50 calls a day, but you can
    bypass this limit by creating a free account and get an API key:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to [https://www.quandl.com](https://www.quandl.com) and create a new account
    by clicking on the **SIGN UP** button on the top right.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fill up the form in three steps of the sign-up wizard. (I chose **Personal**,
    but depending on your situation, you may want to choose **Business** or **Academic**.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the end of the process, you should receive an email confirmation with a link
    to activate the account.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the account is activated, log in to the Quandl platform website and click on
    **Account Settings** in the top right-hand menu, and then go to the **API KEY**
    tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Copy the API key provided in this page. This value will be used to programmatically
    set the key in the `quandl` Python library as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `quandl` library is mainly composed of two APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`quandl.get(dataset, **kwargs)`: This returns a pandas DataFrame or a NumPy
    array for the requested dataset(s). The `dataset` argument can be either a string
    (single dataset) or a list of strings (multi dataset). Each dataset follows the
    syntax `database_code/dataset_code` when `database_code` is a data publisher and
    `dataset_code` related to the resource. (See next how to get a full list of all
    the `database_code` and `dataset_code`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The keyword arguments enable you to refine the query. You can find the full
    list of supported arguments in the `quandl` code on GitHub: [https://github.com/quandl/quandl-python/blob/master/quandl/get.py](https://github.com/quandl/quandl-python/blob/master/quandl/get.py).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'One interesting keyword argument called `returns` controls the data structure
    returned by the method and can take the following two values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`pandas`: Returns a pandas DataFrame'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`: Returns a NumPy array'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quandl.get_table(datatable_code, **kwargs)`: Returns a non-time series dataset
    (called `datatable`) about a resource. We will not be using this method in this
    chapter, but you can find out more about it by looking at the code: [https://github.com/quandl/quandl-python/blob/master/quandl/get_table.py](https://github.com/quandl/quandl-python/blob/master/quandl/get_table.py).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To get the list of `database_code`, we use the Quandl REST API: `https://www.quandl.com/api/v3/databases?api_key=YOUR_API_KEY&page=n`
    which uses pagination.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: In the preceding URL, replace the `YOUR_API_KEY` value with your
    actual API key.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The returned payload is in the following JSON format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode4.json](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode4.json)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use a `while` loop to load all the available pages relying on the `payload[''meta''][''next_page'']`
    value to know when to stop. At each iteration, we append the list of `database_code`
    information into an array called `databases` as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode5.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode5.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `databases` variable now contains an array of JSON objects containing the
    metadata about each `database_code`. We use the PixieDust `display()` API to look at
    the data in a nice searchable table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following screenshot of the PixieDust table, we use the **Filter** button
    described in [Chapter 2](ch02.xhtml "Chapter 2. Python and Jupyter Notebooks to
    Power your Data Analysis"), *Python and Jupyter Notebooks to Power your Data Analysis*,
    to access the statistics about the count of datasets available in each database,
    for example, min, max and mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical exploration of time series](img/B09699_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: List of Quandl database codes
  prefs: []
  type: TYPE_NORMAL
- en: 'After searching for a database that contains stock information from the **New York Stock
    Exchange** (**NYSE**), I found the `XNYS` database as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Make sure to increase the number of the value displayed to `300` in the
    chart options dialog, so all the results are shown in the table.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical exploration of time series](img/B09699_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Looking for a database with stock data from NYSE
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the `XNYS` database is not public and requires a paid subscription.
    I ended up using the `WIKI` database code, which for some reason was not part
    of the list returned by the preceding API request, but which I found in some code
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'I then used the `https://www.quandl.com/api/v3/databases/{database_code}/codes`
    REST API to get the list of datasets. Fortunately, this API returns a CSV compressed
    in a ZIP file, which the PixieDust `sampleData()` method can handle easily, as
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode6.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode6.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the PixieDust table interface, we click on the **Options** dialog to increase
    the number of values displayed to `4000` so that we can fit the entire dataset
    (which is 3,198) and use the search bar to look for particular stocks as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: The search bar only searches for the rows that are displayed in the
    browser, which can be a smaller set when the dataset is too large. Since in this
    case, the dataset is too large, it would be impractical to increase the number
    of rows to display; it is recommended to use the **Filter** instead which guarantees
    to query the entire dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: The CSV file returned by the `quandl` API doesn't have a header, but `PixieDust.sampleData()`
    expects one to be there. This is currently a limitation that will be addressed
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical exploration of time series](img/B09699_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: List of datasets for the WIKI database
  prefs: []
  type: TYPE_NORMAL
- en: 'For the rest of this section, we load the Microsoft stock (ticker symbol MSFT)
    historical time series data for the last several years and start exploring its
    statistical properties. In the following code, we use `quandl.get()` with the
    `WIKI/MSFT` dataset. We add a column called `daily_spread` that computes the daily
    gain/loss by calling the pandas `diff()` method, which returns the difference
    between the current and previous adjusted close price. Note that the returned
    pandas DataFrame uses the dates as an index, but PixieDust does not support plotting
    time series by the index at this time. Therefore, in the following code, we call
    `reset_index()` to convert the `DateTime` index into a new column called `Date`
    that contains the dates information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode7.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode7.py)'
  prefs: []
  type: TYPE_NORMAL
- en: For our first data exploration, we use `display()` to create a line chart of
    the stock adjusted closing price over time using the Bokeh renderer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the **Options** configuration and the resulting
    line chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical exploration of time series](img/B09699_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MSFT Price over time, adjusted for dividend distribution, stock split, and other
    corporate actions
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also generate a chart that shows the daily spread for each day of the
    period, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Statistical exploration of time series](img/B09699_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Daily Spread for the MSFT stock
  prefs: []
  type: TYPE_NORMAL
- en: Hypothetical investment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As an exercise, let's try to create a chart that shows how a hypothetical investment
    of $10,000 in the selected stock (MSFT) would fare over time. To do this, we must
    compute a DataFrame that contains the total investment value for each day of the
    period, factoring in the daily spread that we calculated in the previous paragraph
    and use the PixieDust `display()` API to visualize the data.
  prefs: []
  type: TYPE_NORMAL
- en: We use pandas ability to select rows using a predicate based on dates to first
    filter the DataFrame to select only the data points in the period we are interested
    in. We then calculate the number of shares bought by dividing the initial investment
    of $10,000 by the closing price on the first day of the period and add the initial
    investment value. All this computation is made very easy, thanks to the efficient
    series computation of pandas and the underlying NumPy foundational library. We use
    the `np.cumsum()` method ([https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.cumsum.html](https://docs.scipy.org/doc/numpy-1.14.0/reference/generated/numpy.cumsum.html))
    to compute the cumulative sum of all the daily gains adding the initial investment
    value of $10,000.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we make the chart easier to read by using the `resample()` method that
    converts the frequency from daily to monthly computing the new values using the average
    for the month.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code computes the growth DataFrame using a period starting in May 2016:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode8.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode8.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the graph generated by the `display()` API including
    the configuration options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Hypothetical investment](img/B09699_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Hypothetical portfolio growth
  prefs: []
  type: TYPE_NORMAL
- en: Autocorrelation function (ACF) and partial autocorrelation function (PACF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before trying to generate predictive models, it is essential to understand whether
    the time series has identifiable patterns, such as seasonality or trends. One
    popular technique is to look at how data points correlate with previous data points
    according to a specified time lag. The intuition is that the autocorrelation would
    reveal internal structures, such as for example, identifying periods when high
    correlation (positive or negative) occurs. You can experiment with different lag
    values (that is, for each data point, how many previous points are you taking
    into account) to find the right periodicity.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the ACF usually requires calculating the Pearson R correlation coefficient
    for the set of data points ([https://en.wikipedia.org/wiki/Pearson_correlation_coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient))
    which is not a trivial thing to do. The good news is that the `statsmodels` Python
    library has a `tsa` package (**tsa** stands for **time series analysis**) that
    provides helper methods for computing the ACF, that are tightly integrated with
    pandas Series.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: If not already done, we install the `statsmodels` package using the following
    command, restarting the kernel after completion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code uses `plot_acf()` from the `tsa.api.graphics` package to
    compute and visualize the ACF for the adjusted close price of the MSFT stock time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation function (ACF) and partial autocorrelation function (PACF)](img/B09699_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ACF for MSFT with lags = 100
  prefs: []
  type: TYPE_NORMAL
- en: The preceding chart shows the autocorrelation of the data at a number of previous
    data points (lag) given by the *x* abscissa. So, at lag `0`, you always have an
    autocorrelation of `1.0` (you always correlate perfectly with yourself), lag `1`
    shows the autocorrelation with the previous data point, lag `2` shows the autocorrelation
    with the data point that is two steps behind. We can clearly see that the autocorrelation
    decreases as the lags increase. In the preceding chart, we used only 100 lags,
    and we see that the autocorrelation still remains statistically significant at
    around 0.9, which tells us that data separated by long periods of time is not
    correlated. This suggests that the data has a trend, which is quite obvious when
    glancing at the overall price chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'To confirm this hypothesis, we plot the ACF chart with a bigger `lags` argument,
    say `1000` (which is not unreasonable given the fact that our series has more
    than 10,000 data points), as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation function (ACF) and partial autocorrelation function (PACF)](img/B09699_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ACF for MSFT with lags = 1000
  prefs: []
  type: TYPE_NORMAL
- en: We now clearly see that the autocorrelation falls below the significance level
    at around `600` lags.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better illustrate how the ACF works, let''s generate a time series that
    is periodic, without a trend and see what we can learn. For example, we can use
    `np.cos()` on a series of evenly spaced points generated with `np.linspace()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode9.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode9.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation function (ACF) and partial autocorrelation function (PACF)](img/B09699_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ACF for a periodic series with no trends
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding chart, we can see that the autocorrelation spikes again at
    regular intervals (every 5 lags or so), clearly showing periodicity (also called
    seasonality when dealing with real-world data).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using ACF to detect structure in your time series can sometimes lead to problems,
    especially when you have strong periodicity. In this case, you''ll always see
    a spike in autocorrelation at a multiple of the period, no matter how far back
    you try to autocorrelate your data and this could lead to the wrong interpretation.
    To work around this problem, we use the PACF which uses a shorter lag and unlike
    ACF, doesn''t reuse correlations previously found in shorter time periods. The
    math for ACF and PACF is rather complex, but the reader only needs to understand
    the intuition behind it and happily use libraries such as `statsmodels` to do
    the heavy lifting computation. One resource I used to get more information on
    ACF and PACF can be found here: [https://www.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html](https://www.mathworks.com/help/econ/autocorrelation-and-partial-autocorrelation.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Back to our MSFT stock time series, the following code shows how to plot its
    PACF using the `smt.graphics` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode10.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode10.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Autocorrelation function (ACF) and partial autocorrelation function (PACF)](img/B09699_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Partial autocorrelation for the MSFT stock time series
  prefs: []
  type: TYPE_NORMAL
- en: We'll get back to ACF and PACF later on in this chapter when we discuss time
    series forecasting with the ARIMA model.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we've discussed multiple ways to explore the data. It is of
    course by no means exhaustive, but we get the idea of how tools such as Jupyter,
    pandas, NumPy, and PixieDust make it easier to experiment and fail fast if necessary.
    In the next section, we will build a PixieApp that brings all these charts together.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together with the StockExplorer PixieApp
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the first version of our `StockExplorer` PixieApp, we want to operationalize
    the data exploration of a stock data time series selected by the user. Similar
    to the other PixieApps we''ve built, the first screen has a simple layout with
    an input box where the user can enter a list of stock tickers separated by commas,
    and an **Explore** button to start data exploration. The main screen is composed
    of a vertical navigator bar with a menu for each type of data exploration. To
    make the PixieApp code more modular and easier to maintain and extend, we implement
    each data exploration screen in its own child PixieApp which is triggered by the
    vertical navigation bar. Also, each child PixieApp inherits from a base class
    called `BaseSubApp` that provides common functionalities useful to all the subclasses.
    The following diagram shows the overall UI layout as well as a class diagram for
    all the child PixieApps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Putting it all together with the StockExplorer PixieApp](img/B09699_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: UI layout of the StockExplorer PixieApp
  prefs: []
  type: TYPE_NORMAL
- en: Let's first look at the implementation for the welcome screen. It is implemented
    in the default route for the `StockExplorer` PixieApp class. The following code
    shows a partial implementation of the `StockExplorer` class to include the default
    route only.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Do not try to run this code yet, until the full implementation is provided.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode11.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode11.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code is very similar to the other sample PixieApps we''ve seen
    so far. The **Explore** button contains the following two PixieApp attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A `pd_script` child element, which calls a Python snippet to set the stock
    tickers. We also use the `$val` directive to retrieve the user-entered value for
    the stock tickers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `pd_options` attribute, which points to the `explore` route:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `select_tickers` helper method stores the list of tickers in a dictionary
    member variable and selects the first one as the active ticker. For performance
    reasons, we only load the data when needed, that is, when setting the active ticker
    for the first time or when the user clicks on a particular ticker in the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: As in previous chapters, the `[[StockExplorer]]` notation indicates
    that the code that follows is part of the `StockExplorer` class.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode12.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode12.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The lazy loading of the stock data for a particular ticker symbol into a pandas
    DataFrame is done in `set_active_ticker()`. We first check whether the DataFrame
    has already been loaded by looking if the `df` key is present and, if not, we call the
    `quandl` API with the `dataset_code`: `''WIKI/{ticker}''`. We also add a column
    that computes the daily spread of the stock that will be displayed in the basic
    exploration screen. Finally, we need to call `reset_index()` ([https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html))
    on the DataFrame to convert the index which is a `DateTimeIndex` into its own column
    called `Date`. The reason is that the PixieDust `display()` doesn''t yet support visualization
    of DataFrame with a `DateTimeIndex`.'
  prefs: []
  type: TYPE_NORMAL
- en: In the `explore` route, we return an HTML fragment that builds the layout for
    the whole screen. As shown in the preceding mock-up, we use the `btn-group-vertical`
    and `btn-group-toggle` bootstrap classes to create the vertical navigation bar.
    The list of menus and associated child PixieApp are defined in the `tabs` Python
    variable, and we use Jinja2 `{%for loop%}` to build the content. We also add a
    placeholder `<div>` element with `id ="analytic_screen{{prefix}}"` that will be
    the recipient of the child PixieApp screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `explore` route implementation is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode13.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode13.py)'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, notice that we use the `@templateArgs` decorator because
    we want to use the `tabs` variable, which is created locally to the method implementation,
    in the Jinja2 template.
  prefs: []
  type: TYPE_NORMAL
- en: Each menu in the vertical navigation bar points to the same `analytic_screen{{prefix}}`
    target and invokes the `show_analytic` route with the selected child PixieApp
    class name referenced by `{{subapp}}`.
  prefs: []
  type: TYPE_NORMAL
- en: In turn, the `show_anatytic` route simply returns an HTML fragment with a `<div>` element
    that has a `pd_app` attribute referencing the child PixieApp class name. We also
    use the `pd_render_onload` attribute to ask PixieApp to render the content of the
    `<div>` element as soon as it is loaded in the browser DOM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code is for the `show_analytic` route:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode14.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode14.py)'
  prefs: []
  type: TYPE_NORMAL
- en: BaseSubApp – base class for all the child PixieApps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s now look at the implementation for each of the child PixieApps and how
    the base class `BaseSubApp` is used to provide common functionalities. For each
    child PixieApp we want the user to be able to select a stock ticker through a
    tabbed interface as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![BaseSubApp – base class for all the child PixieApps](img/B09699_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Tab widget for MSFT, IBM, AMZN tickers
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of repeating the HTML fragment for every child PixieApp, we use a technique
    that I particularly like which consists of creating a Python decorator called
    `add_ticker_selection_markup` that dynamically changes how the function behaves
    (for more information on Python decorators, see [https://wiki.python.org/moin/PythonDecorators](https://wiki.python.org/moin/PythonDecorators)).
    This decorator is created in the `BaseSubApp` class and will automatically prepend
    the tab selection widget HTML markup for the route, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode15.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode15.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'At first glance, the preceding code may appear very hard to read as the `add_ticker_selection_markup`
    decorator method contains two levels of anonymous nested methods. Let''s try to
    explain the purpose for each of them including the main `add_ticker_selection_markup`
    decorator method:'
  prefs: []
  type: TYPE_NORMAL
- en: '`add_ticker_selection_markup`: This is the main decorator method that takes
    one argument called `refresh_ids` which will be used in the generated markup.
    This method returns an anonymous function called `deco` that takes a function
    argument.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`deco`: This is the wrapper method that takes one argument called `fn` which
    is a pointer to the original function to which the decorator is applied. This
    method returns an anonymous function called `wrap` which will be called in lieu
    of the original function when it is called in the user code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`wrap`: This is the final wrapper method that takes three arguments:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`self`: Pointer to the host class for the function'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*args`: Any variable arguments that the original method defines (could be
    empty)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`**kwargs`: Any keyword arguments that the original method defines (could be
    empty)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `wrap` method can access the variables that are outside its scope through
    the Python closure mechanism. In this case, it uses the `refresh_ids` to generate
    the tab widget markup, and then calls the `fn` function with the `self`, `args`,
    and `kwargs` arguments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: Do not worry if the preceding explanation is still confusing, even
    after reading it multiple times. You can just use the decorator for now, and it
    won''t affect your ability to understand the rest of the chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: StockExploreSubApp – first child PixieApp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can now implement the first child PixieApp called `StockExploreSubApp`. In
    the main screen, we create two `<div>` elements that each have a `pd_options`
    attribute that calls the `show_chart` route with `Adj. Close` and `daily_spread`
    as values. In turn, the `show_chart` route returns a `<div>` element with a `pd_entity`
    attribute pointing to the `parent_pixieapp.get_active_df()` method with a `<pd_options>`
    element that contains a JSON payload for displaying a Bokeh line chart with `Date`
    as the *x* abscissa and whatever value is passed as an argument as the column
    for the *y* ordinate. We also decorate the route with the `BaseSubApp.add_ticker_selection_markup`
    decorator using the ID of the preceding two `<div>` elements as the `refresh_ids`
    argument.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation for the `StockExplorerSubApp` child PixieApp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode16.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode16.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding `show_chart` route, the `pd_entity` uses the `get_active_df()`
    method from the `parent_pixieapp` which is defined in the `StockExplorer` main
    class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode17.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode17.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, the `StockExploreSubApp` is associated with the menu through
    a tuple in the `tabs` array variable declared in the `Explore` route of the `StockExplorer`
    route:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode18.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode18.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the `StockExploreSubApp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExploreSubApp – first child PixieApp](img/B09699_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: StockExploreSubApp main screen
  prefs: []
  type: TYPE_NORMAL
- en: MovingAverageSubApp – second child PixieApp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The second child PixieApp is `MovingAverageSubApp` which displays a line chart
    of the moving average for the selected stock ticker with a lag that is configurable
    through a slider control. Similar to the ticker selection tab, the lag slider
    will be needed in another child PixieApp. We could use the same decorator technique
    we use for the ticker selection tab control, but here we want to be able to position
    the lag slider anywhere on the page. So instead, we'll use a `pd_widget` control
    called `lag_slider` that we define in the `BaseSubApp` class and return an HTML
    fragment for the slider control. It also adds a `<script>` element that uses the
    jQuery `slider` method available in the jQuery UI module (see [https://api.jqueryui.com/slider](https://api.jqueryui.com/slider)
    for more information). We also add a `change` handler function that is called when
    the user has selected a new value. In this handler, we call the `pixiedust.sendEvent`
    function to publish an event of the `lagSlider` type and a payload containing
    the new value for the lag. It is the responsibility of the caller to add a `<pd_event_handler>`
    element to listen to that event and process the payload.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation of the `lag_slider` `pd_widget`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode19.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode19.py)'
  prefs: []
  type: TYPE_NORMAL
- en: In the `MovingAverageSubApp` we use the `add_ticker_selection_markup` decorator
    with `chart{{prefix}}` as an argument in the default route to add the ticker selection
    tab and add a `<div>` element with `pd_widget` named `lag_slider`, including a
    `<pd_event_handler>` to set the `self.lag` variable and refresh the `chart` div.
    The `chart` div uses a `pd_entity` attribute with the `get_moving_average_df()`
    method that calls the `rolling` method ([https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rolling.html](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rolling.html))
    on the pandas Series returned from the selected pandas DataFrame and calls the
    `mean()` method on it. Because the PixieDust `display()` does not yet support
    pandas Series, we build a pandas DataFrame using the series index as a column
    called `x` and return it in the `get_moving_average_df()` method.
  prefs: []
  type: TYPE_NORMAL
- en: The following code shows the implementation of the `MovingAverageSubApp` child
    PixieApp
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode20.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode20.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the chart displayed by the `MovingAverageSubApp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![MovingAverageSubApp – second child PixieApp](img/B09699_08_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MovingAverageSubApp screenshot
  prefs: []
  type: TYPE_NORMAL
- en: AutoCorrelationSubApp – third child PixieApp
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the third child, PixieApp called `AutoCorrelationSubApp`; we display the
    ACF and PACF of the selected stock DataFrame, which are computed using the `statsmodels`
    package.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation of the `AutoCorrelationSubApp`
    which also uses the `add_ticker_selection_markup` decorator and the `pd_widget`
    named `lag_slider`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode21.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode21.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we define two routes: `show_acf` and `show_pacf` which
    respectively call the `plot_acf` and `plot_pacf` methods of the `smt.graphics`
    package. We also use the `@captureOutput` decorator to signal the PixieApp framework
    to capture the output generated by `plot_acf` and `plot_pacf`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the charts displayed by `AutoCorrelationSubApp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![AutoCorrelationSubApp – third child PixieApp](img/B09699_08_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: AutoCorrelationSubApp screenshot
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we showed how to put together a sample PixieApp that does
    basic data exploration on a time series and display various statistical charts.
    The complete Notebook can be found here: [https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/StockExplorer%20-%20Part%201.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/StockExplorer%20-%20Part%201.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we try to build a time series forecast model using a very
    popular model called **Autoregressive Integrated Moving Average** (**ARIMA**).
  prefs: []
  type: TYPE_NORMAL
- en: Time series forecasting using the ARIMA model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ARIMA is one of the most popular time series forecasting models and as its
    name indicates is made up of three terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AR**: Stands for **autoregression**, which is nothing more than applying
    a linear regression algorithm using one observation and its own lagged observations
    as training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The AR model uses the following formula:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Where ![Time series forecasting using the ARIMA model](img/B09699_08_32.jpg)
    are the weights of the models learned from the previous observations and ![Time
    series forecasting using the ARIMA model](img/B09699_08_33.jpg) is the residual
    error for observation *t*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We also call *p* the order of the autoregression model, which is defined as the number
    of lag observations included in the preceding formula.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*AR(2)* is defined as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '*AR(1)* is defined as:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '**I**: Stands for **integrated**. For the ARIMA model to work, it is assumed
    that the time series is stationary or can be made stationary. A series is said
    to be stationary ([https://en.wikipedia.org/wiki/Stationary_process](https://en.wikipedia.org/wiki/Stationary_process))
    if its mean and variance doesn''t change over time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: There is also the notion of strict stationarity which requires that
    the joint probability distribution of a subset of observations doesn''t change
    when shifted in time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Using mathematical notation, strict stationarity translates to:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_36.jpg) and
    ![Time series forecasting using the ARIMA model](img/B09699_08_37.jpg) are the
    same for any *t*, *m*, and *k,* with *F* being the joint probability distribution.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In practice, this condition is too strong, and the preceding weaker definition
    provided is preferred.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can make a time series stationary through a transformation that uses differencing
    of the log between an observation and the one before that, as shown in the following
    equation:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: It is possible that multiple log differencing transformations are needed before
    the time series is actually made stationary. We call *d* the number of times we transform
    the series using log differencing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*I(0)* is defined as no log differencing needed (the model is then called ARMA).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*I(1)* is defined as 1 log differencing needed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*I(2)* is defined as 2 log differencing needed.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: It is important to remember to do the reverse transformation for
    as many integrations that were made, after predicting a value.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**MA**: Stands for **moving average**. The MA model uses the residual error
    from the mean of the current observation and the weighted residual errors of the
    lagged observations. We can define the model using the following formula:![Time
    series forecasting using the ARIMA model](img/B09699_08_39.jpg)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: is the mean of the time series, ![Time series forecasting using the ARIMA model](img/B09699_08_33.jpg)
    are the residual errors in the series and ![Time series forecasting using the ARIMA model](img/B09699_08_41.jpg)
    are the weights for the lagged residual errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We call *q* the size of the moving average window.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For example:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*MA(0)* is defined as no moving average needed (the model is then called AR).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*MA(1)* is defined as using a moving average window of 1\. The formula becomes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Time series forecasting using the ARIMA model](img/B09699_08_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: As per the preceding definition, we use the notation *ARIMA(p,d,q)* to define
    an ARIMA model with an autoregression model of order *p*, an integration/differencing
    of order *d*, and a moving average window of size *q*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing all the code to build an ARIMA model can be very time-consuming.
    Fortunately, the `statsmodels` library implements an `ARIMA` class in the `statsmodels.tsa.arima_model`
    package that provides all the computation needed to train a model with the `fit()`
    method and predict values with the `predict()` method. It also takes care of the
    log differencing to make the time series stationary. The trick is to find the
    parameters *p*, *d*, and *q* for building the optimal ARIMA model. For this, we
    use the ACF and PACF chart as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The *p* value corresponds to the number of lags (on the *x* abscissa) where
    the ACF chart crosses the statistical significance threshold for the first time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, the *q* value corresponds to the number of lags (on the *x* abscissa)
    where the PACF chart crosses the statistical significance threshold for the first
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Build an ARIMA model for the MSFT stock time series
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a reminder, the price chart for the MSFT stock time series looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MSFT stock series chart
  prefs: []
  type: TYPE_NORMAL
- en: Before we start building our model, let's first withhold the last 14 days of
    the data for testing and use the rest for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines two new variables: `train_set` and `test_set`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: If you''re still not familiar with the preceding slicing notation,
    please refer to the section on NumPy at the beginning of this chapter'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding chart, we can clearly observe a growth trend starting in
    2012 but no clear seasonality. Therefore, we can safely assume that there is no
    stationarity. Let's first try to apply a log differencing transformation once
    and plot the corresponding ACF and PACF chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we build the `logmsft` pandas Series by using `np.log()`
    on the `Adj. Close` column and then build the `logmsft_diff` pandas DataFrame
    using the difference between `logmsft` and the lag of 1 (using the `shift()` method).
    As was done before, we also call `reset_index()` to convert the `Date` index into
    a column so that the PixieDust `display()` can process it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode22.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode22.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: MSFT stock series after log differencing applied
  prefs: []
  type: TYPE_NORMAL
- en: From looking at the preceding graph, we can reasonably think that we've succeeded
    at making the time series stationary with 0 as the mean. We can also use a more
    rigorous way to test for stationarity by using the Dickey-Fuller test ([https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test](https://en.wikipedia.org/wiki/Dickey%E2%80%93Fuller_test))
    which tests the null hypothesis that a unit root is present in an *AR(1)* model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: In statistics, statistical hypothesis testing consists of challenging
    whether a proposed hypothesis is true, by taking a sample and deciding whether
    the claim remains true. We look at the p-value ([https://en.wikipedia.org/wiki/P-value](https://en.wikipedia.org/wiki/P-value))
    which helps determine the significance of the results. More details on statistical
    hypothesis testing can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Statistical_hypothesis_testing](https://en.wikipedia.org/wiki/Statistical_hypothesis_testing)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code uses the `adfuller` method from the `statsmodels.tsa.stattools`
    package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode23.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode23.py)We
    use the `pprint` package which is very useful for *pretty-printing* any Python
    data structures. More info on `pprint` can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://docs.python.org/3/library/pprint.html](https://docs.python.org/3/library/pprint.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results (explained in detail at: [http://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.adfuller.html](http://www.statsmodels.org/devel/generated/statsmodels.tsa.stattools.adfuller.html))
    are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode24.json](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode24.json)'
  prefs: []
  type: TYPE_NORMAL
- en: The p-value is below the significance level; therefore, we can reject the null
    hypothesis that a unit root is present in the *AR(1)* model, which gives us confidence
    that the time series is stationary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then plot the ACF and PACF chart which will give us the *p* and *q* parameters
    of the ARIMA model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code builds the ACF chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode25.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode25.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: ACF for the log difference MSFT DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding ACF graph, we can see that the correlation crosses the statistical
    significance threshold for the first time at a lag of 1\. Therefore, we'll use
    *p = 1* as the AR order of our ARIMA model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We do the same for the PACF:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode26.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode26.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: PACF for the log difference MSFT DataFrame
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding PACF graph, we can also see that the correlation crosses
    the statistical significance threshold for the first time at a lag of 1\. Therefore,
    we'll use *q = 1* as the MA order of our ARIMA model.
  prefs: []
  type: TYPE_NORMAL
- en: We also had to apply the log differencing transformation only once. Therefore
    we'll use *d = 1* for the integrated part of the ARIMA model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Note**: When calling the `ARIMA` class, if you use *d = 0*, then you may
    have to do the log differencing manually and, in this case, you''ll need to revert
    the transformation yourself on the predicted values. If not, the `statsmodels`
    package will take care of reverting the transformation before returning the predicted
    value.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code trains an ARIMA model on the `train_set` time series using
    *p = 1*, *d = 1*, and *q=1* as values to the order tuple argument of the `ARIMA`
    constructor. We then call the `fit()` method to proceed with the training and
    obtain a model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode27.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode27.py)'
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**: We use the `warnings` package to avoid getting the mutiple deprecation
    warnings that may happen if you are using older versions of NumPy and pandas.'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we use `train_set['Adj. Close']` as an argument to the
    `ARIMA` constructor. Since we are using a Series for the data, we also need to
    pass the `train_set['Date']` series for the `dates` argument. Note that if we
    passed a pandas DataFrame instead with a `DateIndex` index, then we wouldn't have
    to use the `dates` argument. The final argument to the `ARIMA` constructor is
    the `order` argument which is a tuple of three values indicating the *p*, *d*,
    and *q* order, as discussed at the beginning of this section.
  prefs: []
  type: TYPE_NORMAL
- en: We then call the `fit()` method that returns the actual ARIMA model that we'll
    use to predict values. For information purposes, we print statistics about the
    residual errors of the model using `arima_model.resid.describe()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: The mean residual error is ![Build an ARIMA model for the MSFT stock time series](img/B09699_08_43.jpg)
    which is very close to zero and therefore shows that the model may be overfitting
    the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a model let's try to diagnose it. We define a method called
    `plot_predict` that takes a model, a series of dates and a number indicating how
    far back we want to look. We then call the ARIMA `plot_predict()` method to create
    a chart with both the predicted and observed values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation for the `plot_predict()` method,
    including calling it twice with `100` and `10`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode28.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode28.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The results are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Observations versus Forecast chart
  prefs: []
  type: TYPE_NORMAL
- en: The preceding charts show how close the predictions are to the actual observations
    from the training set. We now use the test set that was withheld before to further
    diagnose the model. For this part, we use the `forecast()` method which predicts
    the next data point. For each value of the `test_set`, we build a new ARIMA model
    from an array of observations called history that contains the training data augmented
    with each predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation for the `compute_test_set_predictions()`
    method that takes a `train_set` and a `test_set` as arguments and returns a pandas
    DataFrame with a `forecast` column containing all the predicted values and a `test`
    column containing the corresponding actual observed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode29.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode29.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result chart:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Chart of predicted versus acutal values
  prefs: []
  type: TYPE_NORMAL
- en: 'We can measure the error using the popular `mean_squared_error` method ([https://en.wikipedia.org/wiki/Mean_squared_error](https://en.wikipedia.org/wiki/Mean_squared_error))
    of the scikit-learn package ([http://scikit-learn.org](http://scikit-learn.org))
    which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Build an ARIMA model for the MSFT stock time series](img/B09699_08_44.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where ![Build an ARIMA model for the MSFT stock time series](img/B09699_08_45.jpg)
    is the actual value and ![Build an ARIMA model for the MSFT stock time series](img/B09699_08_46.jpg)is
    the predicted value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines a `compute_mean_squared_error` method that takes a test
    and a forecast series and returns the value of the mean squared error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode30.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode30.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The result is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we improve the `StockExplorer` PixieApp by adding a menu that
    provides time series forecasting for the selected stock ticker using an ARIMA
    model. We create a new class called `ForecastArimaSubApp` and update the `tabs`
    variable in the main `StockExplorer` class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode31.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode31.py)'
  prefs: []
  type: TYPE_NORMAL
- en: The `ForecastArimaSubApp` child PixieApp is composed of two screens. The first
    screen displays the time series chart as well as the ACF and the PACF charts.
    The goal of this screen is to provide the user with the necessary data exploration
    to figure out what are the values for the *p*, *d*, and *q* order of the ARIMA
    model, as explained in the previous section. By looking at the time series chart,
    we can figure out whether the time series is stationary (which, as a reminder,
    is a requirement for building the ARIMA model). If not, the user can click on
    the **Add differencing** button to try to make the DataFrame stationery by using
    a log differencing transformation. The three charts are then updated using the
    transformed DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the default route for the `ForecastArimaSubApp` child
    PixieApp:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode32.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode32.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code follows a pattern that we should now be familiar with:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a `setup` method that is guaranteed to be called when the PixieApp starts.
    In this method, we make a copy of the selected DataFrame obtained from the parent
    PixieApp. We also maintain a variable called `self.differencing` that tracks whether
    the user clicked on the **Add differencing** button.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We create a default route that shows the first screen that is composed of the
    following components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A header with two buttons: `Add differencing` for making the time series stationary
    and `Continue to forecast` to display the second screen which we''ll discuss later.
    The `Add differencing` button toggles to `Remove differencing` when the differencing
    has been applied.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A `<div>` element that invokes the `show_chart` route to display the time series
    chart.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A `<div>` element that invokes the `show_acf` route to display the ACF chart.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A `<div>` element that invokes the `show_pacf` route to display the PACF chart.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use an empty array `[]` as an argument to the `@BaseSubApp.add_ticker_selection_markup`
    decorator to make sure that the entire screen is refreshed when the user selects
    another stock ticker, and to restart from the first screen. We also need to reset
    the internal variables. To achieve this, we made a change to the `add_ticker_selection_markup`
    to define a new method in `BaseSubApp` called `set_active_ticker` that is a wrapper
    method to the `set_active_ticker` from the parent PixieApp. The idea is to let
    subclasses override this method and inject extra code if needed. We also change
    the `pd_script` attribute for the tab element to invoke this method when the user selects
    a new ticker symbol as shown in the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode33.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode33.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ForecastArimaSubApp` child PixieApp, we then override the `set_active_tracker`
    method, first calling the super and then calling the `self.setup()` to reinitialize
    the internal variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode34.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode34.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The route implementation for the first forecast screen is pretty straightforward.
    The `Add differencing` / `Remove differencing` button has a `pd_script` attribute
    that calls the `self.toggle_differencing()` method and the `pd_refresh` attribute
    to update the entire page. It also defines the three `<div>` elements that respectively
    call the `show_chart`, `show_acf`, and `show_pacf` routes as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode35.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode35.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `toggle_differencing()` method tracks the current differencing state with
    the `self.differencing` variable and either makes a copy of the active DataFrame
    from the `parent_pixieapp` or applies a log differencing transformation to the
    `self.entity_dataframe` variable as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode36.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode36.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `show_acf` and `show_pacf` routes are pretty straightforward. They respectively
    call the `smt.graphics.plot_acf` and `smt.graphics.plot_pacf` methods. They also
    use the `@captureOutput` decorator to pass through the chart image to the target widget:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode37.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode37.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the data exploration page of the forecast child PixieApp
    without the differencing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model](img/B09699_08_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First forecast screen without applying differencing
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the charts are consistent with a time series that is not stationary.
    When the user clicks on the **Add differencing** button, the following screen
    is shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model](img/B09699_08_27.jpg)'
  prefs: []
  type: TYPE_IMG
- en: First forecast screen with differencing applied
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to implement the `do_forecast` route that is invoked by the
    **Continue to Forecast** button. This route is responsible for building the ARIMA
    model; it starts by showing a configuration page with three input texts that let
    the user enter the *p*, *d*, and *q* orders, which have been inferred by looking
    at the charts in the data exploration screen. We add a `Go` button to proceed
    with the model building using the `build_arima_model` route which we'll discuss
    later in this section. The header also has a `Diagnose Model` button that invokes
    another page responsible for evaluating the accuracy of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the `do_forecast` route is shown here. Note that we use
    the `add_ticker_selection_markup` with an empty array to refresh the entire page when
    the user selects another stock ticker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode38.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode38.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the configuration page of the **Build ARIMA
    model** page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model](img/B09699_08_28.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Configuration page of the Build Arima model page
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Go** button has a `pd_options` attribute that invokes a route with three
    states: `p_order`, `d_order`, and `q_order` with values taken from the three input
    boxes associated with each attribute.'
  prefs: []
  type: TYPE_NORMAL
- en: The route for building the ARIMA model is shown in the following code. It starts
    by splitting the active DataFrame into a training and test set, withholding 14 observations
    for the test set. It then builds the model and computes the residual errors. Once
    the model is successfully built, we return an HTML markup that contains a chart
    showing the predicted values for the training set versus the actual values in
    the training set. This is done by calling the `plot_predict` route. Finally, we
    also show statistics about the residual errors for the model by creating a `<div>`
    element with a `pd_entity` attribute pointing to the residuals variable with a
    `<pd_options>` child element that configures a table view of all the statistics
  prefs: []
  type: TYPE_NORMAL
- en: The chart showing the predictions versus the actual training set is using the
    `plot_predict` route which calls the `plot_predict` method we created earlier
    in the Notebook. We also use the `@captureOutput` decorator to dispatch the chart
    image to the correct widget.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation of the `plot_predict` route is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode39.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode39.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `build_arima_model` route implementation is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode40.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode40.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the result for the **Build Arima model** page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model](img/B09699_08_29.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Model building page
  prefs: []
  type: TYPE_NORMAL
- en: The final screen of the forecast child app is the *diagnose model* screen invoked
    by the `do_diagnose` route. In this screen, we simply display a line chart for
    the DataFrame returned by the `compute_test_set_predictions` method we created
    earlier in the Notebook with the `train_set` and `test_set` variables. The `<div>`
    element for this chart is using a `pd_entity` attribute that calls an intermediary
    class method called `compute_test_set_predictions`. It also has a `<pd_options>`
    child element with the `display()` options for showing the line chart.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows the implementation of the `do_diagnose_screen` route:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can find the code file here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode41.py](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/sampleCode41.py)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the results of the diagnose page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![StockExplorer PixieApp Part 2 – add time series forecasting using the ARIMA
    model](img/B09699_08_30.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Model diagnose screen
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have shown how to improve the `StockExplorer` sample PixieApp to
    include forecasting capabilities using the ARIMA model. Incidentally, we've demonstrated
    how to use the PixieApp programming model to create a three-step wizard that first
    performs some data exploration, then configures the parameters of the model and
    builds it and finally diagnoses the model against the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The complete implementation of the notebook can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/StockExplorer%20-%20Part%202.ipynb](https://github.com/DTAIEB/Thoughtful-Data-Science/blob/master/chapter%208/StockExplorer%20-%20Part%202.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we touched upon the topic of time series analysis and forecasting.
    Of course, we've only scratched the surface, and there is certainly much more
    to explore. It is also a very important field for the industry, especially in
    the finance world, with very active research. For example, we see more and more
    data scientists trying to build time series forecasting models based on recurrent
    neural network ([https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network))
    algorithms, with great success. We've also demonstrated how Jupyter Notebooks
    combined with PixieDust and the ecosystem of libraries, such as `pandas`, `numpy`,
    and `statsmodels,` help accelerate the development of analytics as well as its
    operationalization into applications that are consumable by the line of business
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will look at another important data science use case:
    graphs. We''ll build a sample application related to flight travel and discuss
    how and when we should apply graph algorithms to solve data problems.'
  prefs: []
  type: TYPE_NORMAL
