- en: Database Classification using Support Vector Machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore **Support Vector Machines** (**SVMs**), identify
    various applications for their use, and walk through an example of using a simple
    SVM to classify data in a database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will again organize the topics into the following chief
    areas:'
  prefs: []
  type: TYPE_NORMAL
- en: Database classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Definition and purpose of SVMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common SVM applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using R and an SVM to classify data in a database
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start this chapter with some dialogue around the idea of general or generic
    data classification.
  prefs: []
  type: TYPE_NORMAL
- en: Database classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we''ve said throughout this book, if the reader is a data or database developer,
    or has a similar background, the reader will most likely have heard of and be
    familiar with and comprehend the process of data modeling. This can be (at a high
    level, perhaps) described as the effort of analyzing and understanding the makeup
    and details of some data. Then, this data is organized or classified with the
    objective being that it can be easily understood and consumed by a community of
    users, either named (those already identified, such as financial analysts in an
    organization) or anonymous (such as internet consumers). The following image shows
    the data classification as per requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/023409d2-a36f-4311-9c03-3475dab957a6.png)'
  prefs: []
  type: TYPE_IMG
- en: Anonymous access is the most common system access control method, at least,
    when it comes to websites.
  prefs: []
  type: TYPE_NORMAL
- en: As part of almost any data modeling development project, one might be asked
    to create a class diagram. This (class) diagram details how to split data (or
    a database) into discrete objects, how those objects relate to each other, and
    any known interfaces or connections that they may have. Each class in a class
    diagram can hold both data and its purpose.
  prefs: []
  type: TYPE_NORMAL
- en: So, data classification is the process of sorting and categorizing data into
    various types, forms, or any other distinct classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data classification enables the separation and cataloguing of data according
    to established requirements for the data, for numerous business or other objectives,
    so that a data plan or data model can be developed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09afdc24-eafc-4d77-9583-92b911a30e22.png)'
  prefs: []
  type: TYPE_IMG
- en: Perhaps a real-world illustration, one which a database or data developer would
    recognize or be somewhat familiar with, might be the exercise of classifying or
    reclassifying financial accounts for reporting.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you might find that a typical, recurring exercise during a financial
    reporting project (or perhaps, during a typical accounting period close) is performing
    a detailed review of each account, as well as consolidation of the accounts being
    used, to report an organization's financial results or performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'To the accounting department, the term *classification* (or reclassification)
    is frequently used to define moving an amount from one **general ledger** (**GL**)
    account to another. For example, if an expense (for example, tax preparation)
    was charged to marketing supplies instead of administrative supplies, the correcting
    entry might be read:'
  prefs: []
  type: TYPE_NORMAL
- en: '*To reclassify from marketing supplies to administrative supplies*.'
  prefs: []
  type: TYPE_NORMAL
- en: Here, from an accounting department perspective, we are speaking about an actual
    transaction to deduct an amount from one GL account and add that (same) amount
    to another. From a data developer perspective, there would be no account transaction
    made, but rather a programming change or process would be run that perhaps removes
    a particular account from one parent (or consolidation of a number of accounts)
    to another. This is usually referred to as updating the chart of accounts reporting
    hierarchy, or **c****hart of accounts** (**COA**) maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: Will we discover that these same concepts still apply in the field of statistics?
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections of this chapter, we'll answer that question.
  prefs: []
  type: TYPE_NORMAL
- en: Data classification in statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Statistical data classification is defined by some data scientists or statisticians
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The division of data into meaningful categories for analysis.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The database developer reading this should identify with that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3b7038bc-2721-4550-9fef-3c0582632f47.png)'
  prefs: []
  type: TYPE_IMG
- en: In data science and statistics, classification is defined as identifying to
    which categories (sometimes called sub-populations) a new observation should be
    included, on the basis of a training set of data containing observations (or instances)
    whose category membership has been validated.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists routinely apply statistical formulas to data automatically,
    allowing for processing big data in preparation for statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, a population of unorganized or unclassified data (which is often
    referred to as raw data) collected in real situations and arranged indiscriminately
    does not provide a data scientist with any clear understanding, only hunches based
    upon poorly formed opinions.
  prefs: []
  type: TYPE_NORMAL
- en: The process of data classification attempts to locate similarities within the
    data and condenses the data by dropping out unnecessary details or noise (which
    we discussed in [Chapter 10](d114349e-1538-42d5-b9a6-939081494991.xhtml), *Boosting
    your Database*). Classification facilitates the comparison between different sets
    of data, clearly showing the different points of agreement and disagreement.
  prefs: []
  type: TYPE_NORMAL
- en: Data classification can also help in determining baselines for the use of data.
  prefs: []
  type: TYPE_NORMAL
- en: Classification empowers the data scientist to study the relationship between
    several characteristics and make further statistical treatment like tabulation
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: A very good example of benefitting from statistical data classification is an
    annual population census where people in a population are classified according
    to sex, age, marital status, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines for classifying data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of a guideline (or policy) is to establish a framework for performing
    a task. There exist many schools of thought on the best way or most effective
    method for statistical classification.
  prefs: []
  type: TYPE_NORMAL
- en: In statistical data classification, a data scientist will classify data by assigning
    limits or boundaries. These are most often called **class-limits**. The group
    between any two class-limits is termed a **class** or **class-interval**.
  prefs: []
  type: TYPE_NORMAL
- en: In your statistical travels, you may have also come across the term *decision
    boundary*. Data scientists often use different or similar terms for referring
    to a concept. In statistical-classification problems concerning two classes, they
    may use the terms **decision** **boundary** or **decision** **surface** to define
    a hypersurface (a hypersurface is a generalization of the concept of the hyperplane)
    that partitions the underlying vector space into two sets, one for each class.
    The classifier will classify all the points on one side of the decision boundary
    as belonging to one class, and all those on the other side as belonging to the
    other class.
  prefs: []
  type: TYPE_NORMAL
- en: Common guidelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Guidelines for classification will most often adhere to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: There should not be any doubt about a definition of the classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the classes should preferably have equal width or length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class-limits should be selected in such a way that no value of the item
    in the raw data coincides with the value of the limit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of classes should preferably be between 10 and 20, that is, neither
    too large nor too small
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classes should be exhaustive, that is, each value of the raw data should
    be included in them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classes should be mutually exclusive and non-overlapping, that is, each
    item of the raw data should fit only into one class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification must be suitable for the object of inquiry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The classification should be flexible and items included in each class must
    be consistent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Width of class-interval is determined by first fixing the number of class-intervals
    and then dividing the total range by that number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines are applied to whichever guideline(s) or policies a data scientist
    uses for classification. Those guidelines or policies must apply to all of the
    data being observed within a statistical project from start to completion.
  prefs: []
  type: TYPE_NORMAL
- en: Definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A few key terms that a data scientist must be aware of since they may affect
    how classification is approached, are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confidential data** typically represents any data classified as restricted
    and is often used interchangeably with sensitive data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **Data Steward** is usually a senior-level resource whose responsibilities
    include the overseeing of the life cycle of one or more sets of data to be classified
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A Hyperplane** subspace; an n-dimensional Euclidean space is a flat, n to
    a one-dimensional subset of that space that divides the space into two disconnected
    parts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection** is the act of selecting a subset of relevant features
    (variables, predictors) for use in the construction of a statistical model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature space** simply refers to the collections of features that are used
    to characterize data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistical classification** is the problem of identifying to which set of
    categories (sub-populations), a new observation should be a member of'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Soft margin classification** is a classification where the data scientist
    allows some error while defining or creating a classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Institutional** **data** is defined as all data, maintained or licensed,
    by a university'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Margin classifier** is a classifier which is able to provide an accompanying
    distance from the decision boundary for each example within the data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-public information** is any information that is classified as private
    or restricted according to a guideline, policy, or law'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sensitive data** typically represents data classified as restricted and is
    frequently used interchangeably with confidential data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Property descriptor**;sometimes a classification system will employ descriptors
    to convey the reasons for a given classification; for example, middle age or urban
    are familiar descriptors'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interoperability**; when different applications use different descriptors
    and have different rules about which properties are required and which are optional
    (for a particular classification), how does that support interoperability?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reader should spend some time researching the many related terms related
    to statistical data classification—there are numerous other data defining terms
    within the field of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Definition and purpose of an SVM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: I support Vector Machines, do you?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fa53bba4-b572-4d3c-b670-fc639011578c.png)'
  prefs: []
  type: TYPE_IMG
- en: In the field of machine learning, SVMs are similarly recognized as support vector
    networks and are defined as supervised learning models with accompanying learning
    algorithms that analyze data used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'An important note about SVMs is that they are all about the ability to successfully
    perform pattern recognition. In other words, SVMs promote the ability to extend
    patterns found in data that are:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Not linearly separable by transformations of original data to map into new
    space.*'
  prefs: []
  type: TYPE_NORMAL
- en: Again, everything you will find and come to know about SVMs will align with
    the idea that an SVM is a supervised machine learning algorithm which is most
    often used for classification or regression problems in statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The trick
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will hear most data scientists today refer to the trick or the SVM Trick;
    what they are referring to is that support vector machine algorithms use a method
    referred to as the **kernel trick**.
  prefs: []
  type: TYPE_NORMAL
- en: The kernel trick transforms data and then, based on those transformations it
    performs, works to discover an optimal boundary (remember, we defined classification
    boundaries earlier in this chapter) between the possible outputs or data points.
  prefs: []
  type: TYPE_NORMAL
- en: In statistics, kernel methods are a kind of algorithm which is used for pattern
    analysis. The overall mission of performing pattern analysis is to find and study
    general types of relationships in data.
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the transformations that an SVM performs are some tremendously complex
    data transformations (on your data) which then efficiently figure out how to separate
    the data based on the labels or outputs (sometimes thought of as data scientist
    defined features) the data scientist has previously defined.
  prefs: []
  type: TYPE_NORMAL
- en: Another key (and thought-provoking) point to be aware of when it comes to support
    vector machines is that the data scientist doesn't have to worry about preserving
    the original dimensionality of the data when the SVM performs its transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Consequently, the kernel trick takes the raw data, some previously defined features,
    performs its transformations, and produces the output you might not recognize--sort
    of like uncovering a great labyrinth!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is from [www.yaksis.com/posts/why-use-svm.html](http://www.yaksis.com/posts/why-use-svm.html),
    *Why use SVM?*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"You start with this harmless looking vector of data and after putting it
    through the kernel trick, it''s unraveled and compounded itself until it''s now
    a much larger set of data that can''t be understood by looking at a spreadsheet.*'
  prefs: []
  type: TYPE_NORMAL
- en: '*But here lies the magic, in expanding the dataset there are now more obvious
    boundaries between your classes and the SVM algorithm is able to compute a much
    more optimal hyperplane."*'
  prefs: []
  type: TYPE_NORMAL
- en: Feature space and cheap computations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A huge part of understanding how SVMs work is understanding the trick which
    we've just mentioned. The trick, as we said, uses kernel methods (which use kernel
    functions) and are able to perform well in a high-dimensional feature space.
  prefs: []
  type: TYPE_NORMAL
- en: A feature space is an n-dimensional space where your data variables live. Kernel
    functions are able to operate within this high-dimensional space without having
    to compute the coordinates of the data within that space, but rather by merely
    computing the inner products between the images of all pairs of data in the feature
    space.
  prefs: []
  type: TYPE_NORMAL
- en: This kernel trick can then process the data quicker and more efficiently than
    if it had to explicitly compute the coordinates. This is known as being **computationally
    inexpensive**.
  prefs: []
  type: TYPE_NORMAL
- en: For the reader who is a data or database developer, since database views can
    hide complexity; imagine perhaps comparing the concept of dividing data within
    a database into several compiled views (rather than running queries on all of
    the data as a single source) to how vectors are defined and used by SVMs.
  prefs: []
  type: TYPE_NORMAL
- en: Drawing the line
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A great way to describe the functional (or the practical) steps that a Support
    Vector Machine carries out in an effort to classify data might be to imagine that
    the SVMs are continuously endeavoring to find the line that best separates two
    classes of data points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f38c6c4b-15de-4689-8086-94d71ee47297.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the best line is defined as the line that results in the largest margin
    between the two classifications. The points that lie on this margin are the support
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The great thing about acknowledging these (support) vectors is that we can then
    formulate the problem of finding the maximum-margin hyperplane (the line that
    best separates the two classes) as an optimization problem that only considers
    the support vectors that the SVM has established.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the SVM processing can ignore the vast majority of the data,
    making the classification process go much faster.
  prefs: []
  type: TYPE_NORMAL
- en: More importantly, by presenting the problem in terms of the support vectors
    (the so-called **dual form**), we can apply the kernel trick we defined earlier
    in this chapter to effectively transform the support vector machine into a non-linear
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Linear methods can only solve problems that are linearly separable (usually
    through a hyperplane). Non-linear methods characteristically include applying
    some type of transformation to your input dataset.
  prefs: []
  type: TYPE_NORMAL
- en: More than classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another key point is that SVMs can, in addition to classification, also perform
    regression.
  prefs: []
  type: TYPE_NORMAL
- en: An SVM using a non-linear kernel algorithm means that the boundary that the
    algorithm calculates doesn't have to be a straight line; this means that the SVM
    can capture even more complex relationships between the data points without the
    data scientist having to do all those difficult transformations by hand.
  prefs: []
  type: TYPE_NORMAL
- en: Downside
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, can we find a disadvantage or downside to using support vector machines?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, sometimes the training time required by the SVM is much lengthier and can
    be much more computationally intensive (which we touched on with the idea of computational
    cost earlier in this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Reference resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very understandable book on the topic of SVMs is *An Introduction to Support
    Vector Machines and Other Kernel-based Learning Methods* by NelloChristiani and
    John Shawe-Taylor.
  prefs: []
  type: TYPE_NORMAL
- en: Another respectable reference that offers an insightful association between
    SVMs and a related type of neural network known as a **Radial Basis Function Network**
    is *Neural Networks and Learning Machines* by Simon Haykin, which we also referenced
    in *Chapter 5*, *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting credit scores
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the remaining sections of this chapter, we'll try to review an SVMs example
    based on R thinking like a data developer.
  prefs: []
  type: TYPE_NORMAL
- en: This example is available online and referenced in detail in *Chapter 6* of
    the book, *Mastering Predictive Analytics with R, Second Edition*.
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition](https://www.packtpub.com/big-data-and-business-intelligence/mastering-predictive-analytics-r-second-edition)'
  prefs: []
  type: TYPE_NORMAL
- en: If you're a data developer, you might not be aware that the following website
    offers a very good resource for test datasets at [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
  prefs: []
  type: TYPE_NORMAL
- en: This example uses the particular dataset named *Statlog (German Credit Data)
    Data Set,* which can be found at the site under [https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data))
  prefs: []
  type: TYPE_NORMAL
- en: 'The website is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0490a06-ef25-46ae-a400-f3e6b0273e80.png)'
  prefs: []
  type: TYPE_IMG
- en: This dataset is data from the field of banking and finance. The observations
    in the dataset are loan applications made by individuals at a bank.
  prefs: []
  type: TYPE_NORMAL
- en: The objective of this data is to determine whether a load application constitutes
    a high credit risk.
  prefs: []
  type: TYPE_NORMAL
- en: A data or database developer will most likely observe that rather than downloading
    a dataset, one would create a query to extract the data as records or transactions,
    rather than instances (or observations).
  prefs: []
  type: TYPE_NORMAL
- en: Most likely, if extracting this data from a database, it will not be as easy
    as merely querying a single table because the loan applications will likely be
    in the form of a transaction, pulling data points from multiple database tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other points a data developer should consider are:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset characteristics—field names
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attribute characteristics—datatypes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Date donated—the last update or add date
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The website[ https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/)
    provides two key files; one is the actual data, and one is the data schema:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3adc97ec-25ef-4939-b926-1ed0f78b8e23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The data schema is in the format of an MS Word document (`german.doc`) and
    is used to create the following matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Column name** | **Type** | **Definition** |'
  prefs: []
  type: TYPE_TB
- en: '| checking | Categorical | The status of the existing checking account |'
  prefs: []
  type: TYPE_TB
- en: '| duration | Numerical | The duration of months |'
  prefs: []
  type: TYPE_TB
- en: '| creditHistory | Categorical | The applicant''s credit history |'
  prefs: []
  type: TYPE_TB
- en: '| purpose | Categorical | The purpose of the loan |'
  prefs: []
  type: TYPE_TB
- en: '| credit | Numerical | The credit amount |'
  prefs: []
  type: TYPE_TB
- en: '| savings | Categorical | Savings account/bonds |'
  prefs: []
  type: TYPE_TB
- en: '| employment | Categorical | Present employment since |'
  prefs: []
  type: TYPE_TB
- en: '| installmentRate | Numerical | The instalment rate (as a percentage of disposable
    income) |'
  prefs: []
  type: TYPE_TB
- en: '| personal | Categorical | Personal status and gender |'
  prefs: []
  type: TYPE_TB
- en: '| debtors | Categorical | Other debtors/guarantors |'
  prefs: []
  type: TYPE_TB
- en: '| presentResidence | Numerical | Present residence since |'
  prefs: []
  type: TYPE_TB
- en: '| property | Categorical | The type of property |'
  prefs: []
  type: TYPE_TB
- en: '| age | Numerical | The applicant''s age in years |'
  prefs: []
  type: TYPE_TB
- en: '| otherPlans | Categorical | Other instalment plans |'
  prefs: []
  type: TYPE_TB
- en: '| housing | Categorical | The applicant''s housing situation |'
  prefs: []
  type: TYPE_TB
- en: '| existingBankCredits | Numerical | The number of existing credits at this
    bank |'
  prefs: []
  type: TYPE_TB
- en: '| job | Categorical | The applicant''s job situation |'
  prefs: []
  type: TYPE_TB
- en: '| dependents | Numerical | The number of dependents |'
  prefs: []
  type: TYPE_TB
- en: '| telephone | Categorical | The status of the applicant''s telephone |'
  prefs: []
  type: TYPE_TB
- en: '| foreign | Categorical | Foreign worker |'
  prefs: []
  type: TYPE_TB
- en: '| risk | Binary | Credit risk (1 = good, 2 = bad) |'
  prefs: []
  type: TYPE_TB
- en: 'The raw data looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a98b7f80-e769-4c5c-bb55-7cdf69dfa935.png)'
  prefs: []
  type: TYPE_IMG
- en: Using R and an SVM to classify data in a database
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand the data, we can continue with this particular statistical
    example.
  prefs: []
  type: TYPE_NORMAL
- en: First, the data scientist will need to load the data into an R data frame object.
    This example is calling it `german_raw.`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to provide column names that match our data schema table,
    shown in the preceding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Note from the data schema (the table describing the features in the data) that
    we have a lot of categorical features to deal with. For this reason, a data scientist
    could employ the R `dummyVars()` function (which can be used to create a full
    set of dummy variables) to create dummy binary variables for use. In addition,
    he or she would record the `risk` variable, the output, as a factor with:'
  prefs: []
  type: TYPE_NORMAL
- en: level 0 = good credit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: level 1 = bad credit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As a result of the preceding work, one would have an R data frame object with
    61 features (because several of the categorical input features had numerous levels).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the data scientist would partition or split the data into two subsets:'
  prefs: []
  type: TYPE_NORMAL
- en: Training dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This split can be accomplished using the following R statements:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For the data developer, there are similar approaches to take, such as perhaps
    using the from clause option TABLESAMPLE. With the TAMPLESAMPLE option, you are
    able to get a sample set of data from a table without having to read through the
    entire table or having to assign temporary random values to each row of data.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One particularity of this dataset that is mentioned on the (previously mentioned)
    website is that the data comes from a scenario where the two different types of
    errors defined have different costs associated with them.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the cost of misclassifying a high-risk customer as a low-risk
    customer is five times more expensive for the bank than misclassifying a low-risk
    customer as a high-risk customer. This is understandable, as in the first case,
    the bank stands to lose a lot of money from a loan it gives out that cannot be
    repaid, whereas in the second case, the bank misses out on an opportunity to give
    out a loan that will yield interest for the bank.
  prefs: []
  type: TYPE_NORMAL
- en: This is a practical example where predictive analytics have a direct effect
    on an organization bottom line.
  prefs: []
  type: TYPE_NORMAL
- en: The `svm()` R function has a `class.weights` parameter, which is then used to
    specify the cost of misclassifying (an observation to each class). This is how
    the data scientist incorporates the asymmetric error cost information into the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: First, a vector of class weights is created, noting the need to specify names
    that correspond to the output factor levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the data scientist uses the R `tune()` function to train various SVM
    models with a radial kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The suggested best model here has the cost as 10 and gamma as 0.05 and achieves
    a 74 percent training accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we see how the model performs on the test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The performance on the test set is 73.5 percent and very close to what was seen
    in the training of the model. As expected, the model tends to make many more errors
    that misclassify a low-risk customer as a high-risk customer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unsurprisingly, this takes a toll on the overall classification accuracy, which
    just computes the ratio of correctly classified observations to the overall number
    of observations. In fact, were we to remove this cost imbalance, we would actually
    select a different set of parameters for our model and our performance, from the
    perspective of the unbiased classification accuracy, which would be better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Of course, this last model will tend to make a greater number of costly misclassifications
    of high-risk customers as low-risk customers, which we know is very undesirable.
    We'll conclude this section with two final thoughts. Firstly, we have used relatively
    small ranges for the `gamma` and `cost` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Originally, when this example was presented, it was left as "an exercise for
    the reader" to rerun the analysis with a greater spread of values for these two
    in order to see whether we can get even better performance, which would most likely
    result in longer training times.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, that particular dataset is quite challenging in that its baseline
    accuracy is actually 70 percent. This is because 70 percent of the customers in
    the data are low-risk customers (the two output classes are not balanced).
  prefs: []
  type: TYPE_NORMAL
- en: Whew!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we defined the idea of database and data classification using
    some examples that a data developer may find familiar. Next, we introduced statistical
    data classification and compared that concept with the former.
  prefs: []
  type: TYPE_NORMAL
- en: Classification guidelines were offered along with several important, relevant
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we spoke of support vector machines, how they work, and the advantages
    they offer the data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we aim to provide an explanation of the types of machine
    learning and illustrate to the developer how to use machine learning processes
    to understand database mappings and identify patterns within the data.
  prefs: []
  type: TYPE_NORMAL
