- en: Chapter 4. Data Analysis – Deep Dive
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the principal components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kernel PCA
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features using Singular Value Decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reducing the data dimension with Random Projection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposing Feature matrices using **NMF** (**Non-negative Matrix Factorization**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will look at recipes dealing with dimensionality reduction.
    In the previous chapter, we looked at how to surf through the data to understand
    its characteristics in order to put it to meaningful use. We restricted our discussions
    to only bivariate data. Imagine a dataset with hundreds of columns; how do we
    proceed with the analysis of the data characteristics of such a large dimensional
    dataset? We need efficient tools to handle this hurdle as we work our way through
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Today, high-dimensional data is everywhere. Consider building a product recommendation
    engine for a moderately-sized e-commerce website. Even with the range of thousands
    of products, the number of variables to consider very high. Bioinformatics is
    another area with very high-dimensional data. Gene expression are microarray datasets
    could contain tens of thousands of dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: If your task at hand is to either explore the data or prepare the data for an
    algorithm, the high dimensionality, popularly called the *curse of dimensionality*,
    is a big roadblock. We need efficient methods to handle this. Additionally, the
    complexity of many existing data mining algorithms increases exponentially with
    the increase in the number of dimensions. With increasing dimensions, the algorithms
    become computationally infeasible and thus inapplicable in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction techniques preserve the structure of the data as much
    as possible while reducing the number of dimensions. Thus, in the reduced feature
    space, the execution time of the algorithms is reduced as we have lower dimensions.
    As the structure of data is preserved, the results obtained can be a reliable
    approximation of the original data space. By preserving the structure, we mean
    two things; the first is not tampering with the variations in the original dataset
    and the second is preserving the distance between the data vectors in the new
    projected space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Matrix Decomposition:'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Matrix decomposition yields several techniques for dimensionality reduction.
    Our data is typically a matrix with the instances in rows and features in columns.
    In the previous recipes, we have been storing our data as NumPy matrices all the
    way. For example, in the Iris dataset, our tuples or data instances were represented
    as rows and the features, which included sepal and petal width and length, were
    the columns of the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix decomposition is a way of expressing a matrix. Say that A is a product
    of two other matrices, B and C. The matrix B is supposed to contain vectors that
    can explain the direction of variation in the data. The matrix C is supposed to
    contain the magnitude of this variation. Thus, our original matrix A is now expressed
    as a linear combination of B and C.
  prefs: []
  type: TYPE_NORMAL
- en: The techniques that we will see in the coming sections exploit matrix decomposition
    in order to tackle the dimensionality reduction. There are methods that insist
    that the basic vectors have to be orthogonal to each other, such as the principal
    component analysis, and there are some that don't insist on this requirement,
    such as dictionary learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's buckle up and see some of these techniques in action in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the principal components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first technique that we will look at is the **Principal Component Analysis**
    (**PCA**). PCA is an unsupervised method. In multivariate problems, PCA is used
    to reduce the dimension of the data with minimal information loss, in other words,
    retaining the maximum variation in the data. By variation, we mean the direction
    in which the data is dispersed to the maximum. Let''s look at the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting the principal components](img/B04041_04_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We have a scatter plot of two variables, *x1* and *x2*. The diagonal line indicates
    the maximum variation. By using PCA, our intent is to capture this direction of
    the variation. So, instead of using the direction of two variables, *x1* and *x2*,
    to represent this data, the quest is to find a vector represented by the blue
    line and represent the data with only this vector. Essentially we want to reduce
    the dimension of the data from two to one.
  prefs: []
  type: TYPE_NORMAL
- en: We will leverage the mathematical tools Eigenvalues and Eigenvectors to find
    this blue line vector.
  prefs: []
  type: TYPE_NORMAL
- en: We saw in the previous chapter that the variance measures the amount of dispersion
    or spread in the data. What we saw was an example in one dimension. In case of
    more than one dimension it is easy to express correlation among the variables
    as a matrix, called as Covariance matrix. When the values of the Covariance matrix
    are normalized by standard deviation we get a Correlation matrix. In our case,
    the covariance matrix is a 2 X 2 matrix for two variables, *x1* and *x2*, and
    it measures how much these two variables move in the same direction or generally
    vary together.
  prefs: []
  type: TYPE_NORMAL
- en: When we perform Eigenvalue decomposition, that is, get the Eigenvectors and
    Eigenvalues of the covariance matrix, the principal Eigenvector, which is the
    vector with the largest Eigenvalue, is in the direction of the maximum variance
    in the original data.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, this should be the vector that is represented by the blue line
    in our graph. We will then proceed to project our input data in this blue line
    vector in order to get the reduced dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With a dataset (n x m) with n instances and m dimensions, PCA projects it onto
    a smaller subspace (n x d), where d << m.
  prefs: []
  type: TYPE_NORMAL
- en: A point to note is that PCA is computationally very expensive.
  prefs: []
  type: TYPE_NORMAL
- en: PCA can be performed on both the covariance and correlation matrix. Remember
    when a Covariance matrix of a dataset with unevenly scaled datasets is used in
    PCA, the results may not be very useful. Curious readers can refer to the Book
    A First Course in Multivariate Statistics by Bernard Flury on the topic of using
    either correlation or covariance matrix for PCA.
  prefs: []
  type: TYPE_NORMAL
- en: '[http://www.springer.com/us/book/9780387982069](http://www.springer.com/us/book/9780387982069).'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's use the Iris dataset to understand how to use PCA efficiently in reducing
    the dimension of the dataset. The Iris dataset contains measurements for 150 iris
    flowers from three different species.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three classes in the Iris dataset are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Iris Setosa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Versicolor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Virginica
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the four features in the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: The sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can we use, say, two columns instead of all the four to express most of the
    variations in the data? Our quest is to reduce the dimension of the data. In this
    case, our instances have four columns. Let's say that we are building a classifier
    to predict the type of flower with a new instance; can we do this task using instances
    in the reduced dimension space? Can we reduce the number of columns from four
    to two and still achieve a good accuracy for our classifier?
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA is done using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Standardize the dataset to have a zero mean value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the correlation matrix for the dataset and unit standard deviation value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reduce the Correlation matrix matrix into its Eigenvectors and values.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the top nEigenvectors based on the Eigenvalues sorted in descending order.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Project the input Eigenvectors matrix into the new subspace.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries and call the utility function `load_iris`
    from scikit-learn to get the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will proceed to Standardize this data, with a zero mean and standard
    deviation of one, we will leverage the `numpyscorr_coef` function to find the
    correlation matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then do the Eigenvalue decomposition and project our Iris data on the
    first two principal Eigenvectors. Finally, we will plot the dataset in the reduced
    space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Using function scale. The scale function can perform centering, scaling and
    standardization. Centering is subtracting the mean value from individual values,
    Scaling is dividing each value by the variable's standard deviation and finally
    Standardization is performing centering followed by scaling. Using variables with_mean
    and with_std function scale can be used to perform all three normalization techniques.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Iris dataset has four columns. Though there are not many columns, it will
    serve our purpose. We intend to reduce the dimensionality of the Iris dataset
    to two from four and still retain all the information about the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the Iris data to the `x` and `y` variables using the convenient
    `load_iris` function from scikit-learn. The `x` variable is our data matrix and
    we can inspect its shape as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will scale the data matrix `x` to have zero mean and unit standard deviation.
    The rule of thumb is that if all your columns are measured in the same scale in
    your data and have the same unit of measurement, you don''t have to scale the
    data. This will allow PCA to capture these basic units with the maximum variation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We will proceed to build the correlation matrix of our input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The correlation matrix of n random variables X1, ..., Xn is then × n matrix
    whosei, jentry is corr (Xi, Xj), Wikipedia.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then use the SciPy library to calculate the Eigenvalues and Eigenvectors
    of the matrix.Let''s look at our Eigenvalues and Eigenvectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_26.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In our case, the Eigenvalues are printed in a descending order. A key question
    is how many components should we choose? In the next section, we will explain
    a few ways of choosing the number of components.
  prefs: []
  type: TYPE_NORMAL
- en: You can see that we selected only the first two columns of our right-hand side
    Eigenvectors. The discrimination capability of the retained components on the
    `y` variable is a good test of how much information or variation is retained in
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: We will project the data to the new reduced dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will plot the components in the `x` and `y` axes and color them
    by the target variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that components `1` and `2` are able to discriminate the three classes
    of the iris flowers. Thus we have effectively used PCA in reducing the dimension
    to two from four and still able to discriminate the instances belonging to different
    classes of Iris flower.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we said that we would outline a couple of ways to
    help us select how many components should we include. In our recipe, we included
    only two. The following are a list of ways to select the components more empirically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Eigenvalue criterion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An Eigenvalue of one would mean that the component would explain about one variable's
    worth of variability. So, according to this criterion, a component should at least
    explain one variable's worth of variability. We can say that we will include only
    those Eigenvalues whose value is greater than or equal to one. Based on your data
    set you can set the threshold. In a very large dimensional dataset including components
    capable of explaining only one variable may not be very useful.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The proportion of the variance explained criterion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s run the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output is as follows:![There's more…](img/B04041_04_25.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each component, we printed the Eigenvalue, percentage of the variance explained
    by that component, and cumulative percentage value of the variance explained.
    For example, component `1` has an Eigenvalue of `2.91`; `2.91/4` gives the percentage
    of the variance explained, which is 72.80%. Now, if we include the first two components,
    then we can explain 95.80% of the variance in the data.
  prefs: []
  type: TYPE_NORMAL
- en: The decomposition of a correlation matrix into its Eigenvectors and values is
    a general technique that can be applied to any matrix. In this case, we will apply
    it to a correlation matrix in order to understand the principal axes of data distribution,
    that is, axes through which the maximum variation in the data is observed.
  prefs: []
  type: TYPE_NORMAL
- en: PCA can be used either as an exploratory technique or as a data preparation
    technique for a downstream algorithm. Document classification dataset problems
    typically have very large dimensional feature vectors. PCA can be used to reduce
    the dimension of the dataset in order to include only the most relevant features
    before feeding the data to a classification algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: A drawback of PCA worth mentioning here is that it is computationally expensive
    operation. Finally a point about numpy's corrcoeff function. The corrcoeff function
    will standardize your data internally as a part of its calculation. But since
    we want to explicitly state the reason for scaling, we have included it in our
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**When would PCA work?**'
  prefs: []
  type: TYPE_NORMAL
- en: The input dataset should have correlated columns for PCA to work effectively.
    Without a correlation of the input variables, PCA cannot help us.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Performing Singular Value Decomposition* recipe in [Chapter 4](ch04.xhtml
    "Chapter 4. Data Analysis – Deep Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Kernel PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: PCA makes an assumption that all the principal directions of variation in the
    data are straight lines. This is not true in a lot of real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PCA is limited to only those variables where the variation in the data falls
    in a straight line. In other words, it works only with linearly separable data.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at kernel PCA, which will help us reduce the dimension
    of datasets where the variations in them are not straight lines. We will explicitly
    create such a dataset and apply kernel PCA on it.
  prefs: []
  type: TYPE_NORMAL
- en: In kernel PCA, a kernel function is applied to all the data points. This transforms
    the input data into kernel space. A normal PCA is performed in the kernel space.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will not use the Iris dataset here, but will generate a dataset where variations
    are not straight lines. This way, we cannot apply a simple PCA on this dataset.
    Let's proceed to look at our recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries. We will proceed to make a dataset using
    the `make_circles` function from the scikit-learn library. We will plot this data
    and do a normal PCA on this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then plot the first two principal components of this dataset. We will
    plot the dataset using only the first principal component:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s finish it up by performing a kernal PCA and plotting the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In step 1, we generated a dataset using the scikit's data generation function.
    In this case, we used the `make_circles` function. We can create two concentric
    circles, a large one containing the smaller one, using this function. Each concentric
    circle belongs to a certain class. Thus, we created a two class problem with two
    concentric circles.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s look at the data that we generated. The `make_circles` function
    generated a dataset of size 400 with two dimensions. A plot of the original data
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: This chart describes how our data has been distributed. The outer circle belongs
    to class one and the inner circle belongs to class two. Is there a way we can
    take this data and use it with a linear classifier? We will not be able to do
    it. The variations in the data are not straight lines. We cannot use the normal
    PCA. Hence, we will resort to a kernel PCA in order to transform the data.
  prefs: []
  type: TYPE_NORMAL
- en: Before we venture into kernel PCA, let's see what happens if we apply a normal
    PCA on this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the output plot of the first two components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the components of PCA are unable to distinguish between the
    two classes in a linear fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s plot the first component and see its class distinguishing ability. The
    following graph, where we have plotted only the first component, explains how
    PCA is unable to differentiate the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The normal PCA approach is a linear projection technique that works well if
    the data is linearly separable. In cases where the data is not linearly separable,
    a nonlinear technique is required for the dimensionality reduction of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kernel PCA is a nonlinear technique for data reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s proceed to create a kernel PCA object using the scikit-learn library.
    Here is our object creation code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We selected the **Radial Basis Function** (**RBF**) kernel with a gamma value
    of ten. Gamma is the parameter of the kernel (to handle nonlinearity)—the kernel
    coefficient.
  prefs: []
  type: TYPE_NORMAL
- en: Before we go further, let's look at a little bit of theory about what kernels
    really are. As a simple definition, a kernel is a function that computes the dot
    product, that is, the similarity between two vectors, which are passed to it as
    input.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RBFGaussian kernel is defined as follows for two points, *x* and *x''*
    in some input space:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Where,
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '*The RBF decreases with distance and takes values between 0 and 1\. Hence it
    can be interpreted as a similarity measure. The feature space of the RBF kernel
    has infinite dimensions –Wikipedia*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://en.wikipedia.org/wiki/Radial_basis_function_kernel](http://en.wikipedia.org/wiki/Radial_basis_function_kernel).'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now transform the input from the feature space into the kernel space.
    We will perform a PCA in the kernel space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will plot the first two principal components as a scatter plot.
    The points are colored based on their class value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see in this graph that the points are linearly separated in the kernel
    space.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Scikit-learn''s kernel PCA object also allows other types of kernels, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Polynomial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sigmoid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cosine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precomputed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scikit-learn also provides other types of nonlinear data that is generated.
    The following is another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The data plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B04041_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Extracting features using singular value decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After our discussion on PCA and kernel PCA, we can explain dimensionality reduction
    in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: You can transform the correlated variables into a set of non-correlated variables.
    This way, we will have a less dimension explaining the relationship in the underlying
    data without any loss of information.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find out the principal axes, which has the most data variation recorded.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**) is yet another matrix decomposition
    technique that can be used to tackle the curse of the dimensionality problem.
    It can be used to find the best approximation of the original data using fewer
    dimensions. Unlike PCA, SVD works on the original data matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SVD does not need a covariance or correlation matrix. It works on the original
    data matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'SVD factors an `m` x `n` matrix `A` into a product of three matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Extracting features using singular value decomposition](img/B04041_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, U is an `m` x `k` matrix, V is an `n` x `k` matrix, and S is a `k` x `k`
    matrix. The columns of U are called left singular vectors and columns of V are
    called right singular vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The values on the diagonal of the S matrix are called singular values.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will use the Iris dataset for this exercise. Our task in hand is to reduce
    the dimensionality of the dataset from four to two.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s load the necessary libraries and get the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will demonstrate how to perform an SVD operation on the Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Iris dataset has four columns. Though there are not many columns, it will
    serve our purpose. We intend to reduce the dimensionality of the Iris dataset
    to two from four and still retain all the information about the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will load the Iris data to the `x` and `y` variables using the convenient
    `load_iris` function from scikit-learn. The `x` variable is our data matrix; we
    can inspect its shape in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We center the data matrix `x` using its mean. The rule of thumb is that if
    all your columns are measured in the same scale and have the same unit of measurement
    in the data, you don''t have to scale the data. This will allow PCA to capture
    these basis units with the maximum variation. Note that we used only the mean
    while invoking the function scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Run the SVD method on our scaled input dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the top two singular components. This matrix is a reduced approximation
    of the original input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, plot the columns and color it by the class value:![How it works…](img/B04041_04_07.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SVD is a two-mode factor analysis, where we start with an arbitrary rectangular
    matrix with two types of entities. This is different from our previous recipe
    where we saw PCA that took a correlation matrix as an input. PCA is a one-mode
    factor analysis as the rows and columns in the input square matrix represent the
    same entity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In text mining applications, the input is typically presented as a **Term-document
    Matrix** (**TDM**). In a TDM, the rows correspond to the words and columns are
    the documents. The cell entries are filled with either the term frequency or **Term
    Frequency Inverse Document Frequency** (**TFIDF**) score. It is a rectangular
    matrix with two entities: words and documents that are present in the rows and
    columns of the matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: SVD is widely used in text mining applications to uncover the hidden relationships
    (semantic relationship) between words and documents, documents and documents,
    and words and words.
  prefs: []
  type: TYPE_NORMAL
- en: By applying SVD on a term-document matrix, we transform it into a new semantic
    space, where the words that do not occur together in the same document can still
    be close in the new semantic space. The goal of SVD is to find a useful way to
    model the relationship between the words and documents. After applying SVD, each
    document and word can be represented as a vector of the factor values. We can
    choose to ignore the components with very low values and, hence, avoid noises
    in the underlying dataset. This leads to an approximate representation of our
    text corpus. This is called **Latent Semantic Analysis** (**LSA**).
  prefs: []
  type: TYPE_NORMAL
- en: The ramification of this idea has a very high applicability in document indexing
    for search and information retrieval. Instead of indexing the original words as
    an inverted index, we can now index the output of LSA. This helps avoid problems
    such as synonymy and polysemy. In synonymy, users may tend to use different words
    to represent the same entity. A normal indexing is vulnerable to such scenarios.
    As the underlying document is indexed by regular words, a search may not yield
    the results. For example, if we indexed some documents related to financial instruments,
    typically the words would be currency, money, and similar stuff. Currency and
    money are synonymous words. While a user searches for money, he should be shown
    the documents related to currency as well. However, with regular indexing, the
    search engine would be able to retrieve only the documents having money. With
    latent semantic indexing, the documents with currency will also be retrieved.
    In the latent semantic space, currency and money will be close to each other as
    their neighboring words would be similar in the documents.
  prefs: []
  type: TYPE_NORMAL
- en: Polysemy is about words that have more than one meaning. For example, bank can
    refer to a financial institution or a river bank. Similar to synonymy, polysemy
    can also be handled in the latent semantic space.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on LSA and latent semantic indexing, refer to the paper
    by Deerwester et al at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.108.8490).
    For a comparative study of Eigen Values and Singular Values refer to the book
    Numerical Computing with MATLAB by Cleve Moler. Though the examples are in MATLAB,
    with the help of our recipe you can redo them in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://in.mathworks.com/moler/eigs.pdf](https://in.mathworks.com/moler/eigs.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the data dimension with random projection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The methods that we saw previously for dimensionality reduction are computationally
    expensive and not the fastest ones. Random projection is another way to perform
    dimensionality reduction faster than these methods.
  prefs: []
  type: TYPE_NORMAL
- en: Random projections are attributed to the Johnson-Lindenstrauss lemma. According
    to the lemma, a mapping from a high-dimensional to a low-dimensional Euclidean
    space exists; such that the distance between the points is preserved within an
    epsilon variance. The goal is to preserve the pairwise distances between any two
    points in your data, and still reduce the number of dimensions in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say that if we are given `n`-dimensional data in any Euclidean space,
    according to the lemma, we can map it an Euclidean space of dimension k, such
    that all the distances between the points are preserved up to a multiplicative
    factor of (1-epsilon) and (1+ epsilon).
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this exercise, we will use the 20 newsgroup data ([http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a collection of approximately 20,000 newsgroup documents, partitioned
    (nearly) evenly across 20 different categories of news. Scikit-learn provides
    a convenient function to load this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can load all libraries or a list of categories of interest by providing
    a list of strings of categories. In our case, we will use the `sci.crypt` category.
  prefs: []
  type: TYPE_NORMAL
- en: We will load the input text as a term-document matrix where the features are
    individual words. On this, we will apply random projection in order to reduce
    the number of dimensions. We will try to see if the distances between the documents
    are preserved in the reduced space and an instance is a document.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with loading the necessary libraries. Using scikit''s utility
    function `fetch20newsgroups`, we will load the data. We will select only the `sci.crypt`
    category out of all the data. We will then transform our text data into a vector
    representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Let's now proceed to demonstrate the concept of random projection.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After loading the newsgroup dataset, we will convert it to a matrix through
    `TfidfVectorizer(use_idf=False)`.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we have set `use_idf` to `False`. This creates our input matrix
    where the rows are documents, columns are individual words, and cell values are
    word counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we print our vector using the `print vector.shape` command, we will get
    the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We can see that our input matrix has 595 documents and 16115 words; each word
    is a feature and, hence, a dimension.
  prefs: []
  type: TYPE_NORMAL
- en: We will perform the projection of the data using a dense Gaussian matrix. The
    Gaussian random matrix is generated by sampling elements from a normal distribution
    `N`(0, 1/number of components). In our case, the number of components is 1000\.
    Our intention is to reduce the dimension to 1000 from 16115\. We will then print
    the original and the reduced dimension in order to verify the reduction in the
    dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we would like to validate whether the data characteristics are maintained
    after the projection. We will calculate the Euclidean distances between the vectors.
    We will record the distances in the original space and in the projected space
    as well. We will take a difference between them as in step 7 and plot the difference
    as a heat map:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, the gradient is in the range of 0.000 to 0.105 and indicates
    the difference in distance of vectors in the original and reduced space. The difference
    between the distance in the original space and projected space are pretty much
    in a very small range.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a lot of references for random projections. It is a very active field
    of research. Interested readers can refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments with random projections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://dl.acm.org/citation.cfm?id=719759](http://dl.acm.org/citation.cfm?id=719759)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Experiments with random projections for machine learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.9205](http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.13.9205)'
  prefs: []
  type: TYPE_NORMAL
- en: In our recipe, we used the Gaussian random projection where a Gaussian random
    matrix was generated by sampling from a normal distribution, N(0,1/1000), where
    1000 is the required dimension of the reduced space.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, having a dense matrix can create severe memory-related issues while
    processing. In order to avoid this, Achlioptas proposed sparse random projections.
    Instead of choosing from a standard normal distribution, the entries are picked
    from {-1.0,1} with a probability of `{1/6,2/3,1/6}`. As you can see, the probability
    of having 0 is two-thirds and, hence, the resultant matrix will be sparse. Users
    can refer to the seminal paper by Achlioptas at *Dimitris Achlioptas*, *Database-friendly
    random projections: Johnson-Lindenstrauss with binary coins. Journal of Computer
    and System Sciences, 66(4):671–687, 2003.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The scikit implementation allows the users to choose the density of the resultant
    matrix. Let''s say that if we specify the density as d and s as 1/d, then the
    elements of the matrix are picked from the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B04041_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'With probability of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B04041_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Using kernel PCA* recipe in [Chapter 4](ch04.xhtml "Chapter 4. Data Analysis
    – Deep Dive"), *Analyzing Data - Deep Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decomposing the feature matrices using non-negative matrix factorization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed all the previous matrix decomposition recipes from a data dimensionality
    reduction perspective. Let's discuss this recipe from a collaborative filtering
    perspective to make it more interesting. Though data dimensionality reduction
    is what we are after, **Non-negative Matrix Factorization** (**NMF**) is used
    extensively in recommendation systems using a collaborative filtering algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that our input matrix A is of a dimension `m x n`. NMF factorizes
    the input matrix into two matrices, `A_dash` and `H`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decomposing the feature matrices using non-negative matrix factorization](img/B04041_04_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's say that we want to reduce the dimension of the A matrix to d, that is,
    we want the original m x n matrix to be decomposed into m x d, where d << n.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `A_dash` matrix is of a size m x d and the `H` matrix is of a size d x
    m. NMF solves this as an optimization problem, that is, minimizing the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Decomposing the feature matrices using non-negative matrix factorization](img/B04041_04_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The famous Netflix challenge was solved using NMF. Please refer to the following
    link:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Gábor Takács et al., (2008). Matrix factorization and neighbor based algorithms
    for the Netflix prize problem. In: Proceedings of the 2008 ACM Conference on Recommender
    Systems, Lausanne, Switzerland, October 23 - 25, 267-274:*'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://dl.acm.org/citation.cfm?id=1454049](http://dl.acm.org/citation.cfm?id=1454049)'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to explain NMF, let's create a toy recommendation problem. In a typical
    recommendation system such as the one with MovieLens or Netflix, there is a group
    of users and group of items (movies). If each user has rated a few movies, we
    want to predict their ratings for the movies that they have not rated. We will
    assume that the users have not watched the movies that they have not rated. Our
    prediction algorithm output is the ratings for these movies. We can then recommend
    the movies that have a very high rating from our prediction engine to these users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our toy problem is set as follows; we have the following movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Movie ID | Movie Name |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Star Wars |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | The Matrix |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Inception |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Harry Potter |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | The Hobbit |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Guns of Navarone |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Saving Private Ryan |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | Enemy at the Gates |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | Where Eagles Dare |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | The Great Escape |'
  prefs: []
  type: TYPE_TB
- en: 'We have ten movies, each identified with a movie ID. We also have 10 users
    who have rated these movies as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Movie ID |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **User ID** | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.0 | 5.0 | 4.5 | 4.5 | 5.0 | 3.0 | 2.0 | 2.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4.2 | 4.7 | 5.0 | 3.7 | 3.5 | 0.0 | 2.7 | 2.0 | 1.9 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2.5 | 0.0 | 3.3 | 3.4 | 2.2 | 4.6 | 4.0 | 4.7 | 4.2 | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3.8 | 4.1 | 4.6 | 4.5 | 4.7 | 2.2 | 3.5 | 3.0 | 2.2 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2.1 | 2.6 | 0.0 | 2.1 | 0.0 | 3.8 | 4.8 | 4.1 | 4.3 | 4.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 4.7 | 4.5 | 0.0 | 4.4 | 4.1 | 3.5 | 3.1 | 3.4 | 3.1 | 2.5 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2.8 | 2.4 | 2.1 | 3.3 | 3.4 | 3.8 | 4.4 | 4.9 | 4.0 | 4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 4.5 | 4.7 | 4.7 | 4.5 | 4.9 | 0.0 | 2.9 | 2.9 | 2.5 | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.0 | 3.3 | 2.9 | 3.6 | 3.1 | 4.0 | 4.2 | 0.0 | 4.5 | 4.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 4.1 | 3.6 | 3.7 | 4.6 | 4.0 | 2.6 | 1.9 | 3.0 | 3.6 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: For readability, we have kept it as a matrix where the rows correspond to the
    users and columns correspond to the movies. The cell values are the ratings from
    1 to 5, where 5 signifies a high user affinity to the movie and 1 signifies the
    user's dislike. There are 0 values in the cells that indicate that the user has
    not rated those movies. In this recipe, we will decompose the `user_id` x `movie_id`
    matrix using NMF.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with loading the necessary libraries and then creating our dataset.
    We will store our dataset as a matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now proceed to demonstrate a non-negative matrix transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will load the data to a NumPy matrix A from the list. We will choose to reduce
    the dimension to two as dictated by the `max_components` variable. We will initialize
    the NMF object with the number of components. Finally, we will apply the algorithm
    to get the reduced matrix, `A_dash`.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s all we need to do. The scikit library hides a lot of details for us.
    Let''s now look at what is happening in the background. Formally, NMF decomposes
    the original matrix into two matrices, which when multiplied together, give the
    approximation of our original matrix. Look at the following line in our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The input matrix `A` is transformed into a reduced matrix, A_dash. Let''s look
    at the shape of the new matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The original matrix is reduced to two columns as compared to the original ten
    columns. This is the reduced space. From this data perspective, we can say that
    our algorithm has now grouped our original ten movies into two concepts. The cell
    value indicates the user affinity towards each of the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will print and see how the affinity looks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at user 1; the first line in the preceding image says that user 1
    has a score of 2.14 for concept 1 and 0 for concept 2, indicating that user 1
    has more affinity towards concept 1.
  prefs: []
  type: TYPE_NORMAL
- en: Look at user ID 3; this user has more affinity towards concept 1\. Now we have
    reduced our input dataset to two dimensions, it would be nice to view this in
    a graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our *x* axis, we have component 1 and our *y* axis is component 2\. We will
    plot the various users as a scatter plot. Our graph looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that we have two groups of users; all those with a component 1 score
    of greater than 1.5 and others with less than 1.5\. We are able to group our users
    into two clusters in the reduced feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the other matrix, `F`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The `F` matrix has two rows; each row corresponds to our component and ten columns,
    each corresponding to a movie ID. Another way to look at it is the affinity of
    movies towards these concepts. Let's plot the matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that our *x* axis is the first row and the *y* axis is the second
    row. In step 1, we declared a dictionary. We want this dictionary to annotate
    each point with the movie name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The annotate method takes the string (used to annotate) as the first parameter
    and the `x` and `y` coordinates as a tuple.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the output graph as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How it works…](img/B04041_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: You can see that we have two distinct groups. All the war movies have a very
    low component 1 score and a very high component 2 score. All the fantasy movies
    have a vice versa score. We can safely say that component 1 comprises of war movies
    and the users having high component 1 scores have very high affinity towards war
    movies. The same can be said for fantasy movies.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using NMF, we are able to unearth the hidden features in our input matrix
    with respect to the movies.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We saw how the feature space was reduced from ten dimensions to two dimensions
    for the users. Now, let''s see how this can be used for the recommendation engines.
    Let''s reconstruct the original matrix from the two matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The reconstructed matrix looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![There''s more…](img/B04041_04_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'How different is it from the original matrix? The original matrix is given
    here; look at the highlighted row:'
  prefs: []
  type: TYPE_NORMAL
- en: '|   | Movie ID |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **User ID** | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 5.0 | 5.0 | 4.5 | 4.5 | 5.0 | 3.0 | 2.0 | 2.0 | 0.0 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 4.2 | 4.7 | 5.0 | 3.7 | 3.5 | 0.0 | 2.7 | 2.0 | 1.9 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 2.5 | 0.0 | 3.3 | 3.4 | 2.2 | 4.6 | 4.0 | 4.7 | 4.2 | 3.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 3.8 | 4.1 | 4.6 | 4.5 | 4.7 | 2.2 | 3.5 | 3.0 | 2.2 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2.1 | 2.6 | 0.0 | 2.1 | 0.0 | 3.8 | 4.8 | 4.1 | 4.3 | 4.7 |'
  prefs: []
  type: TYPE_TB
- en: '| **6** | **4.7** | **4.5** | **0.0** | **4.4** | **4.1** | **3.5** | **3.1**
    | **3.4** | **3.1** | **2.5** |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 2.8 | 2.4 | 2.1 | 3.3 | 3.4 | 3.8 | 4.4 | 4.9 | 4.0 | 4.3 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 4.5 | 4.7 | 4.7 | 4.5 | 4.9 | 0.0 | 2.9 | 2.9 | 2.5 | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 0.0 | 3.3 | 2.9 | 3.6 | 3.1 | 4.0 | 4.2 | 0.0 | 4.5 | 4.6 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 4.1 | 3.6 | 3.7 | 4.6 | 4.0 | 2.6 | 1.9 | 3.0 | 3.6 | 0.0 |'
  prefs: []
  type: TYPE_TB
- en: For user 6 and movie 3, we now have a rating. This will help us decide whether
    to recommend this movie to the user, as he has not watched it. Remember that this
    is a toy dataset; real-world scenarios have many movies and users.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Extracting Features Using Singular Value Decomposition* recipe in [Chapter
    4](ch04.xhtml "Chapter 4. Data Analysis – Deep Dive"), *Analyzing Data - Deep
    Dive*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
