<html><head></head><body><div><div><div><div><div><h1 class="title"><a id="ch05" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Chapter 5. Data Mining – Needle in a Haystack</h1></div></div></div><p class="calibre11">In this chapter, we will cover the following topics:</p><div><ul class="itemizedlist"><li class="listitem">Working with distance measures</li><li class="listitem">Learning and using kernel methods</li><li class="listitem">Clustering data using the k-means method</li><li class="listitem">Learning vector quantization</li><li class="listitem">Finding outliers in univariate data</li><li class="listitem">Discovering outliers using the local outlier factor method</li></ul></div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec60" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Introduction</h1></div></div></div><p class="calibre11">In this chapter, we will focus mostly on unsupervised data mining algorithms. We will start with a recipe covering various distance measures. Understanding distance measures and various spaces is critical when building data science applications. Any dataset is usually a set of points that are objects belonging to a particular space. We can define space as a universal set of points from which the points in our dataset are drawn. The most often encountered space is Euclidean. In Euclidean space, the points are vectors real number. The length of the vector denotes the number of dimensions.</p><p class="calibre11">We then have a recipe introducing kernel methods. Kernel methods are a very important topic in machine learning. They help us solve nonlinear data problems using linear methods. We will introduce the concept of the kernel trick.</p><p class="calibre11">We will follow it with some clustering algorithm recipes. Clustering is the process of partitioning a set of points into logical groups. For example, in a supermarket scenario, items are grouped into categories qualitatively. However, we will look at quantitative approaches. Specifically, we will focus our attention on the k-means algorithm and discuss its limitations and advantages.</p><p class="calibre11">Our next recipe is an unsupervised technique called learning vector quantization. It can be used both for clustering and classification tasks.</p><p class="calibre11">Finally, we will look at the outlier detection methods. Outliers are those observations in a dataset that differ significantly from the other observations in that dataset. It is very important to study these outliers as they might be indicative of unusual phenomena or errors in the underlying process that is generating the data. When machine learning models are fitted over data, it is important to understand how to handle outliers before passing the data to algorithms. We will concentrate on a few empirical outlier detection techniques in this chapter.</p><p class="calibre11">We will rely heavily on the Python libraries, NumPy, SciPy, matplotlib, and scikit-learn for most of our recipes. We will also change our coding style from scripting to writing procedures and classes in this chapter.</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec61" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Working with distance measures</h1></div></div></div><p class="calibre11">Distance and <a id="id345" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>similarity measures are key to various data mining tasks. In this recipe, we will see some distance measures in action. Our next recipe will cover similarity measures. Let's define a distance measure before we look at the various distance metrics.</p><p class="calibre11">As data scientists, we are always presented with points or vectors of different dimensions. Mathematically, a set of points is defined as a space. A distance measure in this space is defined as a function d(x,y), which takes two points x and y as arguments in this space and gives a real number as the output. The distance function, that is, the real number output, should satisfy the following axioms:</p><div><ol class="orderedlist"><li class="listitem1">The distance function output should be non-negative, d(x,y) &gt;= 0</li><li class="listitem1">The output of the distance function should be zero only when x = y</li><li class="listitem1">The distance should be symmetric, that is, d(x,y) = d(y,x)</li><li class="listitem1">The distance should obey the triangle inequality, that is, d(x,y) &lt;= d(x,z) + d(z,y)</li></ol></div><p class="calibre11">A careful look at the fourth axiom reveals that distance is the length of the shortest path between two points.</p><p class="calibre11">You can refer to the <a id="id346" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>following link for more information on the axioms:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Metric_%28mathematics%29">http://en.wikipedia.org/wiki/Metric_%28mathematics%29</a>
</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec209" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">We will look at <a id="id347" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distance measures in Euclidean and non-Euclidean spaces. We will start with Euclidean distance and then define Lr–norm distance. Lr-norm is a family of distance measures of which Euclidean is a member. We will then follow it with the cosine distance. In non-Euclidean spaces, we will look at Jaccard's distance and Hamming distance.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec210" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's start by <a id="id348" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>defining the functions to calculate the various distance measures:</p><div><pre class="programlisting">import numpy as np

def euclidean_distance(x,y):
    if len(x) == len(y):
        return np.sqrt(np.sum(np.power((x-y),2)))
    else:
        print "Input should be of equal length"
    return None


def lrNorm_distance(x,y,power):
    if len(x) == len(y):
        return np.power(np.sum (np.power((x-y),power)),(1/(1.0*power)))
    else:
        print "Input should be of equal length"
    return None


def cosine_distance(x,y):
    if len(x) == len(y):
        return np.dot(x,y) / np.sqrt(np.dot(x,x) * np.dot(y,y))
    else:
        print "Input should be of equal length"
    return None


def jaccard_distance(x,y):
    set_x = set(x)
    set_y = set(y)
    return 1 - len(set_x.intersection(set_y)) / len(set_x.union(set_y))

def hamming_distance(x,y):
    diff = 0
    if len(x) == len(y):
        for char1,char2 in zip(x,y):
            if char1 != char2:
                diff+=1
        return diff
    else:
        print "Input should be of equal length"
    return None</pre></div><p class="calibre11">Now, let's write a <a id="id349" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>main routine in order to invoke these various distance measure functions:</p><div><pre class="programlisting">if __name__ == "__main__":

    # Sample data, 2 vectors of dimension 3
    x = np.asarray([1,2,3])
    y = np.asarray([1,2,3])
    # print euclidean distance    
    print euclidean_distance(x,y)
    # Print euclidean by invoking lr norm with
    # r value of 2    
    print lrNorm_distance(x,y,2)
    # Manhattan or citi block Distance
    print lrNorm_distance(x,y,1)
    
    # Sample data for cosine distance
    x =[1,1]
    y =[1,0]
    print 'cosine distance'
    print cosine_distance(x,y)
    
    # Sample data for jaccard distance    
    x = [1,2,3]
    y = [1,2,3]
    print jaccard_distance(x,y)
    
    # Sample data for hamming distance    
    x =[11001]
    y =[11011]
    print hamming_distance(x,y)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec211" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's look at the main function. We created a sample dataset and two vectors of three dimensions and invoked the <code class="literal">euclidean_distance</code> function.</p><p class="calibre11">This is the most common <a id="id350" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>distance measure used is Euclidean distance. It belongs to a family of the Lr-Norm distance. A space is defined as a Euclidean space if the points in this space are vectors composed of real numbers. It's also called the L2-norm distance. The formula for Euclidean distance is as follows:</p><div><img src="img/B04041_05_04.jpg" alt="How it works…" class="calibre84"/></div><p class="calibre11">As you can see, Euclidean distance is derived by finding the distance in each dimension (subtracting the corresponding dimensions), squaring the distance, and finally taking a square root.</p><p class="calibre11">In our code, we leverage NumPy square root and power function in order to implement the preceding formula:</p><div><pre class="programlisting">np.sqrt(np.sum(np.power((x-y),2)))</pre></div><p class="calibre11">Euclidean distance is strictly positive. When x is equal to y, the distance is zero. This should become clear from how we invoked Euclidean distance:</p><div><pre class="programlisting">x = np.asarray([1,2,3])
y = np.asarray([1,2,3])

print euclidean_distance(x,y)</pre></div><p class="calibre11">As you can see, we defined two NumPy arrays, <code class="literal">x</code> and <code class="literal">y</code>. We have kept them the same. Now, when we invoke the <code class="literal">euclidean_distance</code> function with these parameters, our output is zero.</p><p class="calibre11">Let's now invoke the L2-norm function, <code class="literal">lrNorm_distance</code>.</p><p class="calibre11">The Lr-Norm distance metric is from a family of distance metrics of which Euclidean distance is a member. This <a id="id351" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>should become clear as we see its formula:</p><div><img src="img/B04041_05_05.jpg" alt="How it works…" class="calibre85"/></div><p class="calibre11">You can see that we now have a parameter, <code class="literal">r</code>. Let's substitute <code class="literal">r</code> with 2. This will turn the preceding equation to a Euclidean equation. Hence, Euclidean is called the L2-norm distance:</p><div><pre class="programlisting">lrNorm_distance(x,y,power):</pre></div><p class="calibre11">In addition to two vectors, we will also pass a third parameter called <code class="literal">power</code>. This is the <code class="literal">r</code> defined in the formula. Invoking it with a power value set to two will yield the Euclidean distance. You can check it by running the following code:</p><div><pre class="programlisting">print lrNorm_distance(x,y,2)</pre></div><p class="calibre11">This will yield zero as a result, which is similar to the Euclidean distance function.</p><p class="calibre11">Let's define two sample vectors, <code class="literal">x</code> and <code class="literal">y</code>, and invoke the <code class="literal">cosine_distance</code> function.</p><p class="calibre11">In the spaces where the points are considered as directions, the cosine distance yields a cosine of the angle between the given input vectors as a distance value. Both the Euclidean space also the spaces where the points are vectors of integers or Boolean values, are candidate spaces where the cosine distance function can be applied. The cosine of the angle between the input vectors is the ratio of a dot product of the input vectors to the product of an L2-norm of individual input vectors:</p><div><pre class="programlisting">np.dot(x,y) / np.sqrt(np.dot(x,x) * np.dot(y,y))</pre></div><p class="calibre11">Let's look at the numerator where the dot product between the input vector is calculated:</p><div><pre class="programlisting">np.dot(x,y)</pre></div><p class="calibre11">We will use the NumPy dot function to get the dot product value. The dot product for the two vectors, <code class="literal">x</code> and <code class="literal">y</code>, is defined as follows:</p><div><img src="img/B04041_05_06.jpg" alt="How it works…" class="calibre86"/></div><p class="calibre11">Now, let's look at the denominator:</p><div><pre class="programlisting">np.sqrt(np.dot(x,x) * np.dot(y,y))</pre></div><p class="calibre11">We again use the dot function to find the L2-norm of our input vectors:</p><div><pre class="programlisting">np.dot(x,x) is equivalent to 

tot = 0
for i in range(len(x)):
tot+=x[i] * x[i]</pre></div><p class="calibre11">Thus, we can <a id="id352" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the cosine of the angle between the two input vectors.</p><p class="calibre11">We will move on to Jaccard's distance. Similar to the previous invocations, we will define the sample vectors and invoke the <code class="literal">jaccard_distance</code> function.</p><p class="calibre11">From vectors of real values, let's move on to sets. Commonly called Jaccard's coefficient, it is the ratio of the sizes of the intersection and the union of the given input vectors. One minus this value gives the Jaccard's distance. As you can see, in the implementation, we first converted the input lists to sets. This will allows us to leverage the union and intersection operations provided by the Python set datatype:</p><div><pre class="programlisting">set_x = set(x)
set_y = set(y)</pre></div><p class="calibre11">Finally, the distance is calculated as follows:</p><div><pre class="programlisting">1 - len(set_x.intersection(set_y)) / (1.0 * len(set_x.union(set_y)))</pre></div><p class="calibre11">We must use the intersection and union functionalities that are available in the <code class="literal">set</code> datatype in order to calculate the distance.</p><p class="calibre11">Our last distance metric is the Hamming distance. With two bit vectors, the Hamming distance calculates how many bits have differed in these two vectors:</p><div><pre class="programlisting">for char1,char2 in zip(x,y):
    if char1 != char2:
        diff+=1
return diff</pre></div><p class="calibre11">As you can see, we used the <code class="literal">zip</code> functionality to check each of the bits and maintain a counter on how many bits have differed. The Hamming distance is used with a categorical variable.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec212" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">Remember that by subtracting one from our distance values, we can arrive at a similarity value.</p><p class="calibre11">Yet another distance that we didn't go into in detail, but is used prevalently, is the Manhattan or city block distance. It's an L1-norm distance. By passing an r value as 1 to the Lr-norm distance function, we will get the Manhattan distance.</p><p class="calibre11">Depending on the underlying space in which the data is placed, an appropriate distance measure needs to be selected. When using these distances in algorithms, we need to be mindful about the underlying space. For example, in the k-means algorithm, at every step cluster center is calculated as an average of all the points that are close to each other. A nice property of Euclidean is that the average of the points exists and as a point in the same <a id="id353" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>space. Note that our input for the Jaccard's distance was sets. An average of the sets does not make any sense.</p><p class="calibre11">While using the cosine distance, we need to check whether the underlying space is Euclidean or not. If the elements of the vectors are real numbers, then the space is Euclidean, if they are integers, then the space is non-Euclidean. The cosine distance is most commonly used in text mining. In text mining, the words are considered as the axes, and a document is a vector in this space. The cosine of the angle between two document vectors denotes how similar the two documents are.</p><p class="calibre11">SciPy has an <a id="id354" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>implementation of all these distance measures listed and much more at:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://docs.scipy.org/doc/scipy/reference/spatial.distance.html">http://docs.scipy.org/doc/scipy/reference/spatial.distance.html</a>.</p><p class="calibre11">The above URL lists all the distance measures supported by SciPy.</p><p class="calibre11">Additionally, the <a id="id355" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>scikit-learn <code class="literal">pairwise</code> submodule provides you with a method called <code class="literal">pairwise_distance</code>, which can be used to find out the distance matrix from input records. This can be found at:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikitlearn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html">http://scikitlearn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html</a>.</p><p class="calibre11">We had mentioned that the Hamming distance is used with a categorical variable. A point worth mentioning here is the one-hot encoding that is used typically for categorical variables. After the one-hot encoding, the Hamming distance can be used as a similarity/distance measure between the input vectors.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec213" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Reducing data dimension with Random Projections</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec62" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning and using kernel methods</h1></div></div></div><p class="calibre11">In this recipe, we will learn how to use kernel methods for data processing. Having the knowledge of <a id="id356" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kernels in your arsenal of methods will help you in dealing with nonlinear problems. This recipe is an introduction to kernel methods.</p><p class="calibre11">Typically, linear <a id="id357" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>models—models that can separate the data using a straight line or hyper plane—are easy to interpret and understand. Nonlinearity in the data stops us from using linear models effectively. If the data can be transformed into a space where the relationship becomes linear, we can use linear models. However, mathematical computation in the transformed space can turn into a costly operation. This is where the kernel functions come to our rescue.</p><p class="calibre11">Kernels are similarity functions. It takes two input parameters, and the similarity between the two inputs is the output of the kernel function. In this recipe, we will look at how kernel achieves this similarity. We will also discuss what is called a kernel trick.</p><p class="calibre11">Formally defining a kernel K is a similarity function: K(x1,x2) &gt; 0 denotes the similarity of x1 and x2.</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec214" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Let's define it mathematically before looking at the various kernels:</p><div><img src="img/B04041_05_07.jpg" alt="Getting ready" class="calibre87"/></div><p class="calibre11">Here, <code class="literal">xi</code> and, <code class="literal">xj</code> are the input vectors:</p><div><img src="img/B04041_05_08.jpg" alt="Getting ready" class="calibre88"/></div><p class="calibre11">The above mapping function is used to transform the input vectors into a new space. For example, if the input vector is in an n-dimensional space, the transformation function transforms it into a new space of dimension, m, where m &gt;&gt; n:</p><div><img src="img/B04041_05_09.jpg" alt="Getting ready" class="calibre89"/></div><div><img src="img/B04041_05_10.jpg" alt="Getting ready" class="calibre90"/></div><p class="calibre11">The above image denotes the dot product:</p><div><img src="img/B04041_05_09.jpg" alt="Getting ready" class="calibre89"/></div><p class="calibre11">The above image is the dot product, <code class="literal">xi</code> and <code class="literal">xj</code> are now transformed into a new space by the mapping function.</p><p class="calibre11">In this recipe, we will see a simple kernel in action.</p><p class="calibre11">Our mapping function will be as follows:</p><div><img src="img/B04041_05_20.jpg" alt="Getting ready" class="calibre91"/></div><p class="calibre11">When the original data is supplied to this mapping function, it transforms the input into the new space.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec215" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's create two input <a id="id358" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vectors and define the mapping function as described <a id="id359" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>in the previous section:</p><div><pre class="programlisting">import numpy as np
# Simple example to illustrate Kernel Function concept.
# 3 Dimensional input space
x = np.array([10,20,30])
y = np.array([8,9,10])

# Let us find a mapping function to transform this space
# phi(x1,x2,x3) = (x1x2,x1x3,x2x3,x1x1,x2x2,x3x3)
# this will transorm the input space into 6 dimesions

def mapping_function(x):
    output_list  =[]
    for i in range(len(x)):
        output_list.append(x[i]*x[i])
    
    output_list.append(x[0]*x[1])
    output_list.append(x[0]*x[2])
    output_list.append(x[1]*x[0])
    output_list.append(x[1]*x[2])
    output_list.append(x[2]*x[1])
    output_list.append(x[2]*x[0])
    return np.array(output_list)</pre></div><p class="calibre11">Now, let's look at the main routine to invoke the kernel transformation. In the main function, we will define a <a id="id360" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kernel function and pass the input variable to the <a id="id361" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function, and print the output:</p><div><img src="img/B04041_05_15.jpg" alt="How to do it…" class="calibre92"/></div><div><pre class="programlisting">if __name_ == "__main__"
    # Apply the mapping function
    tranf_x = mapping_function(x)
    tranf_y = mapping_function(y)
    # Print the output
    print tranf_x
    print np.dot(tranf_x,tranf_y)
    
    # Print the equivalent kernel functions
    # transformation output.
    output = np.power((np.dot(x,y)),2)
    print output</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec216" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">Let's follow this program from our main function. We created two input vectors, <code class="literal">x</code> and <code class="literal">y</code>. Both the vectors are of three dimensions.</p><p class="calibre11">We then defined a mapping function. The mapping function uses the input vector values and transforms the input vector into a new space with an increased dimension. In this case, the number of the dimension is increased to nine from three.</p><p class="calibre11">Let's now apply a mapping function on these vectors in order to increase their dimension to nine.</p><p class="calibre11">If we print <code class="literal">tranf_x</code>, we will get the following:</p><div><pre class="programlisting">[100 400 900 200 300 200 600 600 300]</pre></div><p class="calibre11">As you can see, we transformed our input, x, from three dimensions to a nine-dimensional vector.</p><p class="calibre11">Now, let's take the dot product in the transformed space and print its output.</p><p class="calibre11">The output is 313600, a scalar value.</p><p class="calibre11">Let's now recap: we first transformed our two input vectors into a higher dimensional space and then calculated the dot product in order to derive a scalar output.</p><p class="calibre11">What we did was a very costly operation of transforming our original three-dimensional vector to a nine-dimensional vector and then performing the dot product operation on it.</p><p class="calibre11">Instead, we can choose a <a id="id362" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kernel function, which can arrive at the same scalar <a id="id363" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>output without explicitly transforming the original space into a new space.</p><p class="calibre11">Our new kernel is defined as follows:</p><div><img src="img/B04041_05_11.jpg" alt="How it works…" class="calibre92"/></div><p class="calibre11">With two inputs, <code class="literal">x</code> and <code class="literal">y</code>, this kernel computes the dot product of the vectors, and squares them.</p><p class="calibre11">After printing the output from the kernel, we get 313600.</p><p class="calibre11">We never did the transformation but still were able to get the same result as the dot product output in the transformed space. This is called the kernel trick.</p><p class="calibre11">There was no magic in choosing this kernel. By expanding the kernel, we can arrive at our mapping <a id="id364" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function. Refer to the following reference for the expansion details:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Polynomial_kernel">http://en.wikipedia.org/wiki/Polynomial_kernel</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec217" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">There are several types of kernels. Based on our data characteristics and algorithm needs, we need to choose the right kernel. Some of them are as follows:</p><p class="calibre11">Linear kernel: This is the <a id="id365" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>simplest kind of kernel function. For two given <a id="id366" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>inputs, it returns the dot product <a id="id367" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>of the input:</p><div><img src="img/B04041_05_12.jpg" alt="There's more..." class="calibre93"/></div><p class="calibre11">Polynomial kernel: This <a id="id368" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is defined as follows:</p><div><img src="img/B04041_05_13.jpg" alt="There's more..." class="calibre94"/></div><p class="calibre11">Here, <code class="literal">x</code> and <code class="literal">y</code> are the input <a id="id369" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>vectors, <code class="literal">d</code> is the degree of the polynomial, and <code class="literal">c</code> is a <a id="id370" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>constant. In our recipe, we used a polynomial kernel of degree 2.</p><p class="calibre11">The following <a id="id371" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>is the scikit implementation of the linear and polynomial <a id="id372" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>kernels:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html#sklearn.metrics.pairwise.linear_kernel</a>
</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel">http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.polynomial_kernel.html#sklearn.metrics.pairwise.polynomial_kernel</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec218" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Using Kernel PCA</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li><li class="listitem"><em class="calibre15">Reducing data dimension with Random Projections</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch04.xhtml" title="Chapter 4. Data Analysis – Deep Dive">Chapter 4</a>, <em class="calibre15">Analyzing Data - Deep Dive</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec63" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Clustering data using the k-means method</h1></div></div></div><p class="calibre11">In this recipe, we <a id="id373" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>will look at the k-means algorithm. K-means is a center-seeking unsupervised algorithm. It is an iterative non-deterministic <a id="id374" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>method. What we mean by iterative is that the algorithm steps are repeated till the convergence of a specified number of steps. Non-deterministic means that a different starting value may lead to a different final cluster assignment. The algorithm requires the number of clusters, <code class="literal">k</code>, as input. There is no good way to select the value of <code class="literal">k</code>, it has to be determined by running the algorithm multiple times.</p><p class="calibre11">For any clustering algorithm, the quality of its output is determined by inter-cluster cohesiveness and intra-cluster separation. Points in the same cluster should be close to each other; points in different clusters should be far away from each other.</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec219" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">Before we jump into how to write the k-means algorithm in Python, there are two key concepts that we need to cover that will help us understand better the quality of the output produced by our algorithm. First is a definition with respect to the quality of the clusters formed, <a id="id375" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and second is a metric <a id="id376" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>that is used to find the quality of the clusters.</p><p class="calibre11">Every cluster detected <a id="id377" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>by k-means can be evaluated using the following measures:</p><div><ol class="orderedlist"><li class="listitem1"><strong class="calibre12">Cluster location</strong>: This is the coordinates of the cluster center. K-means starts with some random points as the cluster center and iteratively finds a new center around which points that are similar are grouped.</li><li class="listitem1"><strong class="calibre12">Cluster radius</strong>: This is the average deviation of all the points from the cluster center.</li><li class="listitem1"><strong class="calibre12">Mass of the cluster</strong>: This is the number of points in a cluster.</li><li class="listitem1"><strong class="calibre12">Density of the cluster</strong>: This is the ratio of mass of the cluster to its radius.</li></ol></div><p class="calibre11">Now, we will measure the quality of our output clusters. As mentioned previously, this is an unsupervised problem and we don't have labels against which to check our output in order to get measures such as precision, recall, accuracy, F1-score, or other similar metrics. The metric that we will use for our k-means algorithm is called a silhouette coefficient. It takes values in the range of -1 to 1. Negative values indicate that the cluster radius is greater than the distance between the clusters so that the clusters overlap. This suggests poor clustering. Large values, that is, values close to 1, indicate good clustering.</p><p class="calibre11">A silhouette coefficient is defined for each point in the cluster. With a cluster, C, and a point, <code class="literal">i</code>, in this cluster, let <code class="literal">xi</code> be the average distance of this point from all the other points in the cluster.</p><p class="calibre11">Now, calculate the average distance that the point <code class="literal">i</code> has from all the points in another cluster, D. Pick the smallest of these values and call it <code class="literal">yi</code>:</p><div><img src="img/B04041_05_14.jpg" alt="Getting ready" class="calibre95"/></div><p class="calibre11">For every cluster, the average of the silhouette coefficient of all the points can serve as a good measure of the cluster quality. An average of the silhouette coefficient of all the data points can serve as an overall quality metric for the clusters formed.</p><p class="calibre11">Let's go ahead and generate some random data:</p><div><pre class="programlisting">import numpy as np
import matplotlib.pyplot as plt

def get_random_data():
    x_1 = np.random.normal(loc=0.2,scale=0.2,size=(100,100))
    x_2 = np.random.normal(loc=0.9,scale=0.1,size=(100,100))
    x = np.r_[x_1,x_2]
    return x</pre></div><p class="calibre11">We sampled two <a id="id378" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sets of data from a normal distribution. The <a id="id379" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>first set was picked up with a mean of <code class="literal">0.2</code> and standard deviation of <code class="literal">0.2</code>. For the second set, our mean value was <code class="literal">0.9</code> and standard deviation was <code class="literal">0.1</code>. Each dataset was a matrix of size 100 * 100—we have <code class="literal">100</code> instances and <code class="literal">100</code> dimensions. Finally, we merged both of them using the row stacking function from NumPy. Our final dataset was of size 200 * 100.</p><p class="calibre11">Let's do a scatter plot of the data:</p><div><pre class="programlisting">x = get_random_data()

plt.cla()
plt.figure(1)
plt.title("Generated Data")
plt.scatter(x[:,0],x[:,1])
plt.show()</pre></div><p class="calibre11">The plot is as follows:</p><div><img src="img/B04041_05_01.jpg" alt="Getting ready" class="calibre96"/></div><p class="calibre11">Though we plotted only the first and second dimension, you can still clearly see that we have two clusters. Let's <a id="id380" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>now jump into writing our k-means <a id="id381" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>clustering algorithm.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec220" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><p class="calibre11">Let's define a function that can perform the k-means clustering for the given data and a parameter, <code class="literal">k</code>. The function fits the clustering on the given data and returns an overall silhouette coefficient.</p><div><pre class="programlisting">from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


def form_clusters(x,k):
    """
    Build clusters
    """
    # k = required number of clusters
    no_clusters = k
    model = KMeans(n_clusters=no_clusters,init='random')
    model.fit(x)
    labels = model.labels_
    print labels
    # Cacluate the silhouette score
    sh_score = silhouette_score(x,labels)
    return sh_score</pre></div><p class="calibre11">Let's invoke the <a id="id382" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>preceding function for the different values of <code class="literal">k</code> <a id="id383" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>and store the returned silhouette coefficient:</p><div><pre class="programlisting">sh_scores = []
for i in range(1,5):
    sh_score = form_clusters(x,i+1)
    sh_scores.append(sh_score)

no_clusters = [i+1 for i in range(1,5)]</pre></div><p class="calibre11">Finally, let's plot the silhouette coefficient for the different values of <code class="literal">k</code>.</p><div><pre class="programlisting">no_clusters = [i+1 for i in range(1,5)]

plt.figure(2)
plt.plot(no_clusters,sh_scores)
plt.title("Cluster Quality")
plt.xlabel("No of clusters k")
plt.ylabel("Silhouette Coefficient")
plt.show()</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec221" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">As mentioned previously, k-means is an iterative algorithm. Roughly, the steps of k-means are as follows:</p><div><ol class="orderedlist"><li class="listitem1">Initialize <code class="literal">k</code> random points from the dataset as initial center points.</li><li class="listitem1">Do the following till the convergence of the specified number of times:<div><ul class="itemizedlist1"><li class="listitem">Assign the points to the closest cluster center. Typically, Euclidean distance is used to find the distance between a point and the cluster center.</li><li class="listitem">Recalculate the new cluster centers based on the assignment in this iteration.</li><li class="listitem">Exit the loop if a cluster assignment of the points remains the same as the previous iteration. The algorithm has converged to an optimal solution.</li></ul></div></li><li class="listitem1">We will leverage the k-means implementation from the scikit-learn library. Our cluster function takes the k value and dataset as a parameter and runs the k-means algorithm:<div><pre class="programlisting1">model = KMeans(n_clusters=no_clusters,init='random')
model.fit(x)</pre></div></li></ol></div><p class="calibre11">The <code class="literal">no_clusters</code> is the <a id="id384" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>parameter that we will pass to the function. Using the init parameter, we set the initial center points as random. When init is <a id="id385" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>set to random, scikit-learn estimates the mean and variance from the data and then samples k centers from a Gaussian distribution.</p><p class="calibre11">Finally, we must call the fit method to run k-means on our dataset:</p><div><pre class="programlisting">labels = model.labels_
sh_score = silhouette_score(x,labels)
return sh_score</pre></div><p class="calibre11">We get the labels, that is, the cluster assignment for each point and find out the silhouette coefficient for all the points in our cluster.</p><p class="calibre11">In real-world scenarios, when we start with the k-means algorithm on a dataset, we don't know the number of clusters present in the data; in other words, we don't know the ideal value for k. However, in our example, we know that k=2 as we generated the data in such a manner that it fits in two clusters. Hence, we need to run k-means for the different values of k:</p><div><pre class="programlisting">sh_scores = []
for i in range(1,5):
sh_score = form_clusters(x,i+1)
sh_scores.append(sh_score)</pre></div><p class="calibre11">For each run, that is, each value of k, we store the silhouette coefficient. A plot of k versus the silhouette coefficient reveals the ideal k value for the dataset:</p><div><pre class="programlisting">no_clusters = [i+1 for i in range(1,5)]

plt.figure(2)
plt.plot(no_clusters,sh_scores)
plt.title("Cluster Quality")
plt.xlabel("No of clusters k")
plt.ylabel("Silhouette Coefficient")
plt.show()</pre></div><div><img src="img/B04041_05_02.jpg" alt="How it works…" class="calibre96"/></div><p class="calibre11">As expected, our silhouette coefficient is very high for k=2.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec222" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">A couple of points to be <a id="id386" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>noted about k-means. The k-means algorithm cannot be used for categorical data, k-medoids is used. Instead of averaging all the points in a cluster in order to find the cluster <a id="id387" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>center, k-medoids selects a point that has the smallest average distance to all the other points in the cluster.</p><p class="calibre11">Care needs to be taken while assigning the initial cluster. If the data is very dense with very widely separated clusters, and if the initial random centers are chosen in the same cluster, k-means may not perform very well.</p><p class="calibre11">Typically, k-means works if the data has star convex clusters. Refer to the following link for more information on star convex-shaped data points:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://mathworld.wolfram.com/StarConvex.html">http://mathworld.wolfram.com/StarConvex.html</a>
</p><p class="calibre11">The presence of <a id="id388" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>nested or other complicated clusters will result in a junk output from k-means.</p><p class="calibre11">The presence of outliers in the data may yield poor results. A good practice is to do a thorough data exploration in order to identify the data characteristics before running k-means.</p><p class="calibre11">An alternative method <a id="id389" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>to initialize the centers during the <a id="id390" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>beginning of the algorithm is the k-means++ method. So, instead of setting the init parameter to random, we can set it using k-means++. Refer to the following paper for k-means++:</p><p class="calibre11">
<em class="calibre15">k-means++: the advantages of careful seeding</em>. <em class="calibre15">ACM-SIAM symposium</em> on <em class="calibre15">Discrete algorithms. 2007</em>
</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec223" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Working with Distance Measures</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Data Mining – Needle in a Haystack">Chapter 5</a>, <em class="calibre15">Data Mining - Finding a needle in a haystack</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec66" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Discovering outliers using the local outlier factor method</h1></div></div></div><p class="calibre11">The Local Outlier Factor (LOF) is an outlier detection algorithm that detects the outliers based on comparing the local density of the data instance with its neighbors. It does this in order to <a id="id420" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>decide if the data instance <a id="id421" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>belongs to a region of similar density. It can detect an outlier in a dataset, in such circumstances where the number of clusters are unknown and the clusters are of different density and sizes. It's inspired by the KNN (K-Nearest Neighbors) algorithm and is widely used.</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec234" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">In the previous recipe, we looked at univariate data. In this one, we will use multivariate data and try to find outliers. Let's use a very small dataset to understand the LOF algorithm for outlier detection.</p><p class="calibre11">We will create a 5 X 2 matrix, and looking at the data, we know that the last tuple is an outlier. Let's also plot it as a scatter plot:</p><div><pre class="programlisting">from collections import defaultdict
import numpy as np

instances = np.matrix([[0,0],[0,1],[1,1],[1,0],[5,0]])

import numpy as np
import matplotlib.pyplot as plt

x = np.squeeze(np.asarray(instances[:,0]))
y = np.squeeze(np.asarray(instances[:,1]))
plt.cla()
plt.figure(1)
plt.scatter(x,y)
plt.show()</pre></div><p class="calibre11">The plot looks as follows:</p><div><img src="img/B04041_05_19.jpg" alt="Getting ready" class="calibre45"/></div><p class="calibre11">LOF works by <a id="id422" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculating the local <a id="id423" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>density of each point. Based on the distance of k-nearest neighbors of a point, the local density of the point is estimated. By comparing the local density of the point with the densities of its neighbors, outliers are detected. Outliers have a low density compared with their neighbors.</p><p class="calibre11">We will need to go through some term definitions in order to understand LOF:</p><div><ul class="itemizedlist"><li class="listitem">The k-distance of object P is the distance between the object P and its kth nearest neighbor. K is a parameter of the algorithm.</li><li class="listitem">The k-distance neighborhood of P is the list of all the objects, Q, whose distance from P is either less than or equal to the distance between P and its kth nearest object.</li><li class="listitem">The reachability distance from P to Q is defined as the maximum of the distance between P and its kth nearest neighbor, and the distance between P and Q. The following notation may help clarify this:<div><pre class="programlisting1">Reachability distance (P ß Q) = &gt; maximum(K-Distance(P), Distance(P,Q))</pre></div></li><li class="listitem">The Local Reachability Density of P (LRD(P)) is the ratio of the k-distance neighborhood of P and the sum of the reachability distance of k and its neighborhood.</li><li class="listitem">The <a id="id424" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Local Outlier <a id="id425" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Factor of P (LOF(P)) is the average of the ratio of the local reachability of P and those of P's k-nearest neighbors.</li></ul></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec235" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><div><ol class="orderedlist"><li class="listitem1">Let's get the <code class="literal">pairwise</code> distance between the points:<div><pre class="programlisting1">k = 2
distance = 'manhattan'


from sklearn.metrics import pairwise_distances
dist = pairwise_distances(instances,metric=distance)</pre></div></li><li class="listitem1">Let's calculate the k-distance. We will use <code class="literal">heapq</code> and get the k-nearest neighbors:<div><pre class="programlisting1"># Calculate K distance
import heapq
k_distance = defaultdict(tuple)
# For each data point
for i in range(instances.shape[0]):
    # Get its distance to all the other points.
    # Convert array into list for convienience
    distances = dist[i].tolist()
    # Get the K nearest neighbours
    ksmallest = heapq.nsmallest(k+1,distances)[1:][k-1]
    # Get their indices
    ksmallest_idx = distances.index(ksmallest)
    # For each data point store the K th nearest neighbour and its distance
    k_distance[i]=(ksmallest,ksmallest_idx)</pre></div></li><li class="listitem1">Calculate <a id="id426" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the k-distance <a id="id427" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neighborhood:<div><pre class="programlisting1">def all_indices(value, inlist):
    out_indices = []
    idx = -1
    while True:
        try:
            idx = inlist.index(value, idx+1)
            out_indices.append(idx)
        except ValueError:
            break
    return out_indices
# Calculate K distance neighbourhood
import heapq
k_distance_neig = defaultdict(list)
# For each data point
for i in range(instances.shape[0]):
    # Get the points distances to its neighbours
    distances = dist[i].tolist()
    print "k distance neighbourhood",i
    print distances
    # Get the 1 to K nearest neighbours
    ksmallest = heapq.nsmallest(k+1,distances)[1:]
    print ksmallest
    ksmallest_set = set(ksmallest)
    print ksmallest_set
    ksmallest_idx = []
    # Get the indices of the K smallest elements
    for x in ksmallest_set:
            ksmallest_idx.append(all_indices(x,distances))
    # Change a list of list to list
    ksmallest_idx = [item for sublist in ksmallest_idx for item in sublist]
    # For each data pont store the K distance neighbourhood
    k_distance_neig[i].extend(zip(ksmallest,ksmallest_idx))</pre></div></li><li class="listitem1">Then, calculate the reachability distance and LRD:<div><pre class="programlisting1">#Local reachable density
local_reach_density = defaultdict(float)
for i in range(instances.shape[0]):
    # LRDs numerator, number of K distance neighbourhood
    no_neighbours = len(k_distance_neig[i])
    denom_sum = 0
    # Reachability distance sum
    for neigh in k_distance_neig[i]:
        # maximum(K-Distance(P), Distance(P,Q))
        denom_sum+=max(k_distance[neigh[1]][0],neigh[0])
    local_reach_density[i] = no_neighbours/(1.0*denom_sum)</pre></div></li><li class="listitem1">Calculate <a id="id428" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>LOF:<div><pre class="programlisting1">lof_list =[]
#Local Outlier Factor
for i in range(instances.shape[0]):
    lrd_sum = 0
    rdist_sum = 0
    for neigh in k_distance_neig[i]:
        lrd_sum+=local_reach_density[neigh[1]]
        rdist_sum+=max(k_distance[neigh[1]][0],neigh[0])
    lof_list.append((i,lrd_sum*rdist_sum))</pre></div></li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec236" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we select our distance metric to be Manhattan and our k value as two. We are looking <a id="id429" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>at the second nearest neighbor for our data point.</p><p class="calibre11">We must then proceed to calculate the pairwise distance between our tuples. The pairwise similarity is stored in the dist matrix. As you can see, the shape of dist is as follows:</p><div><pre class="programlisting">&gt;&gt;&gt; dist.shape
(5, 5)
&gt;&gt;&gt;</pre></div><p class="calibre11">It is a 5 X 5 matrix, where the rows and columns are individual tuples and the cell value indicates the distance between them.</p><p class="calibre11">In step 2, we then import <code class="literal">heapq</code>:</p><div><pre class="programlisting">import heapq</pre></div><p class="calibre11">
<code class="literal">heapq</code> is a data structure that is also known as a priority queue. It is similar to a regular queue except that each element is associated with a priority, and an element with a high priority is served before an element with a low priority.</p><p class="calibre11">Refer to the <a id="id430" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>Wikipedia link for more information on priority queues:</p><p class="calibre11">
<a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://en.wikipedia.org/wiki/Priority_queue">http://en.wikipedia.org/wiki/Priority_queue</a>.</p><p class="calibre11">The Python heapq <a id="id431" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>documentation can be found at <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="https://docs.python.org/2/library/heapq.html">https://docs.python.org/2/library/heapq.html</a>.</p><div><pre class="programlisting">k_distance = defaultdict(tuple)</pre></div><p class="calibre11">Next, we define a dictionary where the key is the tuple ID and the value is the distance of the tuple to its kth nearest neighbor. In our case, it should be the second nearest neighbor.</p><p class="calibre11">We then enter a for loop in order to find the kth nearest neighbor's distance for each of the data points:</p><div><pre class="programlisting">distances = dist[i].tolist()</pre></div><p class="calibre11">From our distance <a id="id432" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>matrix, we extract the ith row. As you can see, the ith row captures the distance between the object <code class="literal">i</code> and all the <a id="id433" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>other objects. Remember that the cell value (<code class="literal">i</code>,<code class="literal">i</code>) holds the distance to itself. We need to ignore this in the next step. We must convert the array to a list for our convenience. Let's try to understand this with an example. The distance matrix looks as follows:</p><div><pre class="programlisting">&gt;&gt;&gt; dist
array([[ 0.,  1.,  2.,  1.,  5.],
       [ 1.,  0.,  1.,  2.,  6.],
       [ 2.,  1.,  0.,  1.,  5.],
       [ 1.,  2.,  1.,  0.,  4.],
       [ 5.,  6.,  5.,  4.,  0.]]) </pre></div><p class="calibre11">Let's assume that we are in the first iteration of our for loop and hence, our <code class="literal">i</code> =<code class="literal">0</code>. (remember that the Python indexing starts with <code class="literal">0</code>).</p><p class="calibre11">So, now our distances list will look as follows:</p><div><pre class="programlisting">[ 0.,  1.,  2.,  1.,  5.]</pre></div><p class="calibre11">From this, we need the kth nearest neighbor, that is, the second nearest neighbor, as we have set K = <code class="literal">2</code> at the beginning of the program.</p><p class="calibre11">Looking at it, we can see that both index 1 and index 3 can be our the kth nearest neighbor as both have a value of <code class="literal">1</code>.</p><p class="calibre11">Now, we use the <code class="literal">heapq.nsmallest</code> function. Remember that we had mentioned that <code class="literal">heapq</code> is a normal queue but with a priority associated with each element. The value of the element is the priority in this case. When we say that give me the n smallest, <code class="literal">heapq</code> will return the smallest elements:</p><div><pre class="programlisting"># Get the Kth nearest neighbours
ksmallest = heapq.nsmallest(k+1,distances)[1:][k-1]</pre></div><p class="calibre11">Let's look at what the <code class="literal">heapq.nsmallest</code> function does:</p><div><pre class="programlisting">&gt;&gt;&gt; help(heapq.nsmallest)
Help on function nsmallest in module heapq:

nsmallest(n, iterable, key=None)
    Find the n smallest elements in a dataset.
    
    Equivalent to:  sorted(iterable, key=key)[:n]</pre></div><p class="calibre11">It returns the n smallest elements from the given dataset. In our case, we need the second nearest neighbor. Additionally, we need to avoid (<code class="literal">i</code>,<code class="literal">i</code>) as mentioned previously. So we must pass n = 3 to <code class="literal">heapq.nsmallest</code>. This ensures that it returns the three smallest elements. We then subset the list to exclude the first element (see [1:] after nsmallest function call) and finally retrieve the second nearest neighbor (see <code class="literal">[k-1]</code> after <code class="literal">[1:]</code>).</p><p class="calibre11">We must also get the <a id="id434" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>index of the second nearest <a id="id435" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>neighbor of <code class="literal">i</code> and store it in our dictionary:</p><div><pre class="programlisting"># Get their indices
ksmallest_idx = distances.index(ksmallest)
# For each data point store the K th nearest neighbour and its distance
k_distance[i]=(ksmallest,ksmallest_idx)</pre></div><p class="calibre11">Let's print our dictionary:</p><div><pre class="programlisting">print k_distance
defaultdict(&lt;type 'tuple'&gt;, {0: (1.0, 1), 1: (1.0, 0), 2: (1.0, 1), 3: (1.0, 0), 4: (5.0, 0)})</pre></div><p class="calibre11">Our tuples have two elements: the distance, and the index of the elements in the distances array. So, for instance <code class="literal">0</code>, the second nearest neighbor is the element in index <code class="literal">1</code>.</p><p class="calibre11">Having calculated the k-distance for all our data points, we then move on to find the k-distance neighborhood.</p><p class="calibre11">In step 3, we find the k-distance neighborhood for each of our data points:</p><div><pre class="programlisting"># Calculate K distance neighbourhood
import heapq
k_distance_neig = defaultdict(list)</pre></div><p class="calibre11">Similar to our previous step, we import the heapq module and declare a dictionary that is going to hold our k-distance neighborhood details. Let's recap what the k-distance neighborhood is:</p><p class="calibre11">The k-distance neighborhood of P is the list of all the objects, Q, whose distance from P is either less than or equal to the distance between P and its kth nearest object:</p><div><pre class="programlisting">distances = dist[i].tolist()
# Get the 1 to K nearest neighbours
ksmallest = heapq.nsmallest(k+1,distances)[1:]
ksmallest_set = set(ksmallest)</pre></div><p class="calibre11">The first two lines should be familiar to you. We did this in our previous step. Look at the second line. Here, we invoked n smallest again with <code class="literal">n=3</code> in our case (K+1), but we selected all the elements in the output list except the first one. (Guess why? The answer is in the previous step.)</p><p class="calibre11">Let's see it in action by printing the values. As usual, in the loop, we assume that we are seeing the first data point or tuple where i=0.</p><p class="calibre11">Our distances list is as follows:</p><div><pre class="programlisting">[0.0, 1.0, 2.0, 1.0, 5.0]</pre></div><p class="calibre11">Our <a id="id436" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>
<code class="literal">heapq.nsmallest</code> function <a id="id437" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>returns the following:</p><div><pre class="programlisting">[1.0, 1.0]</pre></div><p class="calibre11">These are 1 to k-nearest neighbor's distances. We need to find their indices, a simple list.index function will only return the first match, so we will write the <code class="literal">all_indices</code> function in order to retrieve all the indices:</p><div><pre class="programlisting">def all_indices(value, inlist):
    out_indices = []
    idx = -1
    while True:
        try:
            idx = inlist.index(value, idx+1)
            out_indices.append(idx)
        except ValueError:
            break
    return out_indices</pre></div><p class="calibre11">With a value and list, <code class="literal">all_indices</code> will return all the indices where the value occurs in the list. We must convert our k smallest to a set:</p><div><pre class="programlisting">ksmallest_set = set(ksmallest)</pre></div><p class="calibre11">So, [1.0,1.0] becomes a set ([1.0]). Now, using a for loop, we can find all the indices of the elements:</p><div><pre class="programlisting"># Get the indices of the K smallest elements
for x in ksmallest_set:
ksmallest_idx.append(all_indices(x,distances))</pre></div><p class="calibre11">We get two indices for 1.0; they are 1 and 2:</p><div><pre class="programlisting">ksmallest_idx = [item for sublist in ksmallest_idx for item in sublist]</pre></div><p class="calibre11">The next for loop is to convert a list of the lists to a list. The <code class="literal">all_indices</code> function returns a list, and we then append this list to the <code class="literal">ksmallest_idx</code> list. Hence, we flatten it using the next for loop.</p><p class="calibre11">Finally, we add the k smallest neighborhood to our dictionary:</p><div><pre class="programlisting">k_distance_neig[i].extend(zip(ksmallest,ksmallest_idx))</pre></div><p class="calibre11">We then add <a id="id438" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>tuples where the first item <a id="id439" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>in the tuple is the distance and the second item is the index of the nearest neighbor. Let's print the k-distance neighborhood dictionary:</p><div><pre class="programlisting">defaultdict(&lt;type 'list'&gt;, {0: [(1.0, 1), (1.0, 3)], 1: [(1.0, 0), (1.0, 2)], 2: [(1.0, 1), (1.0, 3)], 3: [(1.0, 0), (1.0, 2)], 4: [(4.0, 3), (5.0, 0)]})</pre></div><p class="calibre11">In step 4, we calculate the LRD. The LRD is calculated using the reachability distance. Let's recap both the definitions:</p><div><ul class="itemizedlist"><li class="listitem">The reachability distance from P to Q is defined as the maximum of the distance between P and its kth nearest neighbor, and the distance between P and Q. The following notation may help clarify this:<div><pre class="programlisting1">Reachability distance (P ß Q) = &gt; maximum(K-Distance(P), Distance(P,Q))</pre></div></li><li class="listitem">The Local Reachability density of P (LRD(P)) is the ratio of the k-distance neighborhood of P and the sum of the reachability distance of k and its neighborhood:<div><pre class="programlisting1">#Local reachable density
local_reach_density = defaultdict(float)</pre></div></li></ul></div><p class="calibre11">We will first declare a dictionary in order to store the LRD:</p><div><pre class="programlisting">for i in range(instances.shape[0]):
# LRDs numerator, number of K distance neighbourhood
no_neighbours = len(k_distance_neig[i])
denom_sum = 0
# Reachability distance sum
for neigh in k_distance_neig[i]:
# maximum(K-Distance(P), Distance(P,Q))
denom_sum+=max(k_distance[neigh[1]][0],neigh[0])
   local_reach_density[i] = no_neighbours/(1.0*denom_sum)</pre></div><p class="calibre11">For every point, we will first find the k-distance neighborhood of that point. For example, for i = 0, the numerator would be len (<code class="literal">k_distance_neig[0]</code>), 2.</p><p class="calibre11">Now, in the inner for loop, we calculate the denominator. We then calculate the reachability distance for each k-distance neighborhood point. The ratio is stored in the <code class="literal">local_reach_density</code> dictionary.</p><p class="calibre11">Finally, in step 5, we calculate the LOF for each point:</p><div><pre class="programlisting">for i in range(instances.shape[0]):
lrd_sum = 0
rdist_sum = 0
for neigh in k_distance_neig[i]:
lrd_sum+=local_reach_density[neigh[1]]
rdist_sum+=max(k_distance[neigh[1]][0],neigh[0])
lof_list.append((i,lrd_sum*rdist_sum))</pre></div><p class="calibre11">For each data point, we calculate the LRD sum of its neighbor and the reachability distance sum with its neighbor, and multiply them to get the LOF.</p><p class="calibre11">The point with a very high LOF is considered an outlier. Let's print <code class="literal">lof_list</code>:</p><div><pre class="programlisting">[(0, 4.0), (1, 4.0), (2, 4.0), (3, 4.0), (4, 18.0)]</pre></div><p class="calibre11">As you can <a id="id440" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see, the last point has a very <a id="id441" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>high LOF compared with the others and hence, it's an outlier.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec237" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">You can refer to the following paper in order to understand more about LOF:</p><p class="calibre11">
<em class="calibre15">LOF: Identifying Density-Based Local Outliers</em>
</p><p class="calibre11">
<em class="calibre15">Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, Jörg Sander</em>
</p><p class="calibre11">
<em class="calibre15">Proc. ACM SIGMOD 2000 Int. Conf. On Management of Data, Dalles, TX, 2000</em>
</p></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec64" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Learning vector quantization</h1></div></div></div><p class="calibre11">In this recipe, we will <a id="id391" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>see a model-free method for clustering the data points called Learning Vector Quantization, LVQ for short. LVQ can be used in classification tasks. Not much of an inference can be made between the target variables and prediction variables using this technique. Unlike the other methods, it is tough to make out what relationships exist between the response variable, Y, and predictor, X. They serve very well as a black box approach in many real-world scenarios.</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec224" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">LVQ is an online learning algorithm where the data points are processed one at a time. It makes a very simple intuition. Assume that we have prototype vectors identified for the different classes present in our dataset. The training points will be attracted towards the prototypes of similar classes and will repel the other prototypes.</p><p class="calibre11">The major steps in LVQ are as follows:</p><p class="calibre11">Select k initial prototype vectors for each class in the dataset. If it's a two-class problem and we decide to have two prototype vectors for each class, we will end up with four initial prototype vectors. The initial prototype vectors are selected randomly from the input dataset.</p><p class="calibre11">We will start our iteration. Our iteration will end when our epsilon value has reached either zero or a predefined threshold. We will decide an epsilon value and decrement the epsilon value with every iteration.</p><p class="calibre11">In each iteration, we will <a id="id392" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>sample an input point (with replacement) and find the closest prototype vector to this point. We will use Euclidean distance to find the closest point. We will update the prototype vector of the closest point, as follows:</p><p class="calibre11">If the class label of the prototype vector is the same as the input data point, we will increment the prototype vector with the difference between the prototype vector and data point.</p><p class="calibre11">If the class label is different, we will decrement the prototype vector with the difference between the prototype vector and data point.</p><p class="calibre11">We will use the Iris dataset to demonstrate how LVQ works. As in some of our previous recipe, we will use the convenient data loading function from scikit-learn in order to load the Iris dataset. Iris is a well known classification dataset. However our purpose of using it here is to only demonstrate LVQ's capability. Datasets without class lablels can also be used or processed by LVQ. As we are going to use Euclidean distance, we will scale the data using minmax scaling.</p><div><pre class="programlisting">from sklearn.datasets import load_iris
import numpy as np
from sklearn.metrics import euclidean_distances

data = load_iris()
x = data['data']
y = data['target']

# Scale the variables
from sklearn.preprocessing import MinMaxScaler
minmax = MinMaxScaler()
x = minmax.fit_transform(x)</pre></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec225" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><div><ol class="orderedlist"><li class="listitem1">Let's first declare the parameters for LVQ:<div><pre class="programlisting1">R = 2
n_classes = 3
epsilon = 0.9
epsilon_dec_factor = 0.001</pre></div></li><li class="listitem1">Define a class to hold the prototype vectors:<div><pre class="programlisting1">class prototype(object):
    """
    Class to hold prototype vectors
    """

    def __init__(self,class_id,p_vector,eplsilon):
        self.class_id = class_id
        self.p_vector = p_vector
        self.epsilon = epsilon
        
    def update(self,u_vector,increment=True):
        if increment:
            # Move the prototype vector closer to input vector
            self.p_vector = self.p_vector + self.epsilon*(u_vector - self.p_vector)
        else:
            # Move the prototype vector away from input vector
            self.p_vector = self.p_vector - self.epsilon*(u_vector - self.p_vector)</pre></div></li><li class="listitem1">This is the function to <a id="id393" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>find the closest prototype vector for a given vector:<div><pre class="programlisting1">def find_closest(in_vector,proto_vectors):
    closest = None
    closest_distance = 99999
    for p_v in proto_vectors:
        distance = euclidean_distances(in_vector,p_v.p_vector)
        if distance &lt; closest_distance:
            closest_distance = distance
            closest = p_v
    return closest</pre></div></li><li class="listitem1">A convenient function to find the class ID of the closest prototype vector is as follows:<div><pre class="programlisting1">def find_class_id(test_vector,p_vectors):
    return find_closest(test_vector,p_vectors).class_id</pre></div></li><li class="listitem1">Choose the initial K * number of classes of prototype vectors:<div><pre class="programlisting1"># Choose R initial prototypes for each class        
p_vectors = []
for i in range(n_classes):
    # Select a class
    y_subset = np.where(y == i)
    # Select tuples for choosen class
    x_subset  = x[y_subset]
    # Get R random indices between 0 and 50
    samples = np.random.randint(0,len(x_subset),R)
    # Select p_vectors
    for sample in samples:
        s = x_subset[sample]
        p = prototype(i,s,epsilon)
        p_vectors.append(p)

print "class id \t Initial protype vector\n"
for p_v in p_vectors:
    print p_v.class_id,'\t',p_v.p_vector
       print</pre></div></li><li class="listitem1">Perform iteration to <a id="id394" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>adjust the prototype vector in order to classify/cluster any new incoming points using the existing data points:<div><pre class="programlisting1">while epsilon &gt;= 0.01:
    # Sample a training instance randonly
    rnd_i = np.random.randint(0,149)
    rnd_s = x[rnd_i]
    target_y = y[rnd_i]

    # Decrement epsilon value for next iteration
    epsilon = epsilon - epsilon_dec_factor    
    # Find closes prototype vector to given point
    closest_pvector = find_closest(rnd_s,p_vectors)
    
    # Update closes prototype vector
    if target_y == closest_pvector.class_id:
        closest_pvector.update(rnd_s)
    else:
        closest_pvector.update(rnd_s,False)
    closest_pvector.epsilon = epsilon
        
print "class id \t Final Prototype Vector\n"
for p_vector in p_vectors:
    print p_vector.class_id,'\t',p_vector.p_vector</pre></div></li><li class="listitem1">The following is a small test to verify the correctness of our method:<div><pre class="programlisting1">predicted_y = [find_class_id(instance,p_vectors) for instance in x ]

from sklearn.metrics import classification_report

print
print classification_report(y,predicted_y,target_names=['Iris-Setosa','Iris-Versicolour', 'Iris-Virginica'])</pre></div></li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec226" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we initialize the parameters for the algorithm. We have chosen our R value as two, that is, we have two prototype vectors per class label. The Iris dataset is a three-class problem, so we <a id="id395" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>have six prototype vectors in total. We must choose our epsilon value and epsilon decrement factor.</p><p class="calibre11">We then define a data structure to hold the details of our prototype vector in step 2. Our class stores the following for each point in the dataset:</p><div><pre class="programlisting">self.class_id = class_id
self.p_vector = p_vector
self.epsilon = epsilon</pre></div><p class="calibre11">The class id to which the prototype vector belongs is the vector itself and the epsilon value. It also has a function update that is used to change the prototype values:</p><div><pre class="programlisting">def update(self,u_vector,increment=True):
if increment:
# Move the prototype vector closer to input vector
self.p_vector = self.p_vector + self.epsilon*(u_vector - self.p_vector)
else:
# Move the prototype vector away from input vector
self.p_vector = self.p_vector - self.epsilon*(u_vector - self.p_vector)</pre></div><p class="calibre11">In step 3, we define the following function, which takes any given vector as the input and a list of all the prototype vectors. Out of all the prototype vectors, this function returns the closest prototype vector to the given vector:</p><div><pre class="programlisting">for p_v in proto_vectors:
distance = euclidean_distances(in_vector,p_v.p_vector)
if distance &lt; closest_distance:
closest_distance = distance
closest = p_v</pre></div><p class="calibre11">As you can see, it loops through all the prototype vectors to find the closest one. It uses Euclidean distance to measure the similarity.</p><p class="calibre11">Step 4 is a small function that can return the class ID of the closest prototype vector to the given vector.</p><p class="calibre11">Now that we have finished all the required preprocessing for the LVQ algorithm, we can move on to the actual algorithm in step 5. For each class, we must select the initial prototype vectors. We then select R random points from each class. The outer loop goes through each class, and for each class, we select R random samples and create our prototype object, as follows:</p><div><pre class="programlisting">samples = np.random.randint(0,len(x_subset),R)
# Select p_vectors
for sample in samples:
s = x_subset[sample]
p = prototype(i,s,epsilon)
p_vectors.append(p)</pre></div><p class="calibre11">In step 6, we increment or decrement the prototype vectors iteratively. We loop continuously till our epsilon value falls below a threshold of 0.01.</p><p class="calibre11">We then randomly sample <a id="id396" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>a point from our dataset, as follows:</p><div><pre class="programlisting"># Sample a training instance randonly
rnd_i = np.random.randint(0,149)
rnd_s = x[rnd_i]
target_y = y[rnd_i]</pre></div><p class="calibre11">The point and its corresponding class ID have been retrieved.</p><p class="calibre11">We can then find the closed prototype vector to this point, as follows:</p><div><pre class="programlisting">closest_pvector = find_closest(rnd_s,p_vectors)</pre></div><p class="calibre11">If the current point's class ID matches the prototype's class ID, we call the <code class="literal">update</code> method, with the increment set to <code class="literal">True</code>, or else we will call the <code class="literal">update</code> with the increment set to <code class="literal">False</code>:</p><div><pre class="programlisting"># Update closes prototype vector
if target_y == closest_pvector.class_id:
closest_pvector.update(rnd_s)
else:
closest_pvector.update(rnd_s,False)</pre></div><p class="calibre11">Finally, we update the epsilon value for the closest prototype vector:</p><div><pre class="programlisting">closest_pvector.epsilon = epsilon</pre></div><p class="calibre11">We can print the prototype vectors in order to look at them manually:</p><div><pre class="programlisting">print "class id \t Final Prototype Vector\n"
for p_vector in p_vectors:
print p_vector.class_id,'\t',p_vector.p_vector</pre></div><p class="calibre11">In step 7, we put our prototype vectors into action to do some predictions:</p><div><pre class="programlisting">predicted_y = [find_class_id(instance,p_vectors) for instance in x ]</pre></div><p class="calibre11">We can get the predicted class ID using the <code class="literal">find_class_id</code> function. We pass a point and all the learned prototype vectors to it to get the class ID.</p><p class="calibre11">Finally, we give our predicted output in order to generate a classification report:</p><div><pre class="programlisting">print classification_report(y,predicted_y,target_names=['Iris-Setosa','Iris-Versicolour', 'Iris-Virginica'])</pre></div><p class="calibre11">The classification report function is a convenient function provided by the scikit-learn library to view the <a id="id397" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>classification accuracy scores:</p><div><img src="img/B04041_05_03.jpg" alt="How it works…" class="calibre97"/></div><p class="calibre11">You can see that we have done pretty well with our classification. Keep in mind that we did not keep a separate test set. Never measure the accuracy of your model based on the training data. Always use a test set that is unseen by the training routines. We did it only for illustration purposes.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec227" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more...</h2></div></div></div><p class="calibre11">Keep in mind that this technique does not involve any optimization criteria as in the other classification methods. Hence, it is very difficult to judge how good the prototype vectors have been generated.</p><p class="calibre11">In our recipe, we initialized the prototype vectors as random values. You can use the k-means algorithm to initialize the prototype vectors.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec228" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><a id="id398" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/><em class="calibre15">Clustering of data using K-Means</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch05.xhtml" title="Chapter 5. Data Mining – Needle in a Haystack">Chapter 5</a>, <em class="calibre15">Data Mining - Finding a needle in a haystack</em></li></ul></div></div></div></div>



  
<div><div><div><div><div><h1 class="title1"><a id="ch05lvl1sec65" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Finding outliers in univariate data</h1></div></div></div><p class="calibre11">Outliers are <a id="id399" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>data points that are far away from the other data <a id="id400" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>points in your data. They have to be handled carefully in data science applications. Including them in some of your algorithms unknowingly may lead to wrong results or conclusions. It is very important to account for them properly and have the right algorithms in order to handle them.</p><div><table border="0" width="100%" cellspacing="0" cellpadding="0" class="blockquote1" summary="Block quote"><tr class="calibre98"><td class="calibre99"> </td><td class="calibre99"><p class="calibre74"><em class="calibre15">"Outlier detection is an extremely important problem with a direct application in a wide variety of application domains, including fraud detection (Bolton, 2002), identifying computer network intrusions and bottlenecks (Lane, 1999), criminal activities in e-commerce and detecting suspicious activities (Chiu, 2003)."</em></p></td><td class="calibre99"> </td></tr><tr class="calibre98"><td class="calibre99"> </td><td colspan="2" class="calibre100">--<em class="calibre15">- Jayakumar and Thomas, A New Procedure of Clustering Based on Multivariate Outlier Detection (Journal of Data Science 11(2013), 69-84)</em></td></tr></table></div><p class="calibre11">We will look at the detection of outliers in univariate data in this recipe and then move on to look at outliers in multivariate and text data.</p><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec229" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Getting ready</h2></div></div></div><p class="calibre11">In this recipe, we will look at the following three methods for outlier detection in univariate data:</p><div><ul class="itemizedlist"><li class="listitem">Median absolute deviation</li><li class="listitem">Mean plus or minus three standard deviation</li></ul></div><p class="calibre11">Let's see how we can leverage these methods to spot outliers in univariate data. Before we jump into the next section, let's create a dataset with outliers so that we can evaluate our method empirically:</p><div><pre class="programlisting">import numpy as np
import matplotlib.pyplot as plt

n_samples = 100
fraction_of_outliers = 0.1
number_inliers = int ( (1-fraction_of_outliers) * n_samples )
number_outliers = n_samples - number_inliers</pre></div><p class="calibre11">We will create 100 data points, and 10 percent of them will be outliers:</p><div><pre class="programlisting"># Get some samples from a normal distribution
normal_data = np.random.randn(number_inliers,1)</pre></div><p class="calibre11">We will use the <code class="literal">randn</code> function in the <code class="literal">random</code> module of NumPy to generate our inliers. This will be a sample from a distribution with a mean of zero and a standard deviation of one. Let's verify the mean and standard deviation of our sample:</p><div><pre class="programlisting"># Print the mean and standard deviation
# to confirm the normality of our input data.
mean = np.mean(normal_data,axis=0)
std = np.std(normal_data,axis=0)
print "Mean =(%0.2f) and Standard Deviation (%0.2f)"%(mean[0],std[0])</pre></div><p class="calibre11">We will calculate <a id="id401" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the mean and standard deviation with the <a id="id402" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>functions from NumPy and print the output. Let's inspect the output:</p><div><pre class="programlisting">Mean =(0.24) and Standard Deviation (0.90)</pre></div><p class="calibre11">As you can see, the mean is close to zero and the standard deviation is close to one.</p><p class="calibre11">Now, let's create the outliers. This will be 10 percent of the whole dataset, that is, 10 points, given that our sample size is 100. As you can see, we sampled our outliers from a uniform distribution between -9 and 9. Any points between this range will have an equal chance of being selected. We will concatenate our inlier and outlier data. It will be good to see the data with a scatter plot before we run our outlier detection program:</p><div><pre class="programlisting"># Create outlier data
outlier_data = np.random.uniform(low=-9,high=9,size=(number_outliers,1))
total_data = np.r_[normal_data,outlier_data]
print "Size of input data = (%d,%d)"%(total_data.shape)
# Eyeball the data
plt.cla()
plt.figure(1)
plt.title("Input points")
plt.scatter(range(len(total_data)),total_data,c='b')</pre></div><p class="calibre11">Let's look at the graph that is generated:</p><div><img src="img/B04041_05_16.jpg" alt="Getting ready" class="calibre45"/></div><p class="calibre11">Our <em class="calibre15">y</em> axis is the actual values that we generated and our <em class="calibre15">x</em> axis is a running count. It will be a good exercise to <a id="id403" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>mark the points that you feel are outliers. We can later compare our program output with your manual selections.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec230" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How to do it…</h2></div></div></div><div><ol class="orderedlist"><li class="listitem1">Let's start with the <a id="id404" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>median absolute deviation. Then we will plot our values, with the outliers marked in red:<div><pre class="programlisting1"># Median Absolute Deviation
median = np.median(total_data)
b = 1.4826
mad = b * np.median(np.abs(total_data - median))
outliers = []
# Useful while plotting
outlier_index = []
print "Median absolute Deviation = %.2f"%(mad)
lower_limit = median - (3*mad)
upper_limit = median + (3*mad)
print "Lower limit = %0.2f, Upper limit = %0.2f"%(lower_limit,upper_limit)
for i in range(len(total_data)):
    if total_data[i] &gt; upper_limit or total_data[i] &lt; lower_limit:
        print "Outlier %0.2f"%(total_data[i])
        outliers.append(total_data[i])
        outlier_index.append(i)

plt.figure(2)
plt.title("Outliers using mad")
plt.scatter(range(len(total_data)),total_data,c='b')
plt.scatter(outlier_index,outliers,c='r')
plt.show()</pre></div></li><li class="listitem1">Moving on to <a id="id405" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the mean plus or minus three <a id="id406" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard deviation, we will plot our values, with the outliers colored in red:<div><pre class="programlisting1"># Standard deviation
std = np.std(total_data)
mean = np.mean(total_data)
b = 3
outliers = []
outlier_index = []
lower_limt = mean-b*std
upper_limt = mean+b*std
print "Lower limit = %0.2f, Upper limit = %0.2f"%(lower_limit,upper_limit)
for i in range(len(total_data)):
    x = total_data[i]
    if x &gt; upper_limit or x &lt; lower_limt:
        print "Outlier %0.2f"%(total_data[i])
        outliers.append(total_data[i])
        outlier_index.append(i)


plt.figure(3)
plt.title("Outliers using std")
plt.scatter(range(len(total_data)),total_data,c='b')
plt.scatter(outlier_index,outliers,c='r')
plt.savefig("B04041 04 10.png")
plt.show()</pre></div></li></ol></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec231" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>How it works…</h2></div></div></div><p class="calibre11">In step 1, we use the median absolute deviation to detect the outliers in the data:</p><div><pre class="programlisting">median = np.median(total_data)
b = 1.4826
mad = b * np.median(np.abs(total_data - median))</pre></div><p class="calibre11">We first calculate the median value of our dataset using the median function from NumPy. Next, we declare a variable with a value of 1.4826. This is a constant to be multiplied with the <a id="id407" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>absolute deviation from the median. Finally, we <a id="id408" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>calculate the median of absolute deviations of each entry from the median value and multiply it with the constant, b.</p><p class="calibre11">Any point that is more than or less than three times the median absolute deviation is deemed as an outlier for our method:</p><div><pre class="programlisting">lower_limit = median - (3*mad)
upper_limit = median + (3*mad)

print "Lower limit = %0.2f, Upper limit = %0.2f"%(lower_limit,upper_limit)</pre></div><p class="calibre11">We then calculate the lower and upper limits of the median absolute deviation, as shown previously, and classify every point as either an outlier or inlier, as follows:</p><div><pre class="programlisting">for i in range(len(total_data)):
if total_data[i] &gt; upper_limit or total_data[i] &lt; lower_limit:
print "Outlier %0.2f"%(total_data[i])
outliers.append(total_data[i])
outlier_index.append(i)</pre></div><p class="calibre11">Finally, we have all our outlier points stored in a list by the name of outliers. We must also store the index of the outliers in a separate list called outlier_index. This is done for the ease of plotting, as you will see in the next step.</p><p class="calibre11">We then plot the original points and outliers. The plot looks as follows:</p><div><img src="img/B04041_05_17.jpg" alt="How it works…" class="calibre45"/></div><p class="calibre11">The points marked in red are classified as outliers by the algorithm.</p><p class="calibre11">In step 3, we code up <a id="id409" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the second algorithm, mean plus or minus three standard deviation:</p><div><pre class="programlisting">std = np.std(total_data)
mean = np.mean(total_data)
b = 3</pre></div><p class="calibre11">We then calculate the <a id="id410" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>standard deviation and mean of our dataset. Here, you can see that we have set our <code class="literal">b = 3</code>. As the name of our algorithm suggests, we will need a standard deviation of three, and this b is used for the same:</p><div><pre class="programlisting">lower_limt = mean-b*std
upper_limt = mean+b*std

print "Lower limit = %0.2f, Upper limit = %0.2f"%(lower_limit,upper_limit)

for i in range(len(total_data)):
x = total_data[i]
if x &gt; upper_limit or x &lt; lower_limt:
print "Outlier %0.2f"%(total_data[i])
outliers.append(total_data[i])
outlier_index.append(i)</pre></div><p class="calibre11">We can calculate the <a id="id411" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>lower and upper limits as the mean minus three <a id="id412" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>times the standard deviation. Using these values, we can then classify every point as either an outlier or inlier in the for loop. We then add all the outliers and their indices to the two lists, outliers and outlier_index, to plot.</p><p class="calibre11">Finally, we plot the outliers:</p><div><img src="img/B04041_05_18.jpg" alt="How it works…" class="calibre45"/></div></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec232" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>There's more…</h2></div></div></div><p class="calibre11">As per the definition of outliers, outliers in a given dataset are those points that are far away from the other <a id="id413" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>points in the data source. The estimates of the <a id="id414" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>center of the dataset and the spread of the dataset can be used to detect the outliers. In the methods that we outlined in this recipe, we used the mean and median as the estimates for the center of the data and standard deviation, and the median absolute deviation as the estimates for the spread. Spread is also called scale.</p><p class="calibre11">Let's do a little bit of rationalization about why our methods work in the detection of the outliers. Let's start with the method of using standard deviation. For Gaussian data, we know that 68.27 percent of the data lies with in one standard deviation, 95.45 percent in two, and 99.73 percent lies in three. Thus, according to our rule that any point that is more than three standard deviations from the mean is classified as an outlier. However, this method is not robust. Let's look at a small example.</p><p class="calibre11">Let's sample eight data points from a normal distribution, with the mean as zero and the standard deviation as one.</p><p class="calibre11">Let's use the convenient <a id="id415" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>function from <code class="literal">NumPy .random</code> to generate our numbers:</p><div><pre class="programlisting">np.random.randn(8)</pre></div><p class="calibre11">This gives us the following numbers:</p><div><pre class="programlisting">-1.76334861, -0.75817064,  0.44468944, -0.07724717,  0.12951944,0.43096092, -0.05436724, -0.23719402</pre></div><p class="calibre11">Let's add two outliers to it <a id="id416" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>manually, for example, 45 and 69, to this list.</p><p class="calibre11">Our dataset now looks as follows:</p><div><pre class="programlisting">-1.763348607322289, -0.7581706357821458, 0.4446894368956213, -0.07724717210195432, 0.1295194428816003, 0.4309609200681169, -0.05436724238743103, -0.23719402072058543, 45, 69</pre></div><p class="calibre11">The mean of the preceding dataset is 11.211 and the standard deviation is <code class="literal">23.523</code>.</p><p class="calibre11">Let's look at the upper rule, mean + 3 * std. This is 11.211 + 3 * 23.523 = 81.78.</p><p class="calibre11">Now, according to this upper bound rule, both the points, 45 and 69, are not outliers! Both the mean and the standard deviation are non-robust estimators of the center and scale of the dataset, as they are extremely sensitive to outliers. If we replace one of the points with an extreme point in a dataset with n observations, it will completely change the estimate of the mean and the standard deviation. This property of the estimators is called the finite sample breakdown point.</p><div><div><h3 class="title5"><a id="note23" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>Note</h3><p class="calibre19">The finite sample breakdown point is defined as the proportion of the observations in a sample that can be replaced before the estimator fails to describe the data accurately.</p></div></div><p class="calibre11">Thus, for the mean and standard deviation, the finite sample breakdown point is 0 percent because in a large sample, replacing even a single point would change the estimators drastically.</p><p class="calibre11">In contrast, the median is a more robust estimate. The median is the middle observation in a finite set of observations that is sorted in an ascending order. For the median to change drastically, we have to replace half of the observations in the data that are far away from the median. This gives you a 50 percent finite sample breakdown point for the median.</p><p class="calibre11">The median absolute <a id="id417" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>deviation method is attributed to <a id="id418" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>the <a id="id419" class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2"/>following paper:</p><p class="calibre11">
<em class="calibre15">Leys, C., et al., Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median, Journal of Experimental Social Psychology (2013)</em>, <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="http://dx.doi.org/10.1016/j.jesp.2013.03.013">http://dx.doi.org/10.1016/j.jesp.2013.03.013</a>.</p></div><div><div><div><div><h2 class="title3"><a id="ch05lvl2sec233" class="pcalibre pcalibre3 pcalibre1 calibre8 pcalibre2"/>See also</h2></div></div></div><div><ul class="itemizedlist"><li class="listitem"><em class="calibre15">Performing summary statistics and plots</em> recipe in <a class="calibre4 pcalibre3 pcalibre pcalibre1 pcalibre2" href="ch01.xhtml" title="Chapter 1. Python for Data Science">Chapter 1</a>, <em class="calibre15">Using Python for Data Science</em></li></ul></div></div></div></div>



  </body></html>