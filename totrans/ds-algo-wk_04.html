<html><head></head><body><div><h1 class="header-title">Random Forests</h1>
                
            
            
                
<p class="mce-root">A random forest is a set of random decision trees (similar to the ones described in <a href="a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 3</a>, <em class="calibre18">Decision Trees</em>), each generated on a random subset of data. A random forest classifies the features that belong to the class that is voted for by the majority of the random decision trees. Random forests tend to provide a more accurate classification of a feature than decision trees because of their decreased bias and variance.</p>
<p class="mce-root">In this chapter, we will cover the following topics:</p>
<ul class="calibre14">
<li class="calibre15">The tree bagging (or bootstrap aggregation) technique as part of random forest construction, but which can also be extended to other algorithms and methods in data science in order to reduce bias and variance and, hence, improve accuracy</li>
<li class="calibre15">How to construct a random forest and classify a data item using a random forest constructed through the swim preference example </li>
<li class="calibre15">How to implement an algorithm in Python that will construct a random forest</li>
<li class="calibre15">The differences between the analysis of a problem using the Naive Bayes algorithm, decision trees, and random forest using the example of playing chess</li>
<li class="calibre15">How the random forest algorithm can overcome the shortcomings of the decision tree algorithm and thus outperform it using the example of going shopping </li>
<li class="calibre15">How a random forest can express level of confidence in its classification of a feature using the example of going shopping</li>
<li class="calibre15">How decreasing the variance of a classifier can yield more accurate results, in the <em class="calibre5">Problems</em> section</li>
</ul>


            

            
        
    </div>



  
<div><h1 class="header-title">Introduction to the random forest algorithm</h1>
                
            
            
                
<p class="mce-root">In general, in order to construct a random forest, first we have to choose the number of trees that it will contain. A random forest does not tend to overfit (unless the data is very noisy), so choosing many decision trees will not decrease the accuracy of the prediction. A random forest does not tend to overfit (unless the data is very noisy), so having a higher number of decision trees will not decrease the accuracy of the prediction. It is important to have a sufficient number of decision trees so that more data is used for classification purposes when chosen randomly for the construction of a decision tree. On the other hand, the more decision trees there are, the more computational power is required. Also, increasing the number of decision trees fails to increase the accuracy of the classification by any significant degree.</p>
<p class="mce-root">In practice, you can run the algorithm on a specific number of decision trees, increase their number, and compare the results of the classification of smaller and bigger forests. If the results are very similar, then there is no reason to increase the number of trees.</p>
<p class="mce-root">To simplify the demonstration, throughout this book, we will use a small number of decision trees in a random forest.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Overview of random forest construction</h1>
                
            
            
                
<p class="mce-root">We will describe how each tree is constructed in a random fashion. We construct a decision tree by selecting <em class="calibre18">N</em> training features randomly. This process of selecting the data randomly with a replacement for each tree is called <strong class="calibre8">bootstrap aggregating</strong>, or <strong class="calibre8">tree bagging</strong>. The purpose of bootstrap aggregating is to reduce the variance and bias in the results of the classification.</p>
<p class="mce-root">Say a feature has <em class="calibre18">M</em> variables that are used to classify the feature using the decision tree. When we have to make a branching decision at a node, in the ID3 algorithm, we choose the variable that resulted in the highest information gain. Here, in a random decision tree, at each node, we consider only at most <em class="calibre18">m</em> variables. We do not consider the ones that were already chosen sampled in a random fashion without any replacement from the given <em class="calibre18">M</em> variables. Then, of these <em class="calibre18">m</em> variables, we choose the one that results in the highest information gain.</p>
<p class="mce-root">The remainder of the construction of a random decision tree is carried out just as it was for a decision tree in <a href="a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 3</a>, <em class="calibre18">Decision Trees</em>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Swim preference – analysis involving a random forest</h1>
                
            
            
                
<p class="mce-root">We will use the example from <a href="a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 3</a>, <em class="calibre18">Decision Trees</em> concerning swim preferences. We have the same data table, as follows:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Swimming suit</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Water temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Swim preference</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Small</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Small</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Good</p>
</td>
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre28">
<td class="calibre25">
<p class="calibre26">Good</p>
</td>
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">We would like to construct a random forest from this data and use it to classify an item <kbd class="calibre17">(Good,Cold,?)</kbd>.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">We are given <em class="calibre18">M=3</em> variables, according to which a feature can be classified. In a random forest algorithm, we usually do not use all three variables to form tree branches at each node. We only use a subset (<em class="calibre18">m</em>) of variables from <em class="calibre18">M</em>. So we choose <em class="calibre18">m</em> such that <em class="calibre18">m</em> is less than, or equal to, <em class="calibre18">M</em>. The greater <em class="calibre18">m</em> is, the stronger the classifier is in each constructed tree. However, as mentioned earlier, more data leads to more bias. But, because we use multiple trees (with a lower <em class="calibre18">m</em>), even if each constructed tree is a weak classifier, their combined classification accuracy is strong. As we want to reduce bias in a random forest, we may want to consider choosing an <em class="calibre18">m</em> parameter that is slightly less than <em class="calibre18">M</em>.</p>
<p class="mce-root">Hence, we choose the maximum number of variables considered at the node to be: <em class="calibre18">m=min(M,math.ceil(2*math.sqrt(M)))=min(M,math.ceil(2*math.sqrt(3)))=3</em>.</p>
<p class="mce-root">We are given the following features:</p>
<pre class="calibre22">[['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Small', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Warm', 'Yes']]</pre>
<p class="mce-root">When constructing a random decision tree as part of a random forest, we will choose only a subset of these features randomly, together with their replacements.</p>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Random forest construction</h1>
                
            
            
                
<p class="mce-root">We will construct a random forest that will consist of two random decision trees.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Construction of random decision tree number 0</h1>
                
            
            
                
<p class="mce-root">We are given six features as the input data. Of these, we choose six features at random with a replacement for the construction of this random decision tree:</p>
<pre class="calibre22">[['None', 'Warm', 'No'], ['None', 'Warm', 'No'], ['Small', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]</pre>
<p class="mce-root">We start the construction with the root node to create the first node of the tree. We would like to add children to the [root] node.</p>
<p class="mce-root">We have the following variables available: <kbd class="calibre17">['swimming_suit', 'water_temperature']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. Of these variables, the one with the highest information gain is a swimming suit.</p>
<p class="mce-root">Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">swimming_suit</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=Small: [['Small', 'Cold', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=None: [['None', 'Warm', 'No'], ['None', 'Warm', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=Good: [['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]</kbd></li>
</ul>
<p class="mce-root">Using the preceding partitions, we create the branches and the child nodes.</p>
<p class="mce-root">We now add a child node, <kbd class="calibre17">[swimming_suit=Small]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Small', 'Cold', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=Small]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. The one with the highest information gain is the <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">water_temperature</kbd>, all the remaining features have the same value: <kbd class="calibre17">Cold</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[swim=No]</kbd>.</p>
<p class="mce-root">We now add a child node, <kbd class="calibre17">[swimming_suit=None]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies two features: <kbd class="calibre17">[['None', 'Warm', 'No'], ['None', 'Warm', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=None]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one.The one with the highest information gain is the <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">water_temperature</kbd>, all the remaining features have the same value: <kbd class="calibre17">Warm</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[swim=No]</kbd>.</p>
<p class="mce-root">We now add a child node, <kbd class="calibre17">[swimming_suit=Good]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies three features: <kbd class="calibre17">[['Good', 'Cold', 'No'], ['Good', 'Cold', 'No'], ['Good', 'Cold', 'No']]</kbd></p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=Good]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. The one with the highest information gain is the <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">water_temperature</kbd>, all the remaining features have the same value: <kbd class="calibre17">Cold</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[swim=No]</kbd>.</p>
<p class="mce-root">Now, we have added all the children nodes to the <kbd class="calibre17">[root]</kbd> node.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Construction of random decision tree number 1</h1>
                
            
            
                
<p class="mce-root">We are given six features as input data. Of these, we choose six features at random with a replacement for the construction of this random decision tree:</p>
<pre class="calibre22">[['Good', 'Warm', 'Yes'], ['None', 'Warm', 'No'], ['Good', 'Cold', 'No'], ['None', 'Cold', 'No'], ['None', 'Warm', 'No'], ['Small', 'Warm', 'No']]</pre>
<p class="mce-root">The remainder of the construction of random decision tree number 1 is similar to the construction of the previous random decision tree, number 0. The only difference is that the tree is built using a different randomly generated subset (as seen previously) of the initial data.</p>
<p class="mce-root">We begin construction with the root node to create the first node of the tree. We would like to add children to the <kbd class="calibre17">[root]</kbd> node.</p>
<p class="mce-root">We have the following variables available: <kbd class="calibre17">['swimming_suit', 'water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. Of these variables, the one with the highest information gain is the <kbd class="calibre17">swimming_suit</kbd> variable.</p>
<p class="mce-root">Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">swimming_suit</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=Small: [['Small', 'Warm', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=None: [['None', 'Warm', 'No'], ['None', 'Cold', 'No'], ['None', 'Warm', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">swimming_suit=Good: [['Good', 'Warm', 'Yes'], ['Good', 'Cold', 'No']]</kbd></li>
</ul>
<p class="mce-root">Now, given the partitions, let's create the branches and the child nodes. We add a child node, <kbd class="calibre17">[swimming_suit=Small]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Small', 'Warm', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=Small]</kbd> node.</p>
<p class="mce-root">We have the following variable available:  <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. The one with the highest information gain is the <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">water_temperature</kbd>, all the remaining features have the same value: <kbd class="calibre17">Warm</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[swim=No]</kbd>.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[swimming_suit=None]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies three features: <kbd class="calibre17">[['None', 'Warm', 'No']</kbd>, <kbd class="calibre17">['None', 'Cold', 'No']</kbd>, and <kbd class="calibre17">['None', 'Warm', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=None]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. The one with the highest information gain is the <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">water temperature</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">water_temperature=Cold: [['None', 'Cold', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">water_temperature=Warm: [['None', 'Warm', 'No'], ['None', 'Warm', 'No']]</kbd>; now, given the partitions, let's create the branches and the child nodes</li>
</ul>
<p class="mce-root">We add a child node, <kbd class="calibre17">[water_temperature=Cold]</kbd>, to the <kbd class="calibre17">[swimming_suit=None]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['None', 'Cold', 'No']]</kbd>.</p>
<p class="mce-root">We do not have any available variables on which we could split the node further; therefore, we add a leaf node to the current branch of the tree. We add the <kbd class="calibre17">[swim=No]</kbd> leaf node. We add a child node, <kbd class="calibre17">[water_temperature=Warm]</kbd>, to the <kbd class="calibre17">[swimming_suit=None]</kbd> node. This branch classifies two features: <kbd class="calibre17">[['None', 'Warm', 'No']</kbd>, and  <kbd class="calibre17">['None', 'Warm', 'No']]</kbd>.</p>
<p class="mce-root">We do not have any available variables on which we could split the node further; therefore, we add a leaf node to the current branch of the tree. We add the <kbd class="calibre17">[swim=No]</kbd> leaf node.</p>
<p class="mce-root">Now, we have added all the children nodes to the <kbd class="calibre17">[swimming_suit=None]</kbd> node.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[swimming_suit=Good]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies two features: <kbd class="calibre17">[['Good', 'Warm', 'Yes']</kbd>, and <kbd class="calibre17">['Good', 'Cold', 'No']]</kbd></p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[swimming_suit=Good]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['water_temperature']</kbd>. As there is only one variable here, which is less than the <kbd class="calibre17">m=3</kbd> parameter, we will consider this one. The one with the highest information gain is the  <kbd class="calibre17">water_temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">water temperature</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">water_temperature=Cold: [['Good', 'Cold', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">water_temperature=Warm: [['Good', 'Warm', 'Yes']]</kbd></li>
</ul>
<p class="mce-root">Now, given the partitions, let's create the branches and the child nodes.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[water_temperature=Cold]</kbd>, to the <kbd class="calibre17">[swimming_suit=Good]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Good', 'Cold', 'No']]</kbd></p>
<p class="mce-root">We do not have any available variables on which we could split the node further; therefore, we add a leaf node to the current branch of the tree. We add the <kbd class="calibre17">[swim=No]</kbd> leaf node.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[water_temperature=Warm]</kbd>, to the <kbd class="calibre17">[swimming_suit=Good]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Good', 'Warm', 'Yes']]</kbd>.</p>
<p class="mce-root">We do not have any available variables on which we could split the node further; therefore, we add a leaf node to the current branch of the tree. We add the <kbd class="calibre17">[swim=Yes]</kbd> leaf node.</p>
<p class="mce-root">Now, we have added all the children nodes of the <kbd class="calibre17">[swimming_suit=Good]</kbd> node.</p>
<p class="mce-root">We have also added all the children nodes to the <kbd class="calibre17">[root]</kbd> node.</p>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Constructed random forest</h1>
                
            
            
                
<p class="mce-root">We have completed the construction of the random forest, consisting of two random decision trees, as shown in the following code block:</p>
<pre class="calibre22">Tree 0:<br class="calibre2"/>    Root
    ├── [swimming_suit=Small]
    │ └── [swim=No]
    ├── [swimming_suit=None]
    │ └── [swim=No]
    └── [swimming_suit=Good]
      └── [swim=No]<br class="calibre2"/>Tree 1:
    Root
    ├── [swimming_suit=Small]
    │ └── [swim=No]
    ├── [swimming_suit=None]
    │ ├── [water_temperature=Cold]
    │ │ └── [swim=No]
    │ └──[water_temperature=Warm]
    │   └── [swim=No]
    └── [swimming_suit=Good]
      ├──  [water_temperature=Cold] 
      │ └── [swim=No]
      └──  [water_temperature=Warm]
        └── [swim=Yes]
The total number of trees in the random forest=2.
The maximum number of the variables considered at the node is m=3.</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Classification using random forest</h1>
                
            
            
                
<p class="mce-root">Because we use only a subset of the original data for the construction of the random decision tree, we may not have enough features to form a full tree that is able to classify every feature. In such cases, a tree will not return any class for a feature that should be classified. Therefore, we will only consider trees that classify a feature of a specific class.</p>
<p class="mce-root">The feature we would like to classify is <kbd class="calibre17">['Good', 'Cold', '?']</kbd>. A random decision tree votes for the class to which it classifies a given feature using the same method to classify a feature as in <a href="a0ddbfd8-ce5c-45bc-8fba-ba4cfa048d0f.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 3</a>, <em class="calibre18">Decision Trees</em>. Tree 0 votes for the <kbd class="calibre17">No</kbd> class. Tree 1 votes for the <kbd class="calibre17">No</kbd> class. The class with the maximum number of votes is <kbd class="calibre17">No</kbd>. Therefore, the constructed random forest classifies the feature <kbd class="calibre17">['Good', 'Cold', '?']</kbd> according to the class <kbd class="calibre17">No</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Implementation of the random forest algorithm</h1>
                
            
            
                
<p class="mce-root">We implement a random forest algorithm using a modified decision tree algorithm from the previous chapter. We also add an option to set a verbose mode within the program that can describe the whole process of how the algorithm works on a specific input—how a random forest is constructed with its random decision trees, and how this constructed random forest is used to classify other features.</p>
<p class="mce-root">You are encouraged to consult the <kbd class="calibre17">decision_tree.construct_general_tree</kbd>  function from the previous chapter:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/4/random_forest.py<br class="calibre2"/></strong>import math<strong class="calibre3"><br class="calibre2"/></strong>import random<strong class="calibre3"><br class="calibre2"/></strong>import sys<br class="calibre2"/>sys.path.append('../common')<br class="calibre2"/>import common # noqa<br class="calibre2"/>import decision_tree # noqa<br class="calibre2"/>from common import printfv # noqa<br class="calibre2"/><br class="calibre2"/>#Random forest construction<br class="calibre2"/><strong class="calibre3">def sample_with_replacement(population, size):</strong><br class="calibre2"/>    sample = []<br class="calibre2"/>    for i in range(0, size):<br class="calibre2"/>        sample.append(population[random.randint(0, len(population) - 1)])<br class="calibre2"/>    return sample<br class="calibre2"/><br class="calibre2"/><strong class="calibre3">def construct_random_forest(verbose, heading, complete_data,</strong><br class="calibre2"/><strong class="calibre3">                            enquired_column, m, tree_count):</strong><br class="calibre2"/>    printfv(2, verbose, "*** Random Forest construction ***\n")<br class="calibre2"/>    printfv(2, verbose, "We construct a random forest that will " +<br class="calibre2"/>            "consist of %d random decision trees.\n", tree_count)<br class="calibre2"/>    random_forest = []<br class="calibre2"/>    for i in range(0, tree_count):<br class="calibre2"/>        printfv(2, verbose, "\nConstruction of a random " +<br class="calibre2"/>                "decision tree number %d:\n", i)<br class="calibre2"/>        random_forest.append(construct_random_decision_tree(<br class="calibre2"/>            verbose, heading, complete_data, enquired_column, m))<br class="calibre2"/>    printfv(2, verbose, "\nTherefore we have completed the " +<br class="calibre2"/>            "construction of the random forest consisting of %d " +<br class="calibre2"/>            "random decision trees.\n", tree_count)<br class="calibre2"/>    return random_forest<br class="calibre2"/><br class="calibre2"/><strong class="calibre3">def construct_random_decision_tree(verbose, heading, complete_data,</strong><br class="calibre2"/><strong class="calibre3">                                   enquired_column, m):</strong><br class="calibre2"/>    sample = sample_with_replacement(complete_data, len(complete_data))<br class="calibre2"/>    printfv(2, verbose, "We are given %d features as the input data. " +<br class="calibre2"/>            "Out of these, we choose randomly %d features with the " +<br class="calibre2"/>            "replacement that we will use for the construction of " +<br class="calibre2"/>            "this particular random decision tree:\n" +<br class="calibre2"/>            str(sample) + "\n", len(complete_data),<br class="calibre2"/>            len(complete_data))<br class="calibre2"/># The function construct_general_tree from the module decision_tree<br class="calibre2"/># is written in the implementation section in the previous chapter<br class="calibre2"/># on decision trees.<br class="calibre2"/>    return decision_tree.construct_general_tree(verbose, heading,<br class="calibre2"/>                                                sample,<br class="calibre2"/>                                                enquired_column, m)<br class="calibre2"/><br class="calibre2"/># M is the given number of the decision variables, i.e. properties<br class="calibre2"/># of one feature.<br class="calibre2"/><strong class="calibre3">def choose_m(verbose, M):</strong><br class="calibre2"/>    m = int(min(M, math.ceil(2 * math.sqrt(M))))<br class="calibre2"/>    printfv(2, verbose, "We are given M=" + str(M) +<br class="calibre2"/>            " variables according to which a feature can be " +<br class="calibre2"/>            "classified. ")<br class="calibre2"/>    printfv(3, verbose, "In random forest algorithm we usually do " +<br class="calibre2"/>            "not use all " + str(M) + " variables to form tree " +<br class="calibre2"/>            "branches at each node. ")<br class="calibre2"/>    printfv(3, verbose, "We use only m variables out of M. ")<br class="calibre2"/>    printfv(3, verbose, "So we choose m such that m is less than or " +<br class="calibre2"/>            "equal to M. ")<br class="calibre2"/>    printfv(3, verbose, "The greater m is, a stronger classifier an " +<br class="calibre2"/>            "individual tree constructed is. However, it is more " +<br class="calibre2"/>            "susceptible to a bias as more of the data is considered. " +<br class="calibre2"/>            "Since we in the end use multiple trees, even if each may " +<br class="calibre2"/>            "be a weak classifier, their combined classification " +<br class="calibre2"/>            "accuracy is strong. Therefore as we want to reduce a " +<br class="calibre2"/>            "bias in a random forest, we may want to consider to " +<br class="calibre2"/>            "choose a parameter m to be slightly less than M.\n")<br class="calibre2"/>    printfv(2, verbose, "Thus we choose the maximum number of the " +<br class="calibre2"/>            "variables considered at the node to be " +<br class="calibre2"/>            "m=min(M,math.ceil(2*math.sqrt(M)))" +<br class="calibre2"/>            "=min(M,math.ceil(2*math.sqrt(%d)))=%d.\n", M, m)<br class="calibre2"/>    return m<br class="calibre2"/><br class="calibre2"/>#Classification<br class="calibre2"/><strong class="calibre3">def display_classification(verbose, random_forest, heading,</strong><br class="calibre2"/><strong class="calibre3">                           enquired_column, incomplete_data):</strong><br class="calibre2"/>    if len(incomplete_data) == 0:<br class="calibre2"/>        printfv(0, verbose, "No data to classify.\n")<br class="calibre2"/>    else:<br class="calibre2"/>        for incomplete_feature in incomplete_data:<br class="calibre2"/>            printfv(0, verbose, "\nFeature: " +<br class="calibre2"/>                    str(incomplete_feature) + "\n")<br class="calibre2"/>            display_classification_for_feature(<br class="calibre2"/>                verbose, random_forest, heading,<br class="calibre2"/>                enquired_column, incomplete_feature)<br class="calibre2"/> <br class="calibre2"/><strong class="calibre3">def display_classification_for_feature(verbose, random_forest, heading,</strong><br class="calibre2"/><strong class="calibre3">                                       enquired_column, feature):</strong><br class="calibre2"/>    classification = {}<br class="calibre2"/>    for i in range(0, len(random_forest)):<br class="calibre2"/>        group = decision_tree.classify_by_tree(<br class="calibre2"/>            random_forest[i], heading, enquired_column, feature)<br class="calibre2"/>        common.dic_inc(classification, group)<br class="calibre2"/>        printfv(0, verbose, "Tree " + str(i) +<br class="calibre2"/>                " votes for the class: " + str(group) + "\n")<br class="calibre2"/>    printfv(0, verbose, "The class with the maximum number of votes " +<br class="calibre2"/>            "is '" + str(common.dic_key_max_count(classification)) +<br class="calibre2"/>            "'. Thus the constructed random forest classifies the " +<br class="calibre2"/>            "feature " + str(feature) + " into the class '" +<br class="calibre2"/>            str(common.dic_key_max_count(classification)) + "'.\n")<br class="calibre2"/><br class="calibre2"/><strong class="calibre3"># Program start</strong><br class="calibre2"/>csv_file_name = sys.argv[1]<br class="calibre2"/>tree_count = int(sys.argv[2])<br class="calibre2"/>verbose = int(sys.argv[3])<br class="calibre2"/><br class="calibre2"/>(heading, complete_data, incomplete_data,<br class="calibre2"/> enquired_column) = common.csv_file_to_ordered_data(csv_file_name)<br class="calibre2"/>m = choose_m(verbose, len(heading))<br class="calibre2"/>random_forest = construct_random_forest(<br class="calibre2"/>    verbose, heading, complete_data, enquired_column, m, tree_count)<br class="calibre2"/>display_forest(verbose, random_forest)<br class="calibre2"/>display_classification(verbose, random_forest, heading,<br class="calibre2"/>                       enquired_column, incomplete_data)</pre>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<p class="mce-root">As an input file to the implemented algorithm, we provide the data from the swim preference example:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/4/swim.csv</strong><br class="calibre2"/>swimming_suit,water_temperature,swim<br class="calibre2"/>None,Cold,No<br class="calibre2"/>None,Warm,No<br class="calibre2"/>Small,Cold,No<br class="calibre2"/>Small,Warm,No<br class="calibre2"/>Good,Cold,No<br class="calibre2"/>Good,Warm,Yes<br class="calibre2"/>Good,Cold,?</pre>
<p class="mce-root"><strong class="calibre8">Output</strong>:</p>
<p class="mce-root">We type the following command in the command line to get the output:</p>
<pre class="calibre22"><strong class="calibre3">$ python random_forest.py swim.csv 2 3 &gt; swim.out</strong></pre>
<p class="mce-root"><kbd class="calibre17">2</kbd> means that we would like to construct two decision trees, and <kbd class="calibre17">3</kbd> is the level of the verbosity of the program, which includes detailed explanations of the construction of the random forest, the classification of the feature, and the graph of the random forest. The last part, <kbd class="calibre17">&gt; swim.out</kbd>, means that the output is written to the <kbd class="calibre17">swim.out</kbd> file. This file can be found in the chapter directory <kbd class="calibre17">source_code/4</kbd>. This output of the program was used previously to write the analysis of the swim preference problem.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Playing chess example</h1>
                
            
            
                
<p class="mce-root">We will again use the examples from <a href="" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 2</a>, <em class="calibre18">Naive Bayes,</em> and <a href="4f3fafce-e9a1-4593-bd9d-847a94cde2bf.xhtml" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 3</a>, <em class="calibre18">Decision Tree</em>, as follows:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Wind</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Sunshine</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Play</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Cloudy</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Sunny</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">However, we would like to use a random forest consisting of four random decision trees to find the result of the classification.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">We are given <em class="calibre18">M=4</em> variables from which a feature can be classified. Thus, we choose the maximum number of the variables considered at the node to:</p>
<p class="cdpaligncenter"><img class="fm-editor-equation128" src="img/67f7de1d-b134-48f2-9de2-fc61f30107c0.png" width="7220" height="220"/></p>
<p class="mce-root">We are given the following features:</p>
<pre class="calibre22">[['Cold', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'None', 'Sunny',<br class="calibre2"/>'Yes'], ['Hot', 'None', 'Sunny', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Breeze',<br class="calibre2"/>'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Hot', 'Strong', 'Cloudy', 'Yes'], ['Warm', 'None', 'Cloudy', 'Yes']]</pre>
<p class="mce-root">When constructing a random decision tree as part of a random forest, we will choose only a subset of these features randomly, together with their replacements.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Random forest construction</h1>
                
            
            
                
<p class="mce-root">We will construct a random forest that will consist of four random decision trees.</p>
<p class="mce-root"><strong class="calibre8">Construction of random decision tree number 0</strong></p>
<p class="mce-root">We are given 10 features as input data. Of these, we choose all features randomly with their replacements for the construction of this random decision tree:</p>
<pre class="calibre22">[['Warm', 'Strong', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny', 'Yes']]</pre>
<p class="mce-root">We start the construction with the root node to create the first node of the tree. We would like to add children to the <kbd class="calibre17">[root]</kbd> node.</p>
<p class="mce-root"/>
<p class="mce-root">We have the following variables available: <kbd class="calibre17">['Temperature', 'Wind', 'Sunshine']</kbd>. As there are fewer of them than the <kbd class="calibre17">m=4</kbd> parameter, we consider all of them. Of these variables, the one with the highest information gain is the <kbd class="calibre17">Temperature</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">Temperature</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">Temperature=Cold: [['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'None', 'Sunny', 'Yes'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">Temperature=Warm: [['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Breeze', 'Sunny', 'Yes']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">Temperature=Hot: [['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes']]</kbd></li>
</ul>
<p class="mce-root">Now, given the partitions, let's create the branches and the child nodes.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Temperature=Cold]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies four features: <kbd class="calibre17">[['Cold', 'Breeze', 'Cloudy', 'No']</kbd>, <kbd class="calibre17">['Cold', 'None', 'Sunny', 'Yes']</kbd>, <kbd class="calibre17">['Cold', 'Breeze', 'Cloudy', 'No'],</kbd>  and <kbd class="calibre17"> ['Cold', 'Breeze', 'Cloudy', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Temperature=Cold]</kbd> node.</p>
<p class="mce-root">We have the following variables available: <kbd class="calibre17">['Wind', 'Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Wind</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the water <kbd class="calibre17">Wind</kbd> variable, we partition the data in the current node as follows:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">Wind=None: [['Cold', 'None', 'Sunny', 'Yes']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">Wind=Breeze: [['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No']]</kbd></li>
</ul>
<p class="mce-root">Now, given the partitions, let's create the branches and the child nodes.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Wind=None]</kbd>, to the <kbd class="calibre17">[Temperature=Cold]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Cold', 'None', 'Sunny', 'Yes']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Wind=None]</kbd> node. </p>
<p class="mce-root">We have the following variable: <kbd class="calibre17">available['Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Sunshine</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">Sunshine</kbd>, all the remaining features have the same value: <kbd class="calibre17">Sunny</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[Play=Yes]</kbd>.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Wind=Breeze]</kbd>, to the <kbd class="calibre17">[Temperature=Cold]</kbd> node. This branch classifies three features: <kbd class="calibre17">[['Cold', 'Breeze', 'Cloudy', 'No'], ['Cold', 'Breeze', 'Cloudy', 'No'],</kbd>  and <kbd class="calibre17"> ['Cold', 'Breeze', 'Cloudy', 'No']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Wind=Breeze]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Sunshine</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, Sunshine, all the remaining features have the same value: Cloudy. So, we end the branch with a leaf node, adding [Play=No].</p>
<p class="mce-root">Now, we have added all the children nodes for the <kbd class="calibre17">[Temperature=Cold]</kbd> node.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Temperature=Warm]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies three features: <kbd class="calibre17">[['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No'], </kbd> and  <kbd class="calibre17"> ['Warm', 'Breeze', 'Sunny', 'Yes']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Temperature=Warm]</kbd> node.</p>
<p class="mce-root"/>
<p class="mce-root">The available variables that we still have left are <kbd class="calibre17">['Wind', 'Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Wind</kbd> variable. Thus, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. Using the <kbd class="calibre17">Wind</kbd> variable, we partition the data in the current node, where each partition of the data will be for one of the new branches from the current node, <kbd class="calibre17">[Temperature=Warm]</kbd>. We have the following partitions:</p>
<ul class="calibre14">
<li class="calibre15">Partition for <kbd class="calibre17">Wind=Breeze: [['Warm', 'Breeze', 'Sunny', 'Yes']]</kbd></li>
<li class="calibre15">Partition for <kbd class="calibre17">Wind=Strong: [['Warm', 'Strong', 'Cloudy', 'No'], ['Warm', 'Strong', 'Cloudy', 'No']]</kbd></li>
</ul>
<p class="mce-root">Now, given the partitions, let's form the branches and the child nodes.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Wind=Breeze]</kbd>, to the <kbd class="calibre17">[Temperature=Warm]</kbd> node. This branch classifies a single feature: <kbd class="calibre17">[['Warm', 'Breeze', 'Sunny', 'Yes']]</kbd>.</p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Wind=Breeze]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Sunshine</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">Sunshine</kbd>, all the remaining features have the same value: <kbd class="calibre17">Sunny</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[Play=Yes]</kbd>.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Wind=Strong]</kbd>, to the <kbd class="calibre17">[Temperature=Warm]</kbd> node. This branch classifies two features: <kbd class="calibre17">[['Warm', 'Strong', 'Cloudy', 'No'],</kbd>  and <kbd class="calibre17"> ['Warm', 'Strong', 'Cloudy', 'No']]</kbd></p>
<p class="mce-root">We would like to add children to the <kbd class="calibre17">[Wind=Strong]</kbd> node.</p>
<p class="mce-root">We have the following variable available: <kbd class="calibre17">['Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Sunshine</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">Sunshine</kbd>, all the remaining features have the same value: <kbd class="calibre17">Cloudy</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[Play=No]</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Now, we have added all the children nodes to the <kbd class="calibre17">[Temperature=Warm]</kbd> node.</p>
<p class="mce-root">We add a child node, <kbd class="calibre17">[Temperature=Hot]</kbd>, to the <kbd class="calibre17">[root]</kbd> node. This branch classifies three features: <kbd class="calibre17">[['Hot', 'Breeze', 'Cloudy', 'Yes'], ['Hot', 'Breeze', 'Cloudy', 'Yes'],</kbd>  and <kbd class="calibre17"> ['Hot', 'Breeze', 'Cloudy', 'Yes']]</kbd>.</p>
<p class="mce-root">We would like to add children to the  <kbd class="calibre17">[Temperature=Hot]</kbd> node.</p>
<p class="mce-root">We have the following variables available: <kbd class="calibre17">['Wind', 'Sunshine']</kbd>. As there are fewer of these than the <kbd class="calibre17">m=3</kbd> parameter, we consider both of them. The one with the highest information gain is the <kbd class="calibre17">Wind</kbd> variable. Therefore, we will branch the node further on this variable. We will also remove this variable from the list of available variables for the children of the current node. For the chosen variable, <kbd class="calibre17">Wind</kbd>, all the remaining features have the same value: <kbd class="calibre17">Breeze</kbd>. So, we end the branch with a leaf node, adding <kbd class="calibre17">[Play=Yes]</kbd>.</p>
<p class="mce-root">Now, we have added all the children nodes to the <kbd class="calibre17">[root]</kbd> node.</p>
<p class="mce-root"><strong class="calibre8">Construction of random decision trees numbers 1, 2, and 3</strong></p>
<p class="mce-root">We construct the next three trees in a similar fashion. We should note that, since the construction is random, a reader who performs another correct construction may arrive at a different construction. However, if there are a sufficient number of random decision trees in a random forest, then the result of the classification should be very similar across all the random constructions.</p>
<p class="mce-root">The full construction can be found in the program output in the <kbd class="calibre17">source_code/4/chess.out</kbd> file. </p>
<p class="mce-root"><strong class="calibre8">Constructed random forest</strong>:</p>
<pre class="calibre22">Tree 0:<br class="calibre2"/>Root<br class="calibre2"/>├── [Temperature=Cold]<br class="calibre2"/>│ ├── [Wind=None]<br class="calibre2"/>│ │ └── [Play=Yes]<br class="calibre2"/>│ └── [Wind=Breeze]<br class="calibre2"/>│   └── [Play=No]<br class="calibre2"/>├── [Temperature=Warm]<br class="calibre2"/>│ ├── [Wind=Breeze]<br class="calibre2"/>│ │ └── [Play=Yes]<br class="calibre2"/>│ └── [Wind=Strong]<br class="calibre2"/>│   └── [Play=No]<br class="calibre2"/>└── [Temperature=Hot]<br class="calibre2"/>  └── [Play=Yes]<br class="calibre2"/><br class="calibre2"/>Tree 1:<br class="calibre2"/>Root<br class="calibre2"/>├── [Wind=Breeze]<br class="calibre2"/>│ └── [Play=No]<br class="calibre2"/>├── [Wind=None]<br class="calibre2"/>│ ├── [Temperature=Cold]<br class="calibre2"/>│ │ └── [Play=Yes]<br class="calibre2"/>│ ├── [Temperature=Warm]<br class="calibre2"/>│ │ ├── [Sunshine=Sunny]<br class="calibre2"/>│ │ │ └── [Play=Yes]<br class="calibre2"/>│ │ └── [Sunshine=Cloudy]<br class="calibre2"/>│ │   └── [Play=Yes]<br class="calibre2"/>│ └── [Temperature=Hot]<br class="calibre2"/>│   └── [Play=No]<br class="calibre2"/>└── [Wind=Strong]<br class="calibre2"/>    ├── [Temperature=Cold]<br class="calibre2"/>    │ └── [Play=No]<br class="calibre2"/>    └── [Temperature=Warm]<br class="calibre2"/>        └── [Play=No]<br class="calibre2"/><br class="calibre2"/>Tree 2: <br class="calibre2"/>    Root
    ├── [Wind=Strong]
    │ └── [Play=No]
    ├── [Wind=None]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    └── [Wind=Breeze]
      ├── [Temperature=Hot]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]<br class="calibre2"/><br class="calibre2"/>Tree 3:
    Root
    ├── [Temperature=Cold]
    │ └── [Play=No]
    ├── [Temperature=Warm]
    │ ├──[Wind=Strong]
    │ │ └──[Play=No]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Breeze]
    │   └── [Play=Yes]
    └── [Temperature=Hot]
      ├── [Wind=Strong]
      │ └── [Play=Yes]
      └── [Wind=Breeze]
        └── [Play=Yes]<br class="calibre2"/>The total number of trees in the random forest=4.
The maximum number of the variables considered at the node is m=4.</pre>


            

            
        
    </div>



  
<div><h1 class="header-title">Classification</h1>
                
            
            
                
<p class="mce-root">Given the random forest constructed, we classify the  <kbd class="calibre17">['Warm', 'Strong', 'Sunny', '?']</kbd> feature:</p>
<ul class="calibre14">
<li class="calibre15"><strong class="calibre3">Tree 0 votes for the class</strong>: <kbd class="calibre17">No</kbd></li>
<li class="calibre15"><strong class="calibre3">Tree 1 votes for the class</strong>: <kbd class="calibre17">No</kbd></li>
<li class="calibre15"><strong class="calibre3">Tree 2 votes for the class</strong>: <kbd class="calibre17">No</kbd></li>
<li class="calibre15"><strong class="calibre3">Tree 3 votes for the class</strong>: <kbd class="calibre17">No</kbd></li>
</ul>
<p class="mce-root">The class with the maximum number of votes is <kbd class="calibre17">No</kbd>. Thus, the constructed random forest classifies the feature <kbd class="calibre17">['Warm', 'Strong', 'Sunny', '?']</kbd> into the class <kbd class="calibre17">No</kbd>.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<p class="mce-root">To perform the preceding analysis, we use a program implemented earlier in this chapter. First, we insert the data from the table into the following CSV file:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/4/chess.csv  
</strong>Temperature,Wind,Sunshine,Play  
Cold,Strong,Cloudy,No  
Warm,Strong,Cloudy,No  
Warm,None,Sunny,Yes  
Hot,None,Sunny,No  
Hot,Breeze,Cloudy,Yes  
Warm,Breeze,Sunny,Yes  
Cold,Breeze,Cloudy,No  
Cold,None,Sunny,Yes  
Hot,Strong,Cloudy,Yes  
Warm,None,Cloudy,Yes  
Warm,Strong,Sunny,? </pre>
<p class="mce-root"><strong class="calibre8">Output</strong>:</p>
<p class="mce-root">We produce the output by executing the following command line:</p>
<pre class="calibre22"><strong class="calibre3">$ python random_forest.py chess.csv 4 2 &gt; chess.out</strong></pre>
<p class="mce-root">The number <kbd class="calibre17">4</kbd> here means that we want to construct four decision trees, and <kbd class="calibre17">2</kbd> is the level of verbosity of the program that includes the explanations of how a tree is constructed. The last part, <kbd class="calibre17">&gt; chess.out</kbd>, means that the output is written to the <kbd class="calibre17">chess.out</kbd> file. This file can be found in the chapter directory <kbd class="calibre17">source_code/4</kbd>. We will not put all the output here, as it is very large and repetitive. Instead, some of it was included in the preceding analysis and in the construction of a random forest.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Going shopping – overcoming data inconsistencies with randomness and measuring the level of confidence</h1>
                
            
            
                
<p class="mce-root">We take the problem from the previous chapter. We have the following data relating to the shopping preferences of our friend, Jane:</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Rain</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Shopping</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root">In the previous chapter, decision trees were not able to classify the feature <kbd class="calibre17">(Cold, None)</kbd>. So, this time, we would like to establish whether Jane would go shopping if the temperature was cold and there was no rain using the random forest algorithm.</p>
<p class="mce-root"/>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root">To perform an analysis using the random forest algorithm, we use the program implemented.</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<p class="mce-root">We insert the data from the table into the following CSV file:</p>
<pre class="calibre22"><strong class="calibre3"># source_code/4/shopping.csv  
</strong>Temperature,Rain,Shopping  
Cold,None,Yes  
Warm,None,No  
Cold,Strong,Yes  
Cold,None,No  
Warm,Strong,No  
Warm,None,Yes 
Cold,None,? </pre>
<p class="mce-root"><strong class="calibre8">Output</strong>:</p>
<p class="mce-root">We want to use a slightly higher number of trees than we used in the previous examples and explanations to obtain more accurate results. We want to construct a random forest with 20 trees with low-verbosity output – level 0. Thus, we undertake execution in a terminal:</p>
<pre class="calibre22"><strong class="calibre3">$ python random_forest.py shopping.csv 20 0</strong>
***Classification***
Feature: ['Cold', 'None', '?'] 
Tree 0 votes for the class: Yes 
Tree 1 votes for the class: No 
Tree 2 votes for the class: No 
Tree 3 votes for the class: No 
Tree 4 votes for the class: No 
Tree 5 votes for the class: Yes 
Tree 6 votes for the class: Yes 
Tree 7 votes for the class: Yes 
Tree 8 votes for the class: No 
Tree 9 votes for the class: Yes 
Tree 10 votes for the class: Yes 
Tree 11 votes for the class: Yes 
Tree 12 votes for the class: Yes 
Tree 13 votes for the class: Yes 
Tree 14 votes for the class: Yes 
Tree 15 votes for the class: Yes 
Tree 16 votes for the class: Yes 
Tree 17 votes for the class: No 
Tree 18 votes for the class: No 
Tree 19 votes for the class: No
The class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Cold', 'None', '?'] into the class 'Yes'.
  </pre>
<p class="mce-root">However, we should note that only 12 out of the 20 trees voted for the answer <kbd class="calibre17">Yes</kbd>. Therefore, although we have a definite answer, it might not be that certain, similar to the results we got with an ordinary decision tree. However, unlike in decision trees, where an answer was not produced because of data inconsistency, here, we have an answer.</p>
<p class="mce-root">Furthermore, by measuring the strength of the voting power for each individual class, we can measure the level of confidence that the answer is correct. In this case, the feature <kbd class="calibre17">['Cold', 'None', '?']</kbd> belongs to the <kbd class="calibre17"><em class="calibre5">Yes</em></kbd> class with a confidence level of 12/20, or 60%. To determine the level of certainty of the classification more precisely, an even larger ensemble of random decision trees would be required.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we learned that a random forest is a set of decision trees, where each tree is constructed from a sample chosen randomly from the initial data. This process is called <strong class="calibre8">bootstrap aggregating</strong>. Its purpose is to reduce variance and bias in classifications made by a random forest. Bias is further reduced during the construction of a decision tree by considering only a random subset of the variables for each branch of the tree.</p>
<p class="mce-root">We also learned that once a random forest is constructed, the result of the classification of a random forest is a majority vote from among all the trees in a random forest. The level of the majority also determines the level of confidence that the answer is correct.</p>
<p class="mce-root">Since random forests consist of decision trees, it is good to use them for every problem where a decision tree is a good choice. As random forests reduce the bias and variance that exists in decision tree classifiers, they outperform decision tree algorithms.</p>
<p class="mce-root">In the next chapter, we will learn the technique of clustering data into similar clusters. We will also exploit this technique to classify data into one of the created clusters.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Problems</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre8">Problem</strong> <strong class="calibre8">1</strong>: Let's take the example of playing chess from <a href="" target="_blank" class="pcalibre pcalibre3 pcalibre2 pcalibre1 calibre13">Chapter 2</a>, <em class="calibre18">Naive Bayes</em>. How would you classify a <kbd class="calibre17">(Warm,Strong,Spring,?)</kbd> data sample according to the random forest algorithm?</p>
<table class="msotablegrid" border="1">
<tbody class="calibre23">
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Temperature</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Wind</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Season</strong></p>
</td>
<td class="calibre25">
<p class="calibre26"><strong class="calibre8">Play</strong></p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Winter</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Summer</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">Breeze</p>
</td>
<td class="calibre25">
<p class="calibre26">Winter</p>
</td>
<td class="calibre25">
<p class="calibre26">No</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Cold</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre27">
<td class="calibre25">
<p class="calibre26">Hot</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Summer</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre24">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">None</p>
</td>
<td class="calibre25">
<p class="calibre26">Autumn</p>
</td>
<td class="calibre25">
<p class="calibre26">Yes</p>
</td>
</tr>
<tr class="calibre33">
<td class="calibre25">
<p class="calibre26">Warm</p>
</td>
<td class="calibre25">
<p class="calibre26">Strong</p>
</td>
<td class="calibre25">
<p class="calibre26">Spring</p>
</td>
<td class="calibre25">
<p class="calibre26">?</p>
</td>
</tr>
</tbody>
</table>
<p class="mce-root"> </p>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: Would it be a good idea to use only one tree and a random forest? Justify your answer.</p>
<p class="mce-root"><strong class="calibre8">Problem 3</strong>: Can cross-validation improve the results of the classification by the random forest? Justify your answer.</p>


            

            
        
    </div>



  
<div><h1 class="header-title">Analysis</h1>
                
            
            
                
<p class="mce-root"><strong class="calibre8">Problem 1:</strong> We run the program to construct the random forest and classify the feature (<kbd class="calibre17">Warm, Strong, Spring</kbd>).</p>
<p class="mce-root"><strong class="calibre8">Input</strong>:</p>
<pre class="calibre50"><strong class="calibre3">source_code/4/chess_with_seasons.csv  
</strong>Temperature,Wind,Season,Play  
Cold,Strong,Winter,No  
Warm,Strong,Autumn,No  
Warm,None,Summer,Yes  
Hot,None,Spring,No  
Hot,Breeze,Autumn,Yes  
Warm,Breeze,Spring,Yes  
Cold,Breeze,Winter,No  
Cold,None,Spring,Yes  
Hot,Strong,Summer,Yes  
Warm,None,Autumn,Yes  
Warm,Strong,Spring,? </pre>
<p class="mce-root"><strong class="calibre8">Output</strong>:</p>
<p class="mce-root">We construct four trees in a random forest, as follows:</p>
<pre class="calibre50"><strong class="calibre3">$ python random_forest.py chess_with_seasons.csv 4 2 &gt; chess_with_seasons.out</strong></pre>
<p class="mce-root">The whole construction and the analysis are stored in the <kbd class="calibre17">source_code/4/chess_with_seasons.out</kbd> file. Your construction may differ because of the randomness involved. From the output, we extract the random forest graph, consisting of random decision trees, given the random numbers generated during our run.</p>
<p class="mce-root">Executing the preceding command again will most likely result in a different output and a different random forest graph. Yet there is a high probability that the results of the classification will be similar because of the multiplicity of the random decision trees and their voting power combined. The classification by one random decision tree may be subject to significant variance. However, the majority vote combines the classification from all the trees, thus reducing the variance. To verify your understanding, you can compare your classification results with the classification by the following random forest graph.</p>
<p class="mce-root"><strong class="calibre8">Random forest graph and classification:</strong></p>
<p class="mce-root">Let's have a look at the output of the random forest graph and the classification of the feature:</p>
<pre class="calibre50">Tree 0:<br class="calibre2"/>    Root
    ├── [Wind=None]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   ├── [Season=Autumn]
    │   │ └── [Play=Yes]
    │   └── [Season=Summer]
    │     └── [Play=Yes]
    └── [Wind=Strong]
      ├── [Temperature=Cold]
      │ └── [Play=No]
      └── [Temperature=Warm]
        └── [Play=No]<br class="calibre2"/>Tree 1:
    Root
    ├── [Season=Autumn]
    │ ├──[Wind=Strong]
    │ │ └──[Play=No]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Breeze]
    │   └── [Play=Yes]
    ├── [Season=Summer]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │   └── [Play=No]
    └── [Season=Spring]
      ├── [Temperature=Cold]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]
    
Tree 2:<br class="calibre2"/>    Root
    ├── [Season=Autumn]
    │ ├── [Temperature=Hot]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=No]
    ├── [Season=Spring]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │ └── [Play=No]
    └── [Season=Summer]
      ├── [Temperature=Hot]
      │ └── [Play=Yes]
      └── [Temperature=Warm]
        └── [Play=Yes]<br class="calibre2"/>Tree 3:
    Root
    ├── [Season=Autumn]
    │ ├──[Wind=Breeze]
    │ │ └── [Play=Yes]
    │ ├── [Wind=None]
    │ │ └── [Play=Yes]
    │ └──[Wind=Strong]
    │   └── [Play=No]
    ├── [Season=Spring]
    │ ├── [Temperature=Cold]
    │ │ └── [Play=Yes]
    │ └── [Temperature=Warm]
    │   └── [Play=Yes]
    ├── [Season=Winter]
    │ └── [Play=No]
    └── [Season=Summer]
      └── [Play=Yes]<br class="calibre2"/>The total number of trees in the random forest=4.
The maximum number of the variables considered at the node is m=4.
    
Classication
Feature: ['Warm', 'Strong', 'Spring', '?']<br class="calibre2"/>Tree 0 votes for the class: No
Tree 1 votes for the class: Yes<br class="calibre2"/>Tree 2 votes for the class: Yes<br class="calibre2"/>Tree 3 votes for the class: Yes
The class with the maximum number of votes is 'Yes'. Thus the constructed random forest classifies the feature ['Warm', 'Strong', 'Spring', '?'] into the class 'Yes'.</pre>
<p class="mce-root"><strong class="calibre8">Problem 2</strong>: When we construct a tree in a random forest, we only use a random subset of the data, with replacements. This is to eliminate classifier bias toward certain features. However, if we use only one tree, that tree may happen to contain features with bias and might be missing an important feature to enable it to provide an accurate classification. So, a random forest classifier with one decision tree would likely lead to a very poor classification. Therefore, we should construct more decision trees in a random forest to benefit from reduced bias and variance in the classification.</p>
<p class="mce-root"><strong class="calibre8">Problem 3</strong>: During cross-validation, we divide the data into training and testing data. Training data is used to train the classifier, and testing data is used to evaluate which parameters or methods would be the best fit for improving classification. Another advantage of cross-validation is the reduction in bias, because we only use partial data, thereby decreasing the chance of overfitting the specific dataset.</p>
<p class="mce-root">However, in a decision forest, we address problems that cross-validation addresses in an alternative way. Each random decision tree is constructed only on the subset of the data—reducing the chance of overfitting. In the end, classification is the combination of results from each of these trees. The best decision, in the end, is not made by tuning the parameters on a test dataset, but by taking the majority vote of all the trees with reduced bias.</p>
<p class="cdpalignleft1">Hence, cross-validation for a decision forest algorithm would not be of much use as it is already intrinsic within the algorithm.</p>


            

            
        
    </div>



  </body></html>