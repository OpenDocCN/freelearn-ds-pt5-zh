- en: Understanding AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You now have a good understanding of what data science can do and how we can
    check whether it works. We have covered the main domains of data science, including
    machine learning and deep learning, but still, the inner workings of the algorithms
    are difficult to discern through the fog. In this chapter, we will look at algorithms.
    You will get an intuitive understanding of how the learning process is defined
    using mathematics and statistics. Deep neural networks won't be so mystical anymore,
    and common machine learning jargon will not scare you but provide understanding
    and ideas to complete the ever-growing list of potential projects.
  prefs: []
  type: TYPE_NORMAL
- en: 'You are not the only one who will benefit from reading this chapter. Your new
    knowledge will streamline communication with colleagues, making meetings short
    and purposeful and teamwork more efficient. We will start at the heart of every
    machine learning problem: defining the learning process. To do this, we will start
    with the two subjects that lie at the root of data science: mathematics and statistics.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding mathematical optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking with statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How do machines learn?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding mathematical optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we will explore the concept of mathematical optimization. Optimization
    is the central component of machine learning problem. It turns out that the learning
    process is nothing more than a mere mathematical optimization problem. The trick
    is to define it properly. To come up with a good definition, we first need to
    understand how mathematical optimization works and which problems it can solve.
  prefs: []
  type: TYPE_NORMAL
- en: If you work in the business sector, I bet that you hear the word optimization
    several times a day. To optimize something means to make it more efficient, cut
    costs, increase revenues, and minimize risks. Optimization involves taking a number
    of actions, measuring results, and deciding whether you have ended up in a better
    place.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to optimize your daily route to work, you can minimize the total
    time you spend driving from home to the office. Let's suppose that in your case
    the only thing that matters is time. Thus, optimization means minimization of
    the time. You may try different options such as using another road or going by
    public transport instead of driving your car. To choose the best, you will evaluate
    all routes using the same quantity, that is, the total time to make it from home
    to the office.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better idea of defining optimization problems, let''s consider another
    example. Our friend Jonathan was tired of his day-to-day job at a bank, so he
    has started a rabbit farm. It turns out that rabbits breed fast. To start, he
    bought four rabbits, and after a while he had 16\. A month later, there were 256
    of them. All of those new rabbits causing additional expense. Jonathan''s rabbit
    sale rates fell lower than the rate at which the rabbits bred. Jonathan''s smart
    farmer friend Aron was impressed with his rabbit production rates, so he proposed
    to buy all excess rabbits for a discounted price. Now, Jonathan needs to find
    out how many rabbits to sell to Aron so that he can keep within the following
    boundaries:'
  prefs: []
  type: TYPE_NORMAL
- en: He won't get in a situation where he can't sell a rabbit to someone who desperately
    wants one. The rabbit breeding rate should not fall below the rabbit selling forecasts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: His total expenses for rabbit care stay within the budget.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, we have defined another optimization problem, and the rabbit
    farm started to remind the bank job that Jonathan left before. This optimization
    task is quite different though, it looks harder. In the first problem, we tried
    to minimize commuting time. In this problem, we need to seek the minimum amount
    of rabbits to sell so it does not violate other conditions. We call problems like
    these **constrained optimization**. Additional constraints allow us to model more
    realistic scenarios in complex environments. To name a few, constrained optimization
    can solve planning, budgeting, and routing problems. In the end, Jonathan was
    disappointed with his rabbit farm and sold it to Aron. He then continued his path
    of finding a perfect occupation that wouldn't end up similar to his banking job.
  prefs: []
  type: TYPE_NORMAL
- en: There is one place where profits and losses cease making you mad; that is the mathematics
    department at a technical university. To get a position there, they ask you to
    pass an exam. The first task is to find a minimum of a function, ![](img/bddf2c16-f3e3-4ee5-a413-152095024615.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the plot of this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/911bbd48-87ee-4d8d-b9b9-d7fb2b90ba24.png)'
  prefs: []
  type: TYPE_IMG
- en: While examining the plot, you notice that this function takes values no less
    than 0, so the answer is obviously 0\. The next question looks like the previous
    one but with a twist—*Find a minimum of the function* ![](img/d94e8a03-ff14-400a-8131-1bd7086194e4.png),
    *where ![](img/1def254f-e730-4c92-aaf4-e592208ef0ec.png) is an arbitrary number*.
    To solve it, you draw a bunch of plots and find out that the minimum value is
    always *a*.
  prefs: []
  type: TYPE_NORMAL
- en: The last question takes it to the extreme. It says that you won't be given a
    formula for ![](img/65cb02ad-8d7e-4ab8-ae3d-3a0fb57d2388.png), but you can go
    to your teacher and ask for values of ![](img/93434734-953b-4790-86f1-da915f2d31de.png) for
    some ![](img/d9035d65-10fa-4f72-ace4-b66d9f566858.png) as many times as you want.
    It is impossible to draw a plot. In other plots, the minimum was always the lowest
    point. How can we find that point without looking at the plot? To tackle this
    problem, we will first imagine that we have a plot of this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will draw a line between two arbitrary points of the function, as
    seen in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffd06268-af3c-47f9-aca4-fa7b27087a62.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will call the distance between those points ![](img/15e681ce-db67-4f73-8d4b-505deea6e04e.png).
    If we make ![](img/6a7e6175-b05f-4c10-9d92-f13938a7ba93.png) smaller and smaller,
    two points will be so close that they will visually converge to a single point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d40fa9e2-4e3f-49a0-a572-563935a309c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The line in the preceding plot is called a tangent. It has a very convenient
    property, and the slope of this line can help us find the minimum or maximum of
    a function. If the line is flat, then we have found either the minimum or maximum
    of a function. If all nearby points are higher, then it should be a maximum. If
    all nearby points are lower, then this is the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following plot shows a function (drawn in blue) and its maximum, along
    with the tangent (drawn in orange):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8205186f-79a2-4ac6-8a63-695d7ce6c7ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Drawing a bunch of lines and points becomes mundane after a while. Thankfully,
    there is a simple way to compute a slope of this line between ![](img/b41a95fb-d14c-4d8a-bdbc-4740c20405e3.png) and ![](img/da500071-60c5-4e9f-a9f1-bcbff0c8c5c0.png).
    If you recall the Pythagorean theorem, you will quickly find an answer: [![](img/429f9755-80d6-431a-b87d-9a7467ec2461.png)].
    We can easily find the slope using this formula.'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations, we have just invented our first mathematical optimization algorithm,
    a gradient descent. As always, the name is scary, but the intuition is simple.
    To have a good understanding of function optimization, imagine that you are standing
    on a large hill. You need to descend from it with your eyes closed. You will probably
    test the area around you by moving your feet around. When you feel a descending
    direction, you will take a step there and repeat. In mathematical terminology,
    the hill would be a function, ![](img/05f59972-aef1-4530-89de-6725bf754977.png).
    Each time you evaluate a slope, you calculate the gradient of the function, ![](img/e17eff6c-1b96-4465-8ce1-da2bc478e56a.png).
    You can follow this gradient to find the minimum or maximum of a function. That's
    why it is called gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: You can solve the final task by using gradient descent. You can choose a starting
    point, [![](img/c10f822a-4c2a-4f53-be28-925894d94bf8.png)], ask for the value
    of [![](img/20764d2d-3746-443c-a6f9-55212c8b176d.png)], and calculate the slope
    using the small number, ![](img/212858db-b721-4543-bc1c-b54c9fb0ec9f.png). By
    looking at the slope, you can decide whether your next pick, [![](img/77706a78-632d-40df-87dc-d753b3fa43b4.png)], should
    be greater or less than [![](img/c8400b52-92cc-463f-94a7-17ff7f8bf387.png)]. When
    the slope becomes zero, you can test whether your current value of [![](img/51db79c8-9fd4-4570-97ca-bcf1fa28f884.png)] is
    the minimum or maximum by looking at several nearby values. If every value is
    less than [![](img/afff9b66-3b6c-46b2-abb6-4fb90cc6c18e.png)], then [![](img/8e4a7548-7761-46f4-9756-099ce8f2a4d7.png)] is
    the maximum. Otherwise, it is a minimum.
  prefs: []
  type: TYPE_NORMAL
- en: 'As always, there is a caveat. Let''s examine this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf871086-e906-4a3e-9e7e-59c38b9ca861.png)'
  prefs: []
  type: TYPE_IMG
- en: If we start gradient descent at point **A**, we will end up with a real minimum.
    But if we start at point **B**, we will be stuck at the local minimum. When you
    use gradient descent, you can never actually know whether you are in a local or
    global minimum. One way to check is to repeat the descent from various points
    that are far away from each other. The other way to avoid local minima is to increase
    the step size, ![](img/04a9240d-d9ff-430a-818d-c2b77eb5ee44.png). But be careful;
    if ![](img/60e0e937-c4e9-4c72-b04b-5af0ef22adf6.png) is too large, you will just
    jump over the minima again and again, never reaching your true goal, the global
    minimum.
  prefs: []
  type: TYPE_NORMAL
- en: Like in machine learning, there are many mathematical optimization algorithms
    with different trade-offs. Gradient descent is one of the simplest and easiest
    to get started with. Despite being simple, gradient descent is commonly used to
    train machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review a few key points before moving on:'
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical optimization is the central component of machine learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are two kinds of optimization problems: constrained and unconstrained.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient descent is a simple and widely applied optimization algorithm. To understand
    the intuition behind gradient descent, recall the hill descent analogy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now you have a good grip on the main principles of mathematical optimization
    in your tool belt, we can research the field of statistics—the grandfather of
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking with statistics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistics deal with all things about data, namely, collection, analysis, interpretation,
    inference, and presentation. It is a vast field, incorporating many methods for
    analyzing data. Covering it all is out of the scope of this book, but we will
    look into one concept that lies at the heart of machine learning, that is, **maximum
    likelihood estimation** (**MLE**). As always, do not fear the terminology, as
    the underlying concepts are simple and intuitive. To understand MLE, we will need
    to dive into probability theory, the cornerstone of statistics.
  prefs: []
  type: TYPE_NORMAL
- en: To start, let's look at why we need probabilities when we already are equipped
    with such great mathematical tooling. We use calculus to work with functions on
    an infinitesimal scale and to measure how they change. We developed algebra to
    solve equations, and we have dozens of other areas of mathematics that help us
    to tackle almost any kind of hard problem we can think of. We even came up with
    category theory that provides a universal language for all mathematics that almost
    no one can understand (Haskell programmers included).
  prefs: []
  type: TYPE_NORMAL
- en: The difficult part is that we all live in a chaotic universe where things can't
    be measured exactly. When we study real-world processes we want to learn about
    many random events that distort our experiments. Uncertainty is everywhere, and
    we must tame and use it for our needs. That is when probability theory and statistics
    come into play. Probabilities allow us to quantify and measure uncertain events
    so we can make sound decisions. Daniel Kahneman showed in his widely known book
    *Thinking, Fast and Slow*, that our intuition is notoriously bad in solving statistical
    problems. Probabilistic thinking helps us to avoid biases and act rationally.
  prefs: []
  type: TYPE_NORMAL
- en: Frequentist probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Imagine that a stranger suggested you play a game: he gives you a coin. You
    toss it. If it comes up heads, you get $100\. If it is tails, you lose $75\. Before
    playing a game, you will surely want to check whether or not it is fair. If the
    coin is biased toward tails, you can lose money pretty quickly. How can we approach
    this? Let''s conduct an experiment, wherein we will record 1 if heads come up
    and 0 if we see tails. The fun part is that we will need to make 1,000 tosses
    to be sure that our calculations are right. Imagine we got the following results:
    600 heads (1s) and 400 tails (0s). If we then count how frequent heads or tails
    came up in the past, we will get 60% and 40%, respectively. We can interpret those
    frequencies as probabilities of a coin coming up heads or tails. We call this
    a frequentist view on the probabilities. It turns out that our coin is actually
    biased toward heads. The expected value of this game can be calculated by multiplying
    probabilities with their values and summing everything up (the value in the following
    formula is negative because $40 is a potential loss, not gain):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9ef427c-e4dd-4b65-ac08-e900e07fa5b0.png)'
  prefs: []
  type: TYPE_IMG
- en: The more you play, the more you get. Even after having several consecutive unlucky
    throws in a row, you can be sure that the returns will average out soon. Thus,
    a frequentist probability measures a proportion of some event to all other possible
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Conditional probabilities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is handy to know the probability of an event given some other event has
    occurred. We write the conditional probability of an event ![](img/9f4edf0c-02f4-4f5d-b327-e42b6dcc3a16.png) given
    event ![](img/0550de15-ba34-43f1-89cb-7092ac03c448.png) as ![](img/a2f7111c-68ab-4eff-80fb-67c0fb20a134.png).
    Take rain, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the probability of rain given we hear thunder?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the probability of rain given it is sunny?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see the probabilities of different events
    occurring together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/481bc35d-6f6e-46b7-897d-ad0651b4d5d5.png)'
  prefs: []
  type: TYPE_IMG
- en: From this Euler diagram, we can see that ![](img/28e92085-79d8-4c5a-872b-cc0c512fb639.png),
    meaning that there is always rain when we hear thunder (yes, it is not exactly
    true, but we'll take this as true for the sake of simplicity).
  prefs: []
  type: TYPE_NORMAL
- en: 'What about ![](img/43ee27c6-2ae9-48ce-a87b-33bfbef3869d.png)? Visually, this
    probability is small, but how can we formulate this mathematically to do the exact
    calculations? Conditional probability is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4c54506-0805-410b-aad6-05bd8cf1501f.png)'
  prefs: []
  type: TYPE_IMG
- en: In words, we divide the joint probability of both *Rain* and *Sunny* by the
    probability of sunny weather.
  prefs: []
  type: TYPE_NORMAL
- en: Dependent and independent events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We call a pair of events independent, if the probability of one event does
    not influence the other. For example, take the probability of rolling a die and
    getting a 2 twice in a row. Those events are independent. We can state this as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4da71ad8-0de3-4f03-84b2-5f4d26d8111e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But why does this formula work? First, let''s rename events for the first and
    second tosses as A and B to remove notational clutter and then rewrite the probability
    of a roll explicitly as a joint probability of both rolls we have seen so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50559135-6ee9-4325-b8dc-956b27439e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And now multiply and divide ![](img/44c56de8-4513-4459-a57c-53b0054b58a1.png)
    by ![](img/1d94f705-b256-4be6-97c3-906f2cd98549.png) (nothing changes, it can
    be canceled out), and recall the definition of conditional probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b54ec1b2-12c0-4a51-8eb7-4e828a1b8480.png)'
  prefs: []
  type: TYPE_IMG
- en: If we read the previous expression from right to left, we find that ![](img/f9030a42-ae3c-4599-8d4b-b2fcf30b9b2c.png).
    Basically, this means that A is independent of B! The same argument goes for ![](img/0213653a-f971-423a-9b01-2ec9716cb496.png).
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian view on probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before this point, we always measured probabilities as frequencies. The frequentist
    approach is not the only way to define probabilities. While frequentists think
    about probabilities as proportions, the Bayesian approach takes prior information
    into account. Bayes'' theory is centered around a simple theorem that allows us
    to compute conditional probabilities based on prior knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/044af8c0-e62f-4e60-9fa5-d9d23981ca19.png)'
  prefs: []
  type: TYPE_IMG
- en: In this example, the prior value is ![](img/77bcd102-059d-4827-8e48-ebb7ebe62e33.png).
    If we do not know the real prior value, we can substitute an estimate that is
    based on our experience to make an approximate calculation. This is the beauty
    of Bayes' theorem. You can calculate complex conditional probabilities with simple
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes' theorem has immense value and a vast area of application. The Bayesian
    theory even has its own branch of statistics and inference methods. Many people
    think that the Bayesian view is a lot closer to how we humans understand uncertainties,
    in particular, how prior experience affects decisions we make.
  prefs: []
  type: TYPE_NORMAL
- en: Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Probabilities work with sets of outcomes or events. Many problems we describe
    with probabilities share common properties. In the following plot, you can see
    the bell curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e75f1b9f-bef0-4888-96d2-2268f20267be.png)'
  prefs: []
  type: TYPE_IMG
- en: The bell curve, or Gaussian distribution, is centered around the most probable
    set of outcomes, and the tails on both ends represent the least likely outcomes.
    Because of its mathematical properties, the bell curve appears everywhere in our
    world. Measure the height of lots of random people, and you will see a bell curve;
    look at the height of all grass blades in your lawn, and you will see it again.
    Calculate the probability of people in your city having a certain income, and
    here it is again.
  prefs: []
  type: TYPE_NORMAL
- en: The Gaussian distribution is one of the most common distributions, but there
    are many more. A probability distribution is a mathematical law that tells us
    the probabilities of different possible outcomes of events formulated as a mathematical
    function.
  prefs: []
  type: TYPE_NORMAL
- en: When we measured relative frequencies of a coin-toss event, we calculated the
    so-called empirical probability distribution. Coin tosses also can be formulated
    as a Bernoulli distribution. And if we wanted to calculate the probability of
    heads after *n* trials, we may use a binomial distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is convenient to introduce a concept analogous to a variable that may be
    used in probabilistic environments—a random variable. Random variables are the
    basic building blocks of statistics. Each random variable has a distribution assigned
    to it. Random variables are written in uppercase by convention, and we use the
    ~ symbol to specify a distribution assigned to a variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32946440-4404-446d-bd28-815b1df0db25.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that the random variable *X* is distributed according to a Bernoulli
    law with the probability of success (heads) equal to 0.6.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating statistics from data samples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you are doing research on human height and are eager to publish a mind-blowing
    scientific paper. To complete your research, you need to measure the average person''s
    height in your area. You can do this in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect the heights of every person in your city and calculate average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Statistics allows us to reason about different properties of the population
    without collecting a full dataset for each person in the population. The process
    of selecting a random subset of data from the true population is called sampling.
    A statistic is any function that is used to summarize the data using values from
    the sample. The ubiquitous statistic that is used by almost everyone on a daily
    basis is the sample mean or arithmetic average:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6cfed050-984c-4092-b7ca-c64ae9932998.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have collected a random sample of 16 people to calculate an average height.
    In the following table, we can see the heights over the course of four days:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Heights** | **Average** |'
  prefs: []
  type: TYPE_TB
- en: '| Monday | 162 cm, 155 cm, 160 cm, 171 cm | 162.00 cm |'
  prefs: []
  type: TYPE_TB
- en: '| Tuesday | 180 cm, 200 cm, 210 cm, 179 cm | 192.25 cm |'
  prefs: []
  type: TYPE_TB
- en: '| Wednesday | 160 cm, 170 cm, 158 cm, 176 cm | 166.00 cm |'
  prefs: []
  type: TYPE_TB
- en: '| Thursday | 178 cm, 169 cm, 157 cm, 165 cm | 167.25 cm |'
  prefs: []
  type: TYPE_TB
- en: '| Total |  | 171.88 cm |'
  prefs: []
  type: TYPE_TB
- en: We collected a sample of four heights for each day, a total of 16 heights. Your
    statistician friend Fred told you on Friday that he had already collected a sample
    of 2,000 people and the average height in the area was about 170 cm.
  prefs: []
  type: TYPE_NORMAL
- en: 'To investigate, we can look at how your sample average changed with each new
    data point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/00996332-e5a5-406b-96ab-6d4f476833d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Notice, that on day 2, the average value was unexpectedly high. It may just
    have happened that we had stumbled upon four tall people. The random fluctuations
    in the data are called variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can measure sample variance using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/634c6613-06d9-452d-959d-2541f3dc820c.png)'
  prefs: []
  type: TYPE_IMG
- en: Sample variance summarizes our data, so we can consider it as another statistic.
    The larger the variance is, the more sample size you need to collect before calculating
    the accurate average value, which will be close to the real one. This phenomenon
    has a name—the law of large numbers. The more measurements you make, the better
    your estimate will be.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Statistics is more than simply calculating summary numbers. One of the most
    interesting aspects of statistics is modeling. Statistical modeling studies mathematical
    models that make a set of statistical assumptions about data. To be more clear,
    let''s return to our weather example. We have collected a dataset with random
    variables that describe the current weather:'
  prefs: []
  type: TYPE_NORMAL
- en: Average speed of the wind
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Air humidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Air temperature
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total number of birds seen in the sky in a local area
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistician's mood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using this data, we want to infer which variables are related to rain. To do
    this, we will build a statistical model. Besides the previous data, we have recorded
    a binary rain variable that takes the value 1 if it rained, and 0 otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we pose a set of assumptions in relation to the data:'
  prefs: []
  type: TYPE_NORMAL
- en: Rain probability has a Bernoulli distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rain probability depends on data we have collected. In other words, there is
    a relationship between the data and rain probability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You may find it strange thinking about rain in terms of probability. What does
    it mean to say that last Wednesday, the probability of rain was 45%? Last Wednesday
    is a past date, so we can examine the data and check whether there was rain. The
    trick is to understand that in our dataset, there are many days similar to Wednesday.
    Let''s suppose that we have collected the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day of week** | **Speed of wind** | **Humidity** | **Temperature** | **Outcome**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Monday | 5 m/s | 50% | 30 C | no rain |'
  prefs: []
  type: TYPE_TB
- en: '| Tuesday | 10 m/s | 80% | 25 C | rain |'
  prefs: []
  type: TYPE_TB
- en: '| Wednesday | 5 m/s | 52% | 28 C | rain |'
  prefs: []
  type: TYPE_TB
- en: '| Thursday | 3 m/s | 30% | 23 C | no rain |'
  prefs: []
  type: TYPE_TB
- en: '| Friday | 8 m/s | 35% | 27 C | no rain |'
  prefs: []
  type: TYPE_TB
- en: In this example, Monday and Wednesday are very similar, but their rain outcomes
    are different. In a sufficiently large dataset, we could find two rows that match
    exactly but have different outcomes. Why is this happening? First, our dataset
    does not include all the possible variables that can describe rain. It is impossible
    to collect such a dataset, so we make an assumption that our data is related to
    rain, but does not describe it fully. Measurement errors, the randomness of events,
    and incomplete data make rain probabilistic. You may wonder if rain is probabilistic
    in nature? Or is every period of rain predetermined? To check whether rain events
    are deterministic, we must collect a daily snapshot of the complete state of the
    universe, which is impossible. Statistics and probability theory help us to understand
    our world even if we have imperfect information. For example, imagine that we
    have 10 days similar to last Wednesday in our dataset. By similar, I mean that
    all variables we have collected differ only by a small amount. Out of those 10
    days, 8 were rainy and 2 were sunny. We may say that on a day typical to last
    Wednesday there is an 80% probability of rain. That is the most accurate answer
    we can give using this data.
  prefs: []
  type: TYPE_NORMAL
- en: Having assumptions about data in place, we can proceed to modeling. We can make
    another assumption that there exists some mathematical model **M**, that uses
    data to estimate rain probability. That is, model **M** uses data **d** to learn
    the relationship between the data and rain probability. The model will infer this
    relationship by assigning rain probabilities that are closest to real outcomes
    in our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The main goal of model **M** is not to make accurate predictions, but to find
    and explain relationships. This is where we can draw a line between statistics
    and machine learning. Machine learning seeks to find accurate predictive models,
    while statistics uses models to find explanations and interpretations. Goals differ,
    but the underlying concepts that allow models to learn from data are the same.
    Now, we can finally uncover how this model **M** can learn from data. We will
    disentangle the magic, leaving a straightforward understanding of the mathematics
    behind machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: How do machines learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How do algorithms learn? How can we define learning? As humans, we learn a lot
    throughout our lives. It is a natural task for us. In the first few years of our
    lives, we learn how to control our body, walk, speak, and recognize different
    objects. We constantly get new experiences, and these experiences change the way
    we think, behave, and act. Can a piece of computer code learn like we do? To approach
    machine learning, we first need to come up with a way to transmit experience directly
    to the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practical cases, we are interested in teaching algorithms to perform all
    kinds of specific tasks faster, better, and more reliably that we can do ourselves.
    For now, we will focus on prediction and recognition tasks. Thus, we want to build
    algorithms that are able to recognize patterns and predict future outcomes. The
    following table shows some examples of recognition and prediction tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Recognition tasks | Is this a high-paying customer?How much does this house
    cost in the current market?What are those objects in an image? |'
  prefs: []
  type: TYPE_TB
- en: '| Prediction tasks | Is this customer likely to return his debt in the next
    6 months?How much will we sell in the next quarter?How risky is this investment?
    |'
  prefs: []
  type: TYPE_TB
- en: The first idea may be to approach learning as humans do, and provide explanations
    and examples through speech, images, and sets of examples. Unfortunately, while
    learning in this way, we perform many complex cognitive tasks, such as listening,
    writing, and speaking. A computer algorithm by itself cannot collect new experiences
    the way we do. What if, instead, we take a simplified model of our world in the
    form of digital data? For example, predicting customer churn for Acme Co could
    be done only using data about customer purchases and product ratings. The more
    complete and full the dataset is, the more accurate the model of customer churn
    is likely to be.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at another example. We will build a machine learning project cost
    estimator. This model will use the attributes of a project to calculate the cost
    estimate. Suppose that we have collected the following data attributes for each
    project in our company:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute name** | **Attribute type** | **Attribute description** | **Possible
    values** |'
  prefs: []
  type: TYPE_TB
- en: '| Number of attributes | Integer | Number of data attributes in the project
    dataset | 0 to ∞ |'
  prefs: []
  type: TYPE_TB
- en: '| Number of data scientists | Integer | Number of data scientists requested
    by the customer for project implementation | 0 to ∞ |'
  prefs: []
  type: TYPE_TB
- en: '| Integration | Integer | Integration with customer''s software systems requested
    by the customer | 0 for no integration in project scope1 for integration in project
    scope |'
  prefs: []
  type: TYPE_TB
- en: '| Is a large company | Integer | Indicates if the customer has a large number
    of employees | 0 = customer''s company employee number greater than 1001 = customer''s
    company employee number less or equal to 100 |'
  prefs: []
  type: TYPE_TB
- en: '| Total project cost | Integer | Total cost in USD | 0 to ∞ |'
  prefs: []
  type: TYPE_TB
- en: 'The example dataset containing these attributes is provided in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Number of attributes** | **Number of data scientists** | **Integration**
    | **Is a large company** | **Total project cost** |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 1 | 0 | 135,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 1 | 0 | 1 | 140,000 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 2 | 1 | 0 | 173,200 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 3 | 1 | 1 | 300,000 |'
  prefs: []
  type: TYPE_TB
- en: 'The simplest model we can imagine is a so-called linear model. It sums data
    attributes multiplied by variable coefficients to calculate the project cost estimate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e5bca27-1c7f-4532-ac51-06629d71190b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this simplified scenario, we do not know the real values of cost variables.
    However, we can use statistical methods and estimate them from data. Let''s start
    with a random set of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Base cost = 50,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost per data attribute = 115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cost per data scientist = 40,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration cost = 50,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer relation complexity cost = 5,000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we use the parameters for every project we have in our dataset, we will
    get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '***Total project 1 cost = 50,000 + 115 x 10 + 40,000 x 1 + 50,000 x 1 + 50,000
    x 0 = 141,150***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Total project 2 cost = 50,000 + 115 x 20 + 40,000 x 1 + 50,000 x 0 + 50,000
    x 1 = 142,300***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Total project 3 cost = 50,000 + 115 x 5 + 40,000 x 2 + 50,000 x 1 + 50,000
    x 0 = 180,575***'
  prefs: []
  type: TYPE_NORMAL
- en: '***Total project 4 cost = 50,000 + 115 x 100 + 40,000 x 3 + 50,000 x 1 + 50,000
    x 1 = 281,500***'
  prefs: []
  type: TYPE_NORMAL
- en: 'You have probably noticed that the values differ from the real project costs
    in our dataset. This means that if we use this model on any real project our estimates
    will be erroneous. We can measure this error in multiple ways, but let''s stick
    with one of the most popular choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a0ef489-b139-450a-b4af-da5d63b5b94c.png)'
  prefs: []
  type: TYPE_IMG
- en: There are many ways to quantify prediction errors. All of them introduce different
    trade-offs and limitations. Error measure selection is one of the most important
    technical aspects of building a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: For the overall error, let's take an arithmetic average of individual errors
    of all projects. The number that we calculated is called the **root mean squared
    error** (**RMSE**).
  prefs: []
  type: TYPE_NORMAL
- en: The exact mathematical form of this measure is not a consequence. RMSE has straightforward
    logic behind it. While we can derive the RMSE formula using several technical
    constraints posed on a linear model, mathematical proofs are out of the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: It turns out that we can use optimization algorithms to tweak our cost parameters
    so that RMSE will be minimized. In other words, we can find the best fit for cost
    parameters that minimize the error for all rows in the dataset. We call this procedure
    MLE.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLE gives a way to estimate the parameters of a statistical model given data.
    It seeks to maximize the probability of parameters given data. It may sound difficult,
    but the concept becomes very intuitive if we rephrase the definition as a question:
    what parameters should we set so the results we get will be closest to the data?
    MLE helps us to find the answer to this question.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s focus on another example to get a more general approach. Imagine that
    we have started a coffee subscription service. A customer chooses her favorite
    flavors of coffee in our mobile app and fills in an address and payment information.
    After that, our courier delivers a hot cup of coffee every morning. There is a
    feedback system built in the app. We promote seasonal offerings and discounts
    to clients via push notifications. There was a big growth in subscribers last
    year: almost 2,000 people are already using the service and 100 more are subscribing
    each month. However, our customer churn percentage is growing disturbingly fast.
    Marketing offers do not seem to make a big difference. To solve this problem,
    we have decided to build a machine learning model that predicts customer churn
    in advance. Knowing that a customer will churn, we can tailor an individual offer
    that will turn them into active user again.'
  prefs: []
  type: TYPE_NORMAL
- en: This time, we will be more rigorous and abstract in our definitions. We will
    define a model **M** that takes customer data **X** and historical churn outcomes
    **Y**. We will call **Y** the target variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table describes the attributes of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Attribute name** | **Attribute type** | **Attribute description** | **Possible
    values** |'
  prefs: []
  type: TYPE_TB
- en: '| Months subscribed | Integer | The number of months a user has been subscribed
    to our service | 0 to ∞ |'
  prefs: []
  type: TYPE_TB
- en: '| Special offerings activated | Integer | The number of special offers the
    user activated last month | 0 to ∞ |'
  prefs: []
  type: TYPE_TB
- en: '| Number of cups on weekdays | Float | The average number of cups the user
    orders on weekdays, last month | 1.0 to 5.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Number of cups on the weekend | Float | The average number of cups the user
    orders on weekends, last month | 1.0 to 2.0 |'
  prefs: []
  type: TYPE_TB
- en: This kind of table is called a data dictionary. We can use it to understand
    the data coming in and out of the model, without looking into the code or databases.
    Every data science project must have an up-to-date data dictionary. More complete
    examples of data dictionaries will be shown later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our target variable, **Y**, can be described in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Target variable name** | **Target variable type** | **Target variable description**
    | **Target variable possible values** |'
  prefs: []
  type: TYPE_TB
- en: '| Churn | Integer | Indicates whether the user stopped using our service last
    month | 0 or 1 |'
  prefs: []
  type: TYPE_TB
- en: 'Given the customer description, ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png),
    the model outputs churn probability, ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png).
    A hat over ![](img/e52dbafb-bbd9-49d6-b4b2-c433dcdc1282.png) means that ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png)
    is not a real churn probability, but only an estimate that can contain errors.
    This value will not be strictly zero or one. Instead, the model will output a
    probability between 0% and 100%. For example, for some customer ![](img/47db3c92-704a-4938-a88d-aededf2bbb93.png),
    we got a ![](img/0afcd35b-e59e-4f4d-93c6-8d4a7542ad7f.png) of 76%. We may interpret
    this value like this: based on historical data, the expectancy of this customer
    to churn is 76%. Or, out of 100 customers like customer ![](img/30287e69-02b1-4fbb-9a6d-527793dd6d66.png), 76
    will churn.'
  prefs: []
  type: TYPE_NORMAL
- en: A machine learning model must have some variable parameters that can change
    to better match churn outcomes. Now that we have used formulas, we can't go on
    without introducing at least one Greek letter. All the parameters of our model
    will be represented by ![](img/3457eb94-12b2-4edd-ab4e-622dec683370.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have everything in place:'
  prefs: []
  type: TYPE_NORMAL
- en: Historical customer data ![](img/e5c22cd2-7bee-4f0f-9ab5-4a14b5f72c4a.png) and
    churn outcomes ![](img/b9742446-b92d-4048-988c-bd56185c6a35.png), which we will
    call the training dataset ![](img/03798ff0-0bc9-462d-acdc-7444803b369b.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning algorithm ![](img/5b08d2dd-01fb-4d28-90dd-af668b42920d.png) that
    accepts customer description ![](img/747c1c74-c079-4d1c-b27c-f584b284b30c.png) and
    outputs churn probability ![](img/80513bf6-2166-4fb5-b16d-ba7c898db5cb.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model parameters ![](img/667c9f2f-7ffa-4be7-9f9e-9b705773a02e.png) that can
    be tuned using MLE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will estimate the parameters ![](img/25e1af35-cbdd-413b-9e89-de80eadc420a.png) for
    our model ![](img/0c1721ef-3faa-45cd-98a4-77a06950ed10.png) using MLE on the training
    dataset ![](img/8d37f0bc-1c35-40e5-9b5c-e2533fdc2bcd.png). I have placed a hat
    on top of ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png) to indicate that in
    theory there may be the best parameter set ![](img/2ceb9466-e565-4ecc-8ac6-eeef6dc17921.png),
    but in reality we have limited data. Thus, the best parameter set we can get is
    only an estimate of true that may contain errors.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can finally use our model to make predictions about customer churn
  prefs: []
  type: TYPE_NORMAL
- en: 'probability ![](img/b8b4a596-2252-4c58-9284-940b53d5864b.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af320444-5ea3-465c-b4ac-ac4cefe58fe4.png)'
  prefs: []
  type: TYPE_IMG
- en: The exact interpretation of probability depends heavily on the model **M** we
    used to estimate this probability. Some models can be used to give probabilistic
    interpretations, while others do not have such qualities.
  prefs: []
  type: TYPE_NORMAL
- en: Note that we had not explicitly defined the kind of machine learning model **M** we
    use. We have defined an abstract framework for learning from data that does not
    depend on the specific data or concrete algorithms it uses. This is the beauty
    of mathematics that opens up limitless practical applications. With this abstract
    framework, we can come up with many models **M** that have different trade-offs
    and capabilities. This is how machines learn.
  prefs: []
  type: TYPE_NORMAL
- en: '**How to choose a model**'
  prefs: []
  type: TYPE_NORMAL
- en: There are many different types of machine learning models and many estimation
    methods. Linear regression and MLE are among the simplest examples that show the
    underlying principles that lie beneath many machine learning models. A theorem
    called the **no free lunch theorem** says that there is no model that will give
    you the best results for each task for every dataset. Our machine learning framework
    is abstract, but this does not mean it can yield a perfect algorithm. Some models
    are best for one task, but are terrible for another. One model may classify images
    better than humans do, but it will fail at credit scoring. The process of choosing
    the best model for a given task requires deep knowledge of several disciplines,
    such as machine learning, statistics, and software engineering. It depends on
    many factors, such as statistical data properties, the type of task we are trying
    to solve, business constraints, and risks. That is why only a professional data
    scientist can handle the selection and training machine of learning models. This
    process has many intricacies and explaining all of them is beyond the scope of
    this book. An interested reader may refer to the book list at the end of this
    book. There you can find free books that explain the technical side of machine
    learning in great depth.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you understand the general flow of thought on how to define learning
    processes using mathematics and statistics, we can explore the inner workings
    of machine learning. Machine learning studies algorithms and statistical models
    that are able to learn and perform specific tasks without explicit instruction.
    As every software development manager should have some expertise in computer programming,
    the data science project manager should understand machine learning. Grasping
    the underlying concepts between any machine learning algorithm will allow you
    to understand better the limitations and requirements for your project. It will
    ease communication and improve understanding between you and the data scientists
    on your team. Knowledge of basic machine learning terminology will make you speak
    in the language of data science.
  prefs: []
  type: TYPE_NORMAL
- en: We will now dive into the main intuitions behind popular machine learning algorithms,
    leaving out technical details for the sake of seeing the wood for the trees.
  prefs: []
  type: TYPE_NORMAL
- en: Defining goals of machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we speak about machine learning we speak about accurate predictions and
    recognition. Statisticians often use simple but interpretable models with a rigorous
    mathematical base to explain data and prove points. Machine learning specialists
    build more complex models that are harder to interpret and often work like black
    boxes. Thus, many machine learning algorithms are more suited to prediction quality
    than model interpretability. Trends change slowly, and while more researchers
    look into topics of model interpretation and prediction explanation, the prime
    goal of machine learning continues to be the creation of faster and more accurate
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Using a life cycle to build machine learning models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When creating machine learning models, we typically follow a fixed set of stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Exploratory data analysts**:In this stage, a data scientist uses a set of
    statistical and visualization techniques to have a better understanding of data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preparation**: In this part, a data scientist transforms data into a
    format suitable for applying a machine learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preprocessing**: Here, we clean prepared data and transform it so that
    a machine learning algorithm can properly use every part of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modeling**: In this part, a data scientist trains machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Testing**: In this stage, we evaluate the model using a set of metrics that
    measure its performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This process repeats many times before we achieve sufficiently good results.
    You can apply the life cycle to train many kinds of machine learning models, which
    we will explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most basic type of machine learning models is a linear model. We have already
    seen a detailed example of a linear model in the previous section. Predictions
    of a linear model can be interpreted by looking at coefficients of a model. The
    greater the coefficient, the more its contribution to the final prediction. While
    simple, those models are often not the most accurate. Linear models are fast and
    computationally efficient, which makes them valuable in settings with lots of
    data and limited computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Linear models are fast, efficient, simple, and interpretable. They can solve
    both classification and regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Classification and regression tree** (**CART**) take a very intuitive approach
    for making predictions. CART build a decision tree based on the training data.
    If we use CART for a credit default risk task, we may see a model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bd3570c-9d49-4528-8331-e3d564709153.png)'
  prefs: []
  type: TYPE_IMG
- en: To make a prediction, an algorithm starts from the top of the tree and makes
    consecutive choices based on values in the data. For binary classification, at
    the bottom of the tree you will get a proportion of positive cases for similar
    customers.
  prefs: []
  type: TYPE_NORMAL
- en: 'While simple, CART models suffer from two disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: Low prediction accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many possible trees for a single dataset. One tree may have significantly
    better prediction accuracy than the other.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But how does CART choose columns and values for splits? We will explore the
    general logic of CART on binary classification:'
  prefs: []
  type: TYPE_NORMAL
- en: At first, it takes a single column and divides the data into two parts for each
    value of this column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then it computes a proportion of positive cases for each split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Step 1* and *step 2* are repeated for each column in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We rank each split by how well it divides the data. If the split divides the
    dataset perfectly, then positive cases would be for all values, lower than a threshold
    and negative cases would be on the other side. To illustrate, if **Age > 25** is
    a perfect split, then all customers younger than 25 will have credit defaults
    and all customers older than 25 will have a perfect credit history.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: According to *step 4*, the best split is chosen for the current level of the
    tree. The dataset is divided into two parts according to the split value.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 1* to *5* are repeated for each new dataset part.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The procedure continues before the algorithm meets a stopping criterion. For
    example, we can stop building a tree by looking at the depth of the decision tree
    or the minimum number of data points available for the next split.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also apply CART to regression problems, although the algorithm would
    be slightly more complicated. CART is simple and interpretable, but it produces
    very weak models that are rarely applied in the practice. However, the properties
    of the algorithm and implementation tricks allow us to use their weakest point
    as their main strength. We will learn how to exploit these properties in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Suppose that you own a retail store franchise. Business is growing and you are
    ready to build a new store. The question is, where should you build it? Selection
    of a building location is extremely important as it is permanent and it defines
    the local customer base that will go into your shop.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have several options to make this decision:'
  prefs: []
  type: TYPE_NORMAL
- en: Decide yourself.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask for the advice of the most competent employee.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ask the opinion of many slightly less experienced employees.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Options 1* and *2* encompass one and two persons that make a decision. *Option
    3* encompasses opinions of several of experts. Statistically, *option 3* is likely
    to yield a better decision. Even world-class experts can make a mistake. Several
    professionals, sharing information between each other, are much more likely to
    succeed. That is the reason why living in big communities and working in large
    organizations leads to great results.'
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, this principle works too. Many models can contribute to
    making a single decision in an ensemble. Model ensembles tend to be more accurate
    than single models, including the most advanced ones. Be wary, though; you need
    to build many models to create an ensemble. A large number of models increase
    computational resource requirements quite rapidly, making a tradeoff between prediction
    accuracy and speed.
  prefs: []
  type: TYPE_NORMAL
- en: Tree-based ensembles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A particularly useful model for ensembles is a decision tree. There is an entire
    class of machine learning models devoted to different ways of creating tree ensembles.
    This type of model is the most frequent winner of Kaggle competitions on structured
    data, so it is important to understand how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Trees are good candidates to build ensembles because they have high variance.
    Because of the randomness in a tree-building algorithm, every decision tree differs
    from the previous one, even if the dataset does not change. Each time we build
    a decision tree, we may come up with something that''s different from before.
    Thus, each tree will make different errors. Recall the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d148d81a-041f-4c10-ac00-aee5b322721b.png)'
  prefs: []
  type: TYPE_IMG
- en: It turns out that decision trees have extremely low bias and high variance.
    Imagine that many different trees make hundreds of predictions for each individual,
    creating an ensemble. What would happen if we average all predictions? We will
    be a lot closer to the real answer. When used in an ensemble, decision trees can
    handle complex datasets with high prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see how multiple trees can create an ensemble:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9686fcd0-4f5a-4686-89e7-53a94528923b.png)'
  prefs: []
  type: TYPE_IMG
- en: If you are working with structured data, be sure to try decision tree ensembles
    before jumping into other areas of machine learning, including deep learning.
    Nine times out of ten, the results will satisfy both you and your customer. Media
    often overlooks the value of this algorithm. Praise for ensemble models is rare
    to find, yet it is arguably the most commonly used algorithm family for solving
    practical applied machine learning problems. Be sure to give tree ensembles a
    chance.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another useful application of machine learning is clustering. In contrast to
    other machine learning problems we have studied in this section, clustering is
    an unsupervised learning problem. This means that clustering algorithms can work
    with unlabeled data. To illustrate, let's look at a task that's central to marketing
    departments—customer segmentation. Coming up with marketing offers for each individual
    client may be impossible. For example, if you own a large retail store network,
    you want to apply different discounts at stores depending on customer interests
    to boost sales. To do this, marketing departments create customer segments and
    tailor marketing companies to each specific segment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, you can see six customers assigned to two different
    segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/455c6dc0-8c82-42f8-b509-634317dc759e.png)'
  prefs: []
  type: TYPE_IMG
- en: We may automate customer segmentation by taking all purchase histories for all
    customers and applying a clustering algorithm to group similar customers together.
    The algorithm will assign each customer to a single segment, allowing you to further
    analyze those segments. While exploring data inside each segment, you may find
    interesting patterns that will give insights for new marketing offers targeted
    at this specific segment of customers.
  prefs: []
  type: TYPE_NORMAL
- en: We can apply clustering algorithms to data in an ad hoc manner because they
    don't require prior labeling. However, the situation can get complicated, as many
    of the algorithms suffer from the curse of dimensionality and can't work with
    many columns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most popular clustering algorithm is K-means. In its simplest form, the
    algorithm has only one parameter: the number of clusters to find in the data.
    K-means approaches clustering from a geometrical standpoint. Imagine each data
    row as a point in space. For us, this idea is easy for datasets with two or three
    points, but it works well beyond three dimensions. Having laid out our dataset
    in a geometric space, we can now see that some points will be closer to each other.
    K-means finds center points around which other points cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It does this iteratively, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: It takes the current cluster centers (for the first iteration, it takes random
    points).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It goes through all data rows and assigns them to the closest cluster's center
    point.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It updates cluster centers by averaging the locations of all points from *step
    2*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The algorithm is explained in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/47842a77-e1b1-48d7-a5e7-af9423a38a22.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we conclude the introduction to machine learning. While there
    are many more machine learning algorithms to study, describing them is beyond
    the scope of this book. I am sure you will find that knowledge about regression,
    decision trees, ensemble models, and clustering covers a surprisingly large portion
    of practical applications and will serve you well. Now we are ready to move on
    to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep neural networks that classify images and play Go better than we do create
    an impression of extremely complex models whose internals are inspired by our
    own brain's structure. In fact, the central ideas behind neural networks are easy
    to grasp. While first neural networks were indeed inspired by the physical structure
    of our brain, the analogy no longer holds and the relation to physical processes
    inside the human brain is mostly historical.
  prefs: []
  type: TYPE_NORMAL
- en: 'To demystify neural networks, we will start with the basic building blocks:
    artificial neurons. An artificial neuron is nothing more than two mathematical
    functions. The first takes a bunch of numbers as input and combines them by using
    its internal state—weights. The second, an activation function, takes the output
    of the first and applies special transformations. The activation function tells
    us how active this neuron is to a particular input combination. In the following
    diagram, you can see how an artificial neuron converts the input to output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c3f5af2-af84-4bb6-9cc6-68693be66f4a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the following diagram, we can see the plot of the most popular activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5077ba7a-3bcb-48b0-9387-9512feaad043.png)'
  prefs: []
  type: TYPE_IMG
- en: If the output is less than 0, the function will output 0\. If it is greater,
    it will echo its input. Simple, isn't it? Try to come up with the name for this
    function. I know, naming is hard. Names should be simple, while conveying deep
    insights about the core concept of the thing you are naming. Of course, mathematicians
    knew this and, as we have witnessed many times before, came up with a perfect
    and crystal-clear name—**rectified linear unit** (**ReLU**). An interesting fact
    is that ReLU does not conform to basic requirements for an activation function,
    but still gives better results than other alternatives. Other activation functions
    may be better in specific situations, but none of them beat ReLU in being a sensible
    default.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another important activation function you need to know about is sigmoid. You
    can see it in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/262339b8-8361-415e-9196-6a9a8132fd08.png)'
  prefs: []
  type: TYPE_IMG
- en: Before ReLU came to the throne, sigmoid was a popular choice as an activation
    function. While its value as an activation function has faded, the sigmoid is
    still important for another reason. It often comes up in binary classification
    problems. If you look at the plot closely, you will find it can map any number
    to a range between 0 and 1\. This property makes sigmoid useful when modeling
    binary classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we use sigmoid in binary classification problems not because it conveniently
    maps any number to something between 0 and 1\. The reason behind this useful property
    is that sigmoid, also called the logistic function, is tightly related to the
    Bernoulli probability distribution. This distribution describes events that can
    happen with probability ***p*** between 0 and 1\. For example, a Bernoulli distribution
    can describe a coin toss with ***p = 0.5*** or 50%. As you can see, any binary
    classification problem can be naturally described by the Bernoulli distribution.
    To see how, look at the following questions: *What is the probability of a client
    clicking on an ad?* *What is the probability of a client stating a default while
    being in debt?* We can model these cases as Bernoulli distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we know the main components of an artificial neuron: weights and activation
    function. To make a neuron work, we need to take its input and combine it with
    neuron weights. To do this, we can recall linear regression. Linear regression
    models combine data attributes by multiplying each attribute to a weight and then
    summing them up. Then, apply an activation function, and you will get an artificial
    neuron. If our data row had two columns named *a* and *b*, the neuron would have
    two weights, ![](img/99978480-9cf3-4eea-aaaf-8ea1d21db412.png) and ![](img/9fa78070-be91-46b4-8591-9efde6594188.png).
    A formula for a neuron with ReLU activation is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/126417e1-835a-47ee-9965-2ca8eae45ded.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that ![](img/ccfa7b45-6039-45f6-a898-d338ee0a57aa.png) is a special weight
    called a bias that is not tied to any input.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, an artificial neuron is just a bunch of multiplications and additions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7aa98f4-39cf-4175-8f6a-edebbe2efc87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or, to give you a more concrete example of an actual calculation, take a look
    at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b19e3f6-cd57-4c7f-93e9-0ea1b7d8ec5c.png)'
  prefs: []
  type: TYPE_IMG
- en: The operation of combining numbers by multiplying each term by a constant and
    adding the results is omnipresent in machine learning and statistics. It is called
    a linear combination of two vectors. You can think of a vector as a fixed set
    of numbers. In our example, the first vector would be a data row and the second
    vector would contain the weights for each data attribute.
  prefs: []
  type: TYPE_NORMAL
- en: Building neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are ready to build our first neural network. Let''s start with an example:
    our company is struggling with customer retention. We know a lot about our customers,
    and we can create an offer that would make them want to stay. The problem is,
    we cannot identify which customers will churn. So, our boss, Jane, asks us to
    build a churn prediction model. This model will take customer data and predict
    the probability of churning in the next month. With this probability estimate,
    Jane can decide if she needs to create a personalized marketing offer for this
    client.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have decided to use a neural network to solve this churn prediction problem.
    Our network will comprise multiple layers of neurons. Neurons in each layer will
    be connected to neurons in the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f6042a5e-ca74-4211-8ab6-71fc811bb9e3.png)'
  prefs: []
  type: TYPE_IMG
- en: That is a lot of arrows, isn't it? A connection between two neurons means that
    a neuron will pass its output to the next neuron. If a neuron receives multiple
    inputs, they are all summed up. This type of network is called a **fully connected
    neural network** (**FCNN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'We see how we can make predictions using neural networks, but how can we learn
    what predictions to make? If you look closer, a neural network is nothing more
    than a large function with lots of weights. The model prediction is determined
    by using weights and information incoming through the neuron inputs. Thus, to
    have an accurate neural network, you must set the right weights. We already know
    that we can use mathematical optimization and statistics to minimize prediction
    error by changing the parameters of a function. A neural network is nothing more
    than a large and complex mathematical function with variable weights. Therefore,
    we can use MLE and gradient descent to do the optimization. I will give the formal
    names of each stage in bold, followed by intuitive explanations of each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network initialization**: At first, we can initialize our weights with random
    values.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Forward pass**: We can take an example from our training dataset and make
    a prediction using our current set of weights.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Loss function calculation**: We measure the difference between our prediction
    and ground truth. We want to make this difference as closely to 0 as possible.
    That is, we want to minimize the loss function.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Backward pass**: We can use an optimization algorithm to adjust the weights
    so that the prediction will be more accurate. A special algorithm called backpropagation
    can calculate updates to each layer of neurons, going from the last layer to the
    first.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Steps 1* to *4* are repeated until the desired accuracy level is achieved,
    or until the network stops learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/831809c3-879a-425f-817f-e9794f8d8d9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Backpropagation is the most widely used learning algorithm for training neural
    networks. It takes a prediction error and calculates how much we should change
    each weight in the network to make predictions closer to the ground truth. The
    name backpropagation comes from the specific way of how the algorithm updates
    the weights: it starts from the last layer, propagating changes to every neuron
    until it reaches the network input. When inputs go through the network to calculate
    an output prediction, we call it a forward pass. When we change the weights by
    propagating the error, we call it a backward pass.'
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, there are many different types of building blocks that you can use
    to compose a neural network. Some specific neuron types work better with image
    data, while others can utilize the sequential nature of text. Many specialized
    layers were invented to improve the speed of training and fight overfitting. A
    specific composition of layers in a neural network devoted to solving a specific
    task is called a neural network architecture. All neural network architectures,
    no matter how complex and deep, still conform to basic laws of backpropagation.
    Next, we will explore the domain-specific applications of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, we will look at computer vision. Let''s start with an example. Our client,
    Joe, likes animals. He is the happy owner of six cats and three dogs. Being a
    happy owner, he also likes to take pictures of his pets. Large photo archives
    have accumulated on his computer over the years. Joe has decided that he needs
    to bring order into his dreaded photo folder, containing 50,000 pet photos. To
    help Joe, we have decided to create a neural network that takes an image and decides
    whether a cat or dog is present on the photo. The following diagram shows how
    a neural network classifier works with a cat photo:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b1832d8-40c9-4ec3-9fc2-139563cc4f82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At first, we transform an image into three tables of numbers, one each for
    the red, green and blue channel of every pixel. If we try to use a plain FCNN
    as before, we will see unimpressive results. Deep neural networks shine at computer
    vision tasks because of a specific neuron type called a convolutional filter or
    a convolution. **Convolutional neural networks** (**CNNs**) were invented by a
    French machine learning researcher Yann LeCun. In CNNs, a single neuron can look
    at a small patch of an image, say, 16x16 pixels, instead of taking the entire
    set of pixels as an input. This neuron can go through each 16x16 region of an
    image, detecting some feature of an image it had learned through backpropagation.
    Then, this neuron can pass information to further layers. In the following illustration,
    you can see a single convolutional neuron going through small image patches and
    trying to detect a fur-like pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2936bd0-cc7e-4adb-b9ed-c5b1c69afbd5.png)'
  prefs: []
  type: TYPE_IMG
- en: The remarkable achievement of CNNs is that a single neuron can reuse a small
    number of weights and still cover the entire image. This feature makes CNNs a
    lot faster and lighter than regular neural networks. The idea found implementation
    only in the 2010s, when a CNN surpassed all other computer vision methods in an
    ImageNet competition, where an algorithm had to learn to classify photos between
    21,000 possible categories. The development of CNNs took so long because we lacked
    the computational capabilities to train deep neural networks with a large number
    of parameters on big datasets. To achieve good accuracy, CNNs require significant
    amounts of data. For example, the ImageNet competition includes 1,200,000 training
    images.
  prefs: []
  type: TYPE_NORMAL
- en: At first layers, CNNs tend to detect simple patterns, such as edges and contours
    in an image. As the layer depth progresses, convolutional filters become more
    complex, detecting features such as eyes, noses, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following visualizations, you can see an example visualization of convolutional
    filters at different layers of the neural network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2a479028-5b00-4e7d-a173-4cc0daff5cdf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Many neurons learn to recognize simple patterns that are useful for any computer
    vision task. This observation leads us to a very important idea: a neural network
    that was trained to perform one task well can be retrained to perform another
    task. Moreover, you will need much less training data for the second task, as
    the network has already learned many useful features from the previous training
    dataset. In particular, if you want to train a CNN classifier for two classes
    from scratch, you will need to label tens of thousands of images to reach a good
    performance level. However, if you use a network that was pretrained on ImageNet,
    you will probably get good results with only 1,000 to 2,000 images. This approach
    is called transfer learning. Transfer learning is not limited to computer vision
    tasks. In recent years, researchers made significant progress in using it for
    other domains: natural language processing, reinforcement learning, and sound
    processing.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have an understanding of how deep CNNs work, we will proceed to
    the language domain, where deep learning has changed everything.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to natural language processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the deep learning revolution, **natural language processing** (**NLP**)
    systems were almost fully rule based. Linguists created intricate parsing rules
    and tried to define our language's grammar to automate tasks such as part of speech
    tagging or named entity recognition. Human-level translation between different
    languages and free-form question answering were in the domain of science fiction.
    NLP systems were hard to maintain and took a long time to develop.
  prefs: []
  type: TYPE_NORMAL
- en: As with computer vision, deep learning took the NLP world by storm. Deep-learning-based
    NLP algorithms successfully perform near-human-level translation between different
    languages, can measure the emotional sentiment of a text, can learn to retrieve
    information from a text, and can generate answers on free-form questions. Another
    great benefit of deep learning is a unified approach. A single part-of-speech
    tagging model architecture will work for French, English, Russian, German, and
    other languages. You will need training data for all those languages, but the
    underlying model will be the same. With deep learning, we need not try to hardcode
    the rules of our ambiguous language. While many tasks, such as long-form writing
    and human-level dialogue, are yet unconquerable for deep learning, NLP algorithms
    are a great help in business and daily life.
  prefs: []
  type: TYPE_NORMAL
- en: 'For NLP deep learning, everything began with an idea: the meaning of a word
    is defined by its neighbors. That is, to learn a language and the meaning of words,
    all you need is to understand the context for each word in the text. This idea
    may seem to be too simple to be true. To check its validity, we can create a neural
    network that will predict a word by receiving surrounding words as an input. To
    create a training dataset, we may use any text in any language.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take a context window of two words, then we can generate the following
    training samples for this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: If, we, take, a → will
  prefs: []
  type: TYPE_NORMAL
- en: We, can, following, training → generate
  prefs: []
  type: TYPE_NORMAL
- en: Following, training, for, this → samples
  prefs: []
  type: TYPE_NORMAL
- en: And so on…
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to come up with a way to convert all words to numbers, because
    neural networks only understand numbers. One approach would be to take all unique
    words in a text and assign them to a number:'
  prefs: []
  type: TYPE_NORMAL
- en: Following → 0
  prefs: []
  type: TYPE_NORMAL
- en: Training → 1
  prefs: []
  type: TYPE_NORMAL
- en: Samples → 2
  prefs: []
  type: TYPE_NORMAL
- en: For → 3
  prefs: []
  type: TYPE_NORMAL
- en: This → 4
  prefs: []
  type: TYPE_NORMAL
- en: …
  prefs: []
  type: TYPE_NORMAL
- en: Then, we represent each word by a set of weights inside a neural network. In
    particular, we start with two random numbers between 0 and 1 for each word.
  prefs: []
  type: TYPE_NORMAL
- en: 'We place all the numbers into a table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word identifier** | **Word vector** |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | 0.63, 0.26 |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 0.52, 0.51 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0.72, 0.16 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0.28, 0.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 0.27, 0.71 |'
  prefs: []
  type: TYPE_TB
- en: '| … | … |'
  prefs: []
  type: TYPE_TB
- en: '| N | 0.37, 0.34 |'
  prefs: []
  type: TYPE_TB
- en: Now we have a way to convert every word in the text into a pair of numbers.
    We will take all numbers that we have generated as weights for our neural network.
    It will get four words as input, convert them into eight numbers, and use them
    to predict the identifier for the word in the middle.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for the training sample **Following**, **Training**, **For**,
    **This** → **Samples**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input:'
  prefs: []
  type: TYPE_NORMAL
- en: Following → 0 → 0.63, 0.26
  prefs: []
  type: TYPE_NORMAL
- en: Training → 1 → 0.52, 0.51
  prefs: []
  type: TYPE_NORMAL
- en: For → 3 → 0.28, 0.93
  prefs: []
  type: TYPE_NORMAL
- en: This → 4 → 0.27, 0.71
  prefs: []
  type: TYPE_NORMAL
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: 2 → Samples
  prefs: []
  type: TYPE_NORMAL
- en: We call each pair of numbers associated with a word, a word vector. Our neural
    network will output a vector of probabilities from zero to one. The length of
    this vector will match the total number of unique words in our dataset. Then,
    the number with the largest probability will represent the word that is the most
    likely completion of our input according to the model.
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, we can apply the backpropagation algorithm to adjust word vectors
    until the model matches right words to their contexts. In our example, you can
    imagine each word lives on a coordinate grid. The elements of the word vector
    could represent *X* and *Y* coordinates. If you think about words in this geometric
    fashion, you may conclude that you can add or subtract word vectors to get another
    word vector. In the real world, such vectors contain not two but 100 to 300 elements,
    but the intuition remains the same. After many training iterations, you will see
    remarkable results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Try to calculate the following using word vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: King - Man + Woman = ?
  prefs: []
  type: TYPE_NORMAL
- en: You will get a vector for the word Queen. By learning to put words into their
    surrounding contexts, the model learns how different words relate to each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'The model we have built is called Word2Vec. We can train Word2Vec models in
    two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Predict a word by using its surrounding context. This setup is called a **continuous
    bag of words** (**CBOW**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predict the surrounding context by word. This setup is called **Skipgram**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The two approaches do not differ in anything, except model input and output
    specifications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Word vectors are also referred to as word embeddings. Embeddings contain much
    more information about words than simple numeric identifiers, and NLP models can
    use them to achieve better accuracy. For example, you can train a sentiments classification
    model by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a training dataset that contains user reviews and their sentiment, labeled
    0 as negative and 1 as positive.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Embed the user reviews into sets of word vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train deep learning classifier using this dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Current state-of-the-art models rarely use word embeddings created by training
    a separate model. Newer architectures allow learning task-specific word embeddings
    on the fly, without the need to use Word2Vec. Nonetheless, we have covered word
    embeddings in this chapter as they give us an idea of how computers can understand
    the meaning of a text. While modern models are more complex and robust, this idea
    remains intact.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of embeddings originated in NLP, but now it has found applications
    in recommended systems, face recognition, classification problems with lots of
    categorical data, and many other domains.
  prefs: []
  type: TYPE_NORMAL
- en: To train a classifier that uses word embeddings, you can use a CNN. In a CNN,
    each neuron progressively scans the input text in windows of words. Convolutional
    neurons learn weights that combine word vectors of nearby words into more compact
    representations that are used by the output layer to estimate sentence sentiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see how a single convolutional neuron works on a single sentence in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/646d01d6-47ca-4bfb-90a4-4fe58f5377c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'CNNs process text in a fixed window, which is an oversimplification. In reality,
    a word at the beginning of the sentence can affect its ending, and vice versa.
    Another architecture called **recurrent neural networks** (**RNNs**) can process
    sequences of any length, passing information from start to end. This is possible
    because all recurrent neurons are connected to themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22883415-c01e-4a97-a129-683dfaea54c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Self-connection allows a neuron to cycle through its input, pulling its internal
    state through each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4d0fd715-8483-4a27-bf57-9285aa83c472.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot depicts a single recurrent neuron as it unfolds. With
    each new word, a recurrent neuron changes its previous state. When the final word
    is processed, it returns its internal state as the output. This is the most basic
    recurrent architecture. Neural networks that are used in practice have a more
    complex internal structure, but the idea of recurrent connections holds. When
    speaking about recurrent networks, you will probably hear about **long short-term
    memory networks** (**LSTMs**). While they differ in the details, the flow of thought
    is the same for both RNNs and LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we uncovered the inner workings of machine learning and deep
    learning. We learned the main concepts of mathematical optimization and statistics.
    We connected them to machine learning and, finally, learned how machines learn
    and how we can use optimization algorithms to define learning. Lastly, we covered
    popular machine learning and deep learning algorithms, including linear regression,
    tree ensembles, CNNs, word embeddings, and recurrent neural networks. This chapter
    concludes our introduction to data science.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to build and sustain a data science team
    capable of delivering complex cross-functional projects.
  prefs: []
  type: TYPE_NORMAL
