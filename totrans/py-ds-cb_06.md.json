["```py\n# Load the necesssary Library\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.datasets import load_iris\nimport numpy as np\n\ndef get_iris_data():\n    \"\"\"\n    Returns Iris dataset\n    \"\"\"\n    # Load iris dataset\n    data = load_iris()\n\n    # Extract the dependend and independent variables\n    # y is our class label\n    # x is our instances/records\n    x    = data['data']\n    y    = data['target']\n\n    # For ease we merge them\n    # column merge\n    input_dataset = np.column_stack([x,y])\n\n    # Let us shuffle the dataset\n    # We want records distributed randomly\n    # between our test and train set\n\n    np.random.shuffle(input_dataset)\n\n    return input_dataset\n\n# We need  80/20 split.\n# 80% of our records for Training\n# 20% Remaining for our Test set\ntrain_size = 0.8\ntest_size  = 1-train_size\n\n# get the data\ninput_dataset = get_iris_data()\n# Split the data\ntrain,test = train_test_split(input_dataset,test_size=test_size)\n\n# Print the size of original dataset\nprint \"Dataset size \",input_dataset.shape\n# Print the train/test split\nprint \"Train size \",train.shape\nprint \"Test  size\",test.shape\n```", "```py\n\"\"\"\nGiven an array of class labels\nReturn the class distribution\n\"\"\"\n    distribution = {}\n    set_y = set(y)\n    for y_label in set_y:\n        no_elements = len(np.where(y == y_label)[0])\n        distribution[y_label] = no_elements\n    dist_percentage = {class_label: count/(1.0*sum(distribution.values())) for class_label,count in distribution.items()}\n    return dist_percentage\n\ndef print_class_label_split(train,test):\n  \"\"\"\n  Print the class distribution\n  in test and train dataset\n  \"\"\"  \n    y_train = train[:,-1]\n\n    train_distribution = get_class_distribution(y_train)\n    print \"\\nTrain data set class label distribution\"\n    print \"=========================================\\n\"\n    for k,v in train_distribution.items():\n        print \"Class label =%d, percentage records =%.2f\"%(k,v)\n\n    y_test = test[:,-1]    \n\n    test_distribution = get_class_distribution(y_test)\n\n    print \"\\nTest data set class label distribution\"\n    print \"=========================================\\n\"\n\n    for k,v in test_distribution.items():\n        print \"Class label =%d, percentage records =%.2f\"%(k,v)\n\nprint_class_label_split(train,test)\n```", "```py\n# Perform Split the data\nstratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)\n\nfor train_indx,test_indx in stratified_split:\n    train = input_dataset[train_indx]\n    test =  input_dataset[test_indx]\n    print_class_label_split(train,test)\n```", "```py\nstratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)\n```", "```py\nfrom sklearn.datasets import make_classification\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\ndef get_data():\n    \"\"\"\n    Make a sample classification dataset\n    Returns : Independent variable y, dependent variable x\n    \"\"\"\n    x,y = make_classification(n_features=4)\n    return x,y\n\ndef plot_data(x,y):\n    \"\"\"\n    Plot a scatter plot fo all variable combinations\n    \"\"\"\n    subplot_start = 321\n    col_numbers = range(0,4)\n    col_pairs = itertools.combinations(col_numbers,2)\n\n    for col_pair in col_pairs:\n        plt.subplot(subplot_start)\n        plt.scatter(x[:,col_pair[0]],x[:,col_pair[1]],c=y)\n        title_string = str(col_pair[0]) + \"-\" + str(col_pair[1])\n        plt.title(title_string)\n        x_label = str(col_pair[0])\n        y_label = str(col_pair[1])\n        plt.xlabel(x_label)\n        plt.xlabel(y_label)\n        subplot_start+=1\n\n    plt.show()\n\nx,y = get_data()    \nplot_data(x,y)\n```", "```py\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\n\ndef get_train_test(x,y):\n    \"\"\"\n    Perpare a stratified train and test split\n    \"\"\"\n    train_size = 0.8\n    test_size = 1-train_size\n    input_dataset = np.column_stack([x,y])\n    stratified_split = StratifiedShuffleSplit(input_dataset[:,-1],test_size=test_size,n_iter=1)\n\n    for train_indx,test_indx in stratified_split:\n        train_x = input_dataset[train_indx,:-1]\n        train_y = input_dataset[train_indx,-1]\n        test_x =  input_dataset[test_indx,:-1]\n        test_y = input_dataset[test_indx,-1]\n    return train_x,train_y,test_x,test_y\n\ndef build_model(x,y,k=2):\n    \"\"\"\n    Fit a nearest neighbour model\n    \"\"\"\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x,y)\n    return knn\n\ndef test_model(x,y,knn_model):\n    y_predicted = knn_model.predict(x)\n    print classification_report(y,y_predicted)\n\nif __name__ == \"__main__\":\n\n    # Load the data    \n    x,y = get_data()\n\n    # Scatter plot the data\n    plot_data(x,y)\n\n    # Split the data into train and test    \n    train_x,train_y,test_x,test_y = get_train_test(x,y)\n\n    # Build the model\n    knn_model = build_model(train_x,train_y)\n\n    # Test the model\n    print \"\\nModel evaluation on training set\"\n    print \"================================\\n\"\n    test_model(train_x,train_y,knn_model)\n\n    print \"\\nModel evaluation on test set\"\n    print \"================================\\n\"\n    test_model(test_x,test_y,knn_model)\n```", "```py\nfrom nltk.corpus import movie_reviews\n\n```", "```py\nfrom nltk.corpus import movie_reviews\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\ndef get_data():\n    \"\"\"\n    Get movie review data\n    \"\"\"\n    dataset = []\n    y_labels = []\n    # Extract categories\n    for cat in movie_reviews.categories():\n        # for files in each cateogry    \n        for fileid in movie_reviews.fileids(cat):\n            # Get the words in that category\n            words = list(movie_reviews.words(fileid))\n            dataset.append((words,cat))\n            y_labels.append(cat)\n    return dataset,y_labels\n\ndef get_train_test(input_dataset,ylabels):\n    \"\"\"\n    Perpare a stratified train and test split\n    \"\"\"\n    train_size = 0.7\n    test_size = 1-train_size\n    stratified_split = StratifiedShuffleSplit(ylabels,test_size=test_size,n_iter=1,random_state=77)\n\n    for train_indx,test_indx in stratified_split:\n        train   = [input_dataset[i] for i in train_indx]\n        train_y = [ylabels[i] for i in train_indx]\n\n        test    = [input_dataset[i] for i in test_indx]\n        test_y  = [ylabels[i] for i in test_indx]\n    return train,test,train_y,test_y\n```", "```py\ndef build_word_features(instance):\n    \"\"\"\n    Build feature dictionary\n    Features are binary, name of the feature is word iteslf\n    and value is 1\\. Features are stored in a dictionary\n    called feature_set\n    \"\"\"\n    # Dictionary to store the features\n    feature_set = {}\n    # The first item in instance tuple the word list\n    words = instance[0]\n    # Populate feature dicitonary\n    for word in words:\n        feature_set[word] = 1\n    # Second item in instance tuple is class label\n    return (feature_set,instance[1])\n\ndef build_negate_features(instance):\n    \"\"\"\n    If a word is preceeded by either 'not' or 'no'\n    this function adds a prefix 'Not_' to that word\n    It will also not insert the previous negation word\n    'not' or 'no' in feature dictionary\n    \"\"\"\n    # Retreive words, first item in instance tuple\n    words = instance[0]\n    final_words = []\n    # A boolean variable to track if the \n    # previous word is a negation word\n    negate = False\n    # List of negation words\n    negate_words = ['no','not']\n    # On looping throught the words, on encountering\n    # a negation word, variable negate is set to True\n    # negation word is not added to feature dictionary\n    # if negate variable is set to true\n    # 'Not_' prefix is added to the word\n    for word in words:\n        if negate:\n            word = 'Not_' + word\n            negate = False\n        if word not in negate_words:\n            final_words.append(word)\n        else:\n            negate = True\n    # Feature dictionary\n    feature_set = {}\n    for word in final_words:\n        feature_set[word] = 1\n    return (feature_set,instance[1])\n\ndef remove_stop_words(in_data):\n    \"\"\"\n    Utility function to remove stop words\n    from the given list of words\n    \"\"\"\n    stopword_list = stopwords.words('english')\n    negate_words = ['no','not']\n    # We dont want to remove the negate words\n    # Hence we create a new stop word list excluding\n    # the negate words\n    new_stopwords = [word for word in stopword_list if word not in negate_words]\n    label = in_data[1]\n    # Remove stopw words\n    words = [word for word in in_data[0] if word not in new_stopwords]\n    return (words,label)\n\ndef build_keyphrase_features(instance):\n    \"\"\"\n    A function to extract key phrases\n    from the given text.\n    Key Phrases are words of importance according to a measure\n    In this key our phrase of is our length 2, i.e two words or bigrams\n    \"\"\"\n    feature_set = {}\n    instance = remove_stop_words(instance)\n    words = instance[0]\n\n    bigram_finder  = BigramCollocationFinder.from_words(words)\n    # We use the raw frequency count of bigrams, i.e. bigrams are\n    # ordered by their frequency of occurence in descending order\n    # and top 400 bigrams are selected.\n    bigrams        = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400)\n    for bigram in bigrams:\n        feature_set[bigram] = 1\n    return (feature_set,instance[1])\n```", "```py\ndef build_model(features):\n    \"\"\"\n    Build a naive bayes model\n    with the gvien feature set.\n    \"\"\"\n    model = nltk.NaiveBayesClassifier.train(features)\n    return model    \n\ndef probe_model(model,features,dataset_type = 'Train'):\n    \"\"\"\n    A utility function to check the goodness\n    of our model.\n    \"\"\"\n    accuracy = nltk.classify.accuracy(model,features)\n    print \"\\n\" + dataset_type + \" Accuracy = %0.2f\"%(accuracy*100) + \"%\" \n\ndef show_features(model,no_features=5):\n    \"\"\"\n    A utility function to see how important\n    various features are for our model.\n    \"\"\"\n    print \"\\nFeature Importance\"\n    print \"===================\\n\"\n    print model.show_most_informative_features(no_features)        \n```", "```py\ndef build_model_cycle_1(train_data,dev_data):\n    \"\"\"\n    First pass at trying out our model\n    \"\"\"\n    # Build features for training set\n    train_features =map(build_word_features,train_data)\n    # Build features for test set\n    dev_features = map(build_word_features,dev_data)\n    # Build model\n    model = build_model(train_features)    \n    # Look at the model\n    probe_model(model,train_features)\n    probe_model(model,dev_features,'Dev')\n\n    return model\n\ndef build_model_cycle_2(train_data,dev_data):\n    \"\"\"\n    Second pass at trying out our model\n    \"\"\"\n\n    # Build features for training set\n    train_features =map(build_negate_features,train_data)\n    # Build features for test set\n    dev_features = map(build_negate_features,dev_data)\n    # Build model\n    model = build_model(train_features)    \n    # Look at the model\n    probe_model(model,train_features)\n    probe_model(model,dev_features,'Dev')\n\n    return model\n\ndef build_model_cycle_3(train_data,dev_data):\n    \"\"\"\n    Third pass at trying out our model\n    \"\"\"\n\n    # Build features for training set\n    train_features =map(build_keyphrase_features,train_data)\n    # Build features for test set\n    dev_features = map(build_keyphrase_features,dev_data)\n    # Build model\n    model = build_model(train_features)    \n    # Look at the model\n    probe_model(model,train_features)\n    probe_model(model,dev_features,'Dev')\n    test_features = map(build_keyphrase_features,test_data)\n    probe_model(model,test_features,'Test')\n\n    return model\n```", "```py\nif __name__ == \"__main__\":\n\n    # Load data\n    input_dataset, y_labels = get_data()\n    # Train data    \n    train_data,all_test_data,train_y,all_test_y = get_train_test(input_dataset,y_labels)\n    # Dev data\n    dev_data,test_data,dev_y,test_y = get_train_test(all_test_data,all_test_y)\n\n    # Let us look at the data size in our different \n    # datasets\n    print \"\\nOriginal  Data Size   =\", len(input_dataset)\n    print \"\\nTraining  Data Size   =\", len(train_data)\n    print \"\\nDev       Data Size   =\", len(dev_data)\n    print \"\\nTesting   Data Size   =\", len(test_data)    \n\n    # Different passes of our model building exercise    \n    model_cycle_1 =  build_model_cycle_1(train_data,dev_data)\n    # Print informative features\n    show_features(model_cycle_1)    \n    model_cycle_2 = build_model_cycle_2(train_data,dev_data)\n    show_features(model_cycle_2)\n    model_cycle_3 = build_model_cycle_3(train_data,dev_data)\n    show_features(model_cycle_3)\n```", "```py\n            words = list(movie_reviews.words(fileid))\n```", "```py\n    return dataset,y_labels\n```", "```py\n # Train data    \n    train_data,all_test_data,train_y,all_test_y = get_train_test(input_dataset,y_labels)\n```", "```py\n    # Dev data\n    dev_data,test_data,dev_y,test_y = get_train_test(all_test_data,all_test_y)\n```", "```py\n    # Build features for training set\n    train_features =map(build_negate_features,train_data)\n    # Build features for test set\n    dev_features = map(build_negate_features,dev_data)\n```", "```py\n    # Build model\n    model = build_model(train_features)    \n```", "```py\n    # Look at the model\n    probe_model(model,train_features)\n    probe_model(model,dev_features,'Dev')\n```", "```py\nshow_features(model_cycle_1) \n```", "```py\n\"movie is not good\" to \"movie is not_good\"\n```", "```py\nstopword_list = stopwords.words('english')\n```", "```py\nwords = [word for word in in_data[0] if word not in new_stopwords \\\nand word not in punctuation]\n```", "```py\nnew_stopwords = [word for word in stopword_list if word not in negate_words]\n```", "```py\n    bigram_finder  = BigramCollocationFinder.from_words(words)\n    # We use the raw frequency count of bigrams, i.e. bigrams are\n    # ordered by their frequency of occurence in descending order\n    # and top 400 bigrams are selected.\n    bigrams        = bigram_finder.nbest(BigramAssocMeasures.raw_freq,400) \n```", "```py\n    test_features = map(build_keyphrase_features,test_data)\n    probe_model(model,test_features,'Test')\n```", "```py\nX = {2,2}\n```", "```py\nimport math\n\ndef prob(data,element):\n    \"\"\"Calculates the percentage count of a given element\n\n    Given a list and an element, returns the elements percentage count\n\n    \"\"\"\n    element_count =0\n   \t# Test conditoin to check if we have proper input\n    if len(data) == 0 or element == None \\\n                or not isinstance(element,(int,long,float)):\n        return None      \n    element_count = data.count(element)\n    return element_count / (1.0 * len(data))\n\ndef entropy(data):\n    \"\"\"\"Calcuate entropy\n    \"\"\"\n    entropy =0.0\n\n    if len(data) == 0:\n        return None\n\n    if len(data) == 1:\n        return 0\n    try:\n        for element in data:\n            p = prob(data,element)\n            entropy+=-1*p*math.log(p,2)\n    except ValueError as e:\n        print e.message\n\n    return entropy\n```", "```py\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import StratifiedShuffleSplit\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nimport pprint\n\ndef get_data():\n    \"\"\"\n    Get Iris data\n    \"\"\"\n    data = load_iris()\n    x = data['data']\n    y = data['target']\n    label_names = data['target_names']\n\n    return x,y,label_names.tolist()\n\ndef get_train_test(x,y):\n    \"\"\"\n    Perpare a stratified train and test split\n    \"\"\"\n    train_size = 0.8\n    test_size = 1-train_size\n    input_dataset = np.column_stack([x,y])\n    stratified_split = StratifiedShuffleSplit(input_dataset[:,-1], \\\n            test_size=test_size,n_iter=1,random_state = 77)\n\n    for train_indx,test_indx in stratified_split:\n        train_x = input_dataset[train_indx,:-1]\n        train_y = input_dataset[train_indx,-1]\n        test_x =  input_dataset[test_indx,:-1]\n        test_y = input_dataset[test_indx,-1]\n    return train_x,train_y,test_x,test_y\n```", "```py\ndef build_model(x,y):\n    \"\"\"\n    Fit the model for the given attribute \n    class label pairs\n    \"\"\"\n    model = tree.DecisionTreeClassifier(criterion=\"entropy\")\n    model = model.fit(x,y)\n    return model\n\ndef test_model(x,y,model,label_names):\n    \"\"\"\n    Inspect the model for accuracy\n    \"\"\"\n    y_predicted = model.predict(x)\n    print \"Model accuracy = %0.2f\"%(accuracy_score(y,y_predicted) * 100) + \"%\\n\"\n    print \"\\nConfusion Matrix\"\n    print \"=================\"\n    print pprint.pprint(confusion_matrix(y,y_predicted))\n\n    print \"\\nClassification Report\"\n    print \"=================\"\n\n    print classification_report(y,y_predicted,target_names=label_names)\n```", "```py\nif __name__ == \"__main__\":\n    # Load the data\n    x,y,label_names = get_data()\n    # Split the data into train and test    \n    train_x,train_y,test_x,test_y = get_train_test(x,y)\n    # Build model    \n    model = build_model(train_x,train_y)\n    # Evaluate the model on train dataset    \n    test_model(train_x,train_y,model,label_names)    \n    # Evaluate the model on test dataset\n    test_model(test_x,test_y,model,label_names)\n```", "```py\nmodel = tree.DecisionTreeClassifier(criterion=\"entropy\")\n```", "```py\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix\n```", "```py\ndef get_feature_names():\n    data = load_iris()\n    return data['feature_names']\n\ndef probe_model(x,y,model,label_names):\n\n    feature_names = get_feature_names()\n    feature_importance = model.feature_importances_\n    print \"\\nFeature Importance\\n\"\n    print \"=====================\\n\"\n    for i,feature_name in enumerate(feature_names):\n        print \"%s = %0.3f\"%(feature_name,feature_importance[i])\n\n    # Export the decison tree as a graph\n    tree.export_graphviz(model,out_file='tree.dot')\n```", "```py\n# Export the decison tree as a graph\ntree.export_graphviz(model,out_file='tree.dot')\n```"]