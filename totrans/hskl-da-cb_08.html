<html><head></head><body><div class="chapter" title="Chapter&#xA0;8.&#xA0;Clustering and Classification"><div class="titlepage"><div><div><h1 class="title"><a id="ch08"/>Chapter 8. Clustering and Classification</h1></div></div></div><p>This chapter demonstrates algorithms that intelligently cluster and categorize data:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Implementing the k-means clustering algorithm</li><li class="listitem" style="list-style-type: disc">Implementing hierarchical clustering</li><li class="listitem" style="list-style-type: disc">Using a hierarchical clustering library</li><li class="listitem" style="list-style-type: disc">Finding the number of clusters</li><li class="listitem" style="list-style-type: disc">Clustering words by their lexemes</li><li class="listitem" style="list-style-type: disc">Classifying the parts of speech of words</li><li class="listitem" style="list-style-type: disc">Identifying key words in a corpus of text</li><li class="listitem" style="list-style-type: disc">Training a parts-of-speech tagger</li><li class="listitem" style="list-style-type: disc">Implementing a decision tree classifier</li><li class="listitem" style="list-style-type: disc">Implementing a k-Nearest Neighbors classifier</li><li class="listitem" style="list-style-type: disc">Visualizing points using Graphics.EasyPlot</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec98"/>Introduction</h1></div></div></div><div class="mediaobject"><img src="graphics/ch08.jpg" alt="Introduction"/></div><p>Computer algorithms are becoming better and better at analyzing large datasets. As their performance enhances, their ability to detect interesting patterns in data also improves. </p><p>The first few algorithms in this chapter demonstrate how to look at thousands of points and identify clusters. A <span class="strong"><strong>cluster</strong></span><a id="id456" class="indexterm"/> is simply a congregation of points defined by how closely they lie together. This measure of "closeness" is entirely up to us. One of the most popular closeness metrics is the Euclidian distance. </p><p>We can understand clusters by looking up at the night sky and pointing at stars that appear together. Our ancestors found it convenient to name "clusters" of stars, of which we refer to as constellations. We will be finding our own constellations in the "sky" of data points.</p><p>This chapter also focuses on classifying words. We will label words by their parts of speech as well as topic.</p><p>We will implement our own decision tree to classify practical data. Lastly, we will visualize clusters and points using plotting libraries.</p></div></div>
<div class="section" title="Implementing the k-means clustering algorithm"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec99"/>Implementing the k-means clustering algorithm</h1></div></div></div><p>The k-means clustering algorithm partitions <a id="id457" class="indexterm"/>data into k different groups. These k groupings are called clusters, and the location of these clusters are adjusted iteratively. We compute the arithmetic mean of all the points in a group to obtain a centroid point that we use, replacing the previous cluster location. </p><p>Hopefully, after this succinct <a id="id458" class="indexterm"/>explanation, the name <span class="emphasis"><em>k-means clustering</em></span> no longer sounds completely foreign. One of the best places to learn more about this algorithm is on <a id="id459" class="indexterm"/>Coursera: <a class="ulink" href="https://class.coursera.org/ml-003/lecture/78">https://class.coursera.org/ml-003/lecture/78</a>.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec265"/>How to do it…</h2></div></div></div><p>Create a new file, which we call <code class="literal">Main.hs</code>, and perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the following built-in libraries:<div class="informalexample"><pre class="programlisting">import Data.Map (Map)
import qualified Data.Map as Map
import Data.List (minimumBy, sort, transpose)
import Data.Ord (comparing)</pre></div></li><li class="listitem">Define a type synonym for points shown as follows:<div class="informalexample"><pre class="programlisting">type Point = [Double] </pre></div></li><li class="listitem">Define the Euclidian distance function between two points:<div class="informalexample"><pre class="programlisting">dist :: Point -&gt; Point -&gt; Double

dist a b = sqrt $ sum $ map (^2) $ zipWith (-) a b</pre></div></li><li class="listitem">Define the assignment step in the k-means algorithm. Each point will be assigned to its closest centroid:<div class="informalexample"><pre class="programlisting">assign :: [Point] -&gt; [Point] -&gt; Map Point [Point]

assign centroids points = 
  Map.fromListWith (++) [(assignPoint p, [p]) | p&lt;- points]

  where assignPoint p = 
    minimumBy (comparing (dist p)) centroids</pre></div></li><li class="listitem">Define the relocation step in the k-means algorithm. Each centroid is relocated to the arithmetic mean of its corresponding points:<div class="informalexample"><pre class="programlisting">relocate :: Map Point [Point] -&gt; Map Point [Point]

relocate centroidsMap = 
  Map.foldWithKey insertCenter Map.empty centroidsMap
  where insertCenter _ ps m = Map.insert (center ps) ps m
        center [] = [0,0]
        center ps = map average (transpose ps)
        average xs = sum xs / fromIntegral (length xs)</pre></div></li><li class="listitem">Run the <a id="id460" class="indexterm"/>k-means algorithm repeatedly until the centroids no longer move around:<div class="informalexample"><pre class="programlisting">kmeans :: [Point] -&gt; [Point] -&gt; [Point]

kmeans centroids points = 
if converged 
then centroids 
else kmeans (Map.keys newCentroidsMap) points

where converged = 
        all (&lt; 0.00001) $ zipWith dist 
             (sort centroids) (Map.keys newCentroidsMap)

      newCentroidsMap = 
        relocate (assign centroids points)

      equal a b = dist a b &lt; 0.00001 </pre></div></li><li class="listitem">Test out the clustering with a couple of hardcoded points. The usual way to implement k-means chooses the starting centroids randomly. However, in this recipe, we will simply take the first k points:<div class="informalexample"><pre class="programlisting">main = do
let points = [ [0,0], [1,0], [0,1], [1,1]
             , [7,5], [9,6], [8,7] ]
let centroids = kmeans (take 2 points) points
print centroids </pre></div></li><li class="listitem">After the algorithm converges, the resulting centroids will be as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ runhaskell Main.hs</strong></span>

<span class="strong"><strong>[[0.5,0.5],[8.0,6.0]]</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec266"/>How it works…</h2></div></div></div><p>The algorithm<a id="id461" class="indexterm"/> repeatedly follows two procedures until the clusters are found. The first procedure is to partition the points by assigning each point to its closest centroid. The following diagram shows the data assignment step. Initially, there are three centroids represented by a star, square, and circle around three different points. The first part of the algorithm assigns each point a corresponding centroid.</p><div class="mediaobject"><img src="graphics/6331OS_08_01.jpg" alt="How it works…"/></div><p>The next step is to relocate the centroids to the center, or arithmetic mean, of their corresponding points. In the following diagram, the arithmetic mean of each cluster is computed, and the centroid is shifted to the new center:</p><div class="mediaobject"><img src="graphics/6331OS_08_02.jpg" alt="How it works…"/></div><p>This algorithm continues until the centroids no longer move around. The final categorization of each point is the cluster to which each point belongs.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec267"/>There's more…</h2></div></div></div><p>Although easy<a id="id462" class="indexterm"/> to implement and understand, this algorithm has a couple of limitations. The output of the k-means clustering algorithm is sensitive to the initial centroids chosen. Also, using the Euclidian distance metric forces the clusters to be described only by circular regions. Another limitation of k-means clustering is that the initial number of clusters k must be specified by the user. The user should visualize the data and use their judgment to determine the number of clusters before beginning the algorithm. Moreover, the convergence condition for the algorithm is an issue for special edge-cases.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec268"/>See also</h2></div></div></div><p>For another type of clustering algorithm, see the next recipe on <span class="emphasis"><em>Implementing hierarchical clustering</em></span>.</p></div></div>
<div class="section" title="Implementing hierarchical clustering"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec100"/>Implementing hierarchical clustering</h1></div></div></div><p>Another way to cluster<a id="id463" class="indexterm"/> data is by first assuming each data item as its own cluster. We can then take a step back and merge together two of the nearest clusters. This process forms a hierarchy of clusters.</p><p>Take, for example, an analogy relating to islands and water level. An island is nothing more than a mountain tip surrounded by water. Imagine we have islands scattered across a sea. If we were to<a id="id464" class="indexterm"/> slowly drop the water level of the sea, two nearby small islands would merge into a larger island because they are connected to the same mountain formation. We can stop the water level from dropping any time we have the desired number of larger islands.</p><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec269"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, insert this code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the built-in functions:<div class="informalexample"><pre class="programlisting">import Data.Map (Map, (!), delete)
import qualified Data.Map as Map
import Data.Ord (comparing)
import Data.List (sort, tails, transpose, minimumBy)</pre></div></li><li class="listitem">Define a type synonym for points:<div class="informalexample"><pre class="programlisting">type Point = [Double]</pre></div></li><li class="listitem">Define a convenience function to compute the arithmetic mean of list of points:<div class="informalexample"><pre class="programlisting">center :: [Point] -&gt; Point

center points = map average (transpose points)
  where average xs = sum xs / fromIntegral (length xs)</pre></div></li><li class="listitem">Combine the two clusters that are nearest to each other:<div class="informalexample"><pre class="programlisting">merge :: Map Point [Point] -&gt; Map Point [Point]

merge m = 
        Map.insert (center [a,b]) ((m ! a) ++ (m ! b)) newM

where (a,b) = nearest (Map.keys m)

            newM = Map.delete b (Map.delete a m)

            equal a b = dist a b &lt; 0.00001

            dist a b = sqrt $ sum $ map (^2) $ zipWith (-) a b

            nearest points = 
                  minimumBy (comparing (uncurry dist)) 
                  [(a, b) | (a : rest) &lt;- tails points, b &lt;- rest] </pre></div></li><li class="listitem">Run the hierarchical algorithm until there are k clusters:<div class="informalexample"><pre class="programlisting">run :: Int -&gt; Map Point [Point] -&gt; Map Point [Point]

run k m = if length (Map.keys m) == k 
          then m 
          else run k (merge m)</pre></div></li><li class="listitem">Initialize <a id="id465" class="indexterm"/>so that every point is its own cluster:<div class="informalexample"><pre class="programlisting">initialize :: [Point] -&gt; Map Point [Point]

initialize points = 
  foldl (\m p -&gt; Map.insert p [p] m) Map.empty points</pre></div></li><li class="listitem">Test the clustering algorithm on some input:<div class="informalexample"><pre class="programlisting">main = do
      let points = [ [0,0], [1,0], [0,1], [1,1]
                     , [7,5], [9,6], [8,7]]
      let centroids = Map.keys $ run 2 (initialize points)
      print centroids</pre></div></li><li class="listitem">The algorithm will output the following centroids:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ runhaskell Main.hs</strong></span>

<span class="strong"><strong>[[0.5,0.5],[7.75,5.75]]</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec270"/>How it works…</h2></div></div></div><p>There are two main ways to implement hierarchical clustering. The algorithm described in this recipe implements the<a id="id466" class="indexterm"/> <span class="emphasis"><em>agglomerative</em></span> bottom-up approach<a id="id467" class="indexterm"/>. Each point is pre-emptively considered to be a cluster, and at each step the two closest clusters merge together. However, another approach to implement is top-down in a<a id="id468" class="indexterm"/> <span class="emphasis"><em>divisive</em></span> approach <a id="id469" class="indexterm"/>where every point starts in one massive cluster that iteratively splits the clusters.</p><p>In this recipe, we begin by first assuming that every point is its own cluster. Then we take a step back and merge two of the nearest clusters. This step repeats until a desired convergence state is reached. In our example, we stop once we have exactly two clusters. The following diagram shows the three iterations of a hierarchical clustering algorithm:</p><div class="mediaobject"><img src="graphics/6331OS_08_03.jpg" alt="How it works…"/></div></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec271"/>There's more…</h2></div></div></div><p>Like most <a id="id470" class="indexterm"/>clustering algorithms, the choice of distance metric greatly affects the results. In this recipe, we assumed the Euclidean metric, but depending on the data, perhaps the distance metric should be the Manhattan distance or cosine similarity.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec272"/>See also</h2></div></div></div><p>For a non-hierarchical clustering algorithm, see the previous recipe on <span class="emphasis"><em>Implementing the k-means clustering algorithm</em></span>.</p></div></div>
<div class="section" title="Using a hierarchical clustering library"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec101"/>Using a hierarchical clustering library </h1></div></div></div><p>We will group together a list <a id="id471" class="indexterm"/>of points using a hierarchical clustering approach. We will start by assuming that each point is its own cluster. The two closest clusters merge together and the algorithm repeats until the stopping criteria is met. In this algorithm, we will use a library to run hierarchical clustering until there are a specific number of clusters remaining.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec273"/>Getting ready</h2></div></div></div><p>Install the hierarchical clustering package using cabal as follows (documentation is available at <a class="ulink" href="http://hackage.haskell.org/package/hierarchical-clustering">http://hackage.haskell.org/package/hierarchical-clustering</a>):</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install hierarchical-clustering</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec274"/>How to do it…</h2></div></div></div><p>Insert the following code in a new file, which we call <code class="literal">Main.hs</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the required library:<div class="informalexample"><pre class="programlisting">import Data.Clustering.Hierarchical</pre></div></li><li class="listitem">Define a Point data type:<div class="informalexample"><pre class="programlisting">data Point = Point [Double] deriving Show</pre></div></li><li class="listitem">Define the Euclidian distance metric:<div class="informalexample"><pre class="programlisting">dist :: Point -&gt; Point -&gt; Distance
dist (Point a) (Point b) = sqrt $ sum $ map (^2) $ 
                           zipWith (-) a b</pre></div></li><li class="listitem">Print out the clusters:<div class="informalexample"><pre class="programlisting">printCluster :: Dendrogram Point -&gt; Double -&gt; IO ()

printCluster clusters cut = do
         let es = map elements $ clusters `cutAt` cut
         mapM_ print es</pre></div></li><li class="listitem">Test the clustering algorithm on some points:<div class="informalexample"><pre class="programlisting">main = do
        let points = 
map Point [ [0,0], [1,0], [0,1], [1,1]
          , [7,5], [9,6], [8,7] ]
        let clusters = dendrogram SingleLinkage points dist
        printCluster clusters 2.0</pre></div></li><li class="listitem">Each of the three clusters are printed out as lists of points:<div class="informalexample"><pre class="programlisting">[Point [0.0,1.0], Point [1.0,0.0], Point [0.0,0.0], Point [1.0,1.0]] 
[Point [7.0,5.0]] 
[Point [9.0,6.0], Point [8.0,7.0]]</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec275"/>How it works…</h2></div></div></div><p>The <code class="literal">dendogram</code> function has the type <code class="literal">Linkage -&gt; [a] -&gt; (a -&gt; a -&gt; Distance) -&gt; Dendogram a</code>. The linkage describes how distance is calculated. In this recipe, we use <code class="literal">SingleLinkage</code> as the first argument, which means that the distance between two clusters is the minimum distance between all their elements.</p><p>The second argument is the list of points, followed by a distance metric. The result of this function is a <a id="id472" class="indexterm"/>
<span class="strong"><strong>dendogram</strong></span>, otherwise referred to as a hierarchical tree diagram. We use the defined <code class="literal">printCluster</code> function<a id="id473" class="indexterm"/> to display the clusters.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec276"/>There's more…</h2></div></div></div><p>The other types of linkage in this library include the following mentioned along with their description present on Hackage:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">SingleLinkage</code>: This is<a id="id474" class="indexterm"/> the minimum distance between two clusters.<div class="mediaobject"><img src="graphics/6331OS_08_05.jpg" alt="There's more…"/></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"O(n^2) time and O(n) space, using the SLINK algorithm. This algorithm is optimal in both space and time and gives the same answer as the naive algorithm using a distance matrix."</em></span></p></blockquote></div></li><li class="listitem" style="list-style-type: disc"><code class="literal">CompleteLinkage</code>: This <a id="id475" class="indexterm"/>is the maximum distance between two clusters.<div class="mediaobject"><img src="graphics/6331OS_08_06.jpg" alt="There's more…"/></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix. Use CLINK if you need more performance."</em></span></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">Complete linkage with <span class="strong"><strong>CLINK</strong></span><a id="id476" class="indexterm"/> is the same as the previous linkage type, except that it uses a faster but not always optimal algorithm.<div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"O(n^2) time and O(n) space, using the CLINK algorithm. Note that this algorithm doesn't always give the same answer as the naive algorithm using a distance matrix, but it's much faster."</em></span></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">UPGMA <a id="id477" class="indexterm"/>is the average distance between the two clusters.<div class="mediaobject"><img src="graphics/6331OS_08_07.jpg" alt="There's more…"/></div><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."</em></span></p></blockquote></div></li><li class="listitem" style="list-style-type: disc">And lastly, FakeAverageLinkage<a id="id478" class="indexterm"/> is similar to the previous UPGMA linkage but weighs both clusters equally in its calculations.<div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"O(n^3) time and O(n^2) space, using the naive algorithm with a distance matrix."</em></span></p></blockquote></div></li></ul></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec277"/>See also</h2></div></div></div><p>To use our own hierarchical clustering algorithm, see the previous recipe on <span class="emphasis"><em>Implementing hierarchical clustering</em></span>.</p></div></div>
<div class="section" title="Finding the number of clusters"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec102"/>Finding the number of clusters</h1></div></div></div><p>Sometimes, we do <a id="id479" class="indexterm"/>not know the number of clusters in a dataset, yet most clustering algorithms require this information a priori. One way to find the number of clusters is to run the clustering algorithm on all possible number of clusters and compute the average variance of the clusters. We can then graph the average variance for the number of clusters, and identify the number of clusters by finding the first fluctuation of the curve.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec278"/>Getting ready</h2></div></div></div><p>Review the k-means recipe titled <span class="emphasis"><em>Implementing the k-means clustering algorithm</em></span>. We will be using the <code class="literal">kmeans</code><a id="id480" class="indexterm"/> and <a id="id481" class="indexterm"/>
<code class="literal">assign</code> functions defined in that recipe.</p><p>Install the Statistics package from cabal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install statistics</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec279"/>How to do it…</h2></div></div></div><p>Create a new<a id="id482" class="indexterm"/> file and insert the following code. We name this file <code class="literal">Main.hs</code>.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the <code class="literal">variance</code> function<a id="id483" class="indexterm"/> and the helper <a id="id484" class="indexterm"/><code class="literal">fromList</code> function:<div class="informalexample"><pre class="programlisting">import Statistics.Sample (variance)
import Data.Vector.Unboxed (fromList)</pre></div></li><li class="listitem">Compute the average of the variance of each cluster:<div class="informalexample"><pre class="programlisting">avgVar points centroids = avg [variance . fromList $
  map (dist c) ps | (c, ps) &lt;- Map.assocs m]
  where m = assign centroids points
           avg xs = (sum xs) / (fromIntegral (length xs)) </pre></div></li><li class="listitem">In <code class="literal">main</code>, define a list of points. Notice how there appears to be three clusters:<div class="informalexample"><pre class="programlisting">main = do
  let points = [ [0,0], [1,0], [0,1]
                       , [20,0], [21,0], [20,1]
                       , [40,5], [40,6], [40,8] ]</pre></div></li><li class="listitem">Get the average of the variance of each set of clusters:<div class="informalexample"><pre class="programlisting">      let centroids = [ kmeans (take k points) points | 
                           k &lt;- [1..length points] ]
      let avgVars = map (avgVar points) centroids
      print avgVars</pre></div></li><li class="listitem">The output will be a list of numbers. Once plotted, we can see that the number of clusters is three, which occurs at the knee, or just before local maxima, of the curve as shown in the following image:<div class="mediaobject"><img src="graphics/6331OS_08_04.jpg" alt="How to do it…"/></div></li></ol></div></div></div>
<div class="section" title="Clustering words by their lexemes"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec103"/>Clustering words by their lexemes </h1></div></div></div><p>Words that look <a id="id485" class="indexterm"/>alike can easily be clustered together. The clustering algorithm in the lexeme-clustering package<a id="id486" class="indexterm"/> is based on Janicki's research paper titled, "<span class="emphasis"><em>A Lexeme-Clustering Algorithm for Unsupervised Learning of Morphology</em></span>". A direct link to this paper can be found through the following URL: <a class="ulink" href="http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf">http://skil.informatik.uni-leipzig.de/blog/wp-content/uploads/proceedings/2012/Janicki2012.37.pdf</a>.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec280"/>Getting ready</h2></div></div></div><p>An Internet connection is necessary for this recipe to download the package from GitHub.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec281"/>How to do it…</h2></div></div></div><p>Follow these steps to install and use the library:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Obtain the<a id="id487" class="indexterm"/> lexeme-clustering library from GitHub. If Git is installed, enter the following command, otherwise download it from <a class="ulink" href="https://github.com/BinRoot/lexeme-clustering/archive/master.zip">https://github.com/BinRoot/lexeme-clustering/archive/master.zip</a>:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ git clone https://github.com/BinRoot/lexeme-clustering</strong></span>
</pre></div></li><li class="listitem">Change into the library's directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd lexeme-clustering/</strong></span>
</pre></div></li><li class="listitem">Install the package:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install</strong></span>
</pre></div></li><li class="listitem">Create <a id="id488" class="indexterm"/>an input file with a different word on each line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat input.txt</strong></span>
<span class="strong"><strong>mama</strong></span>
<span class="strong"><strong>papa</strong></span>
<span class="strong"><strong>sissy</strong></span>
<span class="strong"><strong>bro</strong></span>
<span class="strong"><strong>mother</strong></span>
<span class="strong"><strong>father</strong></span>
<span class="strong"><strong>grandfather</strong></span>
<span class="strong"><strong>grandmother</strong></span>
<span class="strong"><strong>uncle</strong></span>
<span class="strong"><strong>mommy</strong></span>
<span class="strong"><strong>daddy</strong></span>
<span class="strong"><strong>ma</strong></span>
<span class="strong"><strong>pa</strong></span>
<span class="strong"><strong>mom</strong></span>
<span class="strong"><strong>dad</strong></span>
<span class="strong"><strong>sister</strong></span>
<span class="strong"><strong>brother</strong></span>
</pre></div></li><li class="listitem">Run the lexeme-clustering algorithm on the input file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ dist/build/lexeme-clustering/lexeme-clustering input.txt</strong></span>
</pre></div></li><li class="listitem">The resulting output clusters are then displayed:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong># Clustering</strong></span>
<span class="strong"><strong>bro, brother</strong></span>
<span class="strong"><strong>dad, daddy</strong></span>
<span class="strong"><strong>grandfather, grandmother</strong></span>
<span class="strong"><strong>father, ma, mama, mom, mommy, mother, pa, papa</strong></span>
<span class="strong"><strong>sissy, sister</strong></span>
<span class="strong"><strong>uncle</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec282"/>How it works…</h2></div></div></div><p>The related words are clustered together by carefully looking at each word's <a id="id489" class="indexterm"/>
<span class="strong"><strong>morpheme</strong></span>, or smallest meaningful component.</p><p>Here's a short excerpt from the abstract of the research paper of which this algorithm is based:</p><div class="blockquote"><blockquote class="blockquote"><p><span class="emphasis"><em>"Initially, a trie of words is built and each node in the trie is considered a candidate for stem. The suffixes, with which it occurs, are clustered according to mutual information in order to identify inflectional paradigms."</em></span></p></blockquote></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec283"/>See also</h2></div></div></div><p>For clustering points of data, see the previous algorithms on <span class="emphasis"><em>Implementing the k-means clustering algorithm</em></span>, <span class="emphasis"><em>Implementing hierarchical clustering</em></span>, and <span class="emphasis"><em>Using a hierarchical clustering library</em></span>.</p></div></div>
<div class="section" title="Classifying the parts of speech of words"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec104"/>Classifying the parts of speech of words</h1></div></div></div><p>This recipe<a id="id490" class="indexterm"/> will demonstrate how to identify the parts of speech of each word in a sentence. We will be using a handy library <a id="id491" class="indexterm"/>called <span class="strong"><strong>chatter</strong></span>, which contains very useful <span class="strong"><strong>Natural Language Processing</strong></span> (<span class="strong"><strong>NLP</strong></span>) tools. It can be obtained from Hackage at <a class="ulink" href="http://hackage.haskell.org/package/chatter">http://hackage.haskell.org/package/chatter</a>.</p><p>NLP is<a id="id492" class="indexterm"/> the study of human language embedded in a machine. Our naturally spoken or written language may seem obvious to us in our day-to-day lives, but producing meaning out of words is still a difficult task for computers.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec284"/>Getting ready</h2></div></div></div><p>Install the<a id="id493" class="indexterm"/> NLP library using cabal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>cabal install chatter</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec285"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, enter the following source code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the parts of speech library and the pack function:<div class="informalexample"><pre class="programlisting">import NLP.POS
import Data.Text (pack)</pre></div></li><li class="listitem">Obtain the default tagger provided by the library:<div class="informalexample"><pre class="programlisting">main = do
tagger &lt;- defaultTagger</pre></div></li><li class="listitem">Feed the <code class="literal">tag</code> function a tagger and a text to see the corresponding parts of speech per each word:<div class="informalexample"><pre class="programlisting">let text = pack "The best jokes have no punchline."
print $ tag tagger text</pre></div></li><li class="listitem">The output will be an association list of the word to its part of speech:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>[[ ("The", Tag "at"), </strong></span>
<span class="strong"><strong>    ("best", Tag "jjt"), </strong></span>
<span class="strong"><strong>    ("jokes", Tag "nns"), </strong></span>
<span class="strong"><strong>    ("have", Tag "hv"), </strong></span>
<span class="strong"><strong>    ("no", Tag "at"), </strong></span>
<span class="strong"><strong>    ("punchline",Tag "nn"), </strong></span>
<span class="strong"><strong>    (".",Tag ".") ]]</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec286"/>How it works…</h2></div></div></div><p>A parts of speech <a id="id494" class="indexterm"/>tagger is <a id="id495" class="indexterm"/>trained from a corpus of text. In this example, we use the default tagger provided by the library, which trains on the corpus in the following directory of the package, <code class="literal">data/models/brown-train.model.gz</code>. This corpus is called the Brown University Standard Corpus of Present-Day American English, created in the 1960s.</p><p>Definitions of each of the abbreviations such as at, <code class="literal">jjt</code>, or <code class="literal">nns</code> can be found on <a class="ulink" href="http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used">http://en.wikipedia.org/wiki/Brown_Corpus#Part-of-speech_tags_used</a>.</p></div><div class="section" title="There's more…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec287"/>There's more…</h2></div></div></div><p>We can also train our own parts of speech taggers by loading a tagger from a file path, <code class="literal">loadTagger :: FilePath -&gt; IO POSTagger</code>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec288"/>See also</h2></div></div></div><p>To categorize words as something other than parts of speech, see the next recipe on <span class="emphasis"><em>Identifying key words in a corpus of text</em></span>.</p></div></div>
<div class="section" title="Identifying key words in a corpus of text"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec105"/>Identifying key words in a corpus of text</h1></div></div></div><p>One way to predict the<a id="id496" class="indexterm"/> topic of a paragraph or sentence is by identifying what the words mean. While the parts of speech give some insight about each word, they still don't reveal the connotation of that word. In this recipe, we will use a Haskell library to tag words by topics such as <code class="literal">PERSON</code>, <code class="literal">CITY</code>, <code class="literal">DATE</code>, and so on.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec289"/>Getting ready</h2></div></div></div><p>An Internet <a id="id497" class="indexterm"/>connection is necessary for this recipe to download the<a id="id498" class="indexterm"/> <code class="literal">sequor</code> package.</p><p>Install it from cabal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install sequor --prefix=`pwd`</strong></span>
</pre></div><p>Otherwise, follow these directions to install it manually:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Obtain the latest version of the <a id="id499" class="indexterm"/>sequor library by opening up a browser and visiting the following URL: <a class="ulink" href="http://hackage.haskell.org/package/sequor">http://hackage.haskell.org/package/sequor</a>.</li><li class="listitem">Under the <span class="strong"><strong>Downloads</strong></span> section, download the cabal source package.</li><li class="listitem">Extract the contents:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">On Windows, it is easiest to using<a id="id500" class="indexterm"/> 7-Zip, an easy-to-use file archiver. Install it on your machine by going to <a class="ulink" href="http://www.7-zip.org">http://www.7-zip.org</a>. Then using 7-Zip, extract the contents of the tarball.</li><li class="listitem" style="list-style-type: disc">On other operating systems, run the following command to extract the tarball. Replace the numbers in the following command to the correct version numbers of your download because a new version (that is, 0.7.3) may be out:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ tar –zxvf sequor-0.7.2.tar.gz</strong></span>
</pre></div></li></ul></div></li><li class="listitem">Go into the directory:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cd sequor-0.7.2</strong></span>
</pre></div></li><li class="listitem">Make sure to read the <code class="literal">README</code> file:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat README.*</strong></span>
</pre></div></li><li class="listitem">Install the library using the following Cabal command:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install –-prefix=`pwd`</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec290"/>How to do it…</h2></div></div></div><p>We will set up an input file to feed into the program.</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create an <code class="literal">input.txt</code> file using the CoNLL format, which requires one token per line, and sentences separated by a blank line:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat input.txt</strong></span>
<span class="strong"><strong>On</strong></span>
<span class="strong"><strong>Tuesday</strong></span>
<span class="strong"><strong>Richard</strong></span>
<span class="strong"><strong>Stallman</strong></span>
<span class="strong"><strong>will</strong></span>
<span class="strong"><strong>visit</strong></span>
<span class="strong"><strong>Charlottesville</strong></span>
<span class="strong"><strong>,</strong></span>
<span class="strong"><strong>Virginia</strong></span>
<span class="strong"><strong>in</strong></span>
<span class="strong"><strong>the</strong></span>
<span class="strong"><strong>United </strong></span>
<span class="strong"><strong>States</strong></span>
</pre></div></li><li class="listitem">Now <a id="id501" class="indexterm"/>run the word tagging on the input:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ bin/seminer en &lt; input.txt &gt; output.txt</strong></span>
</pre></div></li><li class="listitem">The result is saved in the <code class="literal">output.txt</code> file. Open up the file and review the corresponding tags found:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat output.txt</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>B-DATE</strong></span>
<span class="strong"><strong>B-PERSON</strong></span>
<span class="strong"><strong>I-PERSON</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>B-GPE:CITY</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>B-GPE:STATE_PROVINCE</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>B-GPE:COUNTRY</strong></span>
<span class="strong"><strong>I-GPE:COUNTRY</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec291"/>How it works…</h2></div></div></div><p>The library uses Collins' sequence perceptron, based off a paper published in 2002 titled "<span class="emphasis"><em>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</em></span>". His website (<a class="ulink" href="http://www.cs.columbia.edu/~mcollins/">http://www.cs.columbia.edu/~mcollins/</a>) contains comprehensive notes on designing the algorithm used in this recipe.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec292"/>See also</h2></div></div></div><p>To use an existing parts of speech tagger, see the previous recipe on <span class="emphasis"><em>Classifying the parts of speech of words</em></span>. To train our own parts-of-speech tagger, see the next recipe on <span class="emphasis"><em>Training a parts-of-speech tagger</em></span>.</p></div></div>
<div class="section" title="Training a parts-of-speech tagger"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec106"/>Training a parts-of-speech tagger</h1></div></div></div><p>We will use <a id="id502" class="indexterm"/>a Haskell library, sequor, to train our own parts of speech tagger. Then we can use this newly trained model on our own input.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec293"/>Getting ready</h2></div></div></div><p>Please refer to the <span class="emphasis"><em>Getting ready</em></span> section of the previous recipe.</p></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec294"/>How to do it…</h2></div></div></div><p>In a new file, which we name <code class="literal">Main.hs</code>, enter the following source code:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Use the <code class="literal">sequor</code> executable to train the parts of speech tagger:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The first argument to <code class="literal">sequor</code> will be <code class="literal">train</code>, to indicate that we are about to train a tagger</li><li class="listitem" style="list-style-type: disc">The next argument is the template-file, <code class="literal">data/all.features</code></li><li class="listitem" style="list-style-type: disc">Then we provide the train-file, <code class="literal">data/train.conll</code></li><li class="listitem" style="list-style-type: disc">The last file path we need to provide is the location of where to save the trained model</li><li class="listitem" style="list-style-type: disc">We can specify a learning rate using the <code class="literal">-rate</code> flag</li><li class="listitem" style="list-style-type: disc">The beam size can be modified using the <code class="literal">-beam</code> flag</li><li class="listitem" style="list-style-type: disc">Change the number of iterations using the <code class="literal">-iter</code> flag</li><li class="listitem" style="list-style-type: disc">Use hashing instead of a feature dictionary using the <code class="literal">-hash</code> flag</li><li class="listitem" style="list-style-type: disc">Provide a path to the held out data using the <code class="literal">-heldout</code> flag</li><li class="listitem" style="list-style-type: disc">An example of the sequor command in use is as follows:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./bin/sequor train data/all.features data/train.conll \</strong></span>
<span class="strong"><strong>model --rate 0.1 --beam 10 --iter 5 --hash \</strong></span>
<span class="strong"><strong>--heldout data/devel.conll</strong></span>
</pre></div></li></ul></div></li><li class="listitem">Test out the trained model on a sample input:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ ./bin/sequor predict model &lt; data/test.conll &gt; \</strong></span>
<span class="strong"><strong>data/test.labels</strong></span>
</pre></div></li><li class="listitem">The first few lines of the output <code class="literal">test.labels</code> file will be:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>B-NP</strong></span>
<span class="strong"><strong>I-NP</strong></span>
<span class="strong"><strong>B-PP</strong></span>
<span class="strong"><strong>B-NP</strong></span>
<span class="strong"><strong>I-NP</strong></span>
<span class="strong"><strong>O</strong></span>
<span class="strong"><strong>B-VP</strong></span>
<span class="strong"><strong>B-NP</strong></span>
<span class="strong"><strong>B-VP</strong></span>
<span class="strong"><strong>B-NP</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec295"/>How it works…</h2></div></div></div><p>The library <a id="id503" class="indexterm"/>uses Collins' sequence perceptron, based off a paper published in 2002 titled "<span class="emphasis"><em>Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms</em></span>". The Hackage documentation can be found on <a class="ulink" href="http://hackage.haskell.org/package/sequor">http://hackage.haskell.org/package/sequor</a>.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec296"/>See also</h2></div></div></div><p>To use an existing parts of speech tagger, see the previous recipe on <span class="emphasis"><em>Classifying the parts of speech of words</em></span>.</p></div></div>
<div class="section" title="Implementing a decision tree classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec107"/>Implementing a decision tree classifier</h1></div></div></div><p>A decision tree<a id="id504" class="indexterm"/> is a model for classifying data effectively. Each child of a node in the tree represents a feature about the item we are classifying. Traversing down the tree to leaf nodes represent an item's classification. It's often desirable to create the smallest possible tree to represent a large sample of data.</p><p>In this recipe, we <a id="id505" class="indexterm"/>implement the ID3 decision tree algorithm<a id="id506" class="indexterm"/> in Haskell. It is one of the <a id="id507" class="indexterm"/>easiest to implement and produces useful results. However, ID3 does not guarantee an optimal solution, may be computationally inefficient compared to other algorithms, and only supports discrete data. While these issues can be addressed by a more complicated algorithm such as C4.5, the code in this recipe is enough to get up and running with a working decision tree.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec297"/>Getting ready</h2></div></div></div><p>Create a CSV file representing samples of data. The last column should be the classification. Name this file <code class="literal">input.csv</code>.</p><div class="mediaobject"><img src="graphics/6331OS_08_08.jpg" alt="Getting ready"/></div><p>The weather data<a id="id508" class="indexterm"/> is represented <a id="id509" class="indexterm"/>with four attributes, namely outlook, temperature, humidity, and wind. The last column represents whether it is a good idea to play outside.</p><p>Import the CSV helper library:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install csv</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec298"/>How to do it…</h2></div></div></div><p>Insert this code into a new file, which we call <code class="literal">Main.hs</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the built-in libraries:<div class="informalexample"><pre class="programlisting">import Data.List (nub, elemIndices)
import qualified Data.Map as M
import Data.Map (Map, (!))
import Data.List (transpose)
import Text.CSV</pre></div></li><li class="listitem">Define some type synonyms to better understand what data is being passed around:<div class="informalexample"><pre class="programlisting">type Class = String
type Feature = String
type Entropy = Double
type DataSet = [([String], Class)]</pre></div></li><li class="listitem">Define the <a id="id510" class="indexterm"/>main function to read in the CSV file and handle any errors:<div class="informalexample"><pre class="programlisting">main = do
  rawCSV &lt;- parseCSVFromFile "input.csv"
  either handleError doWork rawCSV

handleError = error "invalid file"</pre></div></li><li class="listitem">If the file <a id="id511" class="indexterm"/>was read successfully, remove any invalid CSV records and construct a decision tree out of it:<div class="informalexample"><pre class="programlisting">doWork csv = do
  let removeInvalids = filter (\x -&gt; length x &gt; 1)
  let myData = map (\x -&gt; (init x, last x)) $ removeInvalids csv
  print $ dtree "root" myData</pre></div></li><li class="listitem">Define helper functions to break up the <code class="literal">DataSet</code> tuple into a list of samples or a list of classes:<div class="informalexample"><pre class="programlisting">samples :: DataSet -&gt; [[String]]
samples d = map fst d


classes :: DataSet -&gt; [Class]
classes d = map snd d</pre></div></li><li class="listitem">Calculate the entropy of a list of values:<div class="informalexample"><pre class="programlisting">entropy :: (Eq a) =&gt; [a] -&gt; Entropy

entropy xs = sum $ map (\x -&gt; prob x * info x) $ nub xs
  where prob x = (length' (elemIndices x xs)) / (length' xs)
        info x = negate $ logBase 2 (prob x)
        length' xs = fromIntegral $ length xs</pre></div></li><li class="listitem">Split an attribute by its features:<div class="informalexample"><pre class="programlisting">splitAttr :: [(Feature, Class)] -&gt; Map Feature [Class]

splitAttr fc = foldl (\m (f,c) -&gt; M.insertWith (++) f [c] m)
               M.empty fc</pre></div></li><li class="listitem">Obtain each of the entropies from splitting up an attribute by its features:<div class="informalexample"><pre class="programlisting">splitEntropy :: Map Feature [Class] -&gt; M.Map Feature Entropy

splitEntropy m = M.map entropy m</pre></div></li><li class="listitem">Compute <a id="id512" class="indexterm"/>the information gain from splitting up an attribute by its features:<div class="informalexample"><pre class="programlisting">informationGain :: [Class] -&gt; [(Feature, Class)] -&gt; Double

informationGain s a = entropy s - newInformation
  where eMap = splitEntropy $ splitAttr a
        m = splitAttr a
        toDouble x = read x :: Double
        ratio x y = (fromIntegral x) / (fromIntegral y)
        sumE = M.map (\x -&gt; (fromIntegral.length) x / (fromIntegral.length) s) m
        newInformation = M.foldWithKey (\k a b -&gt; b + a*(eMap!k)) 
          0 sumE</pre></div></li><li class="listitem">Determine<a id="id513" class="indexterm"/> which attribute contributes the highest information gain:<div class="informalexample"><pre class="programlisting">highestInformationGain :: DataSet -&gt; Int
highestInformationGain d = snd $ maximum $ 
  zip (map ((informationGain . classes) d) attrs) [0..]
  where attrs = map (attr d) [0..s-1]
        attr d n = map (\(xs,x) -&gt; (xs!!n,x)) d
        s = (length . fst . head) d</pre></div></li><li class="listitem">Define the data structure for a decision tree that we will soon construct:<div class="informalexample"><pre class="programlisting">data DTree = DTree { feature :: String
                   , children :: [DTree] } 
           | Node String String
           deriving Show</pre></div></li><li class="listitem">Split up the dataset by the attribute that contributes the highest information gain:<div class="informalexample"><pre class="programlisting">datatrees :: DataSet -&gt; Map String DataSet
datatrees d = 
  foldl (\m (x,n) -&gt; M.insertWith (++) (x!!i) [((x `dropAt` i), fst (cs!!n))] m)
    M.empty (zip (samples d) [0..])
  where i = highestInformationGain d
    dropAt xs i = let (a,b) = splitAt i xs in a ++ drop 1 b
        cs = zip (classes d) [0..]</pre></div></li><li class="listitem">Define a helper function to determine if all elements of a list are equal. We use this to check if further splitting of the dataset is necessary by checking if its classes are identical:<div class="informalexample"><pre class="programlisting">allEqual :: Eq a =&gt; [a] -&gt; Bool
allEqual [] = True
allEqual [x] = True
allEqual (x:xs) = x == (head xs) &amp;&amp; allEqual xs</pre></div></li><li class="listitem">Construct <a id="id514" class="indexterm"/>the <a id="id515" class="indexterm"/>decision tree from a labeling and a dataset of samples:<div class="informalexample"><pre class="programlisting">dtree :: String -&gt; DataSet -&gt; DTree

dtree f d 
  | allEqual (classes d) = Node f $ head (classes d)
  | otherwise = DTree f $ 
            M.foldWithKey (\k a b -&gt; b ++ [dtree k a] ) [] 
            (datatrees d)</pre></div></li><li class="listitem">Run the following code to see the tree printed out:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>DTree { feature = "root"</strong></span>
<span class="strong"><strong>      , children = [ DTree { feature = "Sunny"</strong></span>
<span class="strong"><strong>                           , children = [ Node "Normal" "Yes"</strong></span>
<span class="strong"><strong>                                        , Node "High" "No"</strong></span>
<span class="strong"><strong>                                        ]</strong></span>
<span class="strong"><strong>                   , DTree { feature = "Rain"</strong></span>
<span class="strong"><strong>                           , children = [ Node "Weak" "Yes"</strong></span>
<span class="strong"><strong>                                        , Node "Strong" "No" </strong></span>
<span class="strong"><strong>                                        ] </strong></span>
<span class="strong"><strong>                           }</strong></span>
<span class="strong"><strong>                   , Node "Overcast" "Yes" </strong></span>
<span class="strong"><strong>                   ] </strong></span>
<span class="strong"><strong>      }</strong></span>
</pre></div></li></ol></div><p>It can be visualized using the following diagram:</p><div class="mediaobject"><img src="graphics/6331OS_08_09.jpg" alt="How to do it…"/></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec299"/>How it works…</h2></div></div></div><p>The ID3 algorithm <a id="id516" class="indexterm"/>uses the concept of Shannon's entropy to divide up a set of samples by the attribute that maximize the information gain. This process is recursively repeated until we're dealing with samples of the same classification or when we run out of attributes.</p><p>In the field of Information Theory, <span class="strong"><strong>Entropy</strong></span><a id="id517" class="indexterm"/> is the measure of unpredictability. A fair coin has higher entropy than a biased coin. Entropy can be calculated by taking the expected value of the information content, where information content of a random variable X has the form — <span class="emphasis"><em>ln(P(X))</em></span>. When the logarithm in the equation is to the base of 2, the units of entropy are called <span class="emphasis"><em>bits</em></span>.</p><p>Information Gain <a id="id518" class="indexterm"/>is the change in entropy from the prior state to the new state. It has the equation <span class="emphasis"><em>IG = H<sub>1</sub> – H<sub>2</sub></em></span>, where <span class="emphasis"><em>H<sub>1</sub></em></span> is the original entropy of the sample. And <span class="emphasis"><em>H<sub>2</sub></em></span> is the new entropy given an attribute to split.</p></div></div>
<div class="section" title="Implementing a k-Nearest Neighbors classifier"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec108"/>Implementing a k-Nearest Neighbors classifier</h1></div></div></div><p>One simple way to classify an item is to look at only its neighboring data. The k-Nearest Neighbors algorithm <a id="id519" class="indexterm"/>looks at k items located closest to the item in question. The item is then classified as the most common classification of its k neighbors. This heuristic has been very promising for a wide variety of classification tasks.</p><p>In this recipe, we will <a id="id520" class="indexterm"/>implement the k-Nearest Neighbors algorithm using a <a id="id521" class="indexterm"/>
<span class="strong"><strong>k-d tree</strong></span> data structure, which is a binary tree with special properties that allow efficient representation of points in a k-dimensional space.</p><p>Imagine we have a web server for our hip new website. Every time someone requests a web page, our web server will fetch the file and present the page. However, bots can easily hammer a web server with thousands of requests, potentially causing a denial of service attack. In this recipe, we will classify whether a web request is being made by a human or a bot.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec300"/>Getting ready</h2></div></div></div><p>Install the <code class="literal">KdTree</code>, <code class="literal">CSV</code>, and <code class="literal">iproute</code> packages using cabal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install KdTree</strong></span>
<span class="strong"><strong>$ cabal install CSV</strong></span>
<span class="strong"><strong>$ cabal install iproute</strong></span>
</pre></div><p>Create a CSV file containing the IP addresses and number of seconds since last access. The last field of each CSV record should be the classification <span class="emphasis"><em>Human</em></span> or <span class="emphasis"><em>Bot</em></span>. We call our file <code class="literal">input.csv</code>.</p><div class="mediaobject"><img src="graphics/6331OS_08_10.jpg" alt="Getting ready"/></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec301"/>How to do it…</h2></div></div></div><p>After creating a new file called <code class="literal">Main.hs</code>, we perform the following steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import the following packages:<div class="informalexample"><pre class="programlisting">import Data.Trees.KdTree
import Data.IP (IPv4, fromIPv4)
import Text.CSV
import qualified Data.Map as M
import Data.Maybe (fromJust)</pre></div></li><li class="listitem">Convert <a id="id522" class="indexterm"/>an IPv4 address string into its 32-bit representation:<div class="informalexample"><pre class="programlisting">ipToNum :: String -&gt; Double

ipToNum str = fromIntegral $ sum $ 
  zipWith (\a b -&gt; a * 256^b) ns [0..]
  where ns = reverse $ fromIPv4 (read str :: IPv4)</pre></div></li><li class="listitem">Parse data from a CSV file to obtain a list of points and their associated classifications:<div class="informalexample"><pre class="programlisting">parse :: [Record] -&gt; [(Point3d, String)]

parse [] = []
parse xs = map pair (cleanList xs)
  where pair [ip, t, c] = 
          (Point3d (ipToNum ip) (read t) 0.0, c)
        cleanList = filter (\x -&gt; length x == 3)</pre></div></li><li class="listitem">Find the item in a list that occurs most often:<div class="informalexample"><pre class="programlisting">maxFreq :: [String] -&gt; String

maxFreq xs = fst $ foldl myCompare ("", 0) freqs
  where freqs = M.toList $ M.fromListWith (+) 
                                         [(c, 1) | c &lt;- xs]
        myCompare (oldS, oldV) (s,v) = if v &gt; oldV
                                       then (s, v)
                                       else (oldS, oldV)</pre></div></li><li class="listitem">Classify a test point given the KdTree, the number of nearest neighbors to use, and the training set of points:<div class="informalexample"><pre class="programlisting">test :: KdTree Point3d -&gt; Int -&gt; [(Point3d, String)] 
                       -&gt; Point3d -&gt; String

test kdtree k pairList p = maxFreq $ map classify neighbors
  where neighbors = kNearestNeighbors kdtree k p
        classify x = fromJust (lookup x pairList)</pre></div></li><li class="listitem">Define <code class="literal">main</code> to read a CSV file and process the data:<div class="informalexample"><pre class="programlisting">main = do
  rawCSV &lt;- parseCSVFromFile "input.csv"
  either handleError doWork rawCSV</pre></div></li><li class="listitem">Handle an error if the CSV cannot be read properly:<div class="informalexample"><pre class="programlisting">handleError = error "Invalid CSV file"</pre></div></li><li class="listitem">Otherwise create a KdTree from the CSV data and test out a couple of examples:<div class="informalexample"><pre class="programlisting">doWork rawCSV = do
  let ps = parse rawCSV
  let kdtree = fromList (map fst ps)
  let examples = [ ["71.190.100.100", "2000", "?"]
                 , ["216.239.33.1", "1", "?"] ]
  let examplePts = map fst $ parse examples
  print $ map (test kdtree 2 ps) examplePts</pre></div></li><li class="listitem">Run the <a id="id523" class="indexterm"/>code to see the resulting classifications of the example points:<div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ runhaskell Main.hs</strong></span>

<span class="strong"><strong>["Human", "Bot"]</strong></span>
</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec302"/>How it works…</h2></div></div></div><p>The k-Nearest Neighbor algorithm<a id="id524" class="indexterm"/> looks at the k closest points from the training set and returns the most frequent classification between these k points. Since we are dealing with points, each of the coordinates should be orderable. Fortunately, an IP address has a faint sense of hierarchy that we can leverage. We convert an IP to its 32-bit number to obtain a useful ordering that we can treat as a coordinate of a point in space.</p></div></div>
<div class="section" title="Visualizing points using Graphics.EasyPlot"><div class="titlepage"><div><div><h1 class="title"><a id="ch08lvl1sec109"/>Visualizing points using Graphics.EasyPlot</h1></div></div></div><p>Sometimes, it's<a id="id525" class="indexterm"/> convenient to simply visualize <a id="id526" class="indexterm"/>data points before clustering or classifying to inspect the data. This recipe will feed a list of points to a plotting library to easily see a diagram of the data.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec303"/>Getting ready</h2></div></div></div><p>Install easyplot from cabal:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cabal install easyplot</strong></span>
</pre></div><p>Create a CSV file containing two-dimensional points:</p><div class="informalexample"><pre class="programlisting">
<span class="strong"><strong>$ cat input.csv</strong></span>

<span class="strong"><strong>1,2</strong></span>
<span class="strong"><strong>2,3</strong></span>
<span class="strong"><strong>3,1</strong></span>
<span class="strong"><strong>4,5</strong></span>
<span class="strong"><strong>5,3</strong></span>
<span class="strong"><strong>6,1</strong></span>
</pre></div></div><div class="section" title="How to do it…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec304"/>How to do it…</h2></div></div></div><p>In a new file, <code class="literal">Main.hs</code>, follow these steps:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Import <a id="id527" class="indexterm"/>the required library to read in CSV data as well the library to plot points:<div class="informalexample"><pre class="programlisting">import Text.CSV
import Graphics.EasyPlot</pre></div></li><li class="listitem">Create <a id="id528" class="indexterm"/>a helper function to convert a list of string records into a list of doubles. For example, we want to convert <code class="literal">[ "1.0,2.0", "3.5,4.5" ]</code> into <code class="literal">[ (1.0, 2.0), (3.5, 4.5) ]</code>:<div class="informalexample"><pre class="programlisting">tupes :: [[String]] -&gt; [(Double, Double)]

tupes records = [ (read x, read y) | [x, y] &lt;- records ]</pre></div></li><li class="listitem">In <code class="literal">main</code>, parse the CSV file to be used later on:<div class="informalexample"><pre class="programlisting">main = do 
  result &lt;- parseCSVFromFile "input.csv"</pre></div></li><li class="listitem">If the CSV file is valid, plot the points using the <code class="literal">plot :: TerminalType -&gt; a -&gt; IO Bool</code> function:<div class="informalexample"><pre class="programlisting">  case result of
    Left err -&gt; putStrLn "Error reading CSV file"
    Right csv -&gt; do 
      plot X11 $ Data2D [Title "Plot"] [] (tupes csv)
        return ()</pre></div></li></ol></div></div><div class="section" title="How it works…"><div class="titlepage"><div><div><h2 class="title"><a id="ch08lvl2sec305"/>How it works…</h2></div></div></div><p>The first argument to <code class="literal">plot</code> tells gnuplot where its output should be displayed. For example, we use X11 to output to the X Window System on Linux. Depending on the computer, we can choose between different terminal types. The <a id="id529" class="indexterm"/>constructors for <code class="literal">TerminalType</code> are the following:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">Aqua</code>: Output on Mac OS X (Aqua Terminal)</li><li class="listitem" style="list-style-type: disc"><code class="literal">Windows</code>: Output for MS Windows</li><li class="listitem" style="list-style-type: disc"><code class="literal">X11</code>: Output to the X Window System</li><li class="listitem" style="list-style-type: disc"><code class="literal">PS FilePath</code>: Output into a PostScript file</li><li class="listitem" style="list-style-type: disc"><code class="literal">EPS FilePath</code>: Output into an EPS file path</li><li class="listitem" style="list-style-type: disc"><code class="literal">PNG FilePath</code>: Output as Portable Network Graphic into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">PDF FilePath</code>: Output as Portable Document Format into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">SVG FilePath</code>: Output as Scalable Vector Graphic into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">GIF FilePath</code>: Output as Graphics Interchange Format into a file</li><li class="listitem" style="list-style-type: disc"><code class="literal">JPEG FilePath</code>: Output into a JPEG file</li><li class="listitem" style="list-style-type: disc"><code class="literal">Latex FilePath</code>: Output <a id="id530" class="indexterm"/>as LaTeX</li></ul></div><p>The second argument to plot is the graph, which may be a <code class="literal">Graph2D</code>, or <code class="literal">Graph3D</code>, or a list of these.</p></div></div></body></html>