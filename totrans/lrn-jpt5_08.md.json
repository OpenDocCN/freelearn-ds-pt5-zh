["```py\nxcode-select -install \n```", "```py\nbrew cask install java \n```", "```py\nbrew install scala \n```", "```py\nbrew install apache-spark \n```", "```py\nspark-shell \n```", "```py\nWelcome to \n ____              __ \n / __/__  ___ _____/ /__ \n _\\ \\/ _ \\/ _ `/ __/  '_/ \n /__ / .__/\\_,_/_/ /_/\\_\\   version 2.0.0 \n /_/ \n\nUsing Python version 2.7.12 (default, Jul  2 2016 17:43:17) \nSparkSession available as 'spark'. \n>>> \n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\nlines = sc.textFile(\"Spark File Line Lengths.ipynb\")\nlineLengths = lines.map(lambda s: len(s))\ntotalLengths = lineLengths.reduce(lambda a, b: a + b)\nprint(totalLengths)\n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\n\n#load in the file\ntext_file = sc.textFile(\"Spark File Words.ipynb\")\n\n#split file into distinct words\ncounts = text_file.flatMap(lambda line: line.split(\" \")) \\\n    .map(lambda word: (word, 1)) \\\n    .reduceByKey(lambda a, b: a + b)\n\n# print out words found\nfor x in counts.collect():\n    print(x)\n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\n\n#load in the file\ntext_file = sc.textFile(\"Spark Sort Words from File.ipynb\")\n\n#split file into sorted, distinct words\nsorted_counts = text_file.flatMap(lambda line: line.split(\" \")) \\\n    .map(lambda word: (word, 1)) \\\n    .reduceByKey(lambda a, b: a + b) \\\n    .sortByKey()\n\n# print out words found (in sorted order)\nfor x in sorted_counts.collect():\n    print(x)\n```", "```py\nimport pyspark \nimport random \nif not 'sc' in globals(): \n    sc = pyspark.SparkContext() \n\nNUM_SAMPLES = 10000 \nrandom.seed(113) \n\ndef sample(p): \n    x, y = random.random(), random.random() \n    return 1 if x*x + y*y < 1 else 0 \n\ncount = sc.parallelize(range(0, NUM_SAMPLES)) \\ \n    .map(sample) \\ \n    .reduce(lambda a, b: a + b) \n\nprint(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES)) \n```", "```py\n64.242.88.10 - - [07/Mar/2004:16:05:49 -0800] \"GET /twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables HTTP/1.1\" 401 12846 \n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\n\ntextFile = sc.textFile(\"access_log\")\nprint(textFile.count(), \"access records\")\n\ngets = textFile.filter(lambda line: \"GET\" in line)\nprint(gets.count(), \"GETs\")\n\nposts = textFile.filter(lambda line: \"POST\" in line)\nprint(posts.count(), \"POSTs\")\n\nother = textFile.subtract(gets).subtract(posts)\nprint(other.count(), \"Other\")\n\n#curious what Other requests may have been\nfor x in other.collect():\n    print(x)\n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\n\ndef is_it_prime(number):\n\n    #make sure n is a positive integer\n    number = abs(number)\n\n    #simple tests\n    if number < 2:\n        return False\n\n    #2 is special case\n    if number == 2:\n        return True\n\n    #all other even numbers are not prime\n    if not number & 1:\n        return False\n\n    #divisible into it's square root\n    for x in range(3, int(number**0.5)+1, 2):\n        if number % x == 0:\n            return False\n\n    #must be a prime\n    return True\n\n# pick a good range\nnumbers = sc.parallelize(range(100000))\n\n# see how many primes are in that range\nprint(numbers.filter(is_it_prime).count())\n```", "```py\nimport pyspark\nif not 'sc' in globals():\n    sc = pyspark.SparkContext()\n\n#pull out sentences from article\nsentences = sc.textFile('2600raid.txt') \\\n    .glom() \\\n    .map(lambda x: \" \".join(x)) \\\n    .flatMap(lambda x: x.split(\".\"))\nprint(sentences.count(),\"sentences\")\n\n#find the bigrams in the sentences\nbigrams = sentences.map(lambda x:x.split()) \\\n    .flatMap(lambda x: [((x[i],x[i+1]),1) for i in range(0, len(x)-1)])\nprint(bigrams.count(),\"bigrams\")\n\n#find the (10) most common bigrams\nfrequent_bigrams = bigrams.reduceByKey(lambda x,y:x+y) \\\n    .map(lambda x:(x[1], x[0])) \\\n    .sortByKey(False)\nfrequent_bigrams.take(10)\n```", "```py\n1999,actor,1/11/99,Acting,Michael J. Fox \n```", "```py\n#Spark Daily Show Guests\nimport pyspark\nimport csv\nimport operator\nimport itertools\nimport collections\n\nif not 'sc' in globals():\n sc = pyspark.SparkContext()\n\nyears = {}\noccupations = {}\nguests = {}\n\n#file header contains column descriptors:\n#YEAR, GoogleKnowledge_Occupation, Show, Group, Raw_Guest_List\n\nwith open('daily_show_guests.csv', 'rt', errors = 'ignore') as csvfile: \n reader = csv.DictReader(csvfile)\n for row in reader:\n year = row['YEAR']\n if year in years:\n years[year] = years[year] + 1\n else:\n years[year] = 1\n\n occupation = row['GoogleKnowlege_Occupation']\n if occupation in occupations:\n occupations[occupation] = occupations[occupation] + 1\n else:\n occupations[occupation] = 1\n\n guest = row['Raw_Guest_List']\n if guest in guests:\n guests[guest] = guests[guest] + 1\n else:\n guests[guest] = 1\n\n#sort for higher occurrence\nsyears = sorted(years.items(), key = operator.itemgetter(1), reverse = True)\nsoccupations = sorted(occupations.items(), key = operator.itemgetter(1), reverse = True)\nsguests = sorted(guests.items(), key = operator.itemgetter(1), reverse = True)\n\n#print out top 5's\nprint(syears[:5])\nprint(soccupations[:5])\nprint(sguests[:5]) \n```"]