["```py\n    df_1 = pd.DataFrame({'product': ['red shirt', 'red shirt', 'red shirt', 'white dress'],\\n\",\n    'price': [49.33, 49.33, 32.49,\n    199.99]})\\n\",\n    df_2 = pd.DataFrame({'product': ['red shirt', 'blue pants', 'white tuxedo', 'white dress'],\\n\",\n    'in_stock': [True, True, False,\n    False]})\n    ```", "```py\n    df[~df.duplicated()]\n    ```", "```py\n    df[~df['product'].duplicated()]\n    ```", "```py\n    df.drop_duplicates(inplace=True)\n    ```", "```py\n    df.fillna(value=df.price.mean())\n    ```", "```py\n    df.fillna(method='pad')\n    ```", "```py\n    df.in_stock = df.in_stock.map({False: 0, True: 1})\n    ```", "```py\n    from sklearn.preprocessing import LabelEncoder rating_encoder = LabelEncoder()\n    _df = df.copy()\n    _df.rating = rating_encoder.fit_transform(df.rating)\n    _df\n    ```", "```py\n    ordinal_map = {rating: index for index, rating in enumerate(['low', 'medium', 'high'])}\n    print(ordinal_map)\n    df.rating = df.rating.map(ordinal_map)\n    ```", "```py\n    df = pd.get_dummies(df)\n    ```", "```py\n    features = ['price', 'rating', 'product_blue pants', 'product_red shirt', 'product_white dress', 'product_white tuxedo']\n    X = df[features].values target = 'in_stock'\n    y = df[target].values\n    from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3)\n    ```", "```py\n    sns.jointplot('satisfaction_level', 'last_evaluation', data=df, kind='hex')\n    ```", "```py\n    plot_args = dict(shade=True, shade_lowest=False) for i, c in zip((0, 1), ('Reds', 'Blues')):\n    sns.kdeplot(df.loc[df.left==i, 'satisfaction_level'], df.loc[df.left==i, 'last_evaluation'], cmap=c, **plot_args)\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    features = ['satisfaction_level', 'last_evaluation'] X_train, X_test, y_train, y_test = train_test_split(\n    df[features].values, df['left'].values, test_size=0.3, random_state=1)\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler scaler = StandardScaler()\n    X_train_std = scaler.fit_transform(X_train) X_test_std = scaler.transform(X_test)\n    ```", "```py\n    from sklearn.svm import SVC\n    svm = SVC(kernel='linear', C=1, random_state=1) svm.fit(X_train_std, y_train)\n    ```", "```py\n    from sklearn.metrics import accuracy_score y_pred = svm.predict(X_test_std)\n    acc = accuracy_score(y_test, y_pred) print('accuracy = {:.1f}%'.format(acc*100))\n    >> accuracy = 75.9%\n    ```", "```py\n    from sklearn.metrics import confusion_matrix cmat = confusion_matrix(y_test, y_pred)\n    scores = cmat.diagonal() / cmat.sum(axis=1) * 100 print('left = 0 : {:.2f}%'.format(scores[0]))\n    print('left = 1 : {:.2f}%'.format(scores[1]))\n    >> left = 0 : 100.00%\n    >> left = 1 : 0.00%\n    ```", "```py\n    from mlxtend.plotting import plot_decision_regions N_samples = 200\n    X, y = X_train_std[:N_samples], y_train[:N_samples] plot_decision_regions(X, y, clf=svm)\n    ```", "```py\n    check_model_fit(svm, X_test_std, y_test)\n    ```", "```py\n    from sklearn.neighbors import KNeighborsClassifier KNeighborsClassifier?\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbors=3) \n    knn.fit(X_train_std, y_train)\n    check_model_fit(knn, X_test_std, y_test)\n    ```", "```py\n    knn = KNeighborsClassifier(n_neighbors=25) knn.fit(X_train_std, y_train)\n    check_model_fit(knn, X_test_std, y_test)\n    ```", "```py\n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators=50, max_depth=5,\n    random_state=1)\n    forest.fit(X_train, y_train) check_model_fit(forest, X_test, y_test)\n    ```", "```py\n    from sklearn.tree import export_graphviz import graphviz\n    dot_data = export_graphviz(\n    forest.estimators_[0], out_file=None, feature_names=features, class_names=['no', 'yes'], filled=True, rounded=True, special_characters=True)\n    graph = graphviz.Source(dot_data) graph\n    ```", "```py\n    df = pd.read_csv('../data/hr-analytics/hr_data_processed. csv')\n    features = ['satisfaction_level', 'last_evaluation']\n    X = df[features].values y = df.left.values\n    ```", "```py\n    clf = RandomForestClassifier(n_estimators=100, max_depth=5)\n    ```", "```py\n    from sklearn.model_selection import cross_val_score np.random.seed(1)\n    scores = cross_val_score(\n    estimator=clf, X=X,\n    y=y, cv=10)\n    print('accuracy = {:.3f} +/- {:.3f}'.format(scores.mean(), scores.std()))\n    >> accuracy = 0.923 +/- 0.005\n    ```", "```py\n    >> array([ 0.93404397,\t0.91533333,\t0.92266667,\n    0.91866667,\t0.92133333,\n    0.92866667,\t0.91933333,\t0.92\t,\n    0.92795197,\t0.92128085])\n    ```", "```py\n    from sklearn.model_selection import StratifiedKFold\n    …\n    …\n    print('fold: {:d} accuracy: {:s}'.format(k+1, str(class_acc)))\n    return class_accuracy\n    ```", "```py\n    from sklearn.model_selection import cross_val_score np.random.seed(1)\n    …\n    …\n    >> fold: 10 accuracy: [ 0.98861646\t0.70588235]\n    >> accuracy = [ 0.98722476\t0.71715647] +/- [ 0.00330026\n    0.02326823]\n    ```", "```py\n    from sklearn.model_selection import validation_curve\n    clf = RandomForestClassifier(n_estimators=10) max_depths = np.arange(3, 16, 3)\n    train_scores, test_scores = validation_curve( estimator=clf,\n    X=X,\n    y=y, param_name='max_depth', param_range=max_depths,\n    cv=10);\n    ```", "```py\n    plot_validation_curve(train_scores, test_scores,\n    max_depths, xlabel='max_depth')\n    ```", "```py\n    features = ['satisfaction_level', 'last_evaluation', 'number_project',\n    'average_montly_hours', 'time_spend_company', 'work_ accident',\n    …\n    …\n    X = df[features].values y = df.left.values\n    ```", "```py\n    %%time np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=20) max_depths = [3, 4, 5, 6, 7,\n    9, 12, 15, 18, 21]\n    train_scores, test_scores = validation_curve( estimator=clf,\n    X=X,\n    y=y, param_name='max_depth', param_range=max_depths,\n    cv=5);\n    ```", "```py\n    plot_validation_curve(train_scores, test_scores,\n    max_depths, xlabel='max_depth');\n    ```", "```py\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=200, max_depth=6) scores = cross_val_class_score(clf, X, y)\n    print('accuracy = {} +/- {}'\\\n    .format(scores.mean(axis=0), scores.std(axis=0)))\n    >> accuracy = [ 0.99553722\t0.85577359] +/- [ 0.00172575\n    0.02614334]\n    ```", "```py\n    fig = plt.figure(figsize=(5, 7)) sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]),\n    palette=sns.color_palette('Set1')) plt.xlabel('Left')\n    plt.ylabel('Accuracy')\n    ```", "```py\n    pd.Series(clf.feature_importances_, name='Feature importance',\n    index=df[features].columns)\\\n    .sort_values()\\\n    .plot.barh() plt.xlabel('Feature importance')\n    ```", "```py\n    from sklearn.decomposition import PCA pca_features = \\\n    …\n    …\n    pca = PCA(n_components=3)\n    X_pca = pca.fit_transform(X_reduce)\n    ```", "```py\n    >> array([[-0.67733089,\t0.75837169, -0.10493685],\n    >>\t[ 0.73616575,\t0.77155888, -0.11046422],\n    >>\t[ 0.73616575,\t0.77155888, -0.11046422],\n    >>\t...,\n    >>\t[-0.67157059, -0.3337546 ,\t0.70975452],\n    >>\t[-0.67157059, -0.3337546 ,\t0.70975452],\n    >>\t[-0.67157059, -0.3337546 ,\t0.70975452]])\n    ```", "```py\n    df['first_principle_component'] = X_pca.T[0] df['second_principle_component'] = X_pca.T[1]\n    df['third_principle_component'] = X_pca.T[2]\n    ```", "```py\n    features = ['satisfaction_level', 'number_project', 'time_spend_company',\n    'average_montly_hours', 'last_evaluation', 'first_principle_component', 'second_principle_component', 'third_principle_component']\n    X = df[features].values y = df.left.values\n    ```", "```py\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=200, max_depth=6) scores = cross_val_class_score(clf, X, y)\n    print('accuracy = {} +/- {}'\\\n    .format(scores.mean(axis=0), scores.std(axis=0)))\n    >> accuracy = [ 0.99562463\t0.90618594] +/- [ 0.00166047\n    0.01363927]\n    ```", "```py\n    fig = plt.figure(figsize=(5, 7)) sns.boxplot(data=pd.DataFrame(scores, columns=[0, 1]), palette=sns.color_palette('Set1')) plt.xlabel('Left') plt.ylabel('Accuracy')\n    ```", "```py\n    np.random.seed(1)\n    clf = RandomForestClassifier(n_estimators=200, max_depth=6) clf.fit(X, y)\n    ```", "```py\n    from sklearn.externals import joblib joblib.dump(clf, 'random-forest-trained.pkl')\n    ```", "```py\n    clf = joblib.load('random-forest-trained.pkl')\n    ```", "```py\n    sandra = df.iloc[573]X = sandra[features]X\n    >> satisfaction_level              0.360000\n    >> number_project                  2.000000\n    >> time_spend_company              3.000000\n    >> average_montly_hours          148.000000\n    >> last_evaluation                 0.470000\n    >> first_principle_component       0.742801\n    >> second_principle_component     -0.514568\n    >> third_principle_component      -0.677421\n    ```", "```py\n    clf.predict([X])\n    >> array([1])\n    ```", "```py\n    clf.predict_proba([X])\n    >> array([[ 0.06576239,\t0.93423761]])\n    ```", "```py\n    X.average_montly_hours = 100\n    X.time_spend_company = 1 clf.predict_proba([X])\n    >> array([[ 0.61070329,\t0.38929671]])\n    ```"]