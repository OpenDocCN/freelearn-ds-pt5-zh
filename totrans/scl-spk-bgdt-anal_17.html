<html><head></head><body>
        

                            
                    <h1 class="header-title" id="calibre_pb_0">Time to Go to ClusterLand - Deploying Spark on a Cluster</h1>
                
            
            
                
<p>"I see the moon like a clipped piece of silver. Like gilded bees, the stars cluster around her"</p>
<p class="cdpalignright">- Oscar Wilde</p>
<p class="mce-root">In the previous chapters, we have seen how to develop practical applications using different Spark APIs. However, in this chapter, we will see how Spark works in a cluster mode with its underlying architecture. Finally, we will see how to deploy a full Spark application on a cluster. In a nutshell, the following topics will be cover throughout this chapter:</p>
<ul class="calibre9">
<li class="mce-root1">Spark architecture in a cluster</li>
<li class="mce-root1">Spark ecosystem and cluster management</li>
<li class="mce-root1">Deploying Spark on a cluster</li>
<li class="mce-root1">Deploying Spark on a standalone cluster</li>
<li class="mce-root1">Deploying Spark on a Mesos cluster</li>
<li class="mce-root1">Deploying Spark on YARN cluster</li>
<li class="mce-root1">Cloud-based deployment</li>
<li class="mce-root1">Deploying Spark on AWS</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark architecture in a cluster</h1>
                
            
            
                
<p class="mce-root">Hadoop-based <strong class="calibre1">MapReduce</strong> framework has been widely used for the last few years; however, it has some issues with I/O, algorithmic complexity, low-latency streaming jobs, and fully disk-based operation. Hadoop provides the <strong class="calibre1">Hadoop Distributed File System</strong> (<strong class="calibre1">HDFS</strong>) for efficient computing and storing big data cheaply, but you can only do the computations with a high-latency batch model or static data using the Hadoop-based MapReduce framework. The main big data paradigm that Spark has brought for us is the introduction of in-memory computing and caching abstraction. This makes Spark ideal for large-scale data processing and enables the computing nodes to perform multiple operations by accessing the same input data.</p>
<p class="mce-root">Spark's <strong class="calibre1">Resilient Distributed Dataset</strong> (<strong class="calibre1">RDD</strong>) model can do everything that the MapReduce paradigm can, and even more. Nevertheless, Spark can perform iterative computations on your dataset at scale. This option helps to execute machine learning, general purpose data processing, graph analytics, and <strong class="calibre1">Structured Query Language</strong> (<strong class="calibre1">SQL</strong>) algorithms much faster with or without depending upon Hadoop. Therefore, reviving the Spark ecosystem is a demand at this point.</p>
<p class="mce-root">Enough knowing about Spark's beauties and features. At this point, reviving the Spark ecosystem is your demand to know how does Spark work.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Spark ecosystem in brief</h1>
                
            
            
                
<p class="mce-root">To provide you with more advanced and additional big data processing capabilities, your Spark jobs can be running on top of Hadoop-based (aka YARN) or Mesos-based clusters. On the other hand, the core APIs in Spark, which is written in Scala, enable you to develop your Spark application using several programming languages such as Java, Scala, Python, and R. Spark provides several libraries that are part of the Spark ecosystems for additional capabilities for general purpose data processing and analytics, graph processing, large-scale structured SQL, and <strong class="calibre1">machine learning</strong> (<strong class="calibre1">ML</strong>) areas. The Spark ecosystem consists of the following components:</p>
<div><img class="image-border240" src="img/00161.jpeg"/></div>
<div><strong class="calibre1">Figure 1:</strong> Spark ecosystem (up to Spark 2.1.0)</div>
<p class="mce-root">The core engine of Spark is written in Scala but supports different languages to develop your Spark application, such as R, Java, Python, and Scala. The main components/APIs in the Spark core engine are as follows:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><strong class="calibre1">SparkSQL</strong>: This helps in seamlessly mix SQL queries with Spark programs so that you can query structured data inside Spark programs.</li>
<li value="2" class="mce-root1"><strong class="calibre1">Spark Streaming</strong>: This is for large-scale streaming application development that provides seamless integration of Spark with other streaming data sources such as Kafka, Flink, and Twitter.</li>
<li value="3" class="mce-root1"><strong class="calibre1">SparkMLlib</strong> and <strong class="calibre1">SparKML</strong>: These are for RDD and dataset/DataFrame-based machine learning and pipeline creation.</li>
<li value="4" class="mce-root1"><strong class="calibre1">GraphX</strong>: This is for large-scale graph computation and processing to make your graph data object fully connected.</li>
<li value="5" class="mce-root1"><strong class="calibre1">SparkR</strong>: R on Spark helps in basic statistical computations and machine learning.</li>
</ol>
<p class="mce-root">As we have already stated, it is very much possible to combine these APIs seamlessly to develop large-scale machine learning and data analytics applications. Moreover, Spark jobs can be submitted and executed through cluster managers such as Hadoop YARN, Mesos, and standalone, or in the cloud by accessing data storage and sources such as HDFS, Cassandra, HBase, Amazon S3, or even RDBMS. However, to the full facility of Spark, we need to deploy our Spark application on a computing cluster.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Cluster design</h1>
                
            
            
                
<p class="mce-root">Apache Spark is a distributed and parallel processing system and it also provides in-memory computing capabilities. This type of computing paradigm needs an associated storage system so that you can deploy your application on top of a big data cluster. To make this happen, you will have to use distributed storage systems such as HDFS, S3, HBase, and Hive. For moving data, you will be needing other technologies such as Sqoop, Kinesis, Twitter, Flume, and Kafka.</p>
<p class="mce-root">In practice, you can configure a small Hadoop cluster very easily. You only need to have a single master and multiple worker nodes. In your Hadoop cluster, generally, a master node consists of <strong class="calibre1">NameNodes</strong>, <strong class="calibre1">DataNodes</strong>, <strong class="calibre1">JobTracker</strong>, and <strong class="calibre1">TaskTracker</strong>. A worker node, on the other hand, can be configured so that it works both as a DataNode and as a TaskTracker.<br class="title-page-name"/></p>
<p class="mce-root">For security reasons, most of the big data cluster might set up behind a network firewall so that the complexity caused by the firewall can be overcome or at least reduced by the computing nodes. Otherwise, computing nodes cannot be accessed from outside of the network, that is, extranet. The following figure shows a simplified big data cluster that is commonly used in Spark:</p>
<div><img class="image-border241" src="img/00379.jpeg"/></div>
<div><strong class="calibre1">Figure 2:</strong> A general architecture for big data processing with JVM</div>
<p class="mce-root">The above picture shows a cluster consisting of five computing nodes. Here each node has a dedicated executor JVM, one per CPU core, and the Spark Driver JVM sitting outside the cluster. The disk is directly attached to the nodes using the <strong class="calibre1">JBOD</strong> (<strong class="calibre1">Just a bunch of disks</strong>) approach. Very large files are partitioned over the disks, and a virtual file system such as HDFS makes these chunks available as one large virtual file. The following simplified component model shows the driver JVM sitting outside the cluster. It talks to the cluster manager (see <strong class="calibre1">Figure 4</strong>) in order to obtain permission to schedule tasks on the worker nodes because the cluster manager keeps track on resource allocation of all processes running on the cluster.</p>
<p class="mce-root">If you have developed your Spark application using Scala or Java, it means that your job is a JVM-based process. For your JVM-based process, you can simply configure the Java heap space by specifying the following two parameters:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">-Xmx</strong>: <strong class="calibre1">T</strong>his one specifies the upper limit of your Java heap space</li>
<li class="mce-root1"><strong class="calibre1">-Xms</strong>: This one is the lower limit of the Java heap space</li>
</ul>
<p class="mce-root">Once you have sumitted a Spark job, heap memory need to be allocated for your Spark jobs. The following figure provides some insights on how:</p>
<div><img class="image-border242" src="img/00113.jpeg"/></div>
<div><strong class="calibre1">Figure 3:</strong> JVM memory management</div>
<p class="mce-root">As demonstrated in the preceding figure, Spark starts a Spark job with 512 MB of JVM heap space. However, for an uninterrupted processing of your Spark job and to avoid the <strong class="calibre1">Out of Memory</strong> (<strong class="calibre1">OOM</strong>) error, Spark allows the computing nodes to utilize only up to 90% of the heap (that is, ~461 MB), which is eventually increased or decreased by controlling the <kbd class="calibre11">spark.storage.safetyFraction</kbd> parameter in Spark environment. To be more realistic, the JVM can be seen as a concatenation of <strong class="calibre1">Storage</strong> (60% of the Java heap), 20% of the heap for execution (aka <strong class="calibre1">Shuffle</strong>), and the rest of the 20% for other storage.</p>
<p class="mce-root">Moreover, Spark is a cluster computing tool that tries to utilize both in-memory and disk-based computing and allows users to store some data in memory. In reality, Spark utilizes the main memory only for its LRU cache. For uninterrupted caching mechanism, a little amount of memory is required to be reserved for the application specific data processing. Informally, this is around 60% of the Java heap space controlled by the <kbd class="calibre11">spark.memory.fraction</kbd>.</p>
<p class="mce-root">Therefore, if you would like to see or calculate how much application specific data you can cache in memory in your Spark application, you can just sum up all the heap sizes usages by all the executors and multiply it by the <kbd class="calibre11">safetyFraction</kbd> and <kbd class="calibre11">spark.memory.fraction</kbd>. In practice, 54% of the total heap size (276.48 MB) you can allow Spark computing nodes to be used. Now the shuffle memory is calculated as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Shuffle memory= Heap Size * spark.shuffle.safetyFraction * spark.shuffle.memoryFraction</strong>
</pre>
<p class="mce-root">The default values for <kbd class="calibre11">spark.shuffle.safetyFraction</kbd> and <kbd class="calibre11">spark.shuffle.memoryFraction</kbd> are 80% and 20%, respectively. Therefore, in practical, you can use up to <em class="calibre8">0.8*0.2 = 16%</em> of the JVM heap for the shuffle. Finally, unroll memory is the amount of the main memory (in a computing node) that can be utilized by the unroll processes. The calculation goes as follows:</p>
<pre class="calibre19">
<strong class="calibre1">Unroll memory = spark.storage.unrollFraction * spark.storage.memoryFraction * spark.storage.safetyFraction</strong>
</pre>
<p class="mce-root">The above is around 11% of the heap <em class="calibre8">(0.2*0.6*0.9 = 10.8~11%)</em>, that is, 56.32 MB of the Java heap space.</p>
<p>More detailed discussion can be found at <a href="http://spark.apache.org/docs/latest/configuration.html" class="calibre21">http://spark.apache.org/docs/latest/configuration.html.</a></p>
<p class="mce-root">As we will see later, there exist a variety of different cluster managers, some of them also capable of managing other Hadoop workloads or even non-Hadoop applications in parallel to the Spark executors. Note that the executor and driver have bidirectional communication all the time, so network wise they should also be sitting close together.</p>
<div><img class="image-border243" src="img/00167.jpeg"/></div>
<div><strong class="calibre1">Figure 4:</strong> Driver, master, and worker architecture in Spark for cluster</div>
<p class="mce-root">Spark uses the driver (aka the driver program), master, and worker architecture (aka host, slave, or computing nodes). The driver program (or machine) talks to a single coordinator called master node. The master node actually manages all the workers (aka the slave or computing nodes) in which several executors run in parallel in a cluster. It is to be noted that the master is also a computing node having large memory, storage, OS, and underlying computing resources. Conceptually, this architecture can be shown in <strong class="calibre1">Figure 4</strong>. More details will be discussed later in this section.</p>
<p class="mce-root">In a real cluster mode, the cluster manager (aka the resource manager) manages all the resources of computing nodes in a cluster. Generally, firewalls, while adding security to the cluster, also increase the complexity. Ports between system components need to be opened up so that they can talk to each other. For instance, Zookeeper is used by many components for configuration. Apache Kafka, which is a subscribing messaging system, uses Zookeeper for configuring its topics, groups, consumers, and producers. So, client ports to Zookeeper, potentially across the firewall, need to be open.</p>
<p class="mce-root">Finally, the allocation of systems to cluster nodes needs to be considered. For instance, if Apache Spark uses Flume or Kafka, then in-memory channels will be used. Apache Spark should not be competing with other Apache components for memory usage. Depending upon your data flows and memory usage, it might be necessary to have the Spark, Hadoop, Zookeeper, Flume, and other tools on distinct cluster nodes. Alternatively, resource managers such as YARN, Mesos, or Docker, for instance, can be used to tackle this problem as well. In standard Hadoop environments, most likely YARN is there anyway.</p>
<p class="mce-root">The computing nodes that act as workers, or Spark master, will need greater resources than the cluster processing nodes within the firewall. When many Hadoop ecosystem components are deployed on the cluster, all of them will need extra memory on the master server. You should monitor worker nodes for resource usage and adjust in terms of resources and/or application location as necessary. YARN, for instance, is taking care of this.</p>
<p class="mce-root">This section has briefly set the scene for the big data cluster in terms of Apache Spark, Hadoop, and other tools. However, how might the Apache Spark cluster itself, within the big data cluster, be configured? For instance, it is possible to have many types of Spark cluster manager. The next section will examine this and describe each type of Apache Spark cluster manager.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Cluster management</h1>
                
            
            
                
<p class="mce-root">The Spark context can be defined through the Spark configuration object (that is, <kbd class="calibre11">SparkConf</kbd>) and a Spark URL. First, the purpose of the Spark context is to connect the Spark cluster manager in which your Spark jobs will be running. The cluster or resource manager then allocates the required resources across the computing nodes for your application. The second task of the cluster manager is to allocate the executors across the cluster worker nodes so that your Spark jobs get executed. Third, the resource manager also copies the driver program (aka the application JAR file, R code, or Python script) to the computing nodes. Finally, the computing tasks are assigned to the computing nodes by the resource manager.</p>
<p class="mce-root">The following subsections describe the possible Apache Spark cluster manager options available with the current Spark version (that is, Spark 2.1.0 during the writing of this book). To know about the resource management by a resource manager (aka the cluster manager), the following shows how YARN manages all its underlying computing resources. However, this is same for any cluster manager (for example, Mesos or YARN) you use:</p>
<div><img class="image-border244" src="img/00380.jpeg"/></div>
<div><strong class="calibre1">Figure 5:</strong> Resource management using YARN</div>
<p>A detailed discussion can be found at <a href="http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types" class="calibre21">http://spark.apache.org/docs/latest/cluster-overview.html#cluster-manager-types</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Pseudocluster mode (aka Spark local)</h1>
                
            
            
                
<p class="mce-root">As you already know, Spark jobs can be run in local mode. This is sometimes called pseudocluster mode of execution. This is also nondistributed and single JVM-based deployment mode where Spark issues all the execution components, for example, driver program, executor, LocalSchedulerBackend, and master, into your single JVM. This is the only mode where the driver itself is used as an executor. The following figure shows the high-level architecture of the local mode for submitting your Spark jobs:</p>
<div><img class="image-border245" src="img/00214.jpeg"/></div>
<div><strong class="calibre1">Figure 6:</strong> High-level architecture of local mode for Spark jobs (source: <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html" class="calibre44">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-local.html</a>)</div>
<p class="mce-root">Is it too surprising? No, I guess, since you can achieve some short of parallelism as well, where the default parallelism is the number of threads (aka Core used) as specified in the master URL, that is, local [4] for 4 cores/threads and <kbd class="calibre11">local [*]</kbd> for all the available threads. We will discuss this topic later in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Standalone</h1>
                
            
            
                
<p class="mce-root">By specifying a Spark configuration local URL, it is possible to have the application run locally. By specifying <em class="calibre8">local[n]</em>, it is possible to have Spark use <em class="calibre8">n</em> threads to run the application locally. This is a useful development and test option because you can also test some sort of parallelization scenarios but keep all log files on a single machine. The standalone mode uses a basic cluster manager that is supplied with Apache Spark. The spark master URL will be as follows:</p>
<pre class="calibre19">
<strong class="calibre1">spark://&lt;hostname&gt;:7077</strong>
</pre>
<p class="mce-root">Here, <kbd class="calibre11">&lt;hostname&gt;</kbd> is the name of the host on which the Spark master is running. I have specified 7077 as the port, which is the default value, but it is configurable. This simple cluster manager currently only supports <strong class="calibre1">FIFO</strong> (<strong class="calibre1">first in first out</strong>) scheduling. You can contrive to allow concurrent application scheduling by setting the resource configuration options for each application. For example, <kbd class="calibre11">spark.core.max</kbd> is used to share the processor cores between applications. A more detail discussion will be carried out later this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Apache YARN</h1>
                
            
            
                
<p class="mce-root">If the Spark master value is set as YARN-cluster, then the application can be submitted to the cluster and then terminated. The cluster will take care of allocating resources and running tasks. However, if the application master is submitted as YARN-client, then the application stays alive during the life cycle of processing and requests resources from YARN. These are applicable at a larger scale, when integrating with Hadoop YARN. A step-by-step guideline will be provided later in this chapter to configure a single-node YARN cluster for launching your Spark jobs needing minimal resources.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Apache Mesos</h1>
                
            
            
                
<p class="mce-root">Apache Mesos is an open source system for resource sharing across a cluster. It allows multiple frameworks to share a cluster by managing and scheduling resources. It is a cluster manager, which provides isolation using Linux containers, allowing multiple systems such as Hadoop, Spark, Kafka, Storm, and more to share a cluster safely. This is a master-slave based system using Zookeeper for configuration management. This way you can scalae up your Spark jobs to thousands of nodes. For a single master node Mesos cluster, the Spark master URL will be in the following form:</p>
<pre class="calibre19">
<strong class="calibre1">mesos://&lt;hostname&gt;:5050</strong>
</pre>
<p class="mce-root">The consequence of a Spark job submission by specifically using Mesos can be shown visually in the following figure:</p>
<div><img class="image-border246" src="img/00075.jpeg"/></div>
<div><strong class="calibre1">Figure 7:</strong> Mesos in action (image source: <a href="https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html" class="calibre44">https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-architecture.html</a>)</div>
<p class="mce-root">In the preceding figure, where <kbd class="calibre11">&lt;hostname&gt;</kbd> is the hostname of the Mesos master server, and the port is defined as 5050, which is the default Mesos master port (this is configurable). If there are multiple Mesos master servers in a large-scale high availability Mesos cluster, then the Spark master URL would look like the following:</p>
<pre class="calibre19">
<strong class="calibre1">mesos://zk://&lt;hostname&gt;:2181</strong>
</pre>
<p class="mce-root">So, the election of the Mesos master server will be controlled by Zookeeper. The <kbd class="calibre11">&lt;hostname&gt;</kbd> will be the name of a host in the Zookeeper quorum. Also, the port number 2181 is the default master port for Zookeeper.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Cloud-based deployments</h1>
                
            
            
                
<p class="mce-root">There are three different abstraction levels in the cloud computing paradigm:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Infrastructure as a Service</strong> (aka <strong class="calibre1">IaaS</strong>)</li>
<li class="mce-root1"><strong class="calibre1">Platform as a Service</strong> (aka <strong class="calibre1">PaaS</strong>)</li>
<li class="mce-root1"><strong class="calibre1">Software as a Service</strong> (aka <strong class="calibre1">SaaS</strong>)</li>
</ul>
<p class="mce-root">IaaS provides the computing infrastructure through empty virtual machines for your software running as SaaS. This is also true for the Apache Spark on OpenStack.</p>
<p>The advantage of OpenStack is that it can be used among multiple different cloud providers, since it is an open standard and is also based on open source. You even can use OpenStack in a local data center, and transparently and dynamically move workloads between local, dedicated, and public cloud data centers.</p>
<p class="mce-root">PaaS, in contrast, takes away from you the burden of installing and operating an Apache Spark cluster because this is provided as a Service. In other words, you can think it as a layer like what your OS does.</p>
<p>Sometimes, you can even Dockerize your Spark application and deploy on the cloud platform independent manner. However, there is an ongoing discussion whether Docker is IaaS or PaaS, but in our opinion, this is just a form of a lightweight preinstalled virtual machine, so more on the IaaS.</p>
<p class="mce-root">Finally, SaaS is an application layer provided and managed by cloud computing paradigm. To be frank, you won't see or have to worry about the first two layers (IaaS and PaaS).</p>
<p class="mce-root">Google Cloud, Amazon AWS, Digital Ocean, and Microsoft Azure are good examples of cloud computing services that provide these three layers as services. We will show an example of how to deploy your Spark cluster on top of Cloud using Amazon AWS later in this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Deploying the Spark application on a cluster</h1>
                
            
            
                
<p class="mce-root">In this section, we will discuss how to deploy Spark jobs on a computing cluster. We will see how to deploy clusters in three deploy modes: standalone, YARN, and Mesos. The following figure summarizes terms that are needed to refer to cluster concepts in this chapter:</p>
<div><img class="image-border247" src="img/00258.jpeg"/></div>
<div><strong class="calibre1">Figure 8:</strong> Terms that are needed to refer to cluster concepts (source: http://spark.apache.org/docs/latest/cluster-overview.html#glossary)</div>
<p class="mce-root">However, before diving onto deeper, we need to know how to submit a Spark job in general.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Submitting Spark jobs</h1>
                
            
            
                
<p class="mce-root">Once a Spark application is bundled as either a jar file (written in Scala or Java) or a Python file, it can be submitted using the Spark-submit script located under the bin directory in Spark distribution (aka <kbd class="calibre11">$SPARK_HOME/bin</kbd>). According to the API documentation provided in Spark website (<a href="http://spark.apache.org/docs/latest/submitting-applications.html" class="calibre10">http://spark.apache.org/docs/latest/submitting-applications.html</a>), the script takes care of the following:</p>
<ul class="calibre9">
<li class="mce-root1">Setting up the classpath of <kbd class="calibre11">JAVA_HOME</kbd>, <kbd class="calibre11">SCALA_HOME</kbd> with Spark</li>
<li class="mce-root1">Setting up the all the dependencies required to execute the jobs</li>
<li class="mce-root1">Managing different cluster managers</li>
<li class="mce-root1">Finally, deploying models that Spark supports</li>
</ul>
<p class="mce-root">In a nutshell, Spark job submission syntax is as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ spark-submit [options] &lt;app-jar | python-file&gt; [app arguments]</strong>
</pre>
<p class="mce-root">Here, <kbd class="calibre11">[options]</kbd> can be: <kbd class="calibre11">--conf &lt;configuration_parameters&gt; --class &lt;main-class&gt; --master &lt;master-url&gt; --deploy-mode &lt;deploy-mode&gt; ... # other options</kbd></p>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">&lt;main-class&gt;</kbd> is the name of the main class name. This is practically the entry point for our Spark application.</li>
<li class="mce-root1"><kbd class="calibre11">--conf</kbd> signifies all the used Spark parameters and configuration property. The format of a configuration property is a key=value format.</li>
<li class="mce-root1"><kbd class="calibre11">&lt;master-url&gt;</kbd> specifies the master URL for the cluster (for example, <kbd class="calibre11">spark://HOST_NAME:PORT</kbd><em class="calibre8">)</em> for connecting to the master of the Spark standalone cluster, <kbd class="calibre11">local</kbd> for running your Spark jobs locally. By default, it allows you using only one worker thread with no parallelism. The <kbd class="calibre11">local [k]</kbd> can be used for running your Spark job locally with <em class="calibre8">K</em> worker threads. It is to be noted that K is the number of cores on your machine. Finally, if you specify the master with <kbd class="calibre11">local[*]</kbd> for running Spark job locally, you are giving the permission to the <kbd class="calibre11">spark-submit</kbd> script to utilize all the worker threads (logical cores) on your machine have. Finally, you can specify the master as <kbd class="calibre11">mesos://IP_ADDRESS:PORT</kbd> for connecting to the available Mesos cluster. Alternatively, you could specify using <kbd class="calibre11">yarn</kbd> to run your Spark jobs on a YARN-based cluster.</li>
</ul>
<p class="mce-root">For other options on Master URL, please refer to the following figure:</p>
<div><img class="image-border248" src="img/00183.jpeg"/></div>
<div><strong class="calibre1">Figure 9:</strong> Details about the master URLs supported by Spark\</div>
<ul class="calibre9">
<li class="mce-root1"><kbd class="calibre11">&lt;deploy-mode&gt;</kbd> you have to specify this if you want to deploy your driver on the worker nodes (cluster) or locally as an external client (client). Four (4) modes are supported: local, standalone, YARN, and Mesos.</li>
<li class="mce-root1"><kbd class="calibre11">&lt;app-jar&gt;</kbd> is the JAR file you build with with dependencies. Just pass the JAR file while submitting your jobs.</li>
<li class="mce-root1"><kbd class="calibre11">&lt;python-file&gt;</kbd> is the application main source code written using Python. Just pass the <kbd class="calibre11">.py</kbd> file while submitting your jobs.</li>
<li class="mce-root1"><kbd class="calibre11">[app-arguments]</kbd> could be input or output argument specified by an application developer.</li>
</ul>
<p class="mce-root">While submitting the Spark jobs using the spark-submit script, you can specify the main jar of the Spark application (and other related JARS included) using the <kbd class="calibre11">--jars</kbd> option. All the JARS will then be transferred to the cluster. URLs supplied after <kbd class="calibre11">--jars</kbd> must be separated by commas.</p>
<p class="mce-root">However, if you specify the jar using the URLs, it is a good practice to separate the JARS using commas after <kbd class="calibre11">--jars</kbd>. Spark uses the following URL scheme to allow different strategies for disseminating JARS:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">file:</strong> Specifies the absolute paths and <kbd class="calibre11">file:/</kbd></li>
<li class="mce-root1"><strong class="calibre1">hdfs</strong><strong class="calibre1">:</strong>, <strong class="calibre1">http</strong><strong class="calibre1">:</strong>, <strong class="calibre1">https:</strong>, <strong class="calibre1">ftp</strong><strong class="calibre1">:</strong> JARS or any other files will be pull-down from the URLs/URIs you specified as expected</li>
<li class="mce-root1"><strong class="calibre1">local:</strong> A URI starting with <kbd class="calibre11">local:/</kbd> can be used to point local jar files on each computing node</li>
</ul>
<p class="mce-root">It is to be noted that dependent JARs, R codes, Python scripts, or any other associated data files need to be copied or replicated to the working directory for each SparkContext on the computing nodes. This sometimes creates a significant overhead and needs a pretty large amount of disk space. The disk usages increase over time. Therefore, at a certain period of time, unused data objects or associated code files need to be cleaned up. This is, however, quite easy with YARN. YARN handles the cleanup periodically and can be handled automatically. For example, with the Spark standalone mode, automatic cleanup can be configured with the <kbd class="calibre11">spark.worker.cleanup.appDataTtl</kbd> property while submitting the Spark jobs.</p>
<p class="mce-root">Computationally, the Spark is designed such that during the job submission (using <kbd class="calibre11">spark-submit</kbd> script), default Spark config values can be loaded and propagate to Spark applications from a property file. Master node will read the specified options from the configuration file named <kbd class="calibre11">spark-default.conf</kbd>. The exact path is <kbd class="calibre11">SPARK_HOME/conf/spark-defaults.conf</kbd> in your Spark distribution directory. However, if you specify all the parameters in the command line, this will get higher priority and will be used accordingly.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Running Spark jobs locally and in standalone</h1>
                
            
            
                
<p class="mce-root">The examples are shown <a href="part0413.html#C9ROA1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 13</a>, <em class="calibre8">My Name is Bayes, Naive Bayes</em>, and can be made scalable for even larger dataset to solve different purposes. You can package all these three clustering algorithms with all the required dependencies and submit them as Spark job in the cluster. If you don't know how to make a package and create jar files out of the Scala class, you can bundle your application with all the dependencies using SBT or Maven.</p>
<p class="mce-root">According to Spark documentation at <a href="http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management" class="calibre10">http://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management</a>, both the SBT and Maven have assembly plugins for packaging your Spark application as a fat jar. If your application is already bundled with all the dependencies, use the following lines of code to submit your Spark job of k-means clustering, for example (use similar syntax for other classes), for Saratoga NY Homes dataset. For submitting and running a Spark job locally, run the following command on 8 cores:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit  </strong><br class="title-page-name"/><strong class="calibre1">  --class com.chapter15.Clustering.KMeansDemo  </strong><br class="title-page-name"/><strong class="calibre1">  --master local[8]  </strong><br class="title-page-name"/><strong class="calibre1">  KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  </strong><br class="title-page-name"/><strong class="calibre1">  Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">In the preceding code, <kbd class="calibre11">com.chapter15.KMeansDemo</kbd> is the main class file written in Scala. Local [8] is the master URL utilizing eight cores of your machine. <kbd class="calibre11">KMeansDemo-0.1-SNAPSHOT-jar-with-dependencies.jar</kbd> is the application JAR file we just generated by Maven project; <kbd class="calibre11">Saratoga_NY_Homes.txt</kbd> is the input text file for the Saratoga NY Homes dataset. If the application executed successfully, you will find the message including the output in the following figure (abridged):</p>
<div><img class="image-border31" src="img/00015.gif"/></div>
<div><strong class="calibre1">Figure 10:</strong> Spark job output on terminal [local mode]</div>
<p class="mce-root">Now, let's dive into the cluster setup in standalone mode. To install Spark standalone mode, you should place prebuilt versions of Spark with each release on each node on the cluster. Alternatively, you can build it yourself and use it according to the instruction at <a href="http://spark.apache.org/docs/latest/building-spark.html" class="calibre10">http://spark.apache.org/docs/latest/building-spark.html</a>.</p>
<p class="mce-root">To configure the environment as a Spark standalone mode, you will have to provide the prebuilt versions of Spark with the desired version to each node on the cluster. Alternatively, you can build it yourself and use it according to the instruction at <a href="http://spark.apache.org/docs/latest/building-spark.html" class="calibre10">http://spark.apache.org/docs/latest/building-spark.html</a>. Now we will see how to start a standalone cluster manually. You can start a standalone master by executing the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/sbin/start-master.sh</strong>
</pre>
<p class="mce-root">Once started, you should observe the following logs on terminal:</p>
<pre class="calibre19">
<strong class="calibre1">Starting org.apache.spark.deploy.master.Master, logging to &lt;SPARK_HOME&gt;/logs/spark-asif-org.apache.spark.deploy.master.Master-1-ubuntu.out</strong>
</pre>
<p class="mce-root">You should be able to access Spark web UI at <kbd class="calibre11">http://localhost:8080</kbd> by default. Observe the following UI as shown in the following figure:</p>
<div><img class="image-border249" src="img/00321.jpeg"/></div>
<div><strong class="calibre1">Figure 11:</strong> Spark master as standalone</div>
<p class="mce-root">You can change the port number by editing the following parameter:</p>
<pre class="calibre19">
<strong class="calibre1">SPARK_MASTER_WEBUI_PORT=8080</strong>
</pre>
<p class="mce-root">In the <kbd class="calibre11">SPARK_HOME/sbin/start-master.sh</kbd>, just change the port number and then apply the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ sudo chmod +x SPARK_HOME/sbin/start-master.sh.</strong>
</pre>
<p class="mce-root">Alternatively, you can restart the Spark master to effect the preceding change. However, you will have to make a similar change in the <kbd class="calibre11">SPARK_HOME/sbin/start-slave.sh</kbd>.</p>
<p class="mce-root">As you can see here, there are no active workers associated with the master node. Now to create a slave node (aka a worker node or computing node), create workers and connect them to the master using the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/sbin/start-slave.sh &lt;master-spark-URL&gt;</strong>
</pre>
<p class="mce-root">Upon successful completion of the preceding command, you should observe the following logs on terminal:</p>
<pre class="calibre19">
<strong class="calibre1">Starting org.apache.spark.deploy.worker.Worker, logging to &lt;SPARK_HOME&gt;//logs/spark-asif-org.apache.spark.deploy.worker.Worker-1-ubuntu.out </strong>
</pre>
<p class="mce-root">Once you have one of your worker nodes started, you can look at its status on the Spark web UI at <kbd class="calibre11">http://localhost:8081</kbd>. However, if you start another worker node, you can access it's status in the consecutive ports (that is, 8082, 8083, and so on). You should also see the new node listed there, along with its number of CPUs and memory, as shown in the following figure:</p>
<div><img class="image-border250" src="img/00055.jpeg"/></div>
<div><strong class="calibre1">Figure 12:</strong> Spark worker as standalone</div>
<p class="mce-root">Now, if you refresh <kbd class="calibre11">http://localhost:8080</kbd>, you should see that one worker node that is associated with your master node has been added, as shown in the following figure:</p>
<div><img class="image-border251" src="img/00104.jpeg"/></div>
<div><strong class="calibre1">Figure 13:</strong> Spark master has now one worker node as standalone</div>
<p class="mce-root">Finally, as shown in the following figure, these are all the configuration options that can be passed to the master and worker nodes:</p>
<div><img class="image-border252" src="img/00195.jpeg"/></div>
<div><strong class="calibre1">Figure 14:</strong> Configuration options that can be passed to the master and worker nodes (source: <a href="http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually" class="calibre44">http://spark.apache.org/docs/latest/spark-standalone.html#starting-a-cluster-manually</a>)</div>
<p class="cdpalignleft1">Now one of your master node and a worker node are reading and active. Finally, you can submit the same Spark job as standalone rather than local mode using the following commands:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit  <br class="title-page-name"/>--class "com.chapter15.Clustering.KMeansDemo"  <br class="title-page-name"/>--master spark://ubuntu:7077   <br class="title-page-name"/>KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  <br class="title-page-name"/>Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">Once the job started, access Spark web UI at <kbd class="calibre11">http://localhost:80810</kbd> for master and <kbd class="calibre11">http://localhost:8081</kbd> for the worker, you can see the progress of your job as discussed in <a href="part0434.html#CTSK41-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 14</a>, <em class="calibre8">Time to Put Some Order - Cluster Your Data with Spark MLlib</em>.</p>
<p class="mce-root">To summarize this section, we would like to redirect you to the following image (that is, <strong class="calibre1">Figure 15</strong>) that shows the usages of the following shell scripts for launching or stopping your cluster:</p>
<div><img class="image-border253" src="img/00295.jpeg"/></div>
<div><strong class="calibre1">Figure 15:</strong> The usages of the shell scripts for launching or stopping your cluster\</div>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Hadoop YARN</h1>
                
            
            
                
<p class="mce-root">As already discussed, the Apache Hadoop YARN has to main components: a scheduler and an applications manager, as shown in the following figure:</p>
<div><img class="image-border254" src="img/00250.jpeg"/></div>
<div><strong class="calibre1">Figure 16:</strong> Apache Hadoop YARN architecture (blue: system components; yellow and pink: two applications running)</div>
<p class="mce-root">Now that using the scheduler and the applications manager, the following two deploy modes can be configured to launch your Spark jobs on a YARN-based cluster:</p>
<ul class="calibre9">
<li class="mce-root1"><strong class="calibre1">Cluster mode</strong>: In the cluster mode, the Spark driver works within the master process of an application managed by YARN's application manager. Even the client can be terminated or disconnected away when the application has been initiated.</li>
<li class="mce-root1"><strong class="calibre1">Client mode</strong>: In this mode, the Spark driver runs inside the client process. After that, Spark master is used only for requesting computing resources for the computing nodes from YARN (YARN resource manager).</li>
</ul>
<p class="mce-root">In the Spark standalone and Mesos modes, the URL of the master (that is, address) needs to be specified in the <kbd class="calibre11">--master</kbd> parameter. However, in the YARN mode, the address of the resource manager is read from the Hadoop configuration file in your Hadoop setting. Consequently, the <kbd class="calibre11">--master</kbd> parameter is <kbd class="calibre11">yarn</kbd>. Before submitting our Spark jobs, we, however, you need to set up your YARN cluster. The next subsection shows a step-by-step of doing so.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Configuring a single-node YARN cluster</h1>
                
            
            
                
<p class="mce-root">In this subsection, we will see how to set up your YARN cluster before running your Spark jobs on YARN cluster. There are several steps so keep patience and do the following step-by-step:</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 1: Downloading Apache Hadoop</h1>
                
            
            
                
<p class="mce-root">Download the latest distribution from the Hadoop website (<a href="http://hadoop.apache.org/" class="calibre10">http://hadoop.apache.org/</a>). I used the latest stable version 2.7.3 on Ubuntu 14.04 as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$  cd /home</strong><br class="title-page-name"/><strong class="calibre1">$  wget http://mirrors.ibiblio.org/apache/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz</strong>
</pre>
<p class="mce-root">Next, create and extract the package in <kbd class="calibre11">/opt/yarn</kbd> as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$  mkdir –p /opt/yarn</strong><br class="title-page-name"/><strong class="calibre1">$  cd /opt/yarn</strong><br class="title-page-name"/><strong class="calibre1">$  tar xvzf /root/hadoop-2.7.3.tar.gz</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 2: Setting the JAVA_HOME</h1>
                
            
            
                
<p class="mce-root">Refer to the section of Java setup in <a href="part0022.html#KVCC1-21aec46d8593429cacea59dbdcd64e1c" class="calibre10">Chapter 1</a>, <em class="calibre8">Introduction to Scala</em>, for details and apply the same changes.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 3: Creating users and groups</h1>
                
            
            
                
<p class="mce-root">The following <kbd class="calibre11">yarn</kbd>, <kbd class="calibre11">hdfs</kbd>, and <kbd class="calibre11">mapred</kbd> user accounts for <kbd class="calibre11">hadoop</kbd> group can be created as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$  groupadd hadoop</strong><br class="title-page-name"/><strong class="calibre1">$  useradd -g hadoop yarn</strong><br class="title-page-name"/><strong class="calibre1">$  useradd -g hadoop hdfs</strong><br class="title-page-name"/><strong class="calibre1">$  useradd -g hadoop mapred</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 4: Creating data and log directories</h1>
                
            
            
                
<p class="mce-root">To run your Spark jobs using Hadoop, it needs to have the data and the log directories with various permissions. You can use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$  mkdir -p /var/data/hadoop/hdfs/nn</strong><br class="title-page-name"/><strong class="calibre1">$  mkdir -p /var/data/hadoop/hdfs/snn</strong><br class="title-page-name"/><strong class="calibre1">$  mkdir -p /var/data/hadoop/hdfs/dn</strong><br class="title-page-name"/><strong class="calibre1">$  chown hdfs:hadoop /var/data/hadoop/hdfs –R</strong><br class="title-page-name"/><strong class="calibre1">$  mkdir -p /var/log/hadoop/yarn</strong><br class="title-page-name"/><strong class="calibre1">$  chown yarn:hadoop /var/log/hadoop/yarn -R</strong>
</pre>
<p class="mce-root">Now you need to create the log directory where YARN is installed and then set the owner and group as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$  cd /opt/yarn/hadoop-2.7.3</strong><br class="title-page-name"/><strong class="calibre1">$  mkdir logs</strong><br class="title-page-name"/><strong class="calibre1">$  chmod g+w logs</strong><br class="title-page-name"/><strong class="calibre1">$  chown yarn:hadoop . -R</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 5: Configuring core-site.xml</h1>
                
            
            
                
<p class="mce-root">Two properties (that is, <kbd class="calibre11">fs.default.name</kbd> and <kbd class="calibre11">hadoop.http.staticuser.user</kbd>) need to be set to the <kbd class="calibre11">etc/hadoop/core-site.xml</kbd> file. Just copy the following lines of codes:</p>
<pre class="calibre19">
&lt;configuration&gt;<br class="title-page-name"/>       &lt;property&gt;<br class="title-page-name"/>               &lt;name&gt;fs.default.name&lt;/name&gt;<br class="title-page-name"/>               &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;<br class="title-page-name"/>       &lt;/property&gt;<br class="title-page-name"/>       &lt;property&gt;<br class="title-page-name"/>               &lt;name&gt;hadoop.http.staticuser.user&lt;/name&gt;<br class="title-page-name"/>               &lt;value&gt;hdfs&lt;/value&gt;<br class="title-page-name"/>       &lt;/property&gt;<br class="title-page-name"/>&lt;/configuration&gt;
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 6: Configuring hdfs-site.xml</h1>
                
            
            
                
<p class="mce-root">Five properties (that is, <kbd class="calibre11">dfs.replication</kbd> , <kbd class="calibre11">dfs.namenode.name.dir</kbd> , <kbd class="calibre11">fs.checkpoint.dir</kbd> , <kbd class="calibre11">fs.checkpoint.edits.dir</kbd>, and <kbd class="calibre11">dfs.datanode.data.dir</kbd>) need to be set to the <kbd class="calibre11">etc/hadoop/ hdfs-site.xml</kbd> file. Just copy the following lines of codes:</p>
<pre class="calibre19">
&lt;configuration&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;dfs.replication&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;1&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;file:/var/data/hadoop/hdfs/nn&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;fs.checkpoint.dir&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;file:/var/data/hadoop/hdfs/snn&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;fs.checkpoint.edits.dir&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;file:/var/data/hadoop/hdfs/snn&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;file:/var/data/hadoop/hdfs/dn&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/>&lt;/configuration&gt;
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 7: Configuring mapred-site.xml</h1>
                
            
            
                
<p class="mce-root">One property (that is, <kbd class="calibre11">mapreduce.framework.name</kbd>) needs to be set to the <kbd class="calibre11">etc/hadoop/ mapred-site.xml</kbd> file. First, copy and replace the original template file to the <kbd class="calibre11">mapred-site.xml</kbd> as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$  cp mapred-site.xml.template mapred-site.xml</strong>
</pre>
<p class="mce-root">Now, just copy the following lines of codes:</p>
<pre class="calibre19">
&lt;configuration&gt;<br class="title-page-name"/>&lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;mapreduce.framework.name&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;yarn&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/>&lt;/configuration&gt;
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 8: Configuring yarn-site.xml</h1>
                
            
            
                
<p class="mce-root">Two properties (that is, <kbd class="calibre11">yarn.nodemanager.aux-services</kbd> and <kbd class="calibre11">yarn.nodemanager.aux-services.mapreduce.shuffle.class</kbd>) need to be set to the <kbd class="calibre11">etc/hadoop/yarn-site.xml</kbd> file. Just copy the following lines of codes:</p>
<pre class="calibre19">
&lt;configuration&gt;<br class="title-page-name"/>&lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;mapreduce_shuffle&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/> &lt;property&gt;<br class="title-page-name"/>   &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;<br class="title-page-name"/>   &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;<br class="title-page-name"/> &lt;/property&gt;<br class="title-page-name"/>&lt;/configuration&gt;
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 9: Setting Java heap space</h1>
                
            
            
                
<p class="mce-root">To run your Spark job on Hadoop-based YARN cluster, you need to specify enough heap space for the JVM. You need to edit the <kbd class="calibre11">etc/hadoop/hadoop-env.sh</kbd> file. Enable the following properties:</p>
<pre class="calibre19">
HADOOP_HEAPSIZE="500"<br class="title-page-name"/>HADOOP_NAMENODE_INIT_HEAPSIZE="500"
</pre>
<p class="mce-root">Now you also need to edit the <kbd class="calibre11">mapred-env.sh</kbd> file with the following line:</p>
<pre class="calibre19">
HADOOP_JOB_HISTORYSERVER_HEAPSIZE=250
</pre>
<p class="mce-root">Finally, make sure that you have edited <kbd class="calibre11">yarn-env.sh</kbd> to make the changes permanent for Hadoop YARN:</p>
<pre class="calibre19">
JAVA_HEAP_MAX=-Xmx500m<br class="title-page-name"/>YARN_HEAPSIZE=500
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 10: Formatting HDFS</h1>
                
            
            
                
<p class="mce-root">If you want to start your HDFS NameNode, Hadoop needs to initialize the directory where it will store or persist its data for tracking all the metadata for your file system. The formatting will destroy everything and sets up a new file system. Then it uses the values of the parameters set on <kbd class="calibre11">dfs.namenode.name.dir</kbd> in <kbd class="calibre11">etc/hadoop/hdfs-site.xml</kbd>. For doing the format, at first, move to the <kbd class="calibre11">bin</kbd> directory and execute the following commands:</p>
<pre class="calibre19">
<strong class="calibre1">$  su - hdfs</strong><br class="title-page-name"/><strong class="calibre1">$ cd /opt/yarn/hadoop-2.7.3/bin</strong><br class="title-page-name"/><strong class="calibre1">$ ./hdfs namenode -format</strong>
</pre>
<p class="mce-root">If the preceding command executed successfully, you should see the following on your Ubuntu terminal:</p>
<pre class="calibre19">
<strong class="calibre1">INFO common.Storage: Storage directory /var/data/hadoop/hdfs/nn has been successfully formatted</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 11: Starting the HDFS</h1>
                
            
            
                
<p class="mce-root">From the <kbd class="calibre11">bin</kbd> directory in step 10, execute the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ cd ../sbin</strong><br class="title-page-name"/><strong class="calibre1">$ ./hadoop-daemon.sh start namenode</strong>
</pre>
<p class="mce-root">Upon successful execution of the preceding command, you should see the following on your terminal:</p>
<pre class="calibre19">
<strong class="calibre1">starting namenode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-namenode-limulus.out</strong>
</pre>
<p class="mce-root">To start the <kbd class="calibre11">secondarynamenode</kbd> and the <kbd class="calibre11">datanode</kbd>, you should use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ ./hadoop-daemon.sh start secondarynamenode</strong>
</pre>
<p class="mce-root">You should receive the following message on your terminal if the preceding commands succeed:</p>
<pre class="calibre19">
<strong class="calibre1">Starting secondarynamenode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-secondarynamenode-limulus.out</strong>
</pre>
<p class="mce-root">Then use the following command to start the data node:</p>
<pre class="calibre19">
<strong class="calibre1">$ ./hadoop-daemon.sh start datanode</strong>
</pre>
<p class="mce-root">You should receive the following message on your terminal if the preceding commands succeed:</p>
<pre class="calibre19">
<strong class="calibre1">starting datanode, logging to /opt/yarn/hadoop-2.7.3/logs/hadoop-hdfs-datanode-limulus.out</strong>
</pre>
<p class="mce-root">Now make sure that, you check all the services related to those nodes are running use the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ jps</strong>
</pre>
<p class="mce-root">You should observe something like the following:</p>
<pre class="calibre19">
<strong class="calibre1">35180 SecondaryNameNode</strong><br class="title-page-name"/><strong class="calibre1">45915 NameNode</strong><br class="title-page-name"/><strong class="calibre1">656335 Jps</strong><br class="title-page-name"/><strong class="calibre1">75814 DataNode</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 12: Starting YARN</h1>
                
            
            
                
<p class="mce-root">For working with YARN, one <kbd class="calibre11">resourcemanager</kbd> and one node manager have to be started as the user yarn:</p>
<pre class="calibre19">
<strong class="calibre1">$  su - yarn</strong><br class="title-page-name"/><strong class="calibre1">$ cd /opt/yarn/hadoop-2.7.3/sbin</strong><br class="title-page-name"/><strong class="calibre1">$ ./yarn-daemon.sh start resourcemanager</strong>
</pre>
<p class="mce-root">You should receive the following message on your terminal if the preceding commands succeed:</p>
<pre class="calibre19">
<strong class="calibre1">starting resourcemanager, logging to /opt/yarn/hadoop-2.7.3/logs/yarn-yarn-resourcemanager-limulus.out</strong>
</pre>
<p class="mce-root">Tehn execute the following command to start the node manager:</p>
<pre class="calibre19">
$ ./yarn-daemon.sh start nodemanager
</pre>
<p class="mce-root">You should receive the following message on your terminal if the preceding commands succeed:</p>
<pre class="calibre19">
<strong class="calibre1">starting nodemanager, logging to /opt/yarn/hadoop-2.7.3/logs/yarn-yarn-nodemanager-limulus.out</strong>
</pre>
<p class="mce-root">If you want to make sure that every services in those nodes are running, you should use the <kbd class="calibre11">$jsp</kbd> command. Moreover, if you want to stop your resource manager or <kbd class="calibre11">nodemanager</kbd>, use the following <kbd class="calibre11">g</kbd> commands:</p>
<pre class="calibre19">
<strong class="calibre1">$ ./yarn-daemon.sh stop nodemanager</strong><br class="title-page-name"/><strong class="calibre1">$ ./yarn-daemon.sh stop resourcemanager</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 13: Verifying on the web UI</h1>
                
            
            
                
<p class="mce-root">Access <kbd class="calibre11">http://localhost:50070</kbd> to view the status of the NameNode, and access <kbd class="calibre11">http://localhost:8088</kbd> for the resource manager on your browser.</p>
<p>The preceding steps show how to configure a Hadoop-based YARN cluster with only a few nodes. However, if you want to configure your Hadoop-based YARN clusters ranging from a few nodes to extremely large clusters with thousands of nodes, refer to <a href="https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html" class="calibre21">https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/ClusterSetup.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Submitting Spark jobs on YARN cluster</h1>
                
            
            
                
<p class="mce-root">Now that our YARN cluster with the minimum requirement (for executing a small Spark job to be frank) is ready, to launch a Spark application in a cluster mode of YARN, you can use the following submit command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit --classpath.to.your.Class --master yarn --deploy-mode cluster [options] &lt;app jar&gt; [app options]</strong>
</pre>
<p class="mce-root">For running our <kbd class="calibre11">KMeansDemo</kbd>, it should be done like this:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit  <br class="title-page-name"/>    --class "com.chapter15.Clustering.KMeansDemo"  <br class="title-page-name"/>    --master yarn  <br class="title-page-name"/>    --deploy-mode cluster  <br class="title-page-name"/>    --driver-memory 16g  <br class="title-page-name"/>    --executor-memory 4g  <br class="title-page-name"/>    --executor-cores 4  <br class="title-page-name"/>    --queue the_queue  <br class="title-page-name"/>    KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  <br class="title-page-name"/>    Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">The preceding <kbd class="calibre11">submit</kbd> command starts a YARN cluster mode with the default application master. Then <kbd class="calibre11">KMeansDemo</kbd> will be running as a child thread of the application master. For the status updates and for displaying them in the console, the client will periodically poll the application master. When your application (that is, <kbd class="calibre11">KMeansDemo</kbd> in our case) has finished its execution, the client will be exited.</p>
<p>Upon submission of your job, you might want to see the progress using the Spark web UI or Spark history server. Moreover, you should refer to <a href="part0550.html#GCGLC1-21aec46d8593429cacea59dbdcd64e1c" class="calibre21">Chapter 18</a>,﻿ <em class="calibre25">Testing and Debugging Spark</em>) to know how to analyze driver and executor logs.</p>
<p class="mce-root">To launch a Spark application in a client mode, you should use the earlier command, except that you will have to replace the cluster with the client. For those who want to work with Spark shell, use the following in client mode:</p>
<pre class="calibre19">
$ <strong class="calibre1">SPARK_HOME/bin/spark-shell --master yarn --deploy-mode client</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Advance job submissions in a YARN cluster</h1>
                
            
            
                
<p class="mce-root">If you opt for the more advanced way of submitting Spark jobs to be computed in your YARN cluster, you can specify additional parameters. For example, if you want to enable the dynamic resource allocation, make the <kbd class="calibre11">spark.dynamicAllocation.enabled</kbd> parameter true. However, to do so, you also need to specify <kbd class="calibre11">minExecutors</kbd>, <kbd class="calibre11">maxExecutors</kbd>, and <kbd class="calibre11">initialExecutors</kbd> as explained in the following. On the other hand, if you want to enable the shuffling service, set <kbd class="calibre11">spark.shuffle.service.enabled</kbd> as <kbd class="calibre11">true</kbd>. Finally, you could also try specifying how many executor instances will be running using the <kbd class="calibre11">spark.executor.instances</kbd> parameter.</p>
<p class="mce-root">Now, to make the preceding discussion more concrete, you can refer to the following submission command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit   <br class="title-page-name"/>    --class "com.chapter13.Clustering.KMeansDemo"  <br class="title-page-name"/>    --master yarn  <br class="title-page-name"/>    --deploy-mode cluster  <br class="title-page-name"/>    --driver-memory 16g  <br class="title-page-name"/>    --executor-memory 4g  <br class="title-page-name"/>    --executor-cores 4  <br class="title-page-name"/>    --queue the_queue  <br class="title-page-name"/>    --conf spark.dynamicAllocation.enabled=true  <br class="title-page-name"/>    --conf spark.shuffle.service.enabled=true  <br class="title-page-name"/>    --conf spark.dynamicAllocation.minExecutors=1  <br class="title-page-name"/>    --conf spark.dynamicAllocation.maxExecutors=4  <br class="title-page-name"/>    --conf spark.dynamicAllocation.initialExecutors=4  <br class="title-page-name"/>    --conf spark.executor.instances=4  <br class="title-page-name"/>    KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar  <br class="title-page-name"/>    Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">However, the consequence of the preceding job submission script is complex and sometimes nondeterministic. From my previous experience, if you increase the number of partitions from code and the number of executors, then the app will finish faster, which is okay. But if you increase only the executor-cores, the finish time is the same. However, you might expect the time to be lower than initial time. Second, if you launch the preceding code twice, you might expect both jobs to finish in say 60 seconds, but this also might not happen. Often, both jobs might finish after 120 seconds instead. This is a bit weird, isn't it? However, here goes the explanation that would help you understand this scenario.</p>
<p class="mce-root">Suppose you have 16 cores and 8 GB memory on your machine. Now, if you use four executors with one core each, what will happen? Well, when you use an executor, Spark reserves it from YARN and YARN allocates the number of cores (for example, one in our case) and the memory required. The memory is required more than you asked for actually for faster processing. If you ask for 1 GB, it will, in fact, allocate almost 1.5 GB with 500 MB overhead. In addition, it will probably allocate an executor for the driver with probably 1024 MB memory usage (that is, 1 GB).</p>
<p class="mce-root">Sometimes, it doesn't matter how much memory your Spark job wants but how much it reserves. In the preceding example, it will not take 50 MB of the test but around 1.5 GB (including the overhead) per executor. We will discuss how to configure Spark cluster on AWS later this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Apache Mesos</h1>
                
            
            
                
<p class="mce-root">The Mesos master usually replaces the Spark master as the cluster manager (aka the resource manager) when using Mesos. Now, when a driver program creates a Spark job and starts assigning the related tasks for scheduling, Mesos determines which computing nodes handle which tasks. We assume that you have already configured and installed Mesos on your machine.</p>
<p>To get started, following links may be helpful to install Mesos on your machine. <a href="http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/" class="calibre21">http://blog.madhukaraphatak.com/mesos-single-node-setup-ubuntu/,</a> <a href="https://mesos.apache.org/gettingstarted/" class="calibre21">https://mesos.apache.org/gettingstarted/.</a></p>
<p class="mce-root">Depending upon hardware configuration, it takes a while. On my machine (Ubuntu 14.04 64-bit, with Core i7 and 32 GB of RAM), it took 1 hour to complete the build.</p>
<p class="mce-root">To submit and compute your Spark jobs by utilizing the Mesos cluster mode, make sure to check that the Spark binary packages are available in a place accessible by Mesos. Additionally, make sure that your Spark driver program can be configured in such a way that it is automatically connected to Mesos. The second option is installing Spark in the same location as the Mesos slave nodes. Then, you will have to configure the <kbd class="calibre11">spark.mesos.executor.home</kbd> parameter to point the location of Spark distribution. It is to be noted that the default location that could point is the <kbd class="calibre11">SPARK_HOME</kbd>.</p>
<p class="mce-root">When Mesos executes a Spark job on a Mesos worker node (aka computing node) for the first time, the Spark binary packages have to be available on that worker node. This will ensure that the Spark Mesos executor is running in the backend.</p>
<p>The Spark binary packages can be hosted to Hadoop to make them accessible:<br class="calibre23"/>
1. Having the URIs/URLs (including HTTP) via <kbd class="calibre22">http://</kbd>,<br class="calibre23"/>
2. Using the Amazon S3 via <kbd class="calibre22">s3n://</kbd>,<br class="calibre23"/>
3. Using the HDFS via <kbd class="calibre22">hdfs://</kbd>.<br class="calibre23"/>
If you set the <kbd class="calibre22">HADOOP_CONF_DIR</kbd> environment variable, the parameter is usually set as <kbd class="calibre22">hdfs://...</kbd>; otherwise <kbd class="calibre22">file://</kbd>.</p>
<p class="mce-root">You can specify the Master URLs for Mesos as follows:</p>
<ol class="calibre14">
<li value="1" class="mce-root1"><kbd class="calibre11">mesos://host:5050</kbd> for a single-master Mesos cluster, and <kbd class="calibre11">mesos://zk://host1:2181,host2:2181,host3:2181/mesos</kbd> for a multimaster Mesos cluster controlled by the ZooKeeper.</li>
</ol>
<p>For a more detailed discussion, please refer to <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" class="calibre21">http://spark.apache.org/docs/latest/running-on-mesos.html</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Client mode</h1>
                
            
            
                
<p class="mce-root">In this mode, the Mesos framework works in such a way that the Spark job is launched on the client machine directly. It then waits for the computed results, also called the driver output. To interact properly with the Mesos, the driver, however, expects that there are some application-specific configurations specified in <kbd class="calibre11">SPARK_HOME/conf/spark-env.sh</kbd> . To make this happened, modify the <kbd class="calibre11">spark-env.sh.template</kbd> file at <kbd class="calibre11">$SPARK_HOME /conf</kbd>, and before using this client mode, in your <kbd class="calibre11">spark-env.sh</kbd>, set the following environment variables:</p>
<pre class="calibre19">
<strong class="calibre1">$ export MESOS_NATIVE_JAVA_LIBRARY=&lt;path to libmesos.so&gt;</strong>
</pre>
<p class="mce-root">This path is typically <kbd class="calibre11">/usr/local /lib/libmesos.so</kbd> on Ubuntu. On the other hand, on macOS X, the same library is called <kbd class="calibre11">libmesos.dylib</kbd> instead of <kbd class="calibre11">libmesos.so</kbd>:</p>
<pre class="calibre19">
<strong class="calibre1">$ export SPARK_EXECUTOR_URI=&lt;URL of spark-2.1.0.tar.gz uploaded above&gt;</strong>
</pre>
<p class="mce-root">Now, when submitting and starting a Spark application to be executed on the cluster, you will have to pass the Mesos <kbd class="calibre11">:// HOST:PORT</kbd> as the master URL. This is usually done while creating the <kbd class="calibre11">SparkContext</kbd> in your Spark application development as follows:</p>
<pre class="calibre19">
val conf = new SparkConf()              <br class="title-page-name"/>                   .setMaster("mesos://HOST:5050")  <br class="title-page-name"/>                   .setAppName("My app")             <br class="title-page-name"/>                  .set("spark.executor.uri", "&lt;path to spark-2.1.0.tar.gz uploaded above&gt;")<br class="title-page-name"/>val sc = new SparkContext(conf)
</pre>
<p class="mce-root">The second option of doing so is using the <kbd class="calibre11">spark-submit</kbd> script and configure <kbd class="calibre11">spark.executor.uri</kbd> in the <kbd class="calibre11">SPARK_HOME/conf/spark-defaults.conf</kbd> file. When running a shell, the <kbd class="calibre11">spark.executor.uri</kbd> parameter is inherited from <kbd class="calibre11">SPARK_EXECUTOR_URI</kbd>, so it does not need to be redundantly passed in as a system property. Just use the following command to access the client mode from your Spark shell:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-shell --master mesos://host:5050</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Cluster mode</h1>
                
            
            
                
<p class="mce-root">Spark on Mesos also supports cluster mode. If the driver is already launched Spark job (on a cluster) and the computation is also finished, client can access the result (of driver) from the Mesos Web UI. If you have started <kbd class="calibre11">MesosClusterDispatcher</kbd> in your cluster through the <kbd class="calibre11">SPARK_HOME/sbin/start-mesos-dispatcher.sh</kbd> script, you can use the cluster mode.</p>
<p class="mce-root">Again, the condition is that you have to pass the Mesos master URL (for example, <kbd class="calibre11">mesos://host:5050</kbd>) while creating the <kbd class="calibre11">SparkContext</kbd> in your Spark application. Starting the Mesos in the cluster mode also starts the <kbd class="calibre11">MesosClusterDispatcher</kbd> as a daemon running on your host machine.</p>
<p class="mce-root">To gain a more flexible and advanced execution of your Spark jobs, you can also use the <strong class="calibre1">Marathon</strong>. The advantageous thing about using the Marathon is that you can run the <kbd class="calibre11">MesosClusterDispatcher</kbd> with Marathon. If you do that, make sure that the <kbd class="calibre11">MesosClusterDispatcher</kbd> is running in the foreground.</p>
<div><strong class="calibre27">Marathon</strong> is a framework for Mesos that is designed to launch long-running applications, and in Mesosphere, it serves as a replacement for a traditional init system. It has many features that simplify running applications in a clustered environment, such as high-availability, node constraints, application health checks, an API for scriptability and service discovery, and an easy-to-use web user interface. It adds its scaling and self-healing capabilities to the Mesosphere feature set. Marathon can be used to start other Mesos frameworks, and it can also launch any process that can be started in the regular shell. As it is designed for long-running applications, it will ensure that applications it has launched will continue running, even if the slave node(s) they are running on fails. For more information on using Marathon with the Mesosphere, refer to the GitHub page at <a href="https://github.com/mesosphere/marathon" class="calibre21">https://github.com/mesosphere/marathon</a>.</div>
<p class="mce-root">To be more specific, from the client, you can submit a Spark job to your Mesos cluster by using the <kbd class="calibre11">spark-submit</kbd> script and specifying the master URL to the URL of the <kbd class="calibre11">MesosClusterDispatcher</kbd> (for example, <kbd class="calibre11">mesos://dispatcher:7077</kbd>). It goes as follows:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME /bin/spark-class org.apache.spark.deploy.mesos.MesosClusterDispatcher</strong>
</pre>
<p class="mce-root">You can view driver statuses on the Spark cluster web UI. For example, use the following job submission command for doing so:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit   <br class="title-page-name"/>--class com.chapter13.Clustering.KMeansDemo   <br class="title-page-name"/>--master mesos://207.184.161.138:7077    <br class="title-page-name"/>--deploy-mode cluster   <br class="title-page-name"/>--supervise   <br class="title-page-name"/>--executor-memory 20G   <br class="title-page-name"/>--total-executor-cores 100   <br class="title-page-name"/>KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar   <br class="title-page-name"/>Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">Note that JARS or Python files that are passed to Spark-submit should be URIs reachable by Mesos slaves, as the Spark driver doesn't automatically upload local jars. Finally, Spark can run over Mesos in two modes: <em class="calibre8">coarse-grained</em> (default) and <em class="calibre8">fine-grained</em> (deprecated). For more details, please refer to <a href="http://spark.apache.org/docs/latest/running-on-mesos.html" class="calibre10">http://spark.apache.org/docs/latest/running-on-mesos.html</a>.</p>
<p class="mce-root">In a cluster mode, the Spark driver runs on a different machine, that is, driver, master, and computing nodes are different machines. Therefore, if you try adding JARS using <kbd class="calibre11">SparkContext.addJar</kbd>, this will not work. To avoid this issue, make sure that the jar files on the client are also available to <kbd class="calibre11">SparkContext.addJar</kbd>, using the <kbd class="calibre11">--jars</kbd> option in the launch command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/bin/spark-submit --class my.main.Class    <br class="title-page-name"/>     --master yarn    <br class="title-page-name"/>     --deploy-mode cluster    <br class="title-page-name"/>     --jars my-other-jar.jar, my-other-other-jar.jar    <br class="title-page-name"/>     my-main-jar.jar    <br class="title-page-name"/>     app_arg1 app_arg2</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Deploying on AWS</h1>
                
            
            
                
<p class="mce-root">In the previous section, we illustrated how to submit spark jobs in local, standalone, or deploy mode (YARN and Mesos). Here, we are going to show how to run spark application in real cluster mode on AWS EC2. To make our application running on spark cluster mode and for better scalability, we consider the <strong class="calibre1">Amazon Elastic Compute Cloud</strong> (<strong class="calibre1">EC2</strong>) services as IaaS or <strong class="calibre1">Platform as a Service</strong> (<strong class="calibre1">PaaS</strong>). For pricing and related information, please refer to <a href="https://aws.amazon.com/ec2/pricing/" class="calibre10">https://aws.amazon.com/ec2/pricing/</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 1: Key pair and access key configuration</h1>
                
            
            
                
<p class="mce-root">We assume that you have EC2 accounts already created. Well! The first requirement is to create EC2 key pairs and AWS access keys. The EC2 key pair is the private key that you need when you will make a secure connection through SSH to your EC2 server or instances. For making the key, you have to go through AWS console at <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair" class="calibre10">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html#having-ec2-create-your-key-pair</a>. Please refer to the following figure that shows the key-pair creation page for an EC2 account:</p>
<div><img class="image-border255" src="img/00237.jpeg"/></div>
<div><strong class="calibre1">Figure 17:</strong> AWS key-pair generation window</div>
<p class="mce-root">Name it <kbd class="calibre11">aws_key_pair.pem</kbd> once you have downloaded it and save it on your local machine. Then ensure the permission by executing the following command (you should store this file in a secure location for security purpose, say <kbd class="calibre11">/usr/local/key</kbd>):</p>
<pre class="calibre19">
<strong class="calibre1">$ sudo chmod 400 /usr/local/key/aws_key_pair.pem</strong>
</pre>
<p class="mce-root">Now what you need are the AWS access keys and the credentials of your account. These are needed if you want to submit your Spark job to computing nodes from your local machine using the <kbd class="calibre11">spark-ec2</kbd> script. To generate and download the keys, login to your AWS IAM services at <a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey" class="calibre10">http://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey</a>.</p>
<p class="mce-root">Upon the completion of download (that is, <kbd class="calibre11">/usr/local/key</kbd>), you need to set two environment variables in your local machine. Just execute following commands:</p>
<pre class="calibre19">
<strong class="calibre1">$ echo "export AWS_ACCESS_KEY_ID=&lt;access_key_id&gt;" &gt;&gt; ~/.bashrc <br class="title-page-name"/>$ echo " export AWS_SECRET_ACCESS_KEY=&lt;secret_access_key_id&gt;" &gt;&gt; ~/.bashrc <br class="title-page-name"/>$ source ~/.bashrc</strong>
</pre>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 2: Configuring Spark cluster on EC2</h1>
                
            
            
                
<p class="mce-root">Up to Spark 1.6.3 release, Spark distribution (that is, <kbd class="calibre11">/SPARK_HOME/ec2</kbd>) provides a shell script called <strong class="calibre1">spark-ec2</strong> for launching Spark Cluster in EC2 instances from your local machine. This eventually helps in launching, managing, and shutting down the Spark Cluster that you will be using on AWS. However, since Spark 2.x, the same script was moved to AMPLab so that it would be easier to fix bugs and maintain the script itself separately.</p>
<p class="mce-root">The script can be accessed and used from the GitHub repo at <a href="https://github.com/amplab/spark-ec2" class="calibre10">https://github.com/amplab/spark-ec2</a>.</p>
<p>Starting and using a cluster on AWS will cost money. Therefore, it is always a good practice to stop or destroy a cluster when the computation is done. Otherwise, it will incur additional cost to you. For more about AWS pricing, please refer to <a href="https://aws.amazon.com/ec2/pricing/" class="calibre21">https://aws.amazon.com/ec2/pricing/</a>.</p>
<p class="mce-root">You also need to create an IAM Instance profile for your Amazon EC2 instances (Console). For details, refer to <a href="https://github.com/amplab/spark-ec2" class="calibre10">http://docs.aws.amazon.com/codedeploy/latest/userguide/getting-started-create-iam-instance-profile.html</a>. For simplicity, let's download the script and place it under a directory <kbd class="calibre11">ec2</kbd> in Spark home (<kbd class="calibre11">$SPARK_HOME/ec2</kbd>). Once you execute the following command to launch a new instance, it sets up Spark, HDFS, and other dependencies on the cluster automatically:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/spark-ec2 <br class="title-page-name"/>--key-pair=&lt;name_of_the_key_pair&gt; <br class="title-page-name"/>--identity-file=&lt;path_of_the key_pair&gt;  <br class="title-page-name"/>--instance-type=&lt;AWS_instance_type &gt; <br class="title-page-name"/>--region=&lt;region&gt; zone=&lt;zone&gt; <br class="title-page-name"/>--slaves=&lt;number_of_slaves&gt; <br class="title-page-name"/>--hadoop-major-version=&lt;Hadoop_version&gt; <br class="title-page-name"/>--spark-version=&lt;spark_version&gt; <br class="title-page-name"/>--instance-profile-name=&lt;profile_name&gt;<br class="title-page-name"/>launch &lt;cluster-name&gt;</strong>
</pre>
<p class="mce-root">We believe that these parameters are self-explanatory. Alternatively, for more details, please refer to <a href="https://github.com/amplab/spark-ec2#readme" class="calibre10">https://github.com/amplab/spark-ec2#readme</a>.</p>
<div><strong class="calibre27">If you already have a Hadoop cluster and want to deploy spark on it:</strong> If you are using Hadoop-YARN (or even Apache Mesos), running a spark job is relatively easier. Even if you don't use either, Spark can run in standalone mode. Spark runs a driver program, which, in turn, invokes spark executors. This means that you need to tell Spark the nodes where you want your spark daemons to run (in terms of master/slave). In your <kbd class="calibre22">spark/conf</kbd> directory, you can see a file <kbd class="calibre22">slaves</kbd>. Update it to mention all the machines you want to use. You can set up spark from source or use a binary from the website. You always should use the <strong class="calibre27">Fully Qualified Domain Names</strong> (<strong class="calibre27">FQDN</strong>) for all your nodes, and make sure that each of those machines are passwordless SSH accessible from your master node.</div>
<p class="mce-root">Suppose that you have already created and configured an instance profile. Now you are ready to launch the EC2 cluster. For our case, it would be something like the following:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/spark-ec2 <br class="title-page-name"/> --key-pair=aws_key_pair <br class="title-page-name"/> --identity-file=/usr/local/aws_key_pair.pem <br class="title-page-name"/> --instance-type=m3.2xlarge <br class="title-page-name"/>--region=eu-west-1 --zone=eu-west-1a --slaves=2 <br class="title-page-name"/>--hadoop-major-version=yarn <br class="title-page-name"/>--spark-version=2.1.0 <br class="title-page-name"/>--instance-profile-name=rezacsedu_aws<br class="title-page-name"/>launch ec2-spark-cluster-1</strong>
</pre>
<p class="mce-root">The following figure shows your Spark home on AWS:</p>
<div><img class="image-border256" src="img/00063.jpeg"/></div>
<div><strong class="calibre1">Figure 18:</strong> Cluster home on AWS</div>
<p class="mce-root">After the successful completion, spark cluster will be instantiated with two workers (slaves) nodes on your EC2 account. This task, however, sometimes might take half an hour approximately, depending on your Internet speed and hardware configuration. Therefore, you'd love to have a coffee break. Upon successful competition of the cluster setup, you will get the URL of the Spark cluster on the terminal. To make sure if the cluster is really running, check <kbd class="calibre11">https://&lt;master-hostname&gt;:8080</kbd> on your browser, where the <kbd class="calibre11">master-hostname</kbd> is the URL you receive on the terminal. If every think was okay, you will find your cluster running; see cluster home in <strong class="calibre1">Figure 18</strong>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 3: Running Spark jobs on the AWS cluster</h1>
                
            
            
                
<p class="mce-root">Now you master and worker nodes are active and running. This means that you can submit your Spark job to them for computing. However, before that, you need to log in the remote nodes using SSH. For doing so, execute the following command to SSH remote Spark cluster:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/spark-ec2 <br class="title-page-name"/>--key-pair=&lt;name_of_the_key_pair&gt; <br class="title-page-name"/>--identity-file=&lt;path_of_the _key_pair&gt; <br class="title-page-name"/>--region=&lt;region&gt; <br class="title-page-name"/>--zone=&lt;zone&gt;<br class="title-page-name"/>login &lt;cluster-name&gt;  </strong>
</pre>
<p class="mce-root">For our case, it should be something like the following:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/spark-ec2 <br class="title-page-name"/>--key-pair=my-key-pair <br class="title-page-name"/>--identity-file=/usr/local/key/aws-key-pair.pem <br class="title-page-name"/>--region=eu-west-1 <br class="title-page-name"/>--zone=eu-west-1<br class="title-page-name"/>login ec2-spark-cluster-1</strong>
</pre>
<p class="mce-root">Now copy your application, that is, JAR file (or python/R script) to the remote instance (that is, <kbd class="calibre11">ec2-52-48-119-121.eu-west-1.compute.amazonaws.com</kbd> in our case) by executing the following command (in a new terminal):</p>
<pre class="calibre19">
<strong class="calibre1">$ scp -i /usr/local/key/aws-key-pair.pem /usr/local/code/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/</strong>
</pre>
<p class="mce-root">Then you need to copy your data (<kbd class="calibre11">/usr/local/data/Saratoga_NY_Homes.txt</kbd>, in our case) to the same remote instance by executing the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ scp -i /usr/local/key/aws-key-pair.pem /usr/local/data/Saratoga_NY_Homes.txt ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/</strong>
</pre>
<p>Note that if you have already configured HDFS on your remote machine and put your code/data file, you don't need to copy the JAR and data files to the slaves; the master will do it automatically.</p>
<p class="mce-root">Well done! You are almost done! Now, finally, you will have to submit your Spark job to be computed by the slaves or worker nodes. To do so, just execute the following commands:</p>
<pre class="calibre19">
<strong class="calibre1">$SPARK_HOME/bin/spark-submit <br class="title-page-name"/> --class com.chapter13.Clustering.KMeansDemo <br class="title-page-name"/>--master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 <br class="title-page-name"/>file:///home/ec2-user/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar <br class="title-page-name"/>file:///home/ec2-user/Saratoga_NY_Homes.txt</strong>
</pre>
<p>Place your input file under <kbd class="calibre22">file:///input.txt</kbd> if HDFS is not set on your machine.</p>
<p class="mce-root">If you have already put your data on HDFS, you should issue the submit command something like following:</p>
<pre class="calibre19">
<strong class="calibre1">$SPARK_HOME/bin/spark-submit <br class="title-page-name"/> --class com.chapter13.Clustering.KMeansDemo <br class="title-page-name"/>--master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 <br class="title-page-name"/>hdfs://localhost:9000/KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar <br class="title-page-name"/>hdfs://localhost:9000//Saratoga_NY_Homes.txt</strong>
</pre>
<p class="mce-root">Upon successful completion of the job computation, you are supposed to see the status and related statistics of your job at port 8080.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Step 4: Pausing, restarting, and terminating the Spark cluster</h1>
                
            
            
                
<p class="mce-root">When your computation is done, it is better to stop your cluster to avoid additional cost. To stop your clusters, execute the following commands from your local machine:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/ec2/spark-ec2 --region=&lt;ec2-region&gt; stop &lt;cluster-name&gt;</strong>
</pre>
<p class="mce-root">For our case, it would be the following:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/ec2/spark-ec2 --region=eu-west-1 stop ec2-spark-cluster-1</strong>
</pre>
<p class="mce-root">To restart the cluster later on, execute the following command:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/ec2/spark-ec2 -i &lt;key-file&gt; --region=&lt;ec2-region&gt; start &lt;cluster-name&gt;</strong>
</pre>
<p class="mce-root">For our case, it will be something like the following:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/ec2/spark-ec2 --identity-file=/usr/local/key/-key-pair.pem --region=eu-west-1 start ec2-spark-cluster-1</strong>
</pre>
<p class="mce-root">Finally, to terminate your Spark cluster on AWS we use the following code:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME/ec2/spark-ec2 destroy &lt;cluster-name&gt;</strong>
</pre>
<p class="mce-root">In our case, it would be the following:</p>
<pre class="calibre19">
<strong class="calibre1">$ SPARK_HOME /spark-ec2 --region=eu-west-1 destroy ec2-spark-cluster-1</strong>
</pre>
<p class="mce-root">Spot instances are great for reducing AWS costs, sometimes cutting instance costs by a whole order of magnitude. A step-by-step guideline using this facility can be accessed at <a href="http://blog.insightdatalabs.com/spark-cluster-step-by-step/" class="calibre10">http://blog.insightdatalabs.com/spark-cluster-step-by-step/</a>.</p>
<p class="mce-root">Sometimes, it's difficult to move large dataset, say 1 TB of raw data file. In that case, and if you want your application to scale up even more for large-scale datasets, the fastest way of doing so is loading them from Amazon S3 or EBS device to HDFS on your nodes and specifying the data file path using <kbd class="calibre11">hdfs://</kbd>.</p>
<p>The data files or any other files (data, jars, scripts, and so on) can be hosted on HDFS to make them highly accessible:<br class="calibre23"/>
1. Having the URIs/URLs (including HTTP) via <kbd class="calibre22">http://</kbd><br class="calibre23"/>
2. Using the Amazon S3 via <kbd class="calibre22">s3n://</kbd><br class="calibre23"/>
3. Using the HDFS via <kbd class="calibre22">hdfs://</kbd><br class="calibre23"/>
If you set <kbd class="calibre22">HADOOP_CONF_DIR</kbd> environment variable, the parameter is usually set as <kbd class="calibre22">hdfs://...</kbd>; otherwise <kbd class="calibre22">file://</kbd>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title" id="calibre_pb_0">Summary</h1>
                
            
            
                
<p class="mce-root">In this chapter, we discussed how Spark works in a cluster mode with its underlying architecture. You also saw how to deploy a full Spark application on a cluster. You saw how to deploy cluster for running Spark application in different cluster modes such as local, standalone, YARN, and Mesos. Finally, you saw how to configure Spark cluster on AWS using EC2 script. We believe that this chapter will help you to gain some good understanding of Spark. Nevertheless, due to page limitation, we could not cover many APIs and their underlying functionalities.</p>
<p class="mce-root">If you face any issues, please don't forget to report this to Spark user mailing list at <kbd class="calibre11">user@spark.apache.org</kbd>. Before doing so, make sure that you have subscribed to it. In the next chapter, you will see how to test and debug Spark applications.</p>


            

            
        
    </body></html>