- en: Declaring the Objectives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter introduces and explains (yet again, from a developer's perspective)
    the basic objectives behind statistics for data science and introduces the reader
    to the important terms and key concepts (with explanations and examples) that
    are used throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ve broken things down into the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: A primer on the key objectives of data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bringing statistics into data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common terminologies used with statistics and data science
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Key objectives of data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in [Chapter 1](7f3dc6b3-d483-4ffc-b330-22b36da9bdc7.xhtml), *Transitioning
    from Data Developer to Data Scientist*, the idea of how data science is defined
    is a matter of opinion.
  prefs: []
  type: TYPE_NORMAL
- en: 'I personally like the explanation that data science is a progression or, even
    better, an evolution of thought or steps, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f97c563f-aee0-496d-ae4b-68c4eee48a35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This data science evolution (depicted in the preceding figure) consists of
    a series of steps or phases that a data scientist tracks, comprising the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring and visualizing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing (data) and/or applying machine learning (to data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding (or planning) based on acquired insight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although a progression or evolution implies a sequential journey, in practice,
    this is an extremely fluid process; each of the phases may inspire the data scientist
    to reverse and repeat one or more of the phases until they are satisfied. In other
    words, all or some phases of the process may be repeated until the data scientist
    determines that the desired outcome is reached.
  prefs: []
  type: TYPE_NORMAL
- en: For example, after a careful review of a generated visualization (during the
    *Exploring and visualizing data* phase), one may determine that additional processing
    of the data is required or that additional data needs to be collected before any
    reasonable analysis or learning could be of value.
  prefs: []
  type: TYPE_NORMAL
- en: You might loosely compare the data science process to the agile software development
    mythology where a developer performs various tasks, the results are analyzed,
    more work is done, the work is again reviewed, and the process is repeated until
    the desired results or outcomes are obtained.
  prefs: []
  type: TYPE_NORMAL
- en: Let's explain each of the phases of the data science evolution.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This should be somewhat obvious—without (at least some) data, we cannot perform
    any of the subsequent steps (although one might argue the point of inference,
    that would be inappropriate. There is no magic in data science. We, as data scientists,
    don't make something from anything. Inference (which we'll define later in this
    chapter) requires at least some data to begin with.
  prefs: []
  type: TYPE_NORMAL
- en: Some new concepts for collecting data include the fact that data can be collected
    from ample of sources, and the number and types of data sources continue to grow
    daily. In addition, how data is collected might require a perspective new to a
    data developer; data for data science isn't always sourced from a relational database,
    rather from machine-generated logging files, online surveys, performance statistics,
    and so on; again, the list is ever evolving.
  prefs: []
  type: TYPE_NORMAL
- en: Another point to ponder—collecting data also involves supplementation. For example,
    a data scientist might determine that he or she needs to be adding additional
    demographics to a particular pool of application data previously collected, processed,
    and reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The processing (or transformation) of data is where the data scientist's programming
    skills will come in to play (although you can often find a data scientist performing
    some sort of processing in other steps, like collecting, visualizing, or learning).
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that there are many aspects of processing that occur within data
    science. The most common are formatting (and reformatting), which involves activities
    such as mechanically setting data types, aggregating values, reordering or dropping
    columns, and so on, cleansing (or addressing the quality of the data), which is
    solving for such things as default or missing values, incomplete or inapposite
    values, and so on, and profiling, which adds context to the data by creating a
    statistical understanding of the data.
  prefs: []
  type: TYPE_NORMAL
- en: The processing to be completed on the data can be simple (for example, it can
    be a very simple and manual event requiring repetitious updates to data in an
    MS Excel worksheet), or complex (as with the use of programming languages such
    as R or Python), or even more sophisticated (as when processing logic is coded
    into routines that can then be scheduled and rerun automatically on new populations
    of data).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring and visualizing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: During this phase or step in the overall data science pipeline process, the
    data scientist will use various methods to dig deeper into the data. Typically,
    several graphical representations are created (again, either manually or through
    a programming script or tool) emphasizing or validating a data scientist's observation,
    a particular point, or belief. This is a significant step in the overall data
    science process as the data scientist may come to understand that additional processing
    should be done on the data, or additional data needs to be collected, or perhaps
    the original theories appear to be validated. These findings will be cause for
    a pause, reflecting on the next steps that need to be taken. Should the data scientist
    proceed with the formal analysis process, perhaps creating a predictive model
    for automated learning? Or, should the scientist revisit a previous step, collecting
    additional (or different) data for processing?
  prefs: []
  type: TYPE_NORMAL
- en: '**Data visualization** is a key technique permitting data scientists to perform
    analyses, identify key trends or events, and make more confident decisions much
    more quickly.'
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the data and/or applying machine learning to the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this phase, quite a bit of analysis takes place as the data scientist (driven
    by a high level of scientific curiosity and experience) attempts to shape a story
    based upon an observation or the interpretation of their understanding of the
    data (up to this point). The data scientist continues to slice and dice the data,
    using analytics or BI packages—such as Tableau or Pentaho or an open source solution
    such as R or Python—to create a concrete data storyline. Once again, based on
    these analysis results, the data scientist may elect to again go back to a prior
    phase, pulling new data, processing and reprocessing, and creating additional
    visualizations. At some point, when appropriate progress has been made, the data
    scientist may decide that the data is at such point where data analysis can begin.
    Machine learning (defined further later in this chapter) has evolved over time
    from being more of an exercise in pattern recognition to now being defined as
    utilizing a selected statistical method to dig deeper, using the data and results
    of the analysis of this phase to learn and make a prediction, on the project data.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of a data scientist to extract a quantitative result from data through
    machine learning and express it as something that everyone (not just other data
    scientists) can understand immediately is an invaluable skill, and we will talk
    more about this throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding (or planning) based upon acquired insight
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this step, the data scientist hopes to obtain value from their efforts in
    the form of an insight. The insight is gained by performing the preceding described
    phases, aimed at gaining an understanding of a particular situation or phenomena.
    The idea is that this insight can then be used as input to make better decisions.
  prefs: []
  type: TYPE_NORMAL
- en: A fun example that illustrates a creative use of insights mined from data is
    the (as of this writing, experimental) Roztayger personality match process powered
    by IBM Watson. Using either your Facebook or Twitter feeds (or you can enter a
    short bio), Watson will, on-the-fly, perform an analysis of your personality.
    The results are interesting and pretty spot on, and these insights are then used
    to suggest designer labels that may best suit you and your personal style.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this feature at [http://roztayger.com/match](http://roztayger.com/match).
    The Personality Insights service extracts personality characteristics based on
    how a person writes. You can use the service to match individuals to other individuals,
    opportunities, and products, or tailor their experience with personalized messaging
    and recommendations. Characteristics include the Big 5 Personality Traits, Values,
    and Needs. At least 1,200 words of input text are recommended when using this
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the (real-time) data science analysis is complete, the aforementioned
    website not only provides its recommendations but also shares the data behind
    its insights, showing an easy-to-understand, well-organized tabular view of the
    results, and an eye-catching visualization as well, as shown in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91dbe2fb-1ece-4522-88c4-0dd8a8d7096b.png)'
  prefs: []
  type: TYPE_IMG
- en: This illustrates another key aspect of this phase of the data science progression,
    that is, once the data scientist identifies an insight, he must clearly present
    and communicate those data insights/findings.
  prefs: []
  type: TYPE_NORMAL
- en: Thinking like a data scientist
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we've already stressed, agreement on the concepts of what a data scientist
    is and does are still just emerging. The entire field of data science is at best
    roughly defined. Transitioning to data science is perhaps as much about finding
    an organization or group whose needs match your skills as it is about understanding
    what skills and concepts are involved in data science and then working towards
    developing those skills.
  prefs: []
  type: TYPE_NORMAL
- en: Just as a data developer stays up to date and knowledgeable on the trends and
    tools in and around the manipulation of and access to data, so should the would-be
    data scientist.
  prefs: []
  type: TYPE_NORMAL
- en: Bringing statistics into data science
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending on your sources and individual beliefs, you may say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Statistics is data science, and data science is statistics*.'
  prefs: []
  type: TYPE_NORMAL
- en: To clarify this, note that there is a popular opinion that statistics might
    be thought of as a study or process that covers the collection, analysis, interpretation,
    presentation, and organization of data. As you can see, that definition is pretty
    similar to the data science process we described in the previous section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Digging deeper into this topic, one will find that statistics always involves
    (or a collection of) techniques or approaches used to help analyze and present
    data (again, this understanding could also be used to describe data science).
  prefs: []
  type: TYPE_NORMAL
- en: It is commonly accepted that the terms data science and statistics have the
    same meaning, at least within some circles. Again, alignment of terms and concepts
    is still evolving among data scientists.
  prefs: []
  type: TYPE_NORMAL
- en: Common terminology
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Based upon personal experience, research, and various industry experts'' advice,
    someone delving into the art of data science should take every opportunity to
    understand and gain experience as well as proficiency with the following list
    of common data science terms:'
  prefs: []
  type: TYPE_NORMAL
- en: Statistical population
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: False positives
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data mining
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Munging and wrangling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assessment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Outlier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence interval
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical population
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can perhaps think of a statistical population as a recordset (or a set of
    records). This set or group of records will be of similar items or events that
    are of interest to the data scientist for some experiment.
  prefs: []
  type: TYPE_NORMAL
- en: For a data developer, a population of data may be a recordset of all sales transactions
    for a month, and the interest might be reporting to the senior management of an
    organization which products are the fastest sellers and at which time of the year.
  prefs: []
  type: TYPE_NORMAL
- en: For a data scientist, a population may be a recordset of all emergency room
    admissions during a month, and the area of interest might be to determine the
    statistical demographics for emergency room use.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the terms **statistical population** and **statistical model** are
    or can be used interchangeably. Once again, data scientists continue to evolve
    with their alignment on their use of common terms.
  prefs: []
  type: TYPE_NORMAL
- en: Another key point concerning statistical populations is that the recordset may
    be a group of (actually) existing objects or a hypothetical group of objects.
    Using the preceding example, you might draw a comparison of actual objects as
    those actual sales transactions recorded for the month while the hypothetical
    objects as sales transactions are expected, forecast, or presumed (based upon
    observations or experienced assumptions or other logic) to occur during a month.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, through the use of statistical inference (explained later in this chapter),
    the data scientist can select a portion or subset of the recordset (or population)
    with the intention that it will represent the total population for a particular
    area of interest. This subset is known as a **statistical sample**.
  prefs: []
  type: TYPE_NORMAL
- en: If a sample of a population is chosen accurately, characteristics of the entire
    population (that the sample is drawn from) can be estimated from the corresponding
    characteristics of the sample.
  prefs: []
  type: TYPE_NORMAL
- en: Probability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Probability is concerned with the laws governing random events.
  prefs: []
  type: TYPE_NORMAL
- en: -www.britannica.com
  prefs: []
  type: TYPE_NORMAL
- en: When thinking of probability, you think of possible upcoming events and the
    likelihood of them actually occurring. This compares to a statistical thought
    process that involves analyzing the frequency of past events in an attempt to
    explain or make sense of the observations. In addition, the data scientist will
    associate various individual events, studying the relationship of these events.
    How these different events relate to each other governs the methods and rules
    that will need to be followed when we're studying their probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: A probability distribution is a table that is used to show the probabilities
    of various outcomes in a sample population or recordset.
  prefs: []
  type: TYPE_NORMAL
- en: False positives
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The idea of false positives is a very important statistical (data science) concept.
    A false positive is a mistake or an errored result. That is, it is a scenario
    where the results of a process or experiment indicate a fulfilled or true condition
    when, in fact, the condition is not true (not fulfilled). This situation is also
    referred to by some data scientists as a false alarm and is most easily understood
    by considering the idea of a recordset or statistical population (which we discussed
    earlier in this section) that is determined not only by the accuracy of the processing
    but by the characteristics of the sampled population. In other words, the data
    scientist has made errors during the statistical process, or the recordset is
    a population that does not have an appropriate sample (or characteristics) for
    what is being investigated.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What developer at some point in his or her career, had to create a sample or
    test data? For example, I've often created a simple script to generate a random
    number (based upon the number of possible options or choices) and then used that
    number as the selected option (in my test recordset). This might work well for
    data development, but with statistics and data science, this is not sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: To create sample data (or a sample population), the data scientist will use
    a process called **statistical inference**, which is the process of deducing options
    of an underlying distribution through analysis of the data you have or are trying
    to generate for. The process is sometimes called **inferential statistical analysis**
    and includes testing various hypotheses and deriving estimates.
  prefs: []
  type: TYPE_NORMAL
- en: When the data scientist determines that a recordset (or population) should be
    larger than it actually is, it is assumed that the recordset is a sample from
    a larger population, and the data scientist will then utilize statistical inference
    to make up the difference.
  prefs: []
  type: TYPE_NORMAL
- en: The data or recordset in use is referred to by the data scientist as the observed
    data. Inferential statistics can be contrasted with descriptive statistics, which
    is only concerned with the properties of the observed data and does not assume
    that the recordset came from a larger population.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is a process or method (selected by the data scientist as the best
    fit technique for the experiment at hand) used for determining the relationships
    among variables. If you're a programmer, you have a certain understanding of what
    a variable is, but in statistics, we use the term differently. Variables are determined
    to be either dependent or independent.
  prefs: []
  type: TYPE_NORMAL
- en: An independent variable (also known as a **predictor**) is the one that is manipulated
    by the data scientist in an effort to determine its relationship with a dependent
    variable. A dependent variable is a variable that the data scientist is measuring.
  prefs: []
  type: TYPE_NORMAL
- en: It is not uncommon to have more than one independent variable in a data science
    progression or experiment.
  prefs: []
  type: TYPE_NORMAL
- en: More precisely, regression is the process that helps the data scientist comprehend
    how the typical value of the dependent variable (or criterion variable) changes
    when any one or more of the independent variables is varied while the other independent
    variables are held fixed.
  prefs: []
  type: TYPE_NORMAL
- en: Fitting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fitting is the process of measuring how well a statistical model or process
    describes a data scientist's observations pertaining to a recordset or experiment.
    These measures will attempt to point out the discrepancy between observed values
    and probable values. The probable values of a model or process are known as a
    distribution or a probability distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, a probability distribution fitting (or distribution fitting) is when
    the data scientist fits a probability distribution to a series of data concerning
    the repeated measurement of a variable phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: The object of a data scientist performing a distribution fitting is to predict
    the probability or to forecast the frequency of, the occurrence of the phenomenon
    at a certain interval.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most common uses of fitting is to test whether two samples are drawn
    from identical distributions.
  prefs: []
  type: TYPE_NORMAL
- en: There are numerous probability distributions a data scientist can select from.
    Some will fit better to the observed frequency of the data than others will. The
    distribution giving a close fit is supposed to lead to good predictions; therefore,
    the data scientist needs to select a distribution that suits the data well.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we explained how variables in your data can be either independent or
    dependent. Another type of variable definition is a categorical variable. This
    type of variable is one that can take on one of a limited, and typically fixed,
    number of possible values, thus assigning each individual to a particular category.
  prefs: []
  type: TYPE_NORMAL
- en: Often, the collected data's meaning is unclear. Categorical data is a method
    that a data scientist can use to put meaning to the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if a numeric variable is collected (let''s say the values found
    are 4, 10, and 12), the meaning of the variable becomes clear if the values are
    categorized. Let''s suppose that based upon an analysis of how the data was collected,
    we can group (or categorize) the data by indicating that this data describes university
    students, and there is the following number of players:'
  prefs: []
  type: TYPE_NORMAL
- en: 4 tennis players
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 10 soccer players
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 12 football players
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, because we grouped the data into categories, the meaning becomes clear.
  prefs: []
  type: TYPE_NORMAL
- en: Some other examples of categorized data might be individual pet preferences
    (grouped by the type of pet), or vehicle ownership (grouped by the style of a
    car owned), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: So, categorical data, as the name suggests, is data grouped into some sort of
    category or multiple categories. Some data scientists refer to categories as sub-populations
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Categorical data can also be data that is collected as a yes or no answer. For
    example, hospital admittance data may indicate that patients either smoke or do
    not smoke.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Statistical classification of data is the process of identifying which category
    (discussed in the previous section) a data point, observation, or variable should
    be grouped into. The data science process that carries out a classification process
    is known as a **classifier**.
  prefs: []
  type: TYPE_NORMAL
- en: Determining whether a book is fiction or non-fiction is a simple example classification.
    An analysis of data about restaurants might lead to the classification of them
    among several genres.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Clustering is the process of dividing up the data occurrences into groups or
    homogeneous subsets of the dataset, not a predetermined set of groups as in classification
    (described in the preceding section) but groups identified by the execution of
    the data science process based upon similarities that it found among the occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Objects in the same group (a group is also referred to as a cluster) are found
    to be more analogous (in some sense or another) to each other than to those objects
    found in other groups (or found in other clusters). The process of clustering
    is found to be very common in exploratory data mining and is also a common technique
    for statistical data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Statistical comparison
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simply put, when you hear the term statistical comparison, one is usually referring
    to the act of a data scientist performing a process of analysis to view the similarities
    or variances of two or more groups or populations (or recordsets).
  prefs: []
  type: TYPE_NORMAL
- en: As a data developer, one might be familiar with various utilities such as FC
    Compare, UltraCompare, or WinDiff, which aim to provide the developer with a line-by-line
    comparison of the contents of two or more (even binary) files.
  prefs: []
  type: TYPE_NORMAL
- en: In statistics (data science), this process of comparing is a statistical technique
    to compare populations or recordsets. In this method, a data scientist will conduct
    what is called an **Analysis of Variance** (**ANOVA**), compare categorical variables
    (within the recordsets), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: ANOVA is an assortment of statistical methods that are used to analyze the differences
    among group means and their associated procedures (such as variations among and
    between groups, populations, or recordsets). This method eventually evolved into
    the Six Sigma dataset comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Coding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Coding or statistical coding is again a process that a data scientist will use
    to prepare data for analysis. In this process, both quantitative data values (such
    as income or years of education) and qualitative data (such as race or gender)
    are categorized or coded in a consistent way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coding is performed by a data scientist for various reasons such as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: More effective for running statistical models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Computers understand the variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accountability--so the data scientist can run models blind, or without knowing
    what variables stand for, to reduce programming/author bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can imagine the process of coding as the means to transform data into a
    form required for a system or application.
  prefs: []
  type: TYPE_NORMAL
- en: Distributions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The distribution of a statistical recordset (or of a population) is a visualization
    showing all the possible values (or sometimes referred to as intervals) of the
    data and how often they occur. When a distribution of categorical data (which
    we defined earlier in this chapter) is created by a data scientist, it attempts
    to show the number or percentage of individuals in each group or category.
  prefs: []
  type: TYPE_NORMAL
- en: Linking an earlier defined term with this one, a probability distribution, stated
    in simple terms, can be thought of as a visualization showing the probability
    of occurrence of different possible outcomes in an experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Data mining
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](7f3dc6b3-d483-4ffc-b330-22b36da9bdc7.xhtml), *Transitioning from
    Data Developer to Data Scientist*, we said, with data mining, one is usually more
    absorbed in the data relationships (or the potential relationships between points
    of data, sometimes referred to as variables) and cognitive analysis.
  prefs: []
  type: TYPE_NORMAL
- en: To further define this term, we can mention that data mining is sometimes more
    simply referred to as knowledge discovery or even just discovery, based upon processing
    through or analyzing data from new or different viewpoints and summarizing it
    into valuable insights that can be used to increase revenue, cuts costs, or both.
  prefs: []
  type: TYPE_NORMAL
- en: Using software dedicated to data mining is just one of several analytical approaches
    to data mining. Although there are tools dedicated to this purpose (such as IBM
    Cognos BI and Planning Analytics, Tableau, SAS, and so on.), data mining is all
    about the analysis process finding correlations or patterns among dozens of fields
    in the data and that can be effectively accomplished using tools such as MS Excel
    or any number of open source technologies.
  prefs: []
  type: TYPE_NORMAL
- en: A common technique to data mining is through the creation of custom scripts
    using tools such as R or Python. In this way, the data scientist has the ability
    to customize the logic and processing to their exact project needs.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A statistical decision tree uses a diagram that looks like a tree. This structure
    attempts to represent optional decision paths and a predicted outcome for each
    path selected. A data scientist will use a decision tree to support, track, and
    model decision making and their possible consequences, including chance event
    outcomes, resource costs, and utility. It is a common way to display the logic
    of a data science process.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is one of the most intriguing and exciting areas of data science.
    It conjures all forms of images around artificial intelligence which includes
    Neural Networks, **Support Vector Machines** (**SVMs**), and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Fundamentally, we can describe the term machine learning as a method of training
    a computer to make or improve predictions or behaviors based on data or, specifically,
    relationships within that data. Continuing, machine learning is a process by which
    predictions are made based upon recognized patterns identified within data, and
    additionally, it is the ability to continuously learn from the data's patterns,
    therefore continuingly making better predictions.
  prefs: []
  type: TYPE_NORMAL
- en: It is not uncommon for someone to mistake the process of machine learning for
    data mining, but data mining focuses more on exploratory data analysis and is
    known as **unsupervised learning**.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning can be used to learn and establish baseline behavioral profiles
    for various entities and then to find meaningful anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the exciting part: the process of machine learning (using data relationships
    to make predictions) is known as **predictive analytics**.'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive analytics allow the data scientists to produce reliable, repeatable
    decisions and results and uncover hidden insights through learning from historical
    relationships and trends in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Munging and wrangling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The terms **munging** and **wrangling** are buzzwords or jargon meant to describe
    one's efforts to affect the format of data, recordset, or file in some way in
    an effort to prepare the data for continued or otherwise processing and/or evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: With data development, you are most likely familiar with the idea of **Extract**,
    **Transform**, and** Load** (**ETL**). In somewhat the same way, a data developer
    may mung or wrangle data during the transformation steps within an ETL process.
  prefs: []
  type: TYPE_NORMAL
- en: Common munging and wrangling may include removing punctuation or HTML tags,
    data parsing, filtering, all sorts of transforming, mapping, and tying together
    systems and interfaces that were not specifically designed to interoperate. Munging
    can also describe the processing or filtering of raw data into another form, allowing
    for more convenient consumption of the data elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: Munging and wrangling might be performed multiple times within a data science
    process and/or at different steps in the evolving process. Sometimes, data scientists
    use munging to include various data visualization, data aggregation, training
    a statistical model, as well as much other potential work. To this point, munging
    and wrangling may follow a flow beginning with extracting the data in a raw form,
    performing the munging using various logic, and lastly, placing the resulting
    content into a structure for use.
  prefs: []
  type: TYPE_NORMAL
- en: Although there are many valid options for munging and wrangling data, preprocessing
    and manipulation, a tool that is popular with many data scientists today is a
    product named **Trifecta**, which claims that it is the number one (data) wrangling
    solution in many industries.
  prefs: []
  type: TYPE_NORMAL
- en: Trifecta can be downloaded for your personal evaluation from [https://www.trifacta.com/](https://www.trifacta.com/).
    Check it out!
  prefs: []
  type: TYPE_NORMAL
- en: Visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main point (although there are other goals and objectives) when leveraging
    a data visualization technique is to make something complex appear simple. You
    can think of visualization as any technique for creating a graphic (or similar)
    to communicate a message.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other motives for using data visualization include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: To explain the data or put the data in context (which is to highlight demographical
    statistics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To solve a specific problem (for example, identifying problem areas within a
    particular business model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To explore the data to reach a better understanding or add clarity (such as
    what periods of time do this data span?)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To highlight or illustrate otherwise invisible data (such as isolating outliers
    residing in the data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To predict, such as potential sales volumes (perhaps based upon seasonality
    sales statistics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical visualization is used in almost every step in the data science process,
    within the obvious steps such as exploring and visualizing, analyzing and learning,
    but can also be leveraged during collecting, processing, and the end game of using
    the identified insights.
  prefs: []
  type: TYPE_NORMAL
- en: D3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: D3 or `D3.js`, is essentially an open source JavaScript library designed with
    the intention of visualizing data using today's web standards. D3 helps put life
    into your data, utilizing **Scalable Vector Graphics** (**SVG**), Canvas, and
    standard HTML.
  prefs: []
  type: TYPE_NORMAL
- en: D3 combines powerful visualization and interaction techniques with a data-driven
    approach to DOM manipulation, providing data scientists with the full capabilities
    of modern browsers and the freedom to design the right visual interface that best
    depicts the objective or assumption.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to many other libraries, `D3.js` allows inordinate control over
    the visualization of data. D3 is embedded within an HTML webpage and uses prebuilt
    JavaScript functions to select elements, create SVG objects, style them, or add
    transitions, dynamic effects, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization is one possible approach that a data scientist may use for improving
    the results generated from a statistical model or data science process, such as
    when addressing a case of overfitting in statistics and data science.
  prefs: []
  type: TYPE_NORMAL
- en: We defined fitting earlier in this chapter (fitting describes how well a statistical
    model or process describes a data scientist's observations). Overfitting is a
    scenario where a statistical model or process seems to fit too well or appears
    to be too close to the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting usually occurs with an overly simple model. This means that you
    may have only two variables and are drawing conclusions based on the two. For
    example, using our previously mentioned example of *daffodil sales*, one might
    generate a model with temperature as an independent variable and sales as a dependent
    one. You may see the model fail since it is not as simple as concluding that warmer
    temperatures will always generate more sales.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, there is a tendency to add more data to the process or model
    in hopes of achieving a better result. The idea sounds reasonable. For example,
    you have information such as average rainfall, pollen count, fertilizer sales,
    and so on; could these data points be added as explanatory variables?
  prefs: []
  type: TYPE_NORMAL
- en: An explanatory variable is a type of independent variable with a subtle difference.
    When a variable is independent, it is not affected at all by any other variables.
    When a variable isn't independent for certain, it's an explanatory variable.
  prefs: []
  type: TYPE_NORMAL
- en: Continuing to add more and more data to your model will have an effect but will
    probably cause overfitting, resulting in poor predictions since it will closely
    resemble the data, which is mostly just background noise.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this situation, a data scientist can use regularization, introducing
    a tuning parameter (additional factors such as a data points mean value or a minimum
    or maximum limitation, which gives you the ability to change the complexity or
    smoothness of your model) into the data science process to solve an ill-posed
    problem or to prevent overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Assessment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a data scientist evaluates a model or data science process for performance,
    this is referred to as assessment. Performance can be defined in several ways,
    including the model's growth of learning or the model's ability to improve (with)
    learning (to obtain a better score) with additional experience (for example, more
    rounds of training with additional samples of data) or accuracy of its results.
  prefs: []
  type: TYPE_NORMAL
- en: One popular method of assessing a model or processes performance is called **bootstrap
    sampling**. This method examines performance on certain subsets of data, repeatedly
    generating results that can be used to calculate an estimate of accuracy (performance).
  prefs: []
  type: TYPE_NORMAL
- en: The bootstrap sampling method takes a random sample of data, splits it into
    three files--a training file, a testing file, and a validation file. The model
    or process logic is developed based on the data in the training file and then
    evaluated (or tested) using the testing file. This tune and then test process
    is repeated until the data scientist is comfortable with the results of the tests.
    At that point, the model or process is again tested, this time using the validation
    file, and the results should provide a true indication of how it will perform.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine using the bootstrap `sampling` method to develop program logic
    by analyzing test data to determine logic flows and then running (or testing)
    your logic against the test data file. Once you are satisfied that your logic
    handles all of the conditions and exceptions found in your testing data, you can
    run a final test on a new, never-before-seen data file for a final validation
    test.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-validation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cross-validation is a method for assessing a data science process performance.
    Mainly used with predictive modeling to estimate how accurately a model might
    perform in practice, one might see cross-validation used to check how a model
    will potentially generalize, in other words, how the model can apply what it infers
    from samples to an entire population (or recordset).
  prefs: []
  type: TYPE_NORMAL
- en: With cross-validation, you identify a (known) dataset as your validation dataset
    on which training is run along with a dataset of unknown data (or first seen data)
    against which the model will be tested (this is known as your **testing dataset**).
    The objective is to ensure that problems such as overfitting (allowing non-inclusive
    information to influence results) are controlled and also provide an insight into
    how the model will generalize a real problem or on a real data file.
  prefs: []
  type: TYPE_NORMAL
- en: The cross-validation process will consist of separating data into samples of
    similar subsets, performing the analysis on one subset (called the **training
    set**) and validating the analysis on the other subset (called the **validation
    set** or **testing set**). To reduce variability, multiple iterations (also called
    **folds** or **rounds**) of cross-validation are performed using different partitions,
    and the validation results are averaged over the rounds. Typically, a data scientist
    will use a models stability to determine the actual number of rounds of cross-validation
    that should be performed.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks are also called **artificial neural networks** (**ANNs**), and
    the objective is to solve problems in the same way that the human brain would.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google will provide the following explanation of ANN as stated in *Neural Network
    Primer: Part I, by Maureen Caudill, AI Expert, Feb. 1989*:'
  prefs: []
  type: TYPE_NORMAL
- en: A computing system made up of several simple, highly interconnected processing
    elements, which process information by their dynamic state response to external
    inputs.
  prefs: []
  type: TYPE_NORMAL
- en: To oversimplify the idea of neural networks, recall the concept of software
    encapsulation, and consider a computer program with an input layer, a processing
    layer, and an output layer. With this thought in mind, understand that neural
    networks are also organized in a network of these layers, usually with more than
    a single processing layer.
  prefs: []
  type: TYPE_NORMAL
- en: Patterns are presented to the network by way of the input layer, which then
    communicates to one (or more) of the processing layers (where the actual processing
    is done). The processing layers then link to an output layer where the result
    is presented.
  prefs: []
  type: TYPE_NORMAL
- en: Most neural networks will also contain some form of learning rule that modifies
    the weights of the connections (in other words, the network learns which processing
    nodes perform better and gives them a heavier weight) per the input patterns that
    it is presented with. In this way (in a sense), neural networks learn by example
    as a child learns to recognize a cat from being exposed to examples of cats.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In a manner of speaking, boosting is a process generally accepted in data science
    for improving the accuracy of a weak learning data science process.
  prefs: []
  type: TYPE_NORMAL
- en: Data science processes defined as weak learners are those that produce results
    that are only slightly better than if you would randomly guess the outcome. Weak
    learners are basically thresholds or a 1-level decision tree.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, boosting is aimed at reducing bias and variance in supervised
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean by bias and variance? Before going on further about boosting,
    let's take note of what we mean by bias and variance.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists describe bias as a level of favoritism that is present in the
    data collection process, resulting in uneven, disingenuous results and can occur
    in a variety of different ways. A `sampling` method is called **biased** if it
    systematically favors some outcomes over others.
  prefs: []
  type: TYPE_NORMAL
- en: A variance may be defined (by a data scientist) simply as the distance from
    a variable mean (or how far from the average a result is).
  prefs: []
  type: TYPE_NORMAL
- en: The boosting method can be described as a data scientist repeatedly running
    through a data science process (that has been identified as a weak learning process),
    with each iteration running on different and random examples of data sampled from
    the original population recordset. All the results (or classifiers or residue)
    produced by each run are then combined into a single merged result (that is a
    gradient).
  prefs: []
  type: TYPE_NORMAL
- en: This concept of using a random subset of the original recordset for each iteration
    originates from bootstrap sampling in bagging and has a similar variance-reducing
    effect on the combined model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, some data scientists consider boosting a means to convert weak
    learners into strong ones; in fact, to some, the process of boosting simply means
    turning a weak learner into a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: Lift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In data science, the term lift compares the frequency of an observed pattern
    within a recordset or population with how frequently you might expect to see that
    same pattern occur within the data by chance or randomly.
  prefs: []
  type: TYPE_NORMAL
- en: If the lift is very low, then typically, a data scientist will expect that there
    is a very good probability that the pattern identified is occurring just by chance.
    The larger the lift, the more likely it is that the pattern is real.
  prefs: []
  type: TYPE_NORMAL
- en: Mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In statistics and data science, when a data scientist uses the term mode, he
    or she refers to the value that occurs most often within a sample of data. Mode
    is not calculated but is determined manually or through processing of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Outlier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Outliers can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A data point that is way out of keeping with the others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That piece of data that doesn't fit
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either a very high value or a very low value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unusual observations within the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An observation point that is distant from all others
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The development of statistical models and/or data science processes to predict
    future events is called **predictive modeling**.
  prefs: []
  type: TYPE_NORMAL
- en: Big Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Again, we have some variation of the definition of big data. A large assemblage
    of data, data sets that are so large or complex that traditional data processing
    applications are inadequate, and data about every aspect of our lives have all
    been used to define or refer to big data. In 2001, then Gartner analyst Doug Laney
    introduced the 3V's concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can refer to the link: [http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf</span>](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The 3V''s, as per Laney, are volume, variety, and velocity. The V''s make up
    the dimensionality of big data: volume (or the measurable amount of data), variety
    (meaning the number of types of data), and velocity (referring to the speed of
    processing or dealing with that data).'
  prefs: []
  type: TYPE_NORMAL
- en: Confidence interval
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The confidence interval is a range of values that a data scientist will specify
    around an estimate to indicate their margin of error, combined with a probability
    that a value will fall in that range. In other words, confidence intervals are
    good estimates of the unknown population parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Writing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although visualizations grab much more of the limelight when it comes to presenting
    the output or results of a data science process or predictive model, writing skills
    are still not only an important part of how a data scientist communicates but
    still considered an essential skill for all data scientists to be successful.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we said that, currently, how data science is defined is a matter
    of opinion. A practical explanation is that data science is a progression or,
    even better, an evolution of thought, consisting of collecting, processing, exploring,
    and visualizing data, analyzing (data) and/or applying machine learning (to the
    data), and then deciding (or planning) based upon acquired insight(s).
  prefs: []
  type: TYPE_NORMAL
- en: Then, with the goal of thinking like a data scientist, we introduced and defined
    a number of common terms and concepts a data scientist should be comfortable with.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will present and explain how a data developer might
    understand and approach the topic of data cleaning using several common statistical
    methods.
  prefs: []
  type: TYPE_NORMAL
