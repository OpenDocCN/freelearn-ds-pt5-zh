- en: Supervised Learning in Anaconda
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since most of us understand the format of the function *y=f(x)*, it is a good
    idea to use it to explain supervised learning. When having both *y* and *x*, we
    could run various regressions to identify the correct function forms. This is
    the spirit of supervised learning. For supervised learning, we have two datasets:
    the **training data** and **test data**. Usually, the training set has a set of
    input variables, such as *x*, and a related output value such as *y* (that is,
    the supervisory signal). A supervised learning algorithm analyzes the training
    data and produces an inferred function form. Then, we apply this inferred function
    to map our test dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: A glance at supervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of supervised learning via R, Python, Julia, and Octave
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Task view for machine learning in R
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A glance at supervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we discussed unsupervised learning where we have input
    data only. In terms of the function *y=f(x)*, for unsupervised learning we have
    only inputs *x*. Unlike unsupervised learning, we have both inputs *x* and the
    corresponding output *y* for supervised learning. Our task is to find the best
    function, linking *x* with *y*, based on our training dataset. In supervised learning,
    our training dataset consists of an input object, typically a vector, and a desired
    output value, where it could be either binary, categorical, discrete, or continuous.
    A supervised learning algorithm examines a given training dataset and produces
    an inferred best-fit function. To verify the accuracy of this inferred function,
    we use the second dataset, the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In an ideal world, we would want to have a large sample size. However, for many
    occasions, this is not true. In those cases, we could apply the bootstrap method
    for estimating statistical quantities from samples. The **Bootstrap Aggregation**
    algorithm is used to create multiple different models from a single training dataset.
    Actually, random forest is a sort of nickname for *decision tree ensemble through
    bagging*. Because of this, it ends up with a very powerful classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use the famous Titanic tragedy as an example. In 1912, on its maiden
    voyage, the Titanic sank after striking an iceberg. Of its passengers and crew,
    more than 1,500 died. First, let''s look at the simple dataset. The following
    R code can be downloaded from the author''s website at [http://canisius.edu/~yany/RData/titanic.RData](http://canisius.edu/~yany/RData/titanic.RData):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding output, it can be seen that the dataset has `2,201` observations
    with just `4` variables. The `CLASS` is for cabin or economic status, `GENDER`
    is for gender, and `SURVIVED` indicates whether the passenger survived or not.
    The `unique()` function can be used to show their unique values, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Our task is to use the decision tree to find the contributions of those three
    input variables to the survival rate. Let''s use the R package called `Rattle`
    to run a simple decision tree model by using the embedded dataset. To launch the
    `Rattle` package, we have the following two lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The next screenshot shows how to choose a library and a related dataset. Note
    that, depending on the version, the dataset might not be available. An alternative
    way is to download from the author''s web page at [http://canisius.edu/~yany/RData/titanic.RData](http://canisius.edu/~yany/RData/titanic.RData).
    The default setting for the partition is 70% (training), 15% (verification), and
    15% (testing):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b1deab5c-fea1-4335-9897-2a648e7b756d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we choose Model | Tree, and then Execute in the upper left corner, we will
    get the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7dcd68d4-ee7d-4ae0-8f2a-f9767065bfc6.png)'
  prefs: []
  type: TYPE_IMG
- en: The top ratio of **68%** is for the rate of those who did not survive while
    the **32%** is for the survival rate. The first question is whether the passenger
    is a male or not. If the answer is a yes, then the person would have a **21%**
    chance of survival, shown in the lower-left leaf. For a female passenger who booked
    a cabin of **Class 3**, she would have a **44%** chance of survival. For female
    passengers, when they occupy a first- or second-class cabin, she would have a
    **91%** chance of survival. Since the age does not appear in our final result,
    the model considers it useless.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that if you plan to use the loaded dataset called `.titanic`, you could
    use `x<-.titanic`, and then choose it by selecting R Dataset after Rattle is launched.
    For different associated datasets with more variables, it is not a surprise that
    users could reach different conclusions. You can save the log, that is, the code,
    for a later usage. The simplified code is given here. The log is on the Rattle
    menu bar:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The second example is associated with the dataset called `iris`. This time,
    the language used is Python. First, let''s look at the data itself. The code and
    related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The output would look like follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The dataset has five variables: sepal length, sepal width, petal length, petal
    width (all in cm), and class. The last one has three unique values for Setosa,
    Versicolour, or Virginica; see [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)
    for more information. The first several lines of the original dataset are shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a76421af-aa95-4003-8ab6-b768c25ef469.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The dataset can be downloaded from the UCI Machine Learning Repository: [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data).
    First, let''s cut the dataset into a training and testing set. Then we use supervised
    learning to figure out the mapping function, that is, an inferred function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning problems can be further divided into two groups: **classification**
    and **regression**. For the classification problem, the output variable, such
    as *y*, could be a binary variable, that is, 0 or 1, or several categories. For
    a regression, variables or values could be discrete or continuous. In the Titanic
    example, we have 1 for survived and 0 for not survived. For a regression problem,
    the output could be a value, such as, 2.5 or 0.234\. In the previous chapter,
    we discussed the concept of distance between group members within the same group
    and between groups.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic for classification is that the distance between (among) group members
    is shorter than the distance between different groups. Alternatively speaking,
    the similarity between (among) group members is higher than the similarity between
    (among) different groups or categories. Since categorical data cannot be ranked,
    we could use the following method to group them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2adc625e-b24f-4b4c-a061-01ed4fb3e47d.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n[matched]* is the number of matched treatments and *n[total]* is the
    number of total treatments. When both categorical data and numeric data are available,
    we could estimate both types of distances first and then choose appropriate weights
    to combine them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume *d[num]* is for the distance based on the numerical data and *d[cat]*
    is for the distance based on the categorical data. Then, we have the following
    combined distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ae7f744-791d-44da-b3e0-b20c7835aa9f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w[num]* is the weight for the numerical value.
  prefs: []
  type: TYPE_NORMAL
- en: The k-nearest neighbors algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In pattern recognition or grouping, the **k-nearest neighbors** (**k-NN**) algorithm
    is a non-parametric method implemented for classification and regression. For
    those two cases, the input consists of the k-closest training examples in the
    feature space. The following four lines of R code tries to separate plants into
    k-groups by using a dataset called `iris`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d75fcb5b-c007-4b03-85cd-6df7336f2c57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows the five nearest neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7650430a-6ca4-4ab8-90d7-009957e9c655.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code generated by the five nearest neighbors is given here. The code is
    slightly modified from the code offered by others at [https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o/21602#21602](https://stats.stackexchange.com/questions/21572/how-to-plot-decision-boundary-of-a-k-nearest-neighbor-classifier-from-elements-o/21602#21602):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: For more details about each function, readers can find the related manual regarding
    the `ElemStatLearn` package.
  prefs: []
  type: TYPE_NORMAL
- en: Bayes classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is a classification technique based on Bayes theorem with an assumption
    of independence among predictors. In layman''s terms, a naive Bayes classifier
    assumes that the presence of a particular feature in a class is unrelated to the
    presence of any other feature. First, let''s look at the `HouseVotes84` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/221910c7-5292-48d0-88f0-3cc7cd0d0102.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The first variable, `Class`, is a binary one: `republican` or `democrat`. In
    addition, we have 16 traits associated with each individual. The following program
    calls the `naiveBayes()` function which computes the conditional a-posterior probabilities
    of a categorical class variable given independent predictor variables using the
    Bayes rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The final output is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The next example is to predict the survival rate by applying the naive Bayes
    methodology to the Titanic dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: From the previous output, we could find the final prediction in terms of 32
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is a wonderful research area of machine learning. It
    has a root from behavioral psychology. The mechanism would maximize some notion
    of cumulative reward when certain actions were taken in a set of environments
    (that is, an agent tries to learn optimal behavior through trial-and-error interactions
    within a dynamic environment setting).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use an R package called `ReinforcementLearning`. First, let''s look
    at the dataset, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The function called `sampleGridSequence()` is used to generate *n* observations.
    The `State` and `NextState` variables have four unique values: `s1`, `s2`, `s3`,
    and `s4`. The variable, `Action`, has four values: `left`, `right`, `down`, and
    `up`. The variable, `Award`, has two unique values of `-1` and `10`, and we could
    view `-1` as a punishment and `10` as a reward. For the first observation, if
    our current and next state is `s2` and our action is `left`, we would suffer a
    `-1` penalty. The following result shows that if the next status is the same as
    our current one, no matter what our action would take, we would always have a
    negative penalty:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The question is, when at a different state, what is the optimal action we should
    take? For example, at the `s1` state, should we move `left`, `right`, `up`, or
    `down`? Note that the `set.seed()` function is used to guarantee that every user
    would get the same result if they use the same random seed of `123`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Implementation of supervised learning via R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have discussed in the previous chapter, the best choice to conduct various
    tests for supervised learning is applying an R package called `Rattle`. Here,
    we show two more examples. Let''s first look at the `iris` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The next example is using the `diabetes` dataset, shown in the screenshot here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3bc511f-0ba3-494d-82e6-28b475bc696f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, we could choose the logistic model after clicking Model on the
    menu bar. After clicking on Execute, we would have the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b268ab90-54a9-4cca-a904-755a84f47845.png)'
  prefs: []
  type: TYPE_IMG
- en: Based on the significant level of p-values, we could see that in addition to
    the intercept, `x1`*,* `x2`*,* `x3`, and `x6` are statistically significant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next example is from the R package called `LogicReg`. The code is given
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The related graph is shown here. Again, the `set.seed()` function is used for
    an easy replication. If the user omits the function or chooses a different seed,
    they would get quite a different result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4033da8d-3a63-4da6-a1c3-f4b6e0bdd52f.png)'
  prefs: []
  type: TYPE_IMG
- en: Introduction to RTextTools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This R package is about *Automatic Text Classification via Supervised Learning*.
    It is a machine learning package for automatic text classification that makes
    it simple for novice users to get started with machine learning, while allowing
    experienced users to easily experiment with different settings and algorithm combinations.
    The package includes nine algorithms for ensemble classification (svm, slda, boosting,
    bagging, random forests, glmnet, decision trees, neural networks, and maximum
    entropy), comprehensive analytics, and thorough documentation. Here, we use the
    New York Times Times article as an example. First, let''s look at the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'When running the program, users should move this line. The output is shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d443ffe-6d78-497b-b684-adca1173d118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we could look at the types of existing algorithms by using the `print_algorithms()`
    function, shown in the code and output here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `BAGGING` is for zbc, `BOOSTING` is the bbb, `GLMNET` is for the general
    linear model, `MAXENT` is for maximum entropy model, `NNET` is for neural network,
    `RF` is for random forest, `SLDA` is for supervised machine learning algorithms,
    `SVM` is for support vector machine, and `TREE` is for decision trees. The code
    is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Implementation via Python
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter related to unsupervised learning, we have learnt about
    several Python packages. Fortunately, these packages can be applied to supervised
    learning algorithms as well. The following example is for a linear regression
    by using a few Python datasets. The Python dataset can be downloaded from the
    author''s website at [http://www.canisius.edu/~yany/python/ffcMonthly.pkl](http://www.canisius.edu/~yany/python/ffcMonthly.pkl).
    Assume that the data is saved under `c:/temp/`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/94d8d082-d81d-4027-9c23-f028ee1ad464.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We plan to run a linear regression; see the formula here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8440dd65-eba0-4a2c-9086-19b30ad726e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *R[i]* is stock i''s returns, *R[mkt]* is the *market returns*, *R[SMB]*
    is the *portfolio returns of small stocks* minus the *portfolio returns of big
    stocks*, *R[HML]* is the *portfolio returns with high book-to-market ratio* (of
    equity) minus the *portfolio returns of stocks with low book-to-market ratio*.
    The Python program is given next. Note that the Python dataset called `ffDaily.pkl`
    is downloadable at [http://canisius.edu/~yany/python/data/ffDaily.pkl](http://canisius.edu/~yany/python/data/ffDaily.pkl):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e060f888-3d56-4194-8fc3-18ddbe658dd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The next example predicts `iris` categories. The code is given first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The nice output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53ba7d27-ecb4-4f1e-a0a4-028bc4f77698.png)'
  prefs: []
  type: TYPE_IMG
- en: Using the scikit-learn (sklearn) module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following example is borrowed from [http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py](http://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html#sphx-glr-auto-examples-classification-plot-digits-classification-py).
    The program uses the `scikit-learn` module to recognize images of handwritten
    digits. The slightly modified code, for an easy presentation, is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Part of the output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1bfeae7f-59b4-4887-8bf0-ab8655bc2bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementation via Octave
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The next example of running a linear regression and the related datasets could
    be downloaded at [http://canisius.edu/~yany/data/c9_input.csv](http://canisius.edu/~yany/data/c9_input.csv).
    In the following program, the input data set is assumed to be under `c:/temp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The first graph is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42a08750-02e4-427a-8b3b-581c3e0f3ab9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To save space, the long program will not be shown here. Interested readers
    can use the previous link. However, its output graph is shown instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68d39b45-54a0-4a1c-b57f-a9aeefb5561f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the next example, we download an Octave machine library at [https://github.com/partharamanujam/octave-ml](https://github.com/partharamanujam/octave-ml).
    Assume that the location of the directory is `C:\Octave\octave-ml-master` and
    the related `octavelib` is `C:\Octave\octave-ml-master\octavelib`. Then we add
    its path to our Octave program with the following one-liner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many useful programs included under the subdirectory. Note that the
    `.m` extensions are all removed in the following table for brevity. Some useful
    programs included in the directory are presented in the table here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| addBiasTerm | kMeansClosestCentroids |'
  prefs: []
  type: TYPE_TB
- en: '| choosePolynomialForLinearGradDesc | kMeansComputeCentroids |'
  prefs: []
  type: TYPE_TB
- en: '| chooseRBFParamsForSVM | kMeansCostFunction |'
  prefs: []
  type: TYPE_TB
- en: '| chooseRegularizationForLinearGradDesc | kMeansInitCentroids |'
  prefs: []
  type: TYPE_TB
- en: '| chooseRegularizationForLogisticGradDesc | linearRegressionCostFunction |'
  prefs: []
  type: TYPE_TB
- en: '| collaborativeFilteringCostFunction | logisticRegressionCostFunction |'
  prefs: []
  type: TYPE_TB
- en: '| computeCoFiParamsByGradDescFmincg | logisticRegressionOneVsAllError |'
  prefs: []
  type: TYPE_TB
- en: '| computeGaussianParams | logisticRegressionOneVsAllTheta |'
  prefs: []
  type: TYPE_TB
- en: '| computeMultivarGaussianDistribution | normalizeRatings |'
  prefs: []
  type: TYPE_TB
- en: '| computePCA | porterStemmer |'
  prefs: []
  type: TYPE_TB
- en: '| computeScalingParams | predictByLinearGradDesc |'
  prefs: []
  type: TYPE_TB
- en: '| computeThetaByLinearGradDescFminunc | predictByLogisticGradDescOneVsAll |'
  prefs: []
  type: TYPE_TB
- en: '| computeThetaByLogisticGradDescFminunc | predictByNormalEquation |'
  prefs: []
  type: TYPE_TB
- en: '| computeThetaByNormalEquation | projectPCAData |'
  prefs: []
  type: TYPE_TB
- en: '| computeThresholdForMultivarGaussian | recoverPCAData |'
  prefs: []
  type: TYPE_TB
- en: '| fmincg | scaleFeatures |'
  prefs: []
  type: TYPE_TB
- en: '| generateFeaturesPolynomial | sigmoid |'
  prefs: []
  type: TYPE_TB
- en: '| generateKMeansClusters |  |'
  prefs: []
  type: TYPE_TB
- en: '| generateKMeansClustersMinCost |  |'
  prefs: []
  type: TYPE_TB
- en: Table 9.1 Supporting Octave programs under the octavelib directory
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the sample programs and their related input datasets are included
    under the subdirectory called `examples` (see the following table):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Programs** | **Dataset(s)** |'
  prefs: []
  type: TYPE_TB
- en: '| cofi.m | movieList.mat |'
  prefs: []
  type: TYPE_TB
- en: '| extractEmailFeatures.m | email-sample-1.txtemail-sample-2.txtemail-sample-3.txt
    |'
  prefs: []
  type: TYPE_TB
- en: '| gaussian_m.m | anomaly.dat |'
  prefs: []
  type: TYPE_TB
- en: '| initEnv.m | Note: setup program |'
  prefs: []
  type: TYPE_TB
- en: '| k_means.m | bird_small.png |'
  prefs: []
  type: TYPE_TB
- en: '| linear_gd.m | damlevels.mat |'
  prefs: []
  type: TYPE_TB
- en: '| logistic_gd.m | numbers.mat |'
  prefs: []
  type: TYPE_TB
- en: '| normal_eq.m | prices.csv |'
  prefs: []
  type: TYPE_TB
- en: '| pca.m | faces.mat |'
  prefs: []
  type: TYPE_TB
- en: '| svm.m | spam-vocab.txt |'
  prefs: []
  type: TYPE_TB
- en: Table 9.2 Examples and their related datasets
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following program for k-means. The original program is called
    `k_means.m` and its input photo is `bird_small.png`. The program tries to save
    space for a given picture by using the k-means method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The related two photos are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ba844d4-d7ef-42c9-be45-be91b422ac59.png)'
  prefs: []
  type: TYPE_IMG
- en: Implementation via Julia
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first example uses the familiar dataset called `iris` again. Using the
    `kmeans()` function, the program tries to group these plants:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The related output is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b5d7032-066f-46f9-b30c-f85d7123baa6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the next example, we try to sort a set of random numbers into `20` clusters.
    The code is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'To show the value of `x` and `clusters`, we simply type them on the command
    line, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef6bb11b-21c2-42a0-a54d-0142c1756d00.png)'
  prefs: []
  type: TYPE_IMG
- en: Task view for machine learning in R
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'From the previous chapters, we know that there are about three dozen task views
    for different topics. A task view is a set of R packages around a specific topic
    such as finance, econometrics, and the like. In the previous chapter, we briefly
    discussed the task view for clustering. There is no task view with a name of supervised
    learning. Instead, the closest one will be the machine learning task view; see
    the screenshot here for the top part:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14b3127-1f80-4d80-838c-34d07f1f0024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The URL is given in the preceding screenshot. Again, we could issue just three
    lines of R code to install the R packages included in the task view:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: About 105 related R packages were installed as of April 3, 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have discussed supervised learning, such as classification,
    the k-nearest neighbors algorithm, Bayes classifiers, reinforcement learning,
    and the `RTextTools` and `sklearn` modules in R. In addition, we discussed implementations
    of supervised learning via R, Python, Julia, and Octave.
  prefs: []
  type: TYPE_NORMAL
- en: For the next chapter, we will discuss predictive data analytics, modeling and
    validation, some useful datasets, time-series analytics, how to predict the future,
    seasonality, and how to visualize our data. For Python packages, we will mention `predictives-models-building`, `model-catwalk`,
    and `easyML`. For R packages, we will discuss `datarobot`, `LiblineaR`, `eclust`,
    and `AppliedPredictiveModeling`. For Julia packages, we will explain `EmpiricalRisks` and `ValidatedNumerics`.
  prefs: []
  type: TYPE_NORMAL
- en: Review questions and exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does *unsupervised learning* mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the major difference between unsupervised learning and supervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we install the Python package `sklearn`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discuss the relationship between distance and clustering classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does Bayes classification mean?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find out the related functions for Bayes classification in R, Python, Octave,
    and Julia.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many R packages are installed after you run the following three lines of
    R code?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Download the IBM monthly data from Yahoo!Finance, [https://finance.yahoo.com](https://finance.yahoo.com) .
    Then run a Fama-French-Carhart four factor model by using Python. The 4-factor
    model is given here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c8c7fbb6-7041-4152-92a6-7229f522dc89.png)'
  prefs: []
  type: TYPE_IMG
- en: The Python dataset related to those 4-factors can be downloaded from the author's
    website at [http://www.canisius.edu/~yany/python/ffcMonthly.pkl](http://www.canisius.edu/~yany/python/ffcMonthly.pkl).
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the famous iris data set from the UCI Machine Learning Repository.
    The first several lines are given here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/276edafe-1193-447d-adbf-ea0e95089b47.png)'
  prefs: []
  type: TYPE_IMG
- en: Then generate data sets for both R and Python.
  prefs: []
  type: TYPE_NORMAL
- en: To implement machine learning using Octave, how is the usage of the `octavelib` library
    introduced and used in the chapter? How many programs are under it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the provided Octave programs discussed in this chapter, run the program
    called `logistic_gd.m` with an input Octave data set called `numbers.mat`. For
    more details, see Table 9.1 and 9.2 and the related explanations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discuss the `spa` R package and show a few related examples.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download Fama-French-Carhart 4-factor model, see here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/226dc7f5-afa8-4050-8ba8-404450076341.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where R[i] is stock *i*’s returns, R[mkt] is the market returns, R[SMB] is
    the portfolio returns of small stocks might the portfolio returns of big stocks,
    R[HML] is the portfolio returns with high book-to-market ratio (of equity) minus
    the portfolio returns of stocks with low book-to-market ratio, R[MOM] is the momentum
    factor. Write R/Python/Octave/Julia programs to run linear regressions for IBM
    stocks. Source of Data: 1) Prof. French’s Data Library, [http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html](http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html),
    and Yahoo!Finance, [https://finance.yahoo.com](https://finance.yahoo.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Download the Bank Marketing Data Set from the UCI Machine Learning Repository, [https://archive.ics.uci.edu/ml/datasets/Bank+Marketing](https://archive.ics.uci.edu/ml/datasets/Bank+Marketing).
    The data is related to direct marketing campaigns of a Portuguese banking institution.
    The marketing campaigns were based on phone calls. Often, more than one contact
    to the same client was required, in order to access if the product (bank term
    deposit) would be ('yes') or not ('no') subscribed. Use supervised learning to
    identify the most influential factors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download a data set called *Census Income*, [https://archive.ics.uci.edu/ml/datasets/Census+Income](https://archive.ics.uci.edu/ml/datasets/Census+Income)*. *Extraction
    was done by Barry Becker from the 1994 Census database. A set of reasonably clean
    records was extracted using the following conditions: `((AAGE>16) && (AGI>100)
    && (AFNLWGT>1) && (HRSWK>0))` . Write both R and Python programs to predict whether
    a person makes over $50,000 a year.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The utility function for an investor is shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/02120cb7-b0a4-4dfc-aaac-5cf4806349ff.png)'
  prefs: []
  type: TYPE_IMG
- en: where U is the utility function, E(R) is the expected portfolio return and we
    could use its mean to approximate, A is the risk-averse coefficient and σ² is
    the variance of the portfolio. Go to Professor French’s Data Library to download
    10 industry portfolio returns. Assume a risk-averse variable of A varies from
    1 to 100.  Use the k-means method to group investors based on E(R), A, and risk.
  prefs: []
  type: TYPE_NORMAL
- en: Download a data set called Bike Sharing Data Set from the UCI Machine Learning
    Repository, [https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset). 
    The dataset contains the hourly and daily count of rental bikes between years
    2011 and 2012 in Capital bike share system with the corresponding weather and
    seasonal information. Write an R program to group individual riders into k-groups.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download a data set call Bag of Words, [https://archive.ics.uci.edu/ml/datasets/Bag+of+Words](https://archive.ics.uci.edu/ml/datasets/Bag+of+Words). 
    Write both R and Python to conduct a cluster analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Download a data set called Sonar Dataset. The Sonar Dataset involves the prediction
    of whether or not an object is a mine or a rock given the strength of sonar returns
    at different angles.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the data set called Boston to run a regression (Python). The first several
    lines are shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
