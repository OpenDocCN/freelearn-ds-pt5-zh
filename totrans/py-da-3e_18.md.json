["```py\nconda install dask\n```", "```py\npip install dask\n```", "```py\n# import Dask Array\nimport dask.array as da\n\n# Create Dask Array using arange() function and generate values from 0 to 17\na = da.arange(18, chunks=4)\n\n# Compute the array\na.compute()\n```", "```py\narray([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,17])\n```", "```py\n# Check the chunk size\na.chunks\n\n```", "```py\n((4, 4, 4, 4, 2),)\n```", "```py\n# Read csv file using pandas\nimport pandas as pd\n%time temp = pd.read_csv(\"HR_comma_sep.csv\")\n```", "```py\nCPU times: user 17.1 ms, sys: 8.34 ms, total: 25.4 ms\n\nWall time: 36.3 ms\n```", "```py\n# Read csv file using Dask\n\nimport dask.dataframe as dd\n\n%time df = dd.read_csv(\"HR_comma_sep.csv\")\n```", "```py\nCPU times: user 18.8 ms, sys: 5.08 ms, total: 23.9 ms\n\nWall time: 25.8 ms\n```", "```py\n# Import Dask and Pandas DataFrame\nimport dask.dataframe as dd\nimport pandas as pd\n\n# Create Pandas DataFrame\ndf = pd.DataFrame({\"P\": [10, 20, 30], \"Q\": [40, 50, 60]},\nindex=['p', 'q', 'r'])\n\n# Create Dask DataFrame\nddf = dd.from_pandas(df, npartitions=2)\n\n# Check top records\nddf.head()\n```", "```py\nP Q\n\np 10 40\n\nq 20 50\n\nr 30 60\n```", "```py\n# Single Column Selection\nddf['P']\n```", "```py\nDask Series Structure:\n\nnpartitions=1\n\np int64\n\nr ...\nName: P, dtype: int64\nDask Name: getitem, 2 tasks\n```", "```py\n# Multiple Column Selection\nddf[['Q', 'P']]\n```", "```py\n\nDask DataFrame Structure:\n\nQ P\n\nnpartitions=1\n\np int64 int64\n\nr ... ...\n\nDask Name: getitem, 2 tasks\n```", "```py\n# Import Dask and Pandas DataFrame\nimport dask.dataframe as dd\nimport pandas as pd\n\n# Create Pandas DataFrame\ndf = pd.DataFrame({\"X\": [11, 12, 13], \"Y\": [41, 51, 61]})\n\n# Create Dask DataFrame\nddf = dd.from_pandas(df, npartitions=2)\n\n# Check top records\nddf.head()\n```", "```py\nX Y\n\n0 11 41\n\n1 12 51\n\n2 13 61\n```", "```py\nddf.iloc[:, [1, 0]].compute()\n```", "```py\nY X\n0 41 11\n\n1 51 12\n\n2 61 13\n```", "```py\nddf.iloc[0:4, [1, 0]].compute()\n```", "```py\nNotImplementedError: 'DataFrame.iloc' only supports selecting columns. It must be used like 'df.iloc[:, column_indexer]'.\n```", "```py\n# Import Dask DataFrame\nimport dask.dataframe as dd\n\n# Read CSV file\nddf = dd.read_csv('HR_comma_sep.csv')\n\n# See top 5 records\nddf.head(5)\n\n```", "```py\n# Filter employee with low salary\nddf2 = ddf[ddf.salary == 'low']\n\nddf2.compute().head()\n```", "```py\n# Find the average values of all the columns for employee left or stayed\nddf.groupby('left').mean().compute()\n```", "```py\n# Import Dask DataFrame\nfrom dask import dataframe as dd\n\n# Convert pandas dataframe to dask dataframe\nddf = dd.from_pandas(pd_df,chunksize=4)\n\ntype(ddf)\n```", "```py\ndask.dataframe.core.DataFrame\n```", "```py\n# Convert dask DataFrame to pandas DataFrame\npd_df = df.compute()\n\ntype(pd_df)\n```", "```py\npandas.core.frame.DataFrame\n```", "```py\n# Import dask bag\nimport dask.bag as db\n\n# Create a bag of list items\nitems_bag = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=3)\n\n# Take initial two items\nitems_bag.take(2)\n```", "```py\n(1, 2)\n```", "```py\n# Filter the bag of list items\nitems_square=items_bag.filter(lambda x: x if x % 2 != 0 else None)\n\n# Compute the results\nitems_square.compute()\n```", "```py\n[1, 3, 5, 7, 9]\n```", "```py\n# Square the bag of list items\nitems_square=items_b.map(lambda x: x**2)\n\n# Compute the results\nitems_square.compute()\n```", "```py\n[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]\n```", "```py\n# Import dask bag\nimport dask.bag as db\n\n# Create a bag of text file\ntext = db.read_text('sample.txt')\n\n# Show initial 2 items from text\ntext.take(2)\n```", "```py\n('Hi! how are you? \\n', '\\n')\n```", "```py\n# Convert dask bag object into text file\ntext.to_textfiles('/path/to/data/*.text.gz')\n```", "```py\n['/path/to/data/0.text.gz']\n```", "```py\n# Import dask bag\nimport dask.bag as db\n\n# Create a bag of dictionary items\ndict_bag = db.from_sequence([{'item_name': 'Egg', 'price': 5},\n{'item_name': 'Bread', 'price': 20},\n{'item_name': 'Milk', 'price': 54}],\nnpartitions=2)\n\n# Convert bag object into dataframe\ndf = dict_bag.to_dataframe()\n\n# Execute the graph results\ndf.compute()\n```", "```py\n# Import dask delayed and compute\nfrom dask import delayed, compute\n\n# Create delayed function\n@delayed\ndef cube(item):\n    return item ** 3\n\n# Create delayed function\n@delayed\ndef average(items):\n    return sum(items)/len(items)\n\n# create a list\nitem_list = [2, 3, 4]\n\n# Compute cube of given item list\ncube_list= [cube(i) for i in item_list]\n\n# Compute average of cube_list\ncomputation_graph = average(cube_list)\n\n# Compute the results\ncomputation_graph.compute()\n```", "```py\n33.0\n```", "```py\n pip install graphviz\n```", "```py\nbrew install graphviz\n```", "```py\nsudo apt-get install graphviz\n```", "```py\n# Compute the results\ncomputation_graph.visualize()\n```", "```py\n# Import Dask DataFrame\nimport dask.dataframe as dd\n\n# Read CSV file\nddf = dd.read_csv('HR_comma_sep.csv')\n\n# See top 5 records\nddf.head(5)\n```", "```py\n# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate the MinMaxScaler Object\nscaler = MinMaxScaler(feature_range=(0, 100))\n\n# Fit the data on Scaler\nscaler.fit(ddf[['last_evaluation']])\n\n# Transform the data\nperformance_score=scaler.transform(ddf[['last_evaluation']])\n\n# Let's see the scaled performance score\nperformance_score\n```", "```py\narray([[26.5625],\n\n[78.125 ],\n\n[81.25 ],\n\n...,\n\n[26.5625],\n\n[93.75 ],\n\n[25\\. ]])\n```", "```py\n# Import Dask DataFrame\nimport dask.dataframe as dd\n\n# Read CSV file\nddf = dd.read_csv('HR_comma_sep.csv')\n\n# See top 5 records\nddf.head(5)\n```", "```py\n# Import Onehot Encoder\nfrom dask_ml.preprocessing import Categorizer\nfrom dask_ml.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\n\n# Create pipeline with Categorizer and OneHotEncoder\npipe = make_pipeline(Categorizer(), OneHotEncoder())\n\n# Fit and transform the Categorizer and OneHotEncoder\npipe.fit(ddf[['salary',]])\nresult=pipe.transform(ddf[['salary',]])\n\n# See top 5 records\nresult.head()\n```", "```py\n# Import Onehot Encoder\nfrom dask_ml.preprocessing import Categorizer\nfrom dask_ml.preprocessing import OrdinalEncoder\nfrom sklearn.pipeline import make_pipeline\n\n# Create pipeline with Categorizer and OrdinalEncoder\npipe = make_pipeline(Categorizer(), OrdinalEncoder())\n\n# Fit and transform the Categorizer and OneHotEncoder\npipe.fit(ddf[['salary',]])\nresult=pipe.transform(ddf[['salary',]])\n\n# Let's see encoded results\nresult.head()\n```", "```py\nsalary\n\n0 0\n\n1 1\n\n2 1\n\n3 0\n\n4 0\n```", "```py\n# Import Dask DataFrame\nimport pandas as pd\n\n# Read CSV file\ndf = pd.read_csv('HR_comma_sep.csv')\n\n# See top 5 records\ndf.head(5)\n```", "```py\n# select the feature and target columns\ndata=df[['satisfaction_level', 'last_evaluation']]\n\nlabel=df['left']\n```", "```py\n# Import client\nfrom dask.distributed import Client\n\n# Instantiate the Client\nclient = Client()\n```", "```py\n# import dask_ml.joblib\nfrom sklearn.externals.joblib import parallel_backend\n\nwith parallel_backend('dask'):\n    # Write normal scikit-learn code here\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.metrics import accuracy_score\n    from sklearn.model_selection import train_test_split\n\n    # Divide the data into two parts: training and testing set\n    X_train, X_test, y_train, y_test = train_test_split(data,label,\n    test_size=0.2,\n    random_state=0)\n\n    # Instantiate RandomForest Model\n    model = RandomForestClassifier()\n\n    # Fit the model\n    model.fit(X_train,y_train)\n\n    # Predict the classes\n    y_pred = model.predict(X_test)\n\n    # Find model accuracy\n    print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n```", "```py\nAccuracy: 0.92\n```", "```py\n# Read CSV file using Dask\nimport dask.dataframe as dd\n\n# Read Human Resource Data\nddf = dd.read_csv(\"HR_comma_sep.csv\")\n\n# Let's see top 5 records\nddf.head()\n```", "```py\ndata=ddf[['satisfaction_level','last_evaluation']].to_dask_array(lengths=True)\n\nlabel=ddf['left'].to_dask_array(lengths=True)\n```", "```py\n# Import Dask based LogisticRegression\nfrom dask_ml.linear_model import LogisticRegression\n\n# Import Dask based train_test_split\nfrom dask_ml.model_selection import train_test_split\n\n# Split data into training and testing set\nX_train, X_test, y_train, y_test = train_test_split(data, label)\n```", "```py\n# Create logistic regression model\nmodel = LogisticRegression()\n\n# Fit the model\nmodel.fit(X_train,y_train)\n\n# Predict the classes\ny_pred = model.predict(X_test)\n\n# Find model accuracy\nprint(\"Accuracy:\",accuracy_score(y_test, y_pred))\n\n```", "```py\nAccuracy: 0.7753333333333333\n```", "```py\n# Read CSV file using Dask\nimport dask.dataframe as dd\n\n# Read Human Resource Data\nddf = dd.read_csv(\"HR_comma_sep.csv\")\n\n# Let's see top 5 records\nddf.head()\n```", "```py\ndata=ddf[['satisfaction_level', 'last_evaluation']].to_dask_array(lengths=True)\n```", "```py\n# Import Dask based Kmeans\nfrom dask_ml.cluster import KMeans\n\n# Create the Kmeans model\nmodel=KMeans(n_clusters=3)\n\n# Fit the model\nmodel.fit(data)\n\n# Predict the classes\nlabel=model.labels_\n\n# Compute the results\nlabel.compute()\n```", "```py\narray([0, 1, 2, ..., 0, 2, 0], dtype=int32)\n```", "```py\n# Import matplotlib.pyplot\nimport matplotlib.pyplot as plt\n\n# Prepare x,y and cluster_labels\nx=data[:,0].compute()\ny=data[:,1].compute()\ncluster_labels=label.compute()\n\n# Draw scatter plot\nplt.scatter(x,y, c=cluster_labels)\n\n# Add label on X-axis\nplt.xlabel('Satisfaction Level')\n\n# Add label on X-axis\nplt.ylabel('Performance Level')\n\n# Add a title to the graph\nplt.title('Groups of employees who left the Company')\n\n# Show the plot\nplt.show()\n```"]